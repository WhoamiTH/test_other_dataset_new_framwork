nohup: ignoring input
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_3
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5759, train acc 78.38%, f1 0.7843, precision 0.7825, recall 0.7862, auc 0.7838
epoch 201, loss 0.4198, train acc 81.17%, f1 0.8116, precision 0.8120, recall 0.8111, auc 0.8117
epoch 301, loss 0.3924, train acc 82.55%, f1 0.8255, precision 0.8259, recall 0.8251, auc 0.8255
epoch 401, loss 0.3809, train acc 82.96%, f1 0.8295, precision 0.8302, recall 0.8288, auc 0.8296
epoch 501, loss 0.4902, train acc 83.06%, f1 0.8306, precision 0.8309, recall 0.8303, auc 0.8306
epoch 601, loss 0.3490, train acc 83.06%, f1 0.8306, precision 0.8308, recall 0.8303, auc 0.8306
epoch 701, loss 0.3454, train acc 83.04%, f1 0.8303, precision 0.8305, recall 0.8301, auc 0.8304
epoch 801, loss 0.6125, train acc 83.11%, f1 0.8311, precision 0.8312, recall 0.8309, auc 0.8311
epoch 901, loss 0.3045, train acc 83.10%, f1 0.8310, precision 0.8311, recall 0.8309, auc 0.8310
epoch 1001, loss 0.5886, train acc 83.08%, f1 0.8308, precision 0.8309, recall 0.8307, auc 0.8308
epoch 1101, loss 0.4294, train acc 83.08%, f1 0.8308, precision 0.8308, recall 0.8308, auc 0.8308
epoch 1201, loss 0.4774, train acc 83.09%, f1 0.8309, precision 0.8309, recall 0.8309, auc 0.8309
epoch 1301, loss 0.3792, train acc 83.07%, f1 0.8307, precision 0.8308, recall 0.8305, auc 0.8307
epoch 1401, loss 0.3790, train acc 83.08%, f1 0.8308, precision 0.8308, recall 0.8308, auc 0.8308
epoch 1501, loss 0.3775, train acc 83.04%, f1 0.8304, precision 0.8305, recall 0.8303, auc 0.8304
epoch 1601, loss 0.3300, train acc 83.04%, f1 0.8304, precision 0.8303, recall 0.8305, auc 0.8304
epoch 1701, loss 0.3446, train acc 83.14%, f1 0.8314, precision 0.8313, recall 0.8316, auc 0.8314
epoch 1801, loss 0.3819, train acc 83.13%, f1 0.8313, precision 0.8313, recall 0.8313, auc 0.8313
epoch 1901, loss 0.4082, train acc 83.16%, f1 0.8316, precision 0.8316, recall 0.8317, auc 0.8316
epoch 2001, loss 0.3993, train acc 83.20%, f1 0.8320, precision 0.8318, recall 0.8323, auc 0.8320
epoch 2101, loss 0.4065, train acc 83.20%, f1 0.8320, precision 0.8321, recall 0.8320, auc 0.8320
epoch 2201, loss 0.4243, train acc 83.28%, f1 0.8329, precision 0.8326, recall 0.8331, auc 0.8328
epoch 2301, loss 0.4375, train acc 83.28%, f1 0.8328, precision 0.8328, recall 0.8327, auc 0.8328
epoch 2401, loss 0.4512, train acc 83.41%, f1 0.8341, precision 0.8341, recall 0.8340, auc 0.8341
epoch 2501, loss 0.2809, train acc 83.44%, f1 0.8344, precision 0.8345, recall 0.8343, auc 0.8344
epoch 2601, loss 0.2539, train acc 83.57%, f1 0.8357, precision 0.8356, recall 0.8358, auc 0.8357
epoch 2701, loss 0.3674, train acc 83.71%, f1 0.8371, precision 0.8370, recall 0.8372, auc 0.8371
epoch 2801, loss 0.4115, train acc 83.77%, f1 0.8377, precision 0.8378, recall 0.8377, auc 0.8377
epoch 2901, loss 0.3818, train acc 83.88%, f1 0.8388, precision 0.8388, recall 0.8388, auc 0.8388
epoch 3001, loss 0.3533, train acc 84.00%, f1 0.8401, precision 0.8398, recall 0.8403, auc 0.8400
epoch 3101, loss 0.4074, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8423, auc 0.8422
epoch 3201, loss 0.2789, train acc 84.25%, f1 0.8425, precision 0.8425, recall 0.8424, auc 0.8425
epoch 3301, loss 0.2328, train acc 84.43%, f1 0.8443, precision 0.8445, recall 0.8441, auc 0.8443
epoch 3401, loss 0.3673, train acc 84.59%, f1 0.8459, precision 0.8460, recall 0.8458, auc 0.8459
epoch 3501, loss 0.3833, train acc 84.71%, f1 0.8471, precision 0.8471, recall 0.8471, auc 0.8471
epoch 3601, loss 0.3505, train acc 84.79%, f1 0.8479, precision 0.8475, recall 0.8483, auc 0.8479
epoch 3701, loss 0.3739, train acc 84.94%, f1 0.8494, precision 0.8492, recall 0.8497, auc 0.8494
epoch 3801, loss 0.3194, train acc 85.03%, f1 0.8502, precision 0.8508, recall 0.8497, auc 0.8503
epoch 3901, loss 0.3631, train acc 85.16%, f1 0.8515, precision 0.8522, recall 0.8507, auc 0.8516
epoch 4001, loss 0.2857, train acc 85.20%, f1 0.8520, precision 0.8520, recall 0.8520, auc 0.8520
epoch 4101, loss 0.4558, train acc 85.30%, f1 0.8530, precision 0.8530, recall 0.8530, auc 0.8530
epoch 4201, loss 0.3696, train acc 85.37%, f1 0.8537, precision 0.8538, recall 0.8537, auc 0.8537
epoch 4301, loss 0.2349, train acc 85.48%, f1 0.8547, precision 0.8549, recall 0.8546, auc 0.8548
epoch 4401, loss 0.2963, train acc 85.50%, f1 0.8549, precision 0.8555, recall 0.8544, auc 0.8550
epoch 4501, loss 0.4053, train acc 85.66%, f1 0.8566, precision 0.8570, recall 0.8562, auc 0.8566
epoch 4601, loss 0.3041, train acc 85.78%, f1 0.8578, precision 0.8576, recall 0.8580, auc 0.8578
epoch 4701, loss 0.3267, train acc 85.77%, f1 0.8577, precision 0.8574, recall 0.8580, auc 0.8577
epoch 4801, loss 0.2640, train acc 85.76%, f1 0.8575, precision 0.8581, recall 0.8569, auc 0.8576
epoch 4901, loss 0.3185, train acc 85.88%, f1 0.8587, precision 0.8590, recall 0.8584, auc 0.8588
epoch 5001, loss 0.3261, train acc 85.88%, f1 0.8588, precision 0.8592, recall 0.8584, auc 0.8588
epoch 5101, loss 0.3421, train acc 85.95%, f1 0.8595, precision 0.8598, recall 0.8592, auc 0.8595
epoch 5201, loss 0.3308, train acc 86.06%, f1 0.8606, precision 0.8606, recall 0.8606, auc 0.8606
epoch 5301, loss 0.2725, train acc 86.09%, f1 0.8610, precision 0.8606, recall 0.8613, auc 0.8609
epoch 5401, loss 0.2750, train acc 86.14%, f1 0.8615, precision 0.8609, recall 0.8621, auc 0.8614
epoch 5501, loss 0.3153, train acc 86.14%, f1 0.8614, precision 0.8615, recall 0.8613, auc 0.8614
epoch 5601, loss 0.3792, train acc 86.23%, f1 0.8623, precision 0.8621, recall 0.8625, auc 0.8623
epoch 5701, loss 0.3102, train acc 86.27%, f1 0.8628, precision 0.8622, recall 0.8634, auc 0.8627
epoch 5801, loss 0.2900, train acc 86.32%, f1 0.8633, precision 0.8626, recall 0.8640, auc 0.8632
epoch 5901, loss 0.3478, train acc 86.40%, f1 0.8642, precision 0.8631, recall 0.8654, auc 0.8640
epoch 6001, loss 0.3218, train acc 86.45%, f1 0.8645, precision 0.8642, recall 0.8649, auc 0.8645
epoch 6101, loss 0.2735, train acc 86.49%, f1 0.8649, precision 0.8648, recall 0.8650, auc 0.8649
epoch 6201, loss 0.2753, train acc 86.48%, f1 0.8648, precision 0.8645, recall 0.8652, auc 0.8648
epoch 6301, loss 0.1774, train acc 86.56%, f1 0.8655, precision 0.8657, recall 0.8653, auc 0.8656
epoch 6401, loss 0.3964, train acc 86.62%, f1 0.8663, precision 0.8656, recall 0.8670, auc 0.8662
epoch 6501, loss 0.2875, train acc 86.68%, f1 0.8669, precision 0.8660, recall 0.8679, auc 0.8668
epoch 6601, loss 0.2700, train acc 86.75%, f1 0.8676, precision 0.8668, recall 0.8684, auc 0.8675
epoch 6701, loss 0.2896, train acc 86.79%, f1 0.8679, precision 0.8675, recall 0.8684, auc 0.8679
epoch 6801, loss 0.2911, train acc 86.80%, f1 0.8681, precision 0.8674, recall 0.8688, auc 0.8680
epoch 6901, loss 0.3209, train acc 86.88%, f1 0.8688, precision 0.8686, recall 0.8690, auc 0.8688
epoch 7001, loss 0.3092, train acc 86.88%, f1 0.8687, precision 0.8689, recall 0.8686, auc 0.8688
epoch 7101, loss 0.3178, train acc 86.95%, f1 0.8695, precision 0.8693, recall 0.8697, auc 0.8695
epoch 7201, loss 0.3472, train acc 86.95%, f1 0.8695, precision 0.8691, recall 0.8700, auc 0.8695
epoch 7301, loss 0.3306, train acc 86.98%, f1 0.8698, precision 0.8695, recall 0.8702, auc 0.8698
epoch 7401, loss 0.4989, train acc 87.05%, f1 0.8705, precision 0.8700, recall 0.8711, auc 0.8705
epoch 7501, loss 0.4270, train acc 87.05%, f1 0.8705, precision 0.8703, recall 0.8707, auc 0.8705
epoch 7601, loss 0.2919, train acc 87.04%, f1 0.8705, precision 0.8699, recall 0.8711, auc 0.8704
epoch 7701, loss 0.2858, train acc 87.14%, f1 0.8713, precision 0.8721, recall 0.8705, auc 0.8714
epoch 7801, loss 0.2626, train acc 87.14%, f1 0.8714, precision 0.8714, recall 0.8714, auc 0.8714
epoch 7901, loss 0.4273, train acc 87.19%, f1 0.8720, precision 0.8716, recall 0.8723, auc 0.8719
epoch 8001, loss 0.3755, train acc 87.22%, f1 0.8722, precision 0.8721, recall 0.8723, auc 0.8722
epoch 8101, loss 0.3743, train acc 87.25%, f1 0.8725, precision 0.8725, recall 0.8726, auc 0.8725
epoch 8201, loss 0.2334, train acc 87.30%, f1 0.8731, precision 0.8725, recall 0.8736, auc 0.8730
epoch 8301, loss 0.2758, train acc 87.32%, f1 0.8732, precision 0.8733, recall 0.8731, auc 0.8732
epoch 8401, loss 0.3603, train acc 87.35%, f1 0.8735, precision 0.8734, recall 0.8737, auc 0.8735
epoch 8501, loss 0.2760, train acc 87.38%, f1 0.8739, precision 0.8735, recall 0.8743, auc 0.8738
epoch 8601, loss 0.3399, train acc 87.33%, f1 0.8734, precision 0.8726, recall 0.8741, auc 0.8733
epoch 8701, loss 0.2256, train acc 87.44%, f1 0.8744, precision 0.8743, recall 0.8745, auc 0.8744
epoch 8801, loss 0.4659, train acc 87.53%, f1 0.8754, precision 0.8748, recall 0.8760, auc 0.8753
epoch 8901, loss 0.2287, train acc 87.54%, f1 0.8755, precision 0.8751, recall 0.8759, auc 0.8754
epoch 9001, loss 0.2422, train acc 87.56%, f1 0.8758, precision 0.8742, recall 0.8774, auc 0.8756
epoch 9101, loss 0.2830, train acc 87.65%, f1 0.8767, precision 0.8756, recall 0.8777, auc 0.8765
epoch 9201, loss 0.2338, train acc 87.71%, f1 0.8771, precision 0.8766, recall 0.8777, auc 0.8771
epoch 9301, loss 0.3587, train acc 87.68%, f1 0.8769, precision 0.8759, recall 0.8779, auc 0.8768
epoch 9401, loss 0.2945, train acc 87.75%, f1 0.8775, precision 0.8777, recall 0.8773, auc 0.8775
epoch 9501, loss 0.2600, train acc 87.78%, f1 0.8779, precision 0.8771, recall 0.8787, auc 0.8778
epoch 9601, loss 0.2332, train acc 87.80%, f1 0.8783, precision 0.8767, recall 0.8798, auc 0.8780
epoch 9701, loss 0.2859, train acc 87.82%, f1 0.8783, precision 0.8779, recall 0.8786, auc 0.8782
epoch 9801, loss 0.2941, train acc 87.93%, f1 0.8793, precision 0.8789, recall 0.8798, auc 0.8793
epoch 9901, loss 0.2421, train acc 87.96%, f1 0.8796, precision 0.8791, recall 0.8802, auc 0.8796
epoch 10001, loss 0.2716, train acc 88.01%, f1 0.8802, precision 0.8795, recall 0.8808, auc 0.8801
epoch 10101, loss 0.3516, train acc 88.04%, f1 0.8805, precision 0.8802, recall 0.8807, auc 0.8804
epoch 10201, loss 0.2311, train acc 88.08%, f1 0.8810, precision 0.8794, recall 0.8825, auc 0.8808
epoch 10301, loss 0.3472, train acc 88.08%, f1 0.8809, precision 0.8805, recall 0.8812, auc 0.8808
epoch 10401, loss 0.2703, train acc 88.16%, f1 0.8816, precision 0.8816, recall 0.8816, auc 0.8816
epoch 10501, loss 0.3561, train acc 88.15%, f1 0.8817, precision 0.8802, recall 0.8833, auc 0.8815
epoch 10601, loss 0.4170, train acc 88.20%, f1 0.8820, precision 0.8817, recall 0.8823, auc 0.8820
epoch 10701, loss 0.1702, train acc 88.26%, f1 0.8827, precision 0.8817, recall 0.8838, auc 0.8826
epoch 10801, loss 0.1918, train acc 88.34%, f1 0.8833, precision 0.8837, recall 0.8829, auc 0.8834
epoch 10901, loss 0.2687, train acc 88.33%, f1 0.8833, precision 0.8830, recall 0.8836, auc 0.8833
epoch 11001, loss 0.3694, train acc 88.40%, f1 0.8842, precision 0.8828, recall 0.8855, auc 0.8840
epoch 11101, loss 0.2382, train acc 88.47%, f1 0.8847, precision 0.8852, recall 0.8842, auc 0.8847
epoch 11201, loss 0.2748, train acc 88.49%, f1 0.8850, precision 0.8843, recall 0.8857, auc 0.8849
epoch 11301, loss 0.2151, train acc 88.56%, f1 0.8855, precision 0.8862, recall 0.8849, auc 0.8856
epoch 11401, loss 0.3384, train acc 88.56%, f1 0.8856, precision 0.8856, recall 0.8857, auc 0.8856
epoch 11501, loss 0.2931, train acc 88.59%, f1 0.8860, precision 0.8850, recall 0.8870, auc 0.8859
epoch 11601, loss 0.2407, train acc 88.65%, f1 0.8865, precision 0.8864, recall 0.8866, auc 0.8865
epoch 11701, loss 0.2743, train acc 88.64%, f1 0.8864, precision 0.8866, recall 0.8862, auc 0.8864
epoch 11801, loss 0.2168, train acc 88.68%, f1 0.8870, precision 0.8860, recall 0.8880, auc 0.8868
epoch 11901, loss 0.2651, train acc 88.79%, f1 0.8880, precision 0.8871, recall 0.8888, auc 0.8879
epoch 12001, loss 0.1882, train acc 88.84%, f1 0.8885, precision 0.8879, recall 0.8891, auc 0.8884
epoch 12101, loss 0.2593, train acc 88.84%, f1 0.8884, precision 0.8885, recall 0.8884, auc 0.8884
epoch 12201, loss 0.3187, train acc 88.94%, f1 0.8895, precision 0.8886, recall 0.8904, auc 0.8894
epoch 12301, loss 0.2377, train acc 88.88%, f1 0.8888, precision 0.8889, recall 0.8888, auc 0.8888
epoch 12401, loss 0.1842, train acc 89.02%, f1 0.8904, precision 0.8891, recall 0.8917, auc 0.8902
epoch 12501, loss 0.3010, train acc 89.06%, f1 0.8908, precision 0.8895, recall 0.8920, auc 0.8906
epoch 12601, loss 0.2324, train acc 89.08%, f1 0.8908, precision 0.8911, recall 0.8906, auc 0.8908
epoch 12701, loss 0.1561, train acc 89.13%, f1 0.8914, precision 0.8907, recall 0.8922, auc 0.8913
epoch 12801, loss 0.2111, train acc 89.17%, f1 0.8918, precision 0.8913, recall 0.8922, auc 0.8917
epoch 12901, loss 0.2147, train acc 89.22%, f1 0.8924, precision 0.8909, recall 0.8939, auc 0.8922
epoch 13001, loss 0.3087, train acc 89.25%, f1 0.8926, precision 0.8917, recall 0.8934, auc 0.8925
epoch 13101, loss 0.3003, train acc 89.35%, f1 0.8936, precision 0.8927, recall 0.8945, auc 0.8935
epoch 13201, loss 0.2449, train acc 89.36%, f1 0.8938, precision 0.8926, recall 0.8950, auc 0.8936
epoch 13301, loss 0.2553, train acc 89.33%, f1 0.8933, precision 0.8929, recall 0.8937, auc 0.8933
epoch 13401, loss 0.2256, train acc 89.42%, f1 0.8943, precision 0.8940, recall 0.8946, auc 0.8942
epoch 13501, loss 0.2091, train acc 89.46%, f1 0.8946, precision 0.8945, recall 0.8947, auc 0.8946
epoch 13601, loss 0.1828, train acc 89.47%, f1 0.8946, precision 0.8957, recall 0.8934, auc 0.8947
epoch 13701, loss 0.2337, train acc 89.61%, f1 0.8963, precision 0.8948, recall 0.8979, auc 0.8961
epoch 13801, loss 0.2811, train acc 89.60%, f1 0.8960, precision 0.8959, recall 0.8961, auc 0.8960
epoch 13901, loss 0.2466, train acc 89.65%, f1 0.8967, precision 0.8952, recall 0.8981, auc 0.8965
epoch 14001, loss 0.2524, train acc 89.70%, f1 0.8971, precision 0.8963, recall 0.8979, auc 0.8970
epoch 14101, loss 0.2537, train acc 89.72%, f1 0.8973, precision 0.8969, recall 0.8977, auc 0.8972
epoch 14201, loss 0.2273, train acc 89.79%, f1 0.8978, precision 0.8986, recall 0.8970, auc 0.8979
epoch 14301, loss 0.2734, train acc 89.80%, f1 0.8980, precision 0.8975, recall 0.8985, auc 0.8980
epoch 14401, loss 0.2796, train acc 89.89%, f1 0.8990, precision 0.8979, recall 0.9001, auc 0.8989
epoch 14501, loss 0.1769, train acc 89.92%, f1 0.8993, precision 0.8987, recall 0.8999, auc 0.8992
epoch 14601, loss 0.1835, train acc 89.93%, f1 0.8994, precision 0.8990, recall 0.8998, auc 0.8993
epoch 14701, loss 0.2600, train acc 89.96%, f1 0.8998, precision 0.8976, recall 0.9021, auc 0.8996
epoch 14801, loss 0.2026, train acc 90.02%, f1 0.9003, precision 0.8991, recall 0.9015, auc 0.9002
epoch 14901, loss 0.2086, train acc 90.04%, f1 0.9006, precision 0.8989, recall 0.9022, auc 0.9004
epoch 15001, loss 0.3438, train acc 90.12%, f1 0.9012, precision 0.9008, recall 0.9016, auc 0.9012
epoch 15101, loss 0.1835, train acc 90.11%, f1 0.9011, precision 0.9006, recall 0.9016, auc 0.9011
epoch 15201, loss 0.1759, train acc 90.17%, f1 0.9020, precision 0.8994, recall 0.9046, auc 0.9017
epoch 15301, loss 0.3157, train acc 90.24%, f1 0.9025, precision 0.9017, recall 0.9033, auc 0.9024
epoch 15401, loss 0.1869, train acc 90.21%, f1 0.9023, precision 0.9009, recall 0.9036, auc 0.9021
epoch 15501, loss 0.1689, train acc 90.26%, f1 0.9026, precision 0.9026, recall 0.9025, auc 0.9026
epoch 15601, loss 0.2883, train acc 90.34%, f1 0.9035, precision 0.9026, recall 0.9043, auc 0.9034
epoch 15701, loss 0.2758, train acc 90.29%, f1 0.9029, precision 0.9027, recall 0.9031, auc 0.9029
epoch 15801, loss 0.2246, train acc 90.41%, f1 0.9041, precision 0.9042, recall 0.9041, auc 0.9041
epoch 15901, loss 0.1911, train acc 90.44%, f1 0.9045, precision 0.9035, recall 0.9055, auc 0.9044
epoch 16001, loss 0.3378, train acc 90.43%, f1 0.9043, precision 0.9039, recall 0.9047, auc 0.9043
epoch 16101, loss 0.3029, train acc 90.47%, f1 0.9048, precision 0.9039, recall 0.9056, auc 0.9047
epoch 16201, loss 0.2368, train acc 90.46%, f1 0.9048, precision 0.9030, recall 0.9067, auc 0.9046
epoch 16301, loss 0.2204, train acc 90.51%, f1 0.9052, precision 0.9044, recall 0.9060, auc 0.9051
epoch 16401, loss 0.2188, train acc 90.52%, f1 0.9053, precision 0.9047, recall 0.9058, auc 0.9052
epoch 16501, loss 0.1834, train acc 90.60%, f1 0.9061, precision 0.9048, recall 0.9074, auc 0.9060/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.1807, train acc 90.65%, f1 0.9065, precision 0.9063, recall 0.9066, auc 0.9065
epoch 16701, loss 0.2176, train acc 90.73%, f1 0.9075, precision 0.9053, recall 0.9098, auc 0.9073
epoch 16801, loss 0.1493, train acc 90.73%, f1 0.9073, precision 0.9067, recall 0.9080, auc 0.9073
epoch 16901, loss 0.1973, train acc 90.76%, f1 0.9078, precision 0.9061, recall 0.9095, auc 0.9076
epoch 17001, loss 0.2154, train acc 90.74%, f1 0.9074, precision 0.9069, recall 0.9079, auc 0.9074
epoch 17101, loss 0.1371, train acc 90.84%, f1 0.9086, precision 0.9067, recall 0.9105, auc 0.9084
epoch 17201, loss 0.1978, train acc 90.79%, f1 0.9080, precision 0.9067, recall 0.9093, auc 0.9079
epoch 17301, loss 0.2769, train acc 90.93%, f1 0.9094, precision 0.9084, recall 0.9104, auc 0.9093
epoch 17401, loss 0.2713, train acc 90.94%, f1 0.9094, precision 0.9091, recall 0.9097, auc 0.9094
epoch 17501, loss 0.2598, train acc 90.96%, f1 0.9095, precision 0.9106, recall 0.9085, auc 0.9096
epoch 17601, loss 0.1537, train acc 90.96%, f1 0.9097, precision 0.9085, recall 0.9109, auc 0.9096
epoch 17701, loss 0.2012, train acc 91.04%, f1 0.9104, precision 0.9106, recall 0.9103, auc 0.9104
epoch 17801, loss 0.1601, train acc 91.07%, f1 0.9107, precision 0.9105, recall 0.9109, auc 0.9107
epoch 17901, loss 0.2175, train acc 91.06%, f1 0.9107, precision 0.9099, recall 0.9115, auc 0.9106
epoch 18001, loss 0.2218, train acc 91.09%, f1 0.9109, precision 0.9107, recall 0.9112, auc 0.9109
epoch 18101, loss 0.3013, train acc 91.15%, f1 0.9117, precision 0.9103, recall 0.9130, auc 0.9115
epoch 18201, loss 0.2078, train acc 91.25%, f1 0.9126, precision 0.9113, recall 0.9139, auc 0.9125
epoch 18301, loss 0.3018, train acc 91.24%, f1 0.9126, precision 0.9106, recall 0.9146, auc 0.9124
epoch 18401, loss 0.2250, train acc 91.26%, f1 0.9126, precision 0.9122, recall 0.9130, auc 0.9126
epoch 18501, loss 0.2466, train acc 91.28%, f1 0.9128, precision 0.9123, recall 0.9133, auc 0.9128
epoch 18601, loss 0.1720, train acc 91.33%, f1 0.9135, precision 0.9119, recall 0.9151, auc 0.9133
epoch 18701, loss 0.2064, train acc 91.42%, f1 0.9143, precision 0.9138, recall 0.9147, auc 0.9142
epoch 18801, loss 0.1799, train acc 91.47%, f1 0.9147, precision 0.9150, recall 0.9143, auc 0.9147
epoch 18901, loss 0.3000, train acc 91.42%, f1 0.9141, precision 0.9155, recall 0.9127, auc 0.9142
epoch 19001, loss 0.2265, train acc 91.43%, f1 0.9145, precision 0.9124, recall 0.9166, auc 0.9143
epoch 19101, loss 0.2635, train acc 91.50%, f1 0.9151, precision 0.9140, recall 0.9163, auc 0.9150
epoch 19201, loss 0.2787, train acc 91.63%, f1 0.9164, precision 0.9161, recall 0.9166, auc 0.9163
epoch 19301, loss 0.1703, train acc 91.55%, f1 0.9155, precision 0.9149, recall 0.9162, auc 0.9155
epoch 19401, loss 0.2467, train acc 91.64%, f1 0.9165, precision 0.9158, recall 0.9172, auc 0.9164
epoch 19501, loss 0.2419, train acc 91.68%, f1 0.9169, precision 0.9157, recall 0.9181, auc 0.9168
epoch 19601, loss 0.2149, train acc 91.76%, f1 0.9176, precision 0.9169, recall 0.9184, auc 0.9176
epoch 19701, loss 0.2336, train acc 91.74%, f1 0.9175, precision 0.9161, recall 0.9190, auc 0.9174
epoch 19801, loss 0.2221, train acc 91.82%, f1 0.9184, precision 0.9160, recall 0.9207, auc 0.9182
epoch 19901, loss 0.1696, train acc 91.81%, f1 0.9182, precision 0.9168, recall 0.9196, auc 0.9181
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_3
./test_pima/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6637037037037037

the Fscore is 0.6086956521739131

the precision is 0.45794392523364486

the recall is 0.9074074074074074

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_3
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5932, train acc 78.43%, f1 0.7834, precision 0.7866, recall 0.7803, auc 0.7843
epoch 201, loss 0.3867, train acc 81.11%, f1 0.8111, precision 0.8111, recall 0.8111, auc 0.8111
epoch 301, loss 0.3685, train acc 82.56%, f1 0.8257, precision 0.8250, recall 0.8265, auc 0.8256
epoch 401, loss 0.3228, train acc 82.98%, f1 0.8298, precision 0.8295, recall 0.8302, auc 0.8298
epoch 501, loss 0.3580, train acc 83.03%, f1 0.8304, precision 0.8298, recall 0.8310, auc 0.8303
epoch 601, loss 0.2995, train acc 83.05%, f1 0.8306, precision 0.8303, recall 0.8309, auc 0.8305
epoch 701, loss 0.3853, train acc 83.09%, f1 0.8310, precision 0.8306, recall 0.8313, auc 0.8309
epoch 801, loss 0.3936, train acc 83.09%, f1 0.8310, precision 0.8305, recall 0.8315, auc 0.8309
epoch 901, loss 0.4095, train acc 83.19%, f1 0.8320, precision 0.8315, recall 0.8324, auc 0.8319
epoch 1001, loss 0.2968, train acc 83.09%, f1 0.8310, precision 0.8306, recall 0.8313, auc 0.8309
epoch 1101, loss 0.3422, train acc 83.12%, f1 0.8313, precision 0.8308, recall 0.8317, auc 0.8312
epoch 1201, loss 0.3132, train acc 83.15%, f1 0.8315, precision 0.8312, recall 0.8318, auc 0.8315
epoch 1301, loss 0.3988, train acc 83.09%, f1 0.8309, precision 0.8306, recall 0.8313, auc 0.8309
epoch 1401, loss 0.5462, train acc 83.16%, f1 0.8317, precision 0.8313, recall 0.8322, auc 0.8316
epoch 1501, loss 0.4487, train acc 83.16%, f1 0.8318, precision 0.8312, recall 0.8323, auc 0.8316
epoch 1601, loss 0.4228, train acc 83.12%, f1 0.8313, precision 0.8308, recall 0.8318, auc 0.8312
epoch 1701, loss 0.2654, train acc 83.08%, f1 0.8309, precision 0.8302, recall 0.8316, auc 0.8308
epoch 1801, loss 0.3632, train acc 83.12%, f1 0.8314, precision 0.8306, recall 0.8321, auc 0.8312
epoch 1901, loss 0.3431, train acc 83.15%, f1 0.8316, precision 0.8310, recall 0.8322, auc 0.8315
epoch 2001, loss 0.3496, train acc 83.16%, f1 0.8318, precision 0.8311, recall 0.8325, auc 0.8316
epoch 2101, loss 0.4700, train acc 83.18%, f1 0.8320, precision 0.8312, recall 0.8328, auc 0.8318
epoch 2201, loss 0.4051, train acc 83.30%, f1 0.8332, precision 0.8324, recall 0.8341, auc 0.8330
epoch 2301, loss 0.4232, train acc 83.36%, f1 0.8338, precision 0.8328, recall 0.8347, auc 0.8336
epoch 2401, loss 0.3966, train acc 83.41%, f1 0.8343, precision 0.8335, recall 0.8351, auc 0.8341
epoch 2501, loss 0.4029, train acc 83.46%, f1 0.8347, precision 0.8342, recall 0.8352, auc 0.8346
epoch 2601, loss 0.3958, train acc 83.53%, f1 0.8354, precision 0.8351, recall 0.8357, auc 0.8353
epoch 2701, loss 0.2958, train acc 83.63%, f1 0.8364, precision 0.8361, recall 0.8366, auc 0.8363
epoch 2801, loss 0.3866, train acc 83.76%, f1 0.8378, precision 0.8368, recall 0.8388, auc 0.8376
epoch 2901, loss 0.4208, train acc 83.92%, f1 0.8393, precision 0.8391, recall 0.8394, auc 0.8392
epoch 3001, loss 0.4169, train acc 84.03%, f1 0.8406, precision 0.8392, recall 0.8419, auc 0.8403
epoch 3101, loss 0.2816, train acc 84.23%, f1 0.8425, precision 0.8418, recall 0.8432, auc 0.8423
epoch 3201, loss 0.3894, train acc 84.37%, f1 0.8439, precision 0.8428, recall 0.8450, auc 0.8437
epoch 3301, loss 0.3240, train acc 84.45%, f1 0.8446, precision 0.8440, recall 0.8452, auc 0.8445
epoch 3401, loss 0.4218, train acc 84.59%, f1 0.8459, precision 0.8462, recall 0.8456, auc 0.8459
epoch 3501, loss 0.3486, train acc 84.73%, f1 0.8475, precision 0.8464, recall 0.8486, auc 0.8473
epoch 3601, loss 0.3361, train acc 84.83%, f1 0.8485, precision 0.8477, recall 0.8492, auc 0.8483
epoch 3701, loss 0.5151, train acc 84.98%, f1 0.8497, precision 0.8502, recall 0.8493, auc 0.8498
epoch 3801, loss 0.3063, train acc 85.08%, f1 0.8509, precision 0.8506, recall 0.8512, auc 0.8508
epoch 3901, loss 0.3704, train acc 85.23%, f1 0.8524, precision 0.8519, recall 0.8528, auc 0.8523
epoch 4001, loss 0.2957, train acc 85.31%, f1 0.8531, precision 0.8532, recall 0.8531, auc 0.8531
epoch 4101, loss 0.2408, train acc 85.40%, f1 0.8540, precision 0.8536, recall 0.8545, auc 0.8540
epoch 4201, loss 0.3613, train acc 85.43%, f1 0.8544, precision 0.8541, recall 0.8547, auc 0.8543
epoch 4301, loss 0.3417, train acc 85.53%, f1 0.8552, precision 0.8555, recall 0.8548, auc 0.8553
epoch 4401, loss 0.3819, train acc 85.62%, f1 0.8562, precision 0.8563, recall 0.8561, auc 0.8562
epoch 4501, loss 0.3164, train acc 85.76%, f1 0.8576, precision 0.8576, recall 0.8576, auc 0.8576
epoch 4601, loss 0.3525, train acc 85.78%, f1 0.8576, precision 0.8591, recall 0.8560, auc 0.8578
epoch 4701, loss 0.3282, train acc 85.84%, f1 0.8582, precision 0.8592, recall 0.8572, auc 0.8584
epoch 4801, loss 0.2468, train acc 85.92%, f1 0.8592, precision 0.8591, recall 0.8594, auc 0.8592
epoch 4901, loss 0.3720, train acc 86.04%, f1 0.8606, precision 0.8597, recall 0.8615, auc 0.8604
epoch 5001, loss 0.2503, train acc 86.04%, f1 0.8604, precision 0.8601, recall 0.8607, auc 0.8604
epoch 5101, loss 0.3302, train acc 86.08%, f1 0.8609, precision 0.8605, recall 0.8613, auc 0.8608
epoch 5201, loss 0.2814, train acc 86.15%, f1 0.8616, precision 0.8607, recall 0.8625, auc 0.8615
epoch 5301, loss 0.4540, train acc 86.18%, f1 0.8616, precision 0.8630, recall 0.8602, auc 0.8618
epoch 5401, loss 0.2866, train acc 86.22%, f1 0.8622, precision 0.8622, recall 0.8622, auc 0.8622
epoch 5501, loss 0.3905, train acc 86.28%, f1 0.8627, precision 0.8634, recall 0.8620, auc 0.8628
epoch 5601, loss 0.2263, train acc 86.31%, f1 0.8630, precision 0.8634, recall 0.8626, auc 0.8631
epoch 5701, loss 0.2509, train acc 86.37%, f1 0.8637, precision 0.8641, recall 0.8633, auc 0.8637
epoch 5801, loss 0.2982, train acc 86.46%, f1 0.8645, precision 0.8648, recall 0.8642, auc 0.8646
epoch 5901, loss 0.2920, train acc 86.53%, f1 0.8652, precision 0.8656, recall 0.8649, auc 0.8653
epoch 6001, loss 0.3044, train acc 86.53%, f1 0.8654, precision 0.8647, recall 0.8661, auc 0.8653
epoch 6101, loss 0.4076, train acc 86.50%, f1 0.8650, precision 0.8654, recall 0.8645, auc 0.8650
epoch 6201, loss 0.3942, train acc 86.58%, f1 0.8660, precision 0.8651, recall 0.8669, auc 0.8658
epoch 6301, loss 0.3449, train acc 86.63%, f1 0.8663, precision 0.8663, recall 0.8663, auc 0.8663
epoch 6401, loss 0.2342, train acc 86.65%, f1 0.8665, precision 0.8670, recall 0.8659, auc 0.8665
epoch 6501, loss 0.2822, train acc 86.72%, f1 0.8674, precision 0.8662, recall 0.8686, auc 0.8672
epoch 6601, loss 0.3361, train acc 86.72%, f1 0.8673, precision 0.8665, recall 0.8681, auc 0.8672
epoch 6701, loss 0.2799, train acc 86.82%, f1 0.8680, precision 0.8694, recall 0.8666, auc 0.8682
epoch 6801, loss 0.3322, train acc 86.82%, f1 0.8683, precision 0.8679, recall 0.8686, auc 0.8682
epoch 6901, loss 0.2375, train acc 86.84%, f1 0.8685, precision 0.8681, recall 0.8688, auc 0.8684
epoch 7001, loss 0.2416, train acc 86.90%, f1 0.8690, precision 0.8689, recall 0.8692, auc 0.8690
epoch 7101, loss 0.2898, train acc 86.98%, f1 0.8698, precision 0.8698, recall 0.8698, auc 0.8698
epoch 7201, loss 0.2310, train acc 87.00%, f1 0.8701, precision 0.8700, recall 0.8701, auc 0.8700
epoch 7301, loss 0.2963, train acc 87.03%, f1 0.8704, precision 0.8699, recall 0.8708, auc 0.8703
epoch 7401, loss 0.2491, train acc 87.05%, f1 0.8704, precision 0.8713, recall 0.8695, auc 0.8705
epoch 7501, loss 0.1916, train acc 87.03%, f1 0.8703, precision 0.8703, recall 0.8702, auc 0.8703
epoch 7601, loss 0.3357, train acc 87.13%, f1 0.8714, precision 0.8706, recall 0.8722, auc 0.8713
epoch 7701, loss 0.2848, train acc 87.15%, f1 0.8715, precision 0.8712, recall 0.8718, auc 0.8715
epoch 7801, loss 0.2753, train acc 87.21%, f1 0.8722, precision 0.8715, recall 0.8730, auc 0.8721
epoch 7901, loss 0.2837, train acc 87.28%, f1 0.8729, precision 0.8718, recall 0.8741, auc 0.8728
epoch 8001, loss 0.2484, train acc 87.27%, f1 0.8725, precision 0.8737, recall 0.8712, auc 0.8727
epoch 8101, loss 0.2386, train acc 87.29%, f1 0.8731, precision 0.8715, recall 0.8748, auc 0.8729
epoch 8201, loss 0.3139, train acc 87.28%, f1 0.8727, precision 0.8736, recall 0.8718, auc 0.8728/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.3777, train acc 87.41%, f1 0.8741, precision 0.8739, recall 0.8742, auc 0.8741
epoch 8401, loss 0.3239, train acc 87.44%, f1 0.8746, precision 0.8730, recall 0.8762, auc 0.8744
epoch 8501, loss 0.2559, train acc 87.45%, f1 0.8745, precision 0.8747, recall 0.8743, auc 0.8745
epoch 8601, loss 0.2686, train acc 87.48%, f1 0.8746, precision 0.8763, recall 0.8729, auc 0.8748
epoch 8701, loss 0.3023, train acc 87.50%, f1 0.8751, precision 0.8741, recall 0.8761, auc 0.8750
epoch 8801, loss 0.2439, train acc 87.55%, f1 0.8755, precision 0.8757, recall 0.8753, auc 0.8755
epoch 8901, loss 0.2216, train acc 87.58%, f1 0.8757, precision 0.8760, recall 0.8755, auc 0.8758
epoch 9001, loss 0.2077, train acc 87.63%, f1 0.8764, precision 0.8751, recall 0.8778, auc 0.8763
epoch 9101, loss 0.2097, train acc 87.67%, f1 0.8766, precision 0.8775, recall 0.8757, auc 0.8767
epoch 9201, loss 0.2504, train acc 87.75%, f1 0.8774, precision 0.8780, recall 0.8768, auc 0.8775
epoch 9301, loss 0.1907, train acc 87.72%, f1 0.8772, precision 0.8769, recall 0.8776, auc 0.8772
epoch 9401, loss 0.2397, train acc 87.80%, f1 0.8779, precision 0.8792, recall 0.8765, auc 0.8780
epoch 9501, loss 0.1979, train acc 87.85%, f1 0.8784, precision 0.8794, recall 0.8773, auc 0.8785
epoch 9601, loss 0.2733, train acc 87.84%, f1 0.8784, precision 0.8787, recall 0.8780, auc 0.8784
epoch 9701, loss 0.3511, train acc 87.91%, f1 0.8790, precision 0.8792, recall 0.8789, auc 0.8791
epoch 9801, loss 0.3034, train acc 87.97%, f1 0.8795, precision 0.8806, recall 0.8784, auc 0.8797
epoch 9901, loss 0.3201, train acc 87.98%, f1 0.8799, precision 0.8794, recall 0.8805, auc 0.8798
epoch 10001, loss 0.2694, train acc 87.99%, f1 0.8798, precision 0.8803, recall 0.8793, auc 0.8799
epoch 10101, loss 0.2848, train acc 88.02%, f1 0.8802, precision 0.8801, recall 0.8804, auc 0.8802
epoch 10201, loss 0.3421, train acc 88.07%, f1 0.8807, precision 0.8802, recall 0.8812, auc 0.8807
epoch 10301, loss 0.2304, train acc 88.05%, f1 0.8804, precision 0.8811, recall 0.8796, auc 0.8805
epoch 10401, loss 0.3817, train acc 88.13%, f1 0.8813, precision 0.8817, recall 0.8809, auc 0.8813
epoch 10501, loss 0.1994, train acc 88.24%, f1 0.8826, precision 0.8812, recall 0.8841, auc 0.8824
epoch 10601, loss 0.2936, train acc 88.26%, f1 0.8828, precision 0.8814, recall 0.8842, auc 0.8826
epoch 10701, loss 0.3000, train acc 88.32%, f1 0.8833, precision 0.8820, recall 0.8846, auc 0.8832
epoch 10801, loss 0.3026, train acc 88.29%, f1 0.8828, precision 0.8837, recall 0.8818, auc 0.8829
epoch 10901, loss 0.2481, train acc 88.33%, f1 0.8833, precision 0.8834, recall 0.8831, auc 0.8833
epoch 11001, loss 0.3163, train acc 88.42%, f1 0.8842, precision 0.8837, recall 0.8848, auc 0.8842
epoch 11101, loss 0.3211, train acc 88.43%, f1 0.8841, precision 0.8859, recall 0.8823, auc 0.8843
epoch 11201, loss 0.3808, train acc 88.50%, f1 0.8849, precision 0.8854, recall 0.8844, auc 0.8850
epoch 11301, loss 0.3986, train acc 88.50%, f1 0.8853, precision 0.8831, recall 0.8875, auc 0.8850
epoch 11401, loss 0.3109, train acc 88.54%, f1 0.8854, precision 0.8852, recall 0.8857, auc 0.8854
epoch 11501, loss 0.3080, train acc 88.54%, f1 0.8853, precision 0.8862, recall 0.8844, auc 0.8854
epoch 11601, loss 0.2402, train acc 88.59%, f1 0.8859, precision 0.8860, recall 0.8858, auc 0.8859
epoch 11701, loss 0.3062, train acc 88.70%, f1 0.8871, precision 0.8868, recall 0.8873, auc 0.8870
epoch 11801, loss 0.2485, train acc 88.71%, f1 0.8867, precision 0.8897, recall 0.8836, auc 0.8871
epoch 11901, loss 0.2413, train acc 88.76%, f1 0.8876, precision 0.8876, recall 0.8876, auc 0.8876
epoch 12001, loss 0.2467, train acc 88.79%, f1 0.8878, precision 0.8887, recall 0.8868, auc 0.8879
epoch 12101, loss 0.1999, train acc 88.86%, f1 0.8884, precision 0.8898, recall 0.8870, auc 0.8886
epoch 12201, loss 0.2834, train acc 88.94%, f1 0.8893, precision 0.8904, recall 0.8882, auc 0.8894
epoch 12301, loss 0.2315, train acc 88.90%, f1 0.8888, precision 0.8907, recall 0.8869, auc 0.8890
epoch 12401, loss 0.1909, train acc 89.07%, f1 0.8908, precision 0.8900, recall 0.8915, auc 0.8907
epoch 12501, loss 0.3792, train acc 89.01%, f1 0.8897, precision 0.8927, recall 0.8866, auc 0.8901
epoch 12601, loss 0.2636, train acc 89.04%, f1 0.8903, precision 0.8915, recall 0.8891, auc 0.8904
epoch 12701, loss 0.2165, train acc 89.13%, f1 0.8910, precision 0.8932, recall 0.8888, auc 0.8913
epoch 12801, loss 0.2685, train acc 89.09%, f1 0.8909, precision 0.8910, recall 0.8907, auc 0.8909
epoch 12901, loss 0.1556, train acc 89.22%, f1 0.8922, precision 0.8925, recall 0.8918, auc 0.8922
epoch 13001, loss 0.2572, train acc 89.24%, f1 0.8924, precision 0.8921, recall 0.8928, auc 0.8924
epoch 13101, loss 0.1916, train acc 89.23%, f1 0.8922, precision 0.8930, recall 0.8913, auc 0.8923
epoch 13201, loss 0.3131, train acc 89.35%, f1 0.8933, precision 0.8951, recall 0.8915, auc 0.8935
epoch 13301, loss 0.1823, train acc 89.36%, f1 0.8935, precision 0.8948, recall 0.8921, auc 0.8936
epoch 13401, loss 0.2902, train acc 89.35%, f1 0.8934, precision 0.8937, recall 0.8931, auc 0.8935
epoch 13501, loss 0.1346, train acc 89.47%, f1 0.8946, precision 0.8958, recall 0.8934, auc 0.8947
epoch 13601, loss 0.2660, train acc 89.45%, f1 0.8943, precision 0.8955, recall 0.8932, auc 0.8945
epoch 13701, loss 0.2721, train acc 89.53%, f1 0.8950, precision 0.8974, recall 0.8927, auc 0.8953
epoch 13801, loss 0.3330, train acc 89.57%, f1 0.8958, precision 0.8956, recall 0.8960, auc 0.8957
epoch 13901, loss 0.2814, train acc 89.58%, f1 0.8956, precision 0.8975, recall 0.8937, auc 0.8958
epoch 14001, loss 0.3042, train acc 89.68%, f1 0.8969, precision 0.8966, recall 0.8971, auc 0.8968
epoch 14101, loss 0.2054, train acc 89.67%, f1 0.8964, precision 0.8985, recall 0.8944, auc 0.8967
epoch 14201, loss 0.2809, train acc 89.74%, f1 0.8972, precision 0.8986, recall 0.8957, auc 0.8974
epoch 14301, loss 0.1685, train acc 89.70%, f1 0.8969, precision 0.8974, recall 0.8965, auc 0.8970
epoch 14401, loss 0.2303, train acc 89.82%, f1 0.8982, precision 0.8986, recall 0.8977, auc 0.8982
epoch 14501, loss 0.2309, train acc 89.86%, f1 0.8984, precision 0.8999, recall 0.8970, auc 0.8986
epoch 14601, loss 0.2409, train acc 89.87%, f1 0.8986, precision 0.8990, recall 0.8982, auc 0.8987
epoch 14701, loss 0.2145, train acc 89.98%, f1 0.8997, precision 0.9003, recall 0.8991, auc 0.8998
epoch 14801, loss 0.2441, train acc 90.04%, f1 0.9002, precision 0.9018, recall 0.8987, auc 0.9004
epoch 14901, loss 0.2078, train acc 90.03%, f1 0.9002, precision 0.9006, recall 0.8998, auc 0.9003
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_3
./test_pima/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6722222222222223

the Fscore is 0.6181818181818182

the precision is 0.4594594594594595

the recall is 0.9444444444444444

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_3
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5648, train acc 78.14%, f1 0.7814, precision 0.7815, recall 0.7814, auc 0.7814
epoch 201, loss 0.4073, train acc 81.12%, f1 0.8112, precision 0.8112, recall 0.8112, auc 0.8112
epoch 301, loss 0.3886, train acc 82.49%, f1 0.8249, precision 0.8248, recall 0.8249, auc 0.8249
epoch 401, loss 0.3050, train acc 82.85%, f1 0.8285, precision 0.8285, recall 0.8285, auc 0.8285
epoch 501, loss 0.3543, train acc 82.98%, f1 0.8298, precision 0.8298, recall 0.8298, auc 0.8298
epoch 601, loss 0.3429, train acc 83.05%, f1 0.8305, precision 0.8305, recall 0.8304, auc 0.8305
epoch 701, loss 0.3666, train acc 83.13%, f1 0.8313, precision 0.8313, recall 0.8313, auc 0.8313
epoch 801, loss 0.5954, train acc 83.08%, f1 0.8308, precision 0.8308, recall 0.8308, auc 0.8308
epoch 901, loss 0.4679, train acc 83.11%, f1 0.8311, precision 0.8311, recall 0.8311, auc 0.8311
epoch 1001, loss 0.3840, train acc 83.11%, f1 0.8311, precision 0.8310, recall 0.8311, auc 0.8311
epoch 1101, loss 0.4803, train acc 83.15%, f1 0.8315, precision 0.8315, recall 0.8316, auc 0.8315
epoch 1201, loss 0.3512, train acc 83.13%, f1 0.8314, precision 0.8313, recall 0.8314, auc 0.8313
epoch 1301, loss 0.4352, train acc 83.14%, f1 0.8314, precision 0.8314, recall 0.8315, auc 0.8314
epoch 1401, loss 0.3099, train acc 83.14%, f1 0.8314, precision 0.8313, recall 0.8316, auc 0.8314
epoch 1501, loss 0.3744, train acc 83.13%, f1 0.8314, precision 0.8312, recall 0.8315, auc 0.8313
epoch 1601, loss 0.4300, train acc 83.14%, f1 0.8314, precision 0.8313, recall 0.8315, auc 0.8314
epoch 1701, loss 0.4621, train acc 83.17%, f1 0.8317, precision 0.8316, recall 0.8318, auc 0.8317
epoch 1801, loss 0.3765, train acc 83.15%, f1 0.8316, precision 0.8314, recall 0.8318, auc 0.8315
epoch 1901, loss 0.3800, train acc 83.18%, f1 0.8318, precision 0.8316, recall 0.8319, auc 0.8318
epoch 2001, loss 0.5766, train acc 83.21%, f1 0.8322, precision 0.8320, recall 0.8324, auc 0.8321
epoch 2101, loss 0.4452, train acc 83.21%, f1 0.8321, precision 0.8321, recall 0.8321, auc 0.8321
epoch 2201, loss 0.3523, train acc 83.33%, f1 0.8333, precision 0.8332, recall 0.8334, auc 0.8333
epoch 2301, loss 0.3113, train acc 83.37%, f1 0.8337, precision 0.8336, recall 0.8338, auc 0.8337
epoch 2401, loss 0.3376, train acc 83.44%, f1 0.8343, precision 0.8344, recall 0.8343, auc 0.8344
epoch 2501, loss 0.3278, train acc 83.56%, f1 0.8357, precision 0.8354, recall 0.8360, auc 0.8356
epoch 2601, loss 0.2816, train acc 83.60%, f1 0.8360, precision 0.8360, recall 0.8359, auc 0.8360
epoch 2701, loss 0.2361, train acc 83.74%, f1 0.8374, precision 0.8374, recall 0.8374, auc 0.8374
epoch 2801, loss 0.3699, train acc 83.90%, f1 0.8390, precision 0.8392, recall 0.8387, auc 0.8390
epoch 2901, loss 0.3813, train acc 84.03%, f1 0.8403, precision 0.8404, recall 0.8401, auc 0.8403
epoch 3001, loss 0.3699, train acc 84.16%, f1 0.8417, precision 0.8415, recall 0.8418, auc 0.8416
epoch 3101, loss 0.3406, train acc 84.30%, f1 0.8430, precision 0.8431, recall 0.8429, auc 0.8430
epoch 3201, loss 0.3292, train acc 84.43%, f1 0.8443, precision 0.8444, recall 0.8443, auc 0.8443
epoch 3301, loss 0.3485, train acc 84.64%, f1 0.8463, precision 0.8465, recall 0.8461, auc 0.8464
epoch 3401, loss 0.2982, train acc 84.73%, f1 0.8473, precision 0.8470, recall 0.8476, auc 0.8473
epoch 3501, loss 0.3240, train acc 84.85%, f1 0.8485, precision 0.8483, recall 0.8487, auc 0.8485
epoch 3601, loss 0.2703, train acc 84.95%, f1 0.8495, precision 0.8495, recall 0.8494, auc 0.8495
epoch 3701, loss 0.3983, train acc 85.09%, f1 0.8509, precision 0.8510, recall 0.8508, auc 0.8509
epoch 3801, loss 0.4164, train acc 85.19%, f1 0.8519, precision 0.8519, recall 0.8518, auc 0.8519
epoch 3901, loss 0.4205, train acc 85.23%, f1 0.8523, precision 0.8527, recall 0.8518, auc 0.8523
epoch 4001, loss 0.3520, train acc 85.29%, f1 0.8528, precision 0.8532, recall 0.8524, auc 0.8529
epoch 4101, loss 0.2350, train acc 85.40%, f1 0.8539, precision 0.8542, recall 0.8537, auc 0.8540
epoch 4201, loss 0.2755, train acc 85.46%, f1 0.8546, precision 0.8551, recall 0.8541, auc 0.8546
epoch 4301, loss 0.3879, train acc 85.55%, f1 0.8555, precision 0.8553, recall 0.8558, auc 0.8555
epoch 4401, loss 0.3013, train acc 85.63%, f1 0.8564, precision 0.8557, recall 0.8570, auc 0.8563
epoch 4501, loss 0.2294, train acc 85.70%, f1 0.8569, precision 0.8572, recall 0.8566, auc 0.8570
epoch 4601, loss 0.3396, train acc 85.77%, f1 0.8576, precision 0.8583, recall 0.8570, auc 0.8577
epoch 4701, loss 0.3547, train acc 85.83%, f1 0.8582, precision 0.8590, recall 0.8573, auc 0.8583
epoch 4801, loss 0.3233, train acc 85.94%, f1 0.8595, precision 0.8592, recall 0.8598, auc 0.8594
epoch 4901, loss 0.2291, train acc 85.95%, f1 0.8595, precision 0.8596, recall 0.8594, auc 0.8595
epoch 5001, loss 0.3262, train acc 86.03%, f1 0.8603, precision 0.8603, recall 0.8603, auc 0.8603
epoch 5101, loss 0.2999, train acc 86.08%, f1 0.8609, precision 0.8600, recall 0.8617, auc 0.8608
epoch 5201, loss 0.2684, train acc 86.13%, f1 0.8613, precision 0.8614, recall 0.8612, auc 0.8613
epoch 5301, loss 0.2357, train acc 86.13%, f1 0.8613, precision 0.8610, recall 0.8617, auc 0.8613
epoch 5401, loss 0.4481, train acc 86.26%, f1 0.8625, precision 0.8632, recall 0.8618, auc 0.8626
epoch 5501, loss 0.2531, train acc 86.33%, f1 0.8633, precision 0.8636, recall 0.8630, auc 0.8633
epoch 5601, loss 0.2610, train acc 86.34%, f1 0.8635, precision 0.8630, recall 0.8639, auc 0.8634
epoch 5701, loss 0.3709, train acc 86.38%, f1 0.8638, precision 0.8642, recall 0.8634, auc 0.8638
epoch 5801, loss 0.2988, train acc 86.37%, f1 0.8637, precision 0.8638, recall 0.8637, auc 0.8637
epoch 5901, loss 0.3306, train acc 86.40%, f1 0.8640, precision 0.8642, recall 0.8637, auc 0.8640
epoch 6001, loss 0.3273, train acc 86.55%, f1 0.8655, precision 0.8655, recall 0.8655, auc 0.8655
epoch 6101, loss 0.3223, train acc 86.56%, f1 0.8657, precision 0.8652, recall 0.8662, auc 0.8656
epoch 6201, loss 0.2143, train acc 86.64%, f1 0.8665, precision 0.8659, recall 0.8671, auc 0.8664
epoch 6301, loss 0.2819, train acc 86.72%, f1 0.8673, precision 0.8668, recall 0.8678, auc 0.8672
epoch 6401, loss 0.4048, train acc 86.72%, f1 0.8673, precision 0.8663, recall 0.8683, auc 0.8672
epoch 6501, loss 0.3098, train acc 86.75%, f1 0.8676, precision 0.8671, recall 0.8680, auc 0.8675
epoch 6601, loss 0.2665, train acc 86.79%, f1 0.8679, precision 0.8680, recall 0.8679, auc 0.8679
epoch 6701, loss 0.3694, train acc 86.86%, f1 0.8687, precision 0.8679, recall 0.8695, auc 0.8686
epoch 6801, loss 0.3319, train acc 86.86%, f1 0.8687, precision 0.8681, recall 0.8694, auc 0.8686
epoch 6901, loss 0.3085, train acc 86.89%, f1 0.8691, precision 0.8680, recall 0.8702, auc 0.8689
epoch 7001, loss 0.2743, train acc 86.97%, f1 0.8697, precision 0.8697, recall 0.8697, auc 0.8697
epoch 7101, loss 0.2613, train acc 86.96%, f1 0.8695, precision 0.8697, recall 0.8694, auc 0.8696
epoch 7201, loss 0.1880, train acc 87.01%, f1 0.8702, precision 0.8699, recall 0.8705, auc 0.8701
epoch 7301, loss 0.3067, train acc 87.06%, f1 0.8708, precision 0.8695, recall 0.8722, auc 0.8706
epoch 7401, loss 0.3040, train acc 87.06%, f1 0.8707, precision 0.8703, recall 0.8711, auc 0.8706
epoch 7501, loss 0.3325, train acc 87.13%, f1 0.8714, precision 0.8710, recall 0.8717, auc 0.8713
epoch 7601, loss 0.2589, train acc 87.18%, f1 0.8717, precision 0.8722, recall 0.8712, auc 0.8718
epoch 7701, loss 0.3603, train acc 87.21%, f1 0.8720, precision 0.8725, recall 0.8716, auc 0.8721
epoch 7801, loss 0.3757, train acc 87.25%, f1 0.8727, precision 0.8718, recall 0.8735, auc 0.8725
epoch 7901, loss 0.3465, train acc 87.29%, f1 0.8731, precision 0.8719, recall 0.8742, auc 0.8729
epoch 8001, loss 0.3341, train acc 87.29%, f1 0.8729, precision 0.8729, recall 0.8729, auc 0.8729
epoch 8101, loss 0.2316, train acc 87.40%, f1 0.8740, precision 0.8740, recall 0.8740, auc 0.8740
epoch 8201, loss 0.2605, train acc 87.47%, f1 0.8746, precision 0.8749, recall 0.8744, auc 0.8747/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2758, train acc 87.47%, f1 0.8748, precision 0.8739, recall 0.8756, auc 0.8747
epoch 8401, loss 0.3753, train acc 87.45%, f1 0.8745, precision 0.8749, recall 0.8740, auc 0.8745
epoch 8501, loss 0.2625, train acc 87.53%, f1 0.8751, precision 0.8763, recall 0.8739, auc 0.8753
epoch 8601, loss 0.2721, train acc 87.56%, f1 0.8757, precision 0.8752, recall 0.8761, auc 0.8756
epoch 8701, loss 0.2926, train acc 87.60%, f1 0.8760, precision 0.8761, recall 0.8758, auc 0.8760
epoch 8801, loss 0.1799, train acc 87.71%, f1 0.8769, precision 0.8780, recall 0.8758, auc 0.8771
epoch 8901, loss 0.2827, train acc 87.73%, f1 0.8772, precision 0.8779, recall 0.8765, auc 0.8773
epoch 9001, loss 0.3601, train acc 87.79%, f1 0.8778, precision 0.8781, recall 0.8776, auc 0.8779
epoch 9101, loss 0.2352, train acc 87.83%, f1 0.8784, precision 0.8779, recall 0.8789, auc 0.8783
epoch 9201, loss 0.3472, train acc 87.90%, f1 0.8791, precision 0.8783, recall 0.8800, auc 0.8790
epoch 9301, loss 0.3536, train acc 87.95%, f1 0.8796, precision 0.8791, recall 0.8800, auc 0.8795
epoch 9401, loss 0.3820, train acc 87.97%, f1 0.8795, precision 0.8810, recall 0.8781, auc 0.8797
epoch 9501, loss 0.1907, train acc 87.98%, f1 0.8797, precision 0.8802, recall 0.8793, auc 0.8798
epoch 9601, loss 0.2868, train acc 88.09%, f1 0.8810, precision 0.8804, recall 0.8815, auc 0.8809
epoch 9701, loss 0.2301, train acc 88.13%, f1 0.8813, precision 0.8816, recall 0.8810, auc 0.8813
epoch 9801, loss 0.3325, train acc 88.20%, f1 0.8820, precision 0.8816, recall 0.8824, auc 0.8820
epoch 9901, loss 0.3001, train acc 88.26%, f1 0.8826, precision 0.8826, recall 0.8826, auc 0.8826
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_3
./test_pima/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6664814814814815

the Fscore is 0.6153846153846154

the precision is 0.45217391304347826

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_3
----------------------



epoch 1, loss 0.6935, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5806, train acc 77.63%, f1 0.7740, precision 0.7818, recall 0.7664, auc 0.7763
epoch 201, loss 0.4783, train acc 80.99%, f1 0.8101, precision 0.8092, recall 0.8110, auc 0.8099
epoch 301, loss 0.3416, train acc 82.48%, f1 0.8253, precision 0.8230, recall 0.8275, auc 0.8248
epoch 401, loss 0.4164, train acc 82.89%, f1 0.8293, precision 0.8273, recall 0.8314, auc 0.8289
epoch 501, loss 0.3750, train acc 83.02%, f1 0.8307, precision 0.8283, recall 0.8331, auc 0.8302
epoch 601, loss 0.3678, train acc 83.06%, f1 0.8312, precision 0.8284, recall 0.8340, auc 0.8306
epoch 701, loss 0.3980, train acc 83.05%, f1 0.8307, precision 0.8293, recall 0.8321, auc 0.8305
epoch 801, loss 0.3576, train acc 83.11%, f1 0.8314, precision 0.8297, recall 0.8331, auc 0.8311
epoch 901, loss 0.2971, train acc 83.07%, f1 0.8310, precision 0.8297, recall 0.8322, auc 0.8307
epoch 1001, loss 0.3573, train acc 83.07%, f1 0.8310, precision 0.8294, recall 0.8326, auc 0.8307
epoch 1101, loss 0.5375, train acc 83.09%, f1 0.8312, precision 0.8297, recall 0.8327, auc 0.8309
epoch 1201, loss 0.3283, train acc 83.13%, f1 0.8315, precision 0.8304, recall 0.8327, auc 0.8313
epoch 1301, loss 0.3453, train acc 83.14%, f1 0.8315, precision 0.8309, recall 0.8322, auc 0.8314
epoch 1401, loss 0.4244, train acc 83.17%, f1 0.8318, precision 0.8311, recall 0.8325, auc 0.8317
epoch 1501, loss 0.3499, train acc 83.19%, f1 0.8320, precision 0.8311, recall 0.8330, auc 0.8319
epoch 1601, loss 0.3377, train acc 83.13%, f1 0.8315, precision 0.8305, recall 0.8326, auc 0.8313
epoch 1701, loss 0.4147, train acc 83.14%, f1 0.8315, precision 0.8310, recall 0.8320, auc 0.8314
epoch 1801, loss 0.3357, train acc 83.14%, f1 0.8316, precision 0.8306, recall 0.8326, auc 0.8314
epoch 1901, loss 0.4186, train acc 83.17%, f1 0.8318, precision 0.8314, recall 0.8321, auc 0.8317
epoch 2001, loss 0.3839, train acc 83.22%, f1 0.8322, precision 0.8319, recall 0.8325, auc 0.8322
epoch 2101, loss 0.3574, train acc 83.22%, f1 0.8322, precision 0.8322, recall 0.8322, auc 0.8322
epoch 2201, loss 0.3985, train acc 83.20%, f1 0.8321, precision 0.8318, recall 0.8323, auc 0.8320
epoch 2301, loss 0.5549, train acc 83.30%, f1 0.8330, precision 0.8331, recall 0.8328, auc 0.8330
epoch 2401, loss 0.3613, train acc 83.40%, f1 0.8339, precision 0.8344, recall 0.8334, auc 0.8340
epoch 2501, loss 0.3949, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 2601, loss 0.3757, train acc 83.53%, f1 0.8353, precision 0.8352, recall 0.8354, auc 0.8353
epoch 2701, loss 0.4431, train acc 83.64%, f1 0.8364, precision 0.8361, recall 0.8367, auc 0.8364
epoch 2801, loss 0.4062, train acc 83.77%, f1 0.8377, precision 0.8378, recall 0.8375, auc 0.8377
epoch 2901, loss 0.4766, train acc 83.81%, f1 0.8381, precision 0.8385, recall 0.8377, auc 0.8381
epoch 3001, loss 0.4602, train acc 83.96%, f1 0.8397, precision 0.8391, recall 0.8403, auc 0.8396
epoch 3101, loss 0.4014, train acc 84.09%, f1 0.8409, precision 0.8408, recall 0.8410, auc 0.8409
epoch 3201, loss 0.3088, train acc 84.26%, f1 0.8426, precision 0.8426, recall 0.8427, auc 0.8426
epoch 3301, loss 0.2854, train acc 84.41%, f1 0.8441, precision 0.8440, recall 0.8441, auc 0.8441
epoch 3401, loss 0.3338, train acc 84.47%, f1 0.8447, precision 0.8449, recall 0.8445, auc 0.8447
epoch 3501, loss 0.2556, train acc 84.71%, f1 0.8472, precision 0.8469, recall 0.8475, auc 0.8471
epoch 3601, loss 0.3608, train acc 84.85%, f1 0.8484, precision 0.8488, recall 0.8481, auc 0.8485
epoch 3701, loss 0.3451, train acc 84.98%, f1 0.8499, precision 0.8494, recall 0.8504, auc 0.8498
epoch 3801, loss 0.3572, train acc 85.10%, f1 0.8509, precision 0.8511, recall 0.8508, auc 0.8510
epoch 3901, loss 0.2507, train acc 85.17%, f1 0.8516, precision 0.8521, recall 0.8512, auc 0.8517
epoch 4001, loss 0.4122, train acc 85.27%, f1 0.8525, precision 0.8535, recall 0.8514, auc 0.8527
epoch 4101, loss 0.3491, train acc 85.33%, f1 0.8533, precision 0.8533, recall 0.8533, auc 0.8533
epoch 4201, loss 0.2810, train acc 85.41%, f1 0.8542, precision 0.8539, recall 0.8544, auc 0.8541
epoch 4301, loss 0.3063, train acc 85.50%, f1 0.8549, precision 0.8554, recall 0.8545, auc 0.8550
epoch 4401, loss 0.3694, train acc 85.58%, f1 0.8557, precision 0.8560, recall 0.8555, auc 0.8558
epoch 4501, loss 0.2893, train acc 85.67%, f1 0.8567, precision 0.8571, recall 0.8563, auc 0.8567
epoch 4601, loss 0.4725, train acc 85.77%, f1 0.8575, precision 0.8587, recall 0.8562, auc 0.8577
epoch 4701, loss 0.4565, train acc 85.76%, f1 0.8575, precision 0.8585, recall 0.8564, auc 0.8576
epoch 4801, loss 0.3639, train acc 85.85%, f1 0.8584, precision 0.8592, recall 0.8575, auc 0.8585
epoch 4901, loss 0.2578, train acc 85.89%, f1 0.8588, precision 0.8589, recall 0.8588, auc 0.8589
epoch 5001, loss 0.3688, train acc 86.04%, f1 0.8604, precision 0.8600, recall 0.8608, auc 0.8604
epoch 5101, loss 0.3345, train acc 86.10%, f1 0.8610, precision 0.8610, recall 0.8610, auc 0.8610
epoch 5201, loss 0.3101, train acc 86.13%, f1 0.8613, precision 0.8611, recall 0.8616, auc 0.8613
epoch 5301, loss 0.2904, train acc 86.20%, f1 0.8621, precision 0.8615, recall 0.8627, auc 0.8620
epoch 5401, loss 0.2256, train acc 86.20%, f1 0.8618, precision 0.8629, recall 0.8606, auc 0.8620
epoch 5501, loss 0.2330, train acc 86.32%, f1 0.8631, precision 0.8637, recall 0.8625, auc 0.8632
epoch 5601, loss 0.3282, train acc 86.38%, f1 0.8637, precision 0.8643, recall 0.8632, auc 0.8638
epoch 5701, loss 0.2849, train acc 86.38%, f1 0.8637, precision 0.8644, recall 0.8630, auc 0.8638
epoch 5801, loss 0.3178, train acc 86.40%, f1 0.8638, precision 0.8648, recall 0.8628, auc 0.8640
epoch 5901, loss 0.3702, train acc 86.48%, f1 0.8647, precision 0.8654, recall 0.8640, auc 0.8648
epoch 6001, loss 0.3128, train acc 86.58%, f1 0.8657, precision 0.8664, recall 0.8651, auc 0.8658
epoch 6101, loss 0.3783, train acc 86.62%, f1 0.8661, precision 0.8665, recall 0.8658, auc 0.8662
epoch 6201, loss 0.2026, train acc 86.68%, f1 0.8668, precision 0.8669, recall 0.8667, auc 0.8668
epoch 6301, loss 0.2555, train acc 86.70%, f1 0.8668, precision 0.8682, recall 0.8654, auc 0.8670
epoch 6401, loss 0.3119, train acc 86.68%, f1 0.8668, precision 0.8666, recall 0.8671, auc 0.8668
epoch 6501, loss 0.3160, train acc 86.70%, f1 0.8672, precision 0.8664, recall 0.8679, auc 0.8670
epoch 6601, loss 0.3422, train acc 86.73%, f1 0.8672, precision 0.8680, recall 0.8664, auc 0.8673
epoch 6701, loss 0.2832, train acc 86.86%, f1 0.8685, precision 0.8690, recall 0.8681, auc 0.8686
epoch 6801, loss 0.3128, train acc 86.88%, f1 0.8688, precision 0.8688, recall 0.8689, auc 0.8688
epoch 6901, loss 0.3406, train acc 86.90%, f1 0.8690, precision 0.8693, recall 0.8686, auc 0.8690
epoch 7001, loss 0.2768, train acc 86.93%, f1 0.8694, precision 0.8686, recall 0.8702, auc 0.8693
epoch 7101, loss 0.3315, train acc 86.99%, f1 0.8699, precision 0.8699, recall 0.8698, auc 0.8699
epoch 7201, loss 0.3460, train acc 87.02%, f1 0.8702, precision 0.8699, recall 0.8705, auc 0.8702
epoch 7301, loss 0.3717, train acc 87.06%, f1 0.8708, precision 0.8698, recall 0.8717, auc 0.8706
epoch 7401, loss 0.3359, train acc 87.11%, f1 0.8711, precision 0.8711, recall 0.8712, auc 0.8711
epoch 7501, loss 0.2456, train acc 87.15%, f1 0.8714, precision 0.8719, recall 0.8709, auc 0.8715
epoch 7601, loss 0.3106, train acc 87.18%, f1 0.8718, precision 0.8714, recall 0.8723, auc 0.8718
epoch 7701, loss 0.4199, train acc 87.21%, f1 0.8721, precision 0.8719, recall 0.8723, auc 0.8721
epoch 7801, loss 0.2706, train acc 87.26%, f1 0.8726, precision 0.8725, recall 0.8726, auc 0.8726
epoch 7901, loss 0.3364, train acc 87.31%, f1 0.8731, precision 0.8732, recall 0.8729, auc 0.8731
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_3
./test_pima/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.645

the Fscore is 0.6033519553072626

the precision is 0.432

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_3
----------------------



epoch 1, loss 0.6932, train acc 49.95%, f1 0.0001, precision 0.0577, recall 0.0001, auc 0.4995
epoch 101, loss 0.5085, train acc 78.32%, f1 0.7829, precision 0.7840, recall 0.7817, auc 0.7832
epoch 201, loss 0.3957, train acc 81.33%, f1 0.8135, precision 0.8129, recall 0.8141, auc 0.8133
epoch 301, loss 0.3955, train acc 82.50%, f1 0.8251, precision 0.8248, recall 0.8254, auc 0.8250
epoch 401, loss 0.4284, train acc 82.92%, f1 0.8292, precision 0.8289, recall 0.8296, auc 0.8292
epoch 501, loss 0.3976, train acc 83.04%, f1 0.8304, precision 0.8303, recall 0.8306, auc 0.8304
epoch 601, loss 0.5109, train acc 82.98%, f1 0.8299, precision 0.8296, recall 0.8302, auc 0.8298
epoch 701, loss 0.3365, train acc 83.15%, f1 0.8315, precision 0.8315, recall 0.8316, auc 0.8315
epoch 801, loss 0.2962, train acc 83.10%, f1 0.8310, precision 0.8308, recall 0.8312, auc 0.8310
epoch 901, loss 0.3157, train acc 83.08%, f1 0.8308, precision 0.8308, recall 0.8309, auc 0.8308
epoch 1001, loss 0.3819, train acc 83.09%, f1 0.8309, precision 0.8309, recall 0.8309, auc 0.8309
epoch 1101, loss 0.4936, train acc 83.09%, f1 0.8310, precision 0.8309, recall 0.8310, auc 0.8309
epoch 1201, loss 0.2976, train acc 83.13%, f1 0.8313, precision 0.8312, recall 0.8314, auc 0.8313
epoch 1301, loss 0.4189, train acc 83.07%, f1 0.8306, precision 0.8307, recall 0.8306, auc 0.8307
epoch 1401, loss 0.4642, train acc 83.10%, f1 0.8311, precision 0.8310, recall 0.8311, auc 0.8310
epoch 1501, loss 0.3474, train acc 83.13%, f1 0.8313, precision 0.8313, recall 0.8312, auc 0.8313
epoch 1601, loss 0.4094, train acc 83.13%, f1 0.8313, precision 0.8313, recall 0.8313, auc 0.8313
epoch 1701, loss 0.3339, train acc 83.10%, f1 0.8310, precision 0.8312, recall 0.8308, auc 0.8310
epoch 1801, loss 0.4628, train acc 83.09%, f1 0.8309, precision 0.8309, recall 0.8309, auc 0.8309
epoch 1901, loss 0.3438, train acc 83.17%, f1 0.8317, precision 0.8317, recall 0.8317, auc 0.8317
epoch 2001, loss 0.4718, train acc 83.14%, f1 0.8314, precision 0.8312, recall 0.8317, auc 0.8314
epoch 2101, loss 0.3923, train acc 83.13%, f1 0.8313, precision 0.8314, recall 0.8313, auc 0.8313
epoch 2201, loss 0.5183, train acc 83.18%, f1 0.8319, precision 0.8317, recall 0.8321, auc 0.8318
epoch 2301, loss 0.5055, train acc 83.24%, f1 0.8324, precision 0.8324, recall 0.8325, auc 0.8324
epoch 2401, loss 0.4602, train acc 83.32%, f1 0.8333, precision 0.8330, recall 0.8335, auc 0.8332
epoch 2501, loss 0.3859, train acc 83.36%, f1 0.8337, precision 0.8335, recall 0.8338, auc 0.8336
epoch 2601, loss 0.4824, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8347, auc 0.8347
epoch 2701, loss 0.4188, train acc 83.55%, f1 0.8355, precision 0.8353, recall 0.8357, auc 0.8355
epoch 2801, loss 0.3379, train acc 83.62%, f1 0.8362, precision 0.8360, recall 0.8364, auc 0.8362
epoch 2901, loss 0.3552, train acc 83.78%, f1 0.8379, precision 0.8375, recall 0.8384, auc 0.8378
epoch 3001, loss 0.3344, train acc 83.91%, f1 0.8392, precision 0.8389, recall 0.8395, auc 0.8391
epoch 3101, loss 0.3413, train acc 84.08%, f1 0.8408, precision 0.8407, recall 0.8410, auc 0.8408
epoch 3201, loss 0.4019, train acc 84.15%, f1 0.8415, precision 0.8416, recall 0.8414, auc 0.8415
epoch 3301, loss 0.4134, train acc 84.41%, f1 0.8443, precision 0.8433, recall 0.8453, auc 0.8441
epoch 3401, loss 0.3146, train acc 84.47%, f1 0.8447, precision 0.8448, recall 0.8446, auc 0.8447
epoch 3501, loss 0.2604, train acc 84.64%, f1 0.8465, precision 0.8460, recall 0.8469, auc 0.8464
epoch 3601, loss 0.2878, train acc 84.78%, f1 0.8478, precision 0.8476, recall 0.8481, auc 0.8478
epoch 3701, loss 0.3513, train acc 84.90%, f1 0.8489, precision 0.8494, recall 0.8483, auc 0.8490
epoch 3801, loss 0.3053, train acc 85.02%, f1 0.8503, precision 0.8496, recall 0.8510, auc 0.8502
epoch 3901, loss 0.3238, train acc 85.17%, f1 0.8519, precision 0.8505, recall 0.8533, auc 0.8517
epoch 4001, loss 0.3214, train acc 85.26%, f1 0.8526, precision 0.8525, recall 0.8527, auc 0.8526
epoch 4101, loss 0.3161, train acc 85.31%, f1 0.8531, precision 0.8531, recall 0.8532, auc 0.8531
epoch 4201, loss 0.3429, train acc 85.47%, f1 0.8547, precision 0.8546, recall 0.8548, auc 0.8547
epoch 4301, loss 0.4024, train acc 85.55%, f1 0.8556, precision 0.8549, recall 0.8563, auc 0.8555
epoch 4401, loss 0.3504, train acc 85.64%, f1 0.8563, precision 0.8566, recall 0.8561, auc 0.8564
epoch 4501, loss 0.2968, train acc 85.67%, f1 0.8566, precision 0.8567, recall 0.8566, auc 0.8567
epoch 4601, loss 0.3885, train acc 85.76%, f1 0.8576, precision 0.8578, recall 0.8573, auc 0.8576
epoch 4701, loss 0.3153, train acc 85.78%, f1 0.8576, precision 0.8586, recall 0.8566, auc 0.8578
epoch 4801, loss 0.3135, train acc 85.89%, f1 0.8589, precision 0.8590, recall 0.8587, auc 0.8589
epoch 4901, loss 0.2799, train acc 86.02%, f1 0.8601, precision 0.8606, recall 0.8596, auc 0.8602
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_3
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.635

the Fscore is 0.5966850828729281

the precision is 0.4251968503937008

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_3
----------------------



epoch 1, loss 0.6932, train acc 50.01%, f1 0.6666, precision 0.5000, recall 0.9995, auc 0.5001
epoch 101, loss 0.4923, train acc 77.68%, f1 0.7688, precision 0.7973, recall 0.7424, auc 0.7768
epoch 201, loss 0.3709, train acc 81.09%, f1 0.8106, precision 0.8117, recall 0.8096, auc 0.8109
epoch 301, loss 0.4183, train acc 82.47%, f1 0.8256, precision 0.8215, recall 0.8297, auc 0.8247
epoch 401, loss 0.3391, train acc 82.94%, f1 0.8301, precision 0.8265, recall 0.8337, auc 0.8294
epoch 501, loss 0.2915, train acc 83.05%, f1 0.8314, precision 0.8271, recall 0.8357, auc 0.8305
epoch 601, loss 0.2522, train acc 83.05%, f1 0.8314, precision 0.8268, recall 0.8361, auc 0.8305
epoch 701, loss 0.3334, train acc 83.09%, f1 0.8316, precision 0.8280, recall 0.8353, auc 0.8309
epoch 801, loss 0.2686, train acc 83.14%, f1 0.8322, precision 0.8282, recall 0.8363, auc 0.8314
epoch 901, loss 0.4628, train acc 83.14%, f1 0.8322, precision 0.8285, recall 0.8359, auc 0.8314
epoch 1001, loss 0.2975, train acc 83.12%, f1 0.8319, precision 0.8283, recall 0.8355, auc 0.8312
epoch 1101, loss 0.3602, train acc 83.11%, f1 0.8319, precision 0.8283, recall 0.8355, auc 0.8311
epoch 1201, loss 0.3447, train acc 83.12%, f1 0.8318, precision 0.8290, recall 0.8346, auc 0.8313
epoch 1301, loss 0.3522, train acc 83.09%, f1 0.8316, precision 0.8281, recall 0.8352, auc 0.8309
epoch 1401, loss 0.3576, train acc 83.10%, f1 0.8313, precision 0.8294, recall 0.8332, auc 0.8310
epoch 1501, loss 0.4674, train acc 83.14%, f1 0.8319, precision 0.8293, recall 0.8346, auc 0.8314
epoch 1601, loss 0.3219, train acc 83.13%, f1 0.8316, precision 0.8300, recall 0.8333, auc 0.8313
epoch 1701, loss 0.5669, train acc 83.14%, f1 0.8318, precision 0.8299, recall 0.8337, auc 0.8314
epoch 1801, loss 0.2972, train acc 83.17%, f1 0.8322, precision 0.8296, recall 0.8348, auc 0.8317
epoch 1901, loss 0.3377, train acc 83.15%, f1 0.8317, precision 0.8309, recall 0.8324, auc 0.8315
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_3
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.585

the Fscore is 0.5654450261780105

the precision is 0.39416058394160586

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_3
----------------------



epoch 1, loss 0.6933, train acc 49.50%, f1 0.6622, precision 0.4950, recall 1.0000, auc 0.5000
epoch 101, loss 0.5707, train acc 77.21%, f1 0.7747, precision 0.7587, recall 0.7914, auc 0.7723
epoch 201, loss 0.4624, train acc 80.83%, f1 0.8074, precision 0.8030, recall 0.8118, auc 0.8083
epoch 301, loss 0.3957, train acc 82.33%, f1 0.8211, precision 0.8227, recall 0.8195, auc 0.8232
epoch 401, loss 0.3655, train acc 82.89%, f1 0.8273, precision 0.8266, recall 0.8280, auc 0.8289
epoch 501, loss 0.3024, train acc 83.02%, f1 0.8291, precision 0.8259, recall 0.8324, auc 0.8302
epoch 601, loss 0.4955, train acc 83.04%, f1 0.8290, precision 0.8275, recall 0.8305, auc 0.8304
epoch 701, loss 0.4286, train acc 83.06%, f1 0.8295, precision 0.8267, recall 0.8323, auc 0.8306
epoch 801, loss 0.4911, train acc 83.11%, f1 0.8297, precision 0.8282, recall 0.8311, auc 0.8311
epoch 901, loss 0.4682, train acc 83.16%, f1 0.8297, precision 0.8305, recall 0.8289, auc 0.8316
epoch 1001, loss 0.4275, train acc 83.14%, f1 0.8297, precision 0.8297, recall 0.8296, auc 0.8314
epoch 1101, loss 0.3323, train acc 83.15%, f1 0.8299, precision 0.8294, recall 0.8304, auc 0.8315
epoch 1201, loss 0.3401, train acc 83.15%, f1 0.8302, precision 0.8283, recall 0.8321, auc 0.8315
epoch 1301, loss 0.3012, train acc 83.15%, f1 0.8300, precision 0.8292, recall 0.8308, auc 0.8315
epoch 1401, loss 0.3578, train acc 83.14%, f1 0.8304, precision 0.8270, recall 0.8338, auc 0.8314
epoch 1501, loss 0.4028, train acc 83.17%, f1 0.8298, precision 0.8308, recall 0.8288, auc 0.8317
epoch 1601, loss 0.3555, train acc 83.15%, f1 0.8301, precision 0.8286, recall 0.8316, auc 0.8315
epoch 1701, loss 0.4491, train acc 83.15%, f1 0.8302, precision 0.8285, recall 0.8319, auc 0.8315
epoch 1801, loss 0.3139, train acc 83.11%, f1 0.8299, precision 0.8273, recall 0.8325, auc 0.8311
epoch 1901, loss 0.4080, train acc 83.14%, f1 0.8302, precision 0.8280, recall 0.8324, auc 0.8314
epoch 2001, loss 0.2900, train acc 83.05%, f1 0.8293, precision 0.8269, recall 0.8318, auc 0.8305
epoch 2101, loss 0.3758, train acc 83.14%, f1 0.8310, precision 0.8248, recall 0.8372, auc 0.8314
epoch 2201, loss 0.4343, train acc 83.07%, f1 0.8290, precision 0.8289, recall 0.8291, auc 0.8306
epoch 2301, loss 0.3121, train acc 83.09%, f1 0.8300, precision 0.8259, recall 0.8342, auc 0.8309
epoch 2401, loss 0.3605, train acc 83.12%, f1 0.8304, precision 0.8259, recall 0.8350, auc 0.8312
epoch 2501, loss 0.3647, train acc 83.21%, f1 0.8303, precision 0.8305, recall 0.8302, auc 0.8321
epoch 2601, loss 0.3755, train acc 83.12%, f1 0.8301, precision 0.8272, recall 0.8329, auc 0.8312
epoch 2701, loss 0.3577, train acc 83.14%, f1 0.8301, precision 0.8281, recall 0.8321, auc 0.8314
epoch 2801, loss 0.3570, train acc 83.14%, f1 0.8303, precision 0.8274, recall 0.8332, auc 0.8314
epoch 2901, loss 0.4881, train acc 83.22%, f1 0.8304, precision 0.8305, recall 0.8303, auc 0.8321
epoch 3001, loss 0.3814, train acc 83.25%, f1 0.8314, precision 0.8284, recall 0.8344, auc 0.8325
epoch 3101, loss 0.3564, train acc 83.38%, f1 0.8332, precision 0.8281, recall 0.8383, auc 0.8339
epoch 3201, loss 0.3988, train acc 83.40%, f1 0.8331, precision 0.8294, recall 0.8368, auc 0.8340
epoch 3301, loss 0.5280, train acc 83.43%, f1 0.8332, precision 0.8306, recall 0.8357, auc 0.8343
epoch 3401, loss 0.3950, train acc 83.49%, f1 0.8328, precision 0.8349, recall 0.8308, auc 0.8349
epoch 3501, loss 0.3808, train acc 83.54%, f1 0.8340, precision 0.8325, recall 0.8355, auc 0.8354
epoch 3601, loss 0.2535, train acc 83.67%, f1 0.8362, precision 0.8305, recall 0.8421, auc 0.8368
epoch 3701, loss 0.2931, train acc 83.76%, f1 0.8371, precision 0.8314, recall 0.8427, auc 0.8376
epoch 3801, loss 0.4077, train acc 83.89%, f1 0.8373, precision 0.8371, recall 0.8374, auc 0.8388
epoch 3901, loss 0.3277, train acc 84.09%, f1 0.8394, precision 0.8389, recall 0.8398, auc 0.8409
epoch 4001, loss 0.3429, train acc 84.10%, f1 0.8402, precision 0.8361, recall 0.8443, auc 0.8411
epoch 4101, loss 0.3843, train acc 84.42%, f1 0.8424, precision 0.8437, recall 0.8411, auc 0.8442
epoch 4201, loss 0.2906, train acc 84.53%, f1 0.8442, precision 0.8419, recall 0.8465, auc 0.8453
epoch 4301, loss 0.4863, train acc 84.67%, f1 0.8457, precision 0.8427, recall 0.8486, auc 0.8467
epoch 4401, loss 0.3864, train acc 84.77%, f1 0.8463, precision 0.8453, recall 0.8473, auc 0.8477
epoch 4501, loss 0.2842, train acc 84.83%, f1 0.8472, precision 0.8452, recall 0.8492, auc 0.8483
epoch 4601, loss 0.3584, train acc 84.97%, f1 0.8484, precision 0.8470, recall 0.8498, auc 0.8497
epoch 4701, loss 0.3788, train acc 85.05%, f1 0.8495, precision 0.8468, recall 0.8522, auc 0.8505
epoch 4801, loss 0.2633, train acc 85.13%, f1 0.8499, precision 0.8491, recall 0.8507, auc 0.8513
epoch 4901, loss 0.4149, train acc 85.29%, f1 0.8519, precision 0.8493, recall 0.8546, auc 0.8530
epoch 5001, loss 0.2930, train acc 85.41%, f1 0.8529, precision 0.8509, recall 0.8550, auc 0.8541
epoch 5101, loss 0.3137, train acc 85.43%, f1 0.8532, precision 0.8513, recall 0.8550, auc 0.8543
epoch 5201, loss 0.3790, train acc 85.67%, f1 0.8555, precision 0.8539, recall 0.8571, auc 0.8567
epoch 5301, loss 0.3575, train acc 85.70%, f1 0.8556, precision 0.8550, recall 0.8562, auc 0.8570
epoch 5401, loss 0.2828, train acc 85.70%, f1 0.8548, precision 0.8596, recall 0.8500, auc 0.8570
epoch 5501, loss 0.3038, train acc 85.86%, f1 0.8578, precision 0.8544, recall 0.8611, auc 0.8587
epoch 5601, loss 0.3227, train acc 85.89%, f1 0.8576, precision 0.8568, recall 0.8584, auc 0.8589
epoch 5701, loss 0.4059, train acc 86.02%, f1 0.8583, precision 0.8612, recall 0.8553, auc 0.8601
epoch 5801, loss 0.3296, train acc 85.97%, f1 0.8586, precision 0.8564, recall 0.8609, auc 0.8597
epoch 5901, loss 0.3118, train acc 86.09%, f1 0.8598, precision 0.8574, recall 0.8623, auc 0.8609
epoch 6001, loss 0.3310, train acc 86.15%, f1 0.8599, precision 0.8608, recall 0.8591, auc 0.8615
epoch 6101, loss 0.3434, train acc 86.20%, f1 0.8611, precision 0.8579, recall 0.8643, auc 0.8620
epoch 6201, loss 0.3318, train acc 86.23%, f1 0.8607, precision 0.8621, recall 0.8593, auc 0.8623
epoch 6301, loss 0.3606, train acc 86.31%, f1 0.8625, precision 0.8579, recall 0.8671, auc 0.8631
epoch 6401, loss 0.3204, train acc 86.36%, f1 0.8624, precision 0.8613, recall 0.8634, auc 0.8636
epoch 6501, loss 0.2509, train acc 86.42%, f1 0.8631, precision 0.8612, recall 0.8650, auc 0.8642
epoch 6601, loss 0.3142, train acc 86.45%, f1 0.8637, precision 0.8601, recall 0.8672, auc 0.8645
epoch 6701, loss 0.2412, train acc 86.49%, f1 0.8634, precision 0.8642, recall 0.8625, auc 0.8649
epoch 6801, loss 0.3336, train acc 86.54%, f1 0.8642, precision 0.8633, recall 0.8650, auc 0.8654
epoch 6901, loss 0.2741, train acc 86.61%, f1 0.8653, precision 0.8616, recall 0.8691, auc 0.8661
epoch 7001, loss 0.2968, train acc 86.64%, f1 0.8655, precision 0.8628, recall 0.8682, auc 0.8664
epoch 7101, loss 0.3460, train acc 86.64%, f1 0.8648, precision 0.8660, recall 0.8637, auc 0.8663
epoch 7201, loss 0.4009, train acc 86.71%, f1 0.8659, precision 0.8647, recall 0.8672, auc 0.8671
epoch 7301, loss 0.2636, train acc 86.80%, f1 0.8672, precision 0.8637, recall 0.8708, auc 0.8681
epoch 7401, loss 0.3337, train acc 86.75%, f1 0.8665, precision 0.8642, recall 0.8689, auc 0.8675
epoch 7501, loss 0.1713, train acc 86.84%, f1 0.8672, precision 0.8666, recall 0.8678, auc 0.8684
epoch 7601, loss 0.3207, train acc 86.86%, f1 0.8677, precision 0.8647, recall 0.8706, auc 0.8686
epoch 7701, loss 0.2393, train acc 86.88%, f1 0.8675, precision 0.8677, recall 0.8672, auc 0.8688
epoch 7801, loss 0.3225, train acc 86.96%, f1 0.8683, precision 0.8680, recall 0.8686, auc 0.8696
epoch 7901, loss 0.2919, train acc 87.00%, f1 0.8692, precision 0.8660, recall 0.8724, auc 0.8701
epoch 8001, loss 0.3684, train acc 86.97%, f1 0.8685, precision 0.8675, recall 0.8696, auc 0.8697
epoch 8101, loss 0.3574, train acc 87.07%, f1 0.8696, precision 0.8680, recall 0.8712, auc 0.8707
epoch 8201, loss 0.3147, train acc 87.06%, f1 0.8690, precision 0.8711, recall 0.8668, auc 0.8706
epoch 8301, loss 0.3164, train acc 87.13%, f1 0.8704, precision 0.8677, recall 0.8732, auc 0.8714
epoch 8401, loss 0.3363, train acc 87.19%, f1 0.8713, precision 0.8663, recall 0.8764, auc 0.8719
epoch 8501, loss 0.3348, train acc 87.24%, f1 0.8714, precision 0.8697, recall 0.8730, auc 0.8724
epoch 8601, loss 0.2821, train acc 87.25%, f1 0.8714, precision 0.8705, recall 0.8722, auc 0.8725
epoch 8701, loss 0.1955, train acc 87.33%, f1 0.8722, precision 0.8711, recall 0.8734, auc 0.8733
epoch 8801, loss 0.2688, train acc 87.39%, f1 0.8732, precision 0.8687, recall 0.8778, auc 0.8739
epoch 8901, loss 0.2551, train acc 87.40%, f1 0.8731, precision 0.8709, recall 0.8752, auc 0.8740
epoch 9001, loss 0.2675, train acc 87.42%, f1 0.8730, precision 0.8729, recall 0.8730, auc 0.8742
epoch 9101, loss 0.2964, train acc 87.48%, f1 0.8743, precision 0.8694, recall 0.8792, auc 0.8749
epoch 9201, loss 0.2566, train acc 87.52%, f1 0.8739, precision 0.8739, recall 0.8738, auc 0.8751
epoch 9301, loss 0.3263, train acc 87.61%, f1 0.8754, precision 0.8714, recall 0.8795, auc 0.8762
epoch 9401, loss 0.3416, train acc 87.62%, f1 0.8752, precision 0.8736, recall 0.8768, auc 0.8762
epoch 9501, loss 0.2186, train acc 87.63%, f1 0.8752, precision 0.8741, recall 0.8763, auc 0.8763
epoch 9601, loss 0.2087, train acc 87.72%, f1 0.8764, precision 0.8733, recall 0.8796, auc 0.8773
epoch 9701, loss 0.2877, train acc 87.84%, f1 0.8775, precision 0.8754, recall 0.8796, auc 0.8784
epoch 9801, loss 0.3284, train acc 87.87%, f1 0.8778, precision 0.8756, recall 0.8799, auc 0.8787
epoch 9901, loss 0.2484, train acc 87.89%, f1 0.8780, precision 0.8758, recall 0.8802, auc 0.8789
epoch 10001, loss 0.2448, train acc 87.94%, f1 0.8787, precision 0.8746, recall 0.8829, auc 0.8794
epoch 10101, loss 0.2225, train acc 88.03%, f1 0.8793, precision 0.8781, recall 0.8805, auc 0.8803
epoch 10201, loss 0.3149, train acc 88.05%, f1 0.8792, precision 0.8800, recall 0.8785, auc 0.8805
epoch 10301, loss 0.2848, train acc 88.12%, f1 0.8802, precision 0.8786, recall 0.8817, auc 0.8812
epoch 10401, loss 0.3175, train acc 88.18%, f1 0.8806, precision 0.8805, recall 0.8807, auc 0.8818
epoch 10501, loss 0.3429, train acc 88.21%, f1 0.8816, precision 0.8767, recall 0.8866, auc 0.8822
epoch 10601, loss 0.2654, train acc 88.27%, f1 0.8815, precision 0.8822, recall 0.8808, auc 0.8827
epoch 10701, loss 0.2202, train acc 88.30%, f1 0.8820, precision 0.8803, recall 0.8837, auc 0.8830
epoch 10801, loss 0.2700, train acc 88.36%, f1 0.8822, precision 0.8842, recall 0.8802, auc 0.8836
epoch 10901, loss 0.2555, train acc 88.42%, f1 0.8829, precision 0.8839, recall 0.8818, auc 0.8841
epoch 11001, loss 0.3032, train acc 88.51%, f1 0.8842, precision 0.8821, recall 0.8864, auc 0.8851
epoch 11101, loss 0.2600, train acc 88.60%, f1 0.8850, precision 0.8837, recall 0.8863, auc 0.8860
epoch 11201, loss 0.1833, train acc 88.61%, f1 0.8853, precision 0.8831, recall 0.8875, auc 0.8861
epoch 11301, loss 0.2930, train acc 88.71%, f1 0.8857, precision 0.8875, recall 0.8839, auc 0.8870
epoch 11401, loss 0.2844, train acc 88.77%, f1 0.8865, precision 0.8868, recall 0.8861, auc 0.8876
epoch 11501, loss 0.3021, train acc 88.80%, f1 0.8869, precision 0.8863, recall 0.8876, auc 0.8880
epoch 11601, loss 0.2113, train acc 88.89%, f1 0.8878, precision 0.8879, recall 0.8876, auc 0.8889
epoch 11701, loss 0.2130, train acc 88.96%, f1 0.8884, precision 0.8890, recall 0.8878, auc 0.8896
epoch 11801, loss 0.2249, train acc 88.97%, f1 0.8884, precision 0.8902, recall 0.8865, auc 0.8897
epoch 11901, loss 0.3454, train acc 89.05%, f1 0.8895, precision 0.8885, recall 0.8905, auc 0.8905
epoch 12001, loss 0.2346, train acc 89.07%, f1 0.8895, precision 0.8904, recall 0.8886, auc 0.8907
epoch 12101, loss 0.3646, train acc 89.20%, f1 0.8909, precision 0.8912, recall 0.8907, auc 0.8920
epoch 12201, loss 0.3635, train acc 89.20%, f1 0.8910, precision 0.8907, recall 0.8913, auc 0.8920
epoch 12301, loss 0.2399, train acc 89.25%, f1 0.8912, precision 0.8934, recall 0.8889, auc 0.8925
epoch 12401, loss 0.3138, train acc 89.30%, f1 0.8922, precision 0.8904, recall 0.8940, auc 0.8930
epoch 12501, loss 0.1832, train acc 89.32%, f1 0.8916, precision 0.8953, recall 0.8880, auc 0.8931
epoch 12601, loss 0.2889, train acc 89.44%, f1 0.8932, precision 0.8945, recall 0.8918, auc 0.8944
epoch 12701, loss 0.2290, train acc 89.42%, f1 0.8932, precision 0.8929, recall 0.8935, auc 0.8942
epoch 12801, loss 0.1418, train acc 89.55%, f1 0.8943, precision 0.8956, recall 0.8930, auc 0.8955
epoch 12901, loss 0.1074, train acc 89.56%, f1 0.8942, precision 0.8972, recall 0.8913, auc 0.8956
epoch 13001, loss 0.2455, train acc 89.58%, f1 0.8950, precision 0.8923, recall 0.8978, auc 0.8958
epoch 13101, loss 0.1708, train acc 89.71%, f1 0.8960, precision 0.8966, recall 0.8954, auc 0.8971
epoch 13201, loss 0.2700, train acc 89.80%, f1 0.8966, precision 0.8994, recall 0.8938, auc 0.8979
epoch 13301, loss 0.2478, train acc 89.76%, f1 0.8970, precision 0.8934, recall 0.9006, auc 0.8976
epoch 13401, loss 0.1973, train acc 89.83%, f1 0.8970, precision 0.8994, recall 0.8946, auc 0.8983
epoch 13501, loss 0.2538, train acc 89.88%, f1 0.8980, precision 0.8962, recall 0.8997, auc 0.8988
epoch 13601, loss 0.2132, train acc 89.98%, f1 0.8984, precision 0.9017, recall 0.8950, auc 0.8997
epoch 13701, loss 0.1963, train acc 90.00%, f1 0.8991, precision 0.8981, recall 0.9001, auc 0.9000
epoch 13801, loss 0.1656, train acc 90.07%, f1 0.8998, precision 0.8983, recall 0.9014, auc 0.9007
epoch 13901, loss 0.2521, train acc 90.09%, f1 0.8998, precision 0.9010, recall 0.8986, auc 0.9009
epoch 14001, loss 0.1700, train acc 90.20%, f1 0.9009, precision 0.9020, recall 0.8999, auc 0.9020
epoch 14101, loss 0.1870, train acc 90.25%, f1 0.9011, precision 0.9051, recall 0.8971, auc 0.9025
epoch 14201, loss 0.2270, train acc 90.33%, f1 0.9023, precision 0.9028, recall 0.9018, auc 0.9033
epoch 14301, loss 0.2995, train acc 90.43%, f1 0.9037, precision 0.9001, recall 0.9073, auc 0.9043
epoch 14401, loss 0.2707, train acc 90.44%, f1 0.9036, precision 0.9018, recall 0.9054, auc 0.9044
epoch 14501, loss 0.2180, train acc 90.53%, f1 0.9044, precision 0.9040, recall 0.9047, auc 0.9053
epoch 14601, loss 0.2455, train acc 90.61%, f1 0.9052, precision 0.9047, recall 0.9057, auc 0.9061
epoch 14701, loss 0.3136, train acc 90.64%, f1 0.9053, precision 0.9064, recall 0.9043, auc 0.9064
epoch 14801, loss 0.1950, train acc 90.64%, f1 0.9055, precision 0.9054, recall 0.9056, auc 0.9064
epoch 14901, loss 0.2190, train acc 90.76%, f1 0.9070, precision 0.9039, recall 0.9101, auc 0.9076
epoch 15001, loss 0.2294, train acc 90.78%, f1 0.9068, precision 0.9077, recall 0.9059, auc 0.9078
epoch 15101, loss 0.1767, train acc 90.80%, f1 0.9071, precision 0.9065, recall 0.9077, auc 0.9080
epoch 15201, loss 0.1758, train acc 90.92%, f1 0.9080, precision 0.9104, recall 0.9057, auc 0.9092
epoch 15301, loss 0.2311, train acc 90.95%, f1 0.9086, precision 0.9077, recall 0.9096, auc 0.9095
epoch 15401, loss 0.1245, train acc 90.97%, f1 0.9091, precision 0.9057, recall 0.9125, auc 0.9097
epoch 15501, loss 0.2859, train acc 91.09%, f1 0.9104, precision 0.9062, recall 0.9146, auc 0.9109
epoch 15601, loss 0.1452, train acc 91.14%, f1 0.9105, precision 0.9109, recall 0.9101, auc 0.9114
epoch 15701, loss 0.2714, train acc 91.22%, f1 0.9112, precision 0.9123, recall 0.9101, auc 0.9122
epoch 15801, loss 0.2079, train acc 91.30%, f1 0.9120, precision 0.9131, recall 0.9110, auc 0.9130
epoch 15901, loss 0.1860, train acc 91.33%, f1 0.9124, precision 0.9123, recall 0.9125, auc 0.9133
epoch 16001, loss 0.1131, train acc 91.37%, f1 0.9129, precision 0.9125, recall 0.9132, auc 0.9137
epoch 16101, loss 0.3290, train acc 91.42%, f1 0.9136, precision 0.9108, recall 0.9164, auc 0.9142
epoch 16201, loss 0.1616, train acc 91.56%, f1 0.9145, precision 0.9174, recall 0.9117, auc 0.9156
epoch 16301, loss 0.2010, train acc 91.46%, f1 0.9138, precision 0.9132, recall 0.9143, auc 0.9146
epoch 16401, loss 0.1966, train acc 91.64%, f1 0.9154, precision 0.9166, recall 0.9142, auc 0.9164
epoch 16501, loss 0.2171, train acc 91.55%, f1 0.9149, precision 0.9120, recall 0.9177, auc 0.9155/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.2004, train acc 91.62%, f1 0.9155, precision 0.9140, recall 0.9170, auc 0.9163
epoch 16701, loss 0.1700, train acc 91.71%, f1 0.9162, precision 0.9169, recall 0.9154, auc 0.9171
epoch 16801, loss 0.2330, train acc 91.76%, f1 0.9167, precision 0.9168, recall 0.9167, auc 0.9176
epoch 16901, loss 0.2663, train acc 91.82%, f1 0.9174, precision 0.9170, recall 0.9178, auc 0.9182
epoch 17001, loss 0.1758, train acc 91.88%, f1 0.9181, precision 0.9171, recall 0.9190, auc 0.9188
epoch 17101, loss 0.2607, train acc 91.85%, f1 0.9178, precision 0.9163, recall 0.9192, auc 0.9185
epoch 17201, loss 0.2347, train acc 91.95%, f1 0.9185, precision 0.9206, recall 0.9163, auc 0.9194
epoch 17301, loss 0.1538, train acc 91.94%, f1 0.9185, precision 0.9193, recall 0.9177, auc 0.9194
epoch 17401, loss 0.1660, train acc 92.01%, f1 0.9193, precision 0.9195, recall 0.9190, auc 0.9201
epoch 17501, loss 0.1821, train acc 92.12%, f1 0.9201, precision 0.9234, recall 0.9168, auc 0.9212
epoch 17601, loss 0.1908, train acc 92.11%, f1 0.9205, precision 0.9185, recall 0.9225, auc 0.9211
epoch 17701, loss 0.2818, train acc 92.13%, f1 0.9207, precision 0.9189, recall 0.9225, auc 0.9213
epoch 17801, loss 0.1990, train acc 92.18%, f1 0.9210, precision 0.9207, recall 0.9214, auc 0.9218
epoch 17901, loss 0.2741, train acc 92.20%, f1 0.9211, precision 0.9224, recall 0.9198, auc 0.9220
epoch 18001, loss 0.1922, train acc 92.29%, f1 0.9224, precision 0.9193, recall 0.9255, auc 0.9229
epoch 18101, loss 0.1366, train acc 92.32%, f1 0.9224, precision 0.9230, recall 0.9217, auc 0.9232
epoch 18201, loss 0.1867, train acc 92.32%, f1 0.9224, precision 0.9227, recall 0.9221, auc 0.9232
epoch 18301, loss 0.1981, train acc 92.38%, f1 0.9230, precision 0.9233, recall 0.9226, auc 0.9238
epoch 18401, loss 0.1717, train acc 92.41%, f1 0.9233, precision 0.9229, recall 0.9237, auc 0.9241
epoch 18501, loss 0.2262, train acc 92.41%, f1 0.9233, precision 0.9239, recall 0.9227, auc 0.9241
epoch 18601, loss 0.1262, train acc 92.48%, f1 0.9241, precision 0.9233, recall 0.9249, auc 0.9248
epoch 18701, loss 0.2153, train acc 92.55%, f1 0.9248, precision 0.9242, recall 0.9254, auc 0.9255
epoch 18801, loss 0.2229, train acc 92.58%, f1 0.9250, precision 0.9249, recall 0.9252, auc 0.9258
epoch 18901, loss 0.1772, train acc 92.64%, f1 0.9257, precision 0.9252, recall 0.9261, auc 0.9264
epoch 19001, loss 0.1108, train acc 92.61%, f1 0.9253, precision 0.9265, recall 0.9241, auc 0.9261
epoch 19101, loss 0.2501, train acc 92.64%, f1 0.9258, precision 0.9245, recall 0.9271, auc 0.9265
epoch 19201, loss 0.2132, train acc 92.66%, f1 0.9258, precision 0.9261, recall 0.9255, auc 0.9266
epoch 19301, loss 0.1270, train acc 92.69%, f1 0.9262, precision 0.9251, recall 0.9274, auc 0.9269
epoch 19401, loss 0.1711, train acc 92.75%, f1 0.9267, precision 0.9278, recall 0.9256, auc 0.9275
epoch 19501, loss 0.1292, train acc 92.74%, f1 0.9264, precision 0.9291, recall 0.9238, auc 0.9273
epoch 19601, loss 0.1889, train acc 92.77%, f1 0.9270, precision 0.9259, recall 0.9282, auc 0.9277
epoch 19701, loss 0.2728, train acc 92.89%, f1 0.9279, precision 0.9314, recall 0.9244, auc 0.9289
epoch 19801, loss 0.1470, train acc 92.82%, f1 0.9272, precision 0.9299, recall 0.9245, auc 0.9281
epoch 19901, loss 0.1834, train acc 92.89%, f1 0.9280, precision 0.9299, recall 0.9262, auc 0.9289
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_3
./test_pima/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6922222222222222

the Fscore is 0.6335403726708074

the precision is 0.4766355140186916

the recall is 0.9444444444444444

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_3
----------------------



epoch 1, loss 0.6930, train acc 49.99%, f1 0.6666, precision 0.4999, recall 1.0000, auc 0.5000
epoch 101, loss 0.5575, train acc 77.57%, f1 0.7756, precision 0.7760, recall 0.7751, auc 0.7757
epoch 201, loss 0.3914, train acc 80.94%, f1 0.8095, precision 0.8090, recall 0.8100, auc 0.8094
epoch 301, loss 0.3906, train acc 82.37%, f1 0.8238, precision 0.8233, recall 0.8242, auc 0.8237
epoch 401, loss 0.3796, train acc 82.86%, f1 0.8283, precision 0.8293, recall 0.8273, auc 0.8286
epoch 501, loss 0.4031, train acc 83.08%, f1 0.8305, precision 0.8320, recall 0.8289, auc 0.8308
epoch 601, loss 0.3537, train acc 83.07%, f1 0.8308, precision 0.8300, recall 0.8316, auc 0.8307
epoch 701, loss 0.3520, train acc 83.07%, f1 0.8309, precision 0.8296, recall 0.8322, auc 0.8307
epoch 801, loss 0.3769, train acc 83.12%, f1 0.8317, precision 0.8292, recall 0.8341, auc 0.8312
epoch 901, loss 0.3274, train acc 83.16%, f1 0.8314, precision 0.8322, recall 0.8306, auc 0.8316
epoch 1001, loss 0.3850, train acc 83.09%, f1 0.8309, precision 0.8309, recall 0.8309, auc 0.8309
epoch 1101, loss 0.3927, train acc 83.14%, f1 0.8312, precision 0.8322, recall 0.8302, auc 0.8314
epoch 1201, loss 0.3856, train acc 83.18%, f1 0.8322, precision 0.8303, recall 0.8341, auc 0.8318
epoch 1301, loss 0.4002, train acc 83.18%, f1 0.8318, precision 0.8318, recall 0.8317, auc 0.8318
epoch 1401, loss 0.4044, train acc 83.12%, f1 0.8308, precision 0.8323, recall 0.8294, auc 0.8312
epoch 1501, loss 0.3629, train acc 83.09%, f1 0.8309, precision 0.8307, recall 0.8310, auc 0.8309
epoch 1601, loss 0.3370, train acc 83.10%, f1 0.8313, precision 0.8296, recall 0.8331, auc 0.8310
epoch 1701, loss 0.3052, train acc 83.06%, f1 0.8313, precision 0.8280, recall 0.8345, auc 0.8306
epoch 1801, loss 0.3388, train acc 83.09%, f1 0.8309, precision 0.8310, recall 0.8307, auc 0.8309
epoch 1901, loss 0.3569, train acc 83.11%, f1 0.8311, precision 0.8312, recall 0.8309, auc 0.8311
epoch 2001, loss 0.4494, train acc 83.13%, f1 0.8314, precision 0.8307, recall 0.8322, auc 0.8313
epoch 2101, loss 0.3613, train acc 83.14%, f1 0.8323, precision 0.8278, recall 0.8369, auc 0.8314
epoch 2201, loss 0.3048, train acc 83.11%, f1 0.8309, precision 0.8319, recall 0.8299, auc 0.8311
epoch 2301, loss 0.3817, train acc 83.12%, f1 0.8316, precision 0.8293, recall 0.8340, auc 0.8312
epoch 2401, loss 0.4219, train acc 83.12%, f1 0.8311, precision 0.8316, recall 0.8306, auc 0.8312
epoch 2501, loss 0.4442, train acc 83.14%, f1 0.8317, precision 0.8303, recall 0.8330, auc 0.8314
epoch 2601, loss 0.5413, train acc 83.14%, f1 0.8315, precision 0.8306, recall 0.8325, auc 0.8314
epoch 2701, loss 0.4545, train acc 83.14%, f1 0.8309, precision 0.8330, recall 0.8288, auc 0.8314
epoch 2801, loss 0.4801, train acc 83.16%, f1 0.8318, precision 0.8304, recall 0.8332, auc 0.8316
epoch 2901, loss 0.3654, train acc 83.17%, f1 0.8318, precision 0.8312, recall 0.8324, auc 0.8317
epoch 3001, loss 0.2889, train acc 83.29%, f1 0.8332, precision 0.8317, recall 0.8347, auc 0.8329
epoch 3101, loss 0.3609, train acc 83.32%, f1 0.8334, precision 0.8321, recall 0.8347, auc 0.8332
epoch 3201, loss 0.3709, train acc 83.38%, f1 0.8338, precision 0.8336, recall 0.8340, auc 0.8338
epoch 3301, loss 0.3242, train acc 83.38%, f1 0.8342, precision 0.8321, recall 0.8363, auc 0.8338
epoch 3401, loss 0.3850, train acc 83.41%, f1 0.8346, precision 0.8320, recall 0.8372, auc 0.8341
epoch 3501, loss 0.3046, train acc 83.59%, f1 0.8368, precision 0.8320, recall 0.8417, auc 0.8359
epoch 3601, loss 0.4279, train acc 83.66%, f1 0.8374, precision 0.8330, recall 0.8419, auc 0.8366
epoch 3701, loss 0.3913, train acc 83.81%, f1 0.8380, precision 0.8387, recall 0.8373, auc 0.8381
epoch 3801, loss 0.2122, train acc 83.86%, f1 0.8391, precision 0.8360, recall 0.8423, auc 0.8386
epoch 3901, loss 0.2948, train acc 84.00%, f1 0.8404, precision 0.8379, recall 0.8430, auc 0.8400
epoch 4001, loss 0.3676, train acc 84.13%, f1 0.8410, precision 0.8424, recall 0.8395, auc 0.8413
epoch 4101, loss 0.3748, train acc 84.28%, f1 0.8437, precision 0.8389, recall 0.8485, auc 0.8428
epoch 4201, loss 0.2295, train acc 84.39%, f1 0.8440, precision 0.8435, recall 0.8444, auc 0.8439
epoch 4301, loss 0.3296, train acc 84.55%, f1 0.8458, precision 0.8441, recall 0.8476, auc 0.8455
epoch 4401, loss 0.3134, train acc 84.68%, f1 0.8472, precision 0.8447, recall 0.8497, auc 0.8468
epoch 4501, loss 0.3858, train acc 84.87%, f1 0.8484, precision 0.8501, recall 0.8468, auc 0.8487
epoch 4601, loss 0.4108, train acc 84.95%, f1 0.8502, precision 0.8464, recall 0.8540, auc 0.8495
epoch 4701, loss 0.3178, train acc 85.02%, f1 0.8506, precision 0.8484, recall 0.8528, auc 0.8502
epoch 4801, loss 0.2717, train acc 85.16%, f1 0.8518, precision 0.8503, recall 0.8532, auc 0.8516
epoch 4901, loss 0.3095, train acc 85.24%, f1 0.8526, precision 0.8515, recall 0.8538, auc 0.8524
epoch 5001, loss 0.3942, train acc 85.37%, f1 0.8539, precision 0.8524, recall 0.8555, auc 0.8537
epoch 5101, loss 0.3527, train acc 85.45%, f1 0.8546, precision 0.8542, recall 0.8550, auc 0.8545
epoch 5201, loss 0.2793, train acc 85.55%, f1 0.8558, precision 0.8541, recall 0.8575, auc 0.8555
epoch 5301, loss 0.2855, train acc 85.64%, f1 0.8562, precision 0.8577, recall 0.8546, auc 0.8564
epoch 5401, loss 0.3148, train acc 85.78%, f1 0.8573, precision 0.8600, recall 0.8546, auc 0.8578
epoch 5501, loss 0.2861, train acc 85.82%, f1 0.8587, precision 0.8552, recall 0.8623, auc 0.8582
epoch 5601, loss 0.2983, train acc 85.87%, f1 0.8590, precision 0.8573, recall 0.8607, auc 0.8587
epoch 5701, loss 0.4109, train acc 85.99%, f1 0.8602, precision 0.8585, recall 0.8619, auc 0.8599
epoch 5801, loss 0.3299, train acc 86.01%, f1 0.8601, precision 0.8605, recall 0.8596, auc 0.8601
epoch 5901, loss 0.2634, train acc 86.07%, f1 0.8608, precision 0.8597, recall 0.8620, auc 0.8607
epoch 6001, loss 0.2937, train acc 86.21%, f1 0.8626, precision 0.8591, recall 0.8661, auc 0.8621
epoch 6101, loss 0.3401, train acc 86.19%, f1 0.8623, precision 0.8598, recall 0.8648, auc 0.8619
epoch 6201, loss 0.2948, train acc 86.25%, f1 0.8625, precision 0.8623, recall 0.8626, auc 0.8625
epoch 6301, loss 0.3309, train acc 86.25%, f1 0.8627, precision 0.8617, recall 0.8636, auc 0.8625
epoch 6401, loss 0.3223, train acc 86.33%, f1 0.8641, precision 0.8591, recall 0.8691, auc 0.8633
epoch 6501, loss 0.2715, train acc 86.42%, f1 0.8645, precision 0.8625, recall 0.8665, auc 0.8642
epoch 6601, loss 0.3138, train acc 86.41%, f1 0.8639, precision 0.8654, recall 0.8624, auc 0.8641
epoch 6701, loss 0.2667, train acc 86.41%, f1 0.8649, precision 0.8598, recall 0.8701, auc 0.8641
epoch 6801, loss 0.3920, train acc 86.50%, f1 0.8654, precision 0.8629, recall 0.8679, auc 0.8650
epoch 6901, loss 0.3013, train acc 86.54%, f1 0.8657, precision 0.8636, recall 0.8679, auc 0.8654
epoch 7001, loss 0.2567, train acc 86.64%, f1 0.8667, precision 0.8645, recall 0.8689, auc 0.8664
epoch 7101, loss 0.3146, train acc 86.55%, f1 0.8661, precision 0.8621, recall 0.8700, auc 0.8655
epoch 7201, loss 0.2140, train acc 86.64%, f1 0.8663, precision 0.8673, recall 0.8653, auc 0.8664
epoch 7301, loss 0.3732, train acc 86.68%, f1 0.8667, precision 0.8671, recall 0.8663, auc 0.8668
epoch 7401, loss 0.4333, train acc 86.80%, f1 0.8681, precision 0.8671, recall 0.8692, auc 0.8680
epoch 7501, loss 0.3922, train acc 86.80%, f1 0.8682, precision 0.8673, recall 0.8691, auc 0.8680
epoch 7601, loss 0.3363, train acc 86.78%, f1 0.8681, precision 0.8659, recall 0.8704, auc 0.8678
epoch 7701, loss 0.3257, train acc 86.82%, f1 0.8682, precision 0.8685, recall 0.8679, auc 0.8682
epoch 7801, loss 0.4185, train acc 86.81%, f1 0.8683, precision 0.8670, recall 0.8697, auc 0.8681
epoch 7901, loss 0.4073, train acc 86.87%, f1 0.8687, precision 0.8686, recall 0.8687, auc 0.8687
epoch 8001, loss 0.2522, train acc 86.93%, f1 0.8693, precision 0.8689, recall 0.8697, auc 0.8693
epoch 8101, loss 0.2004, train acc 86.93%, f1 0.8697, precision 0.8672, recall 0.8722, auc 0.8693
epoch 8201, loss 0.3277, train acc 86.95%, f1 0.8697, precision 0.8681, recall 0.8713, auc 0.8695/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.3026, train acc 87.02%, f1 0.8708, precision 0.8672, recall 0.8744, auc 0.8702
epoch 8401, loss 0.2406, train acc 87.01%, f1 0.8704, precision 0.8686, recall 0.8722, auc 0.8701
epoch 8501, loss 0.3668, train acc 87.08%, f1 0.8707, precision 0.8713, recall 0.8701, auc 0.8708
epoch 8601, loss 0.2043, train acc 87.06%, f1 0.8707, precision 0.8703, recall 0.8711, auc 0.8706
epoch 8701, loss 0.3612, train acc 87.09%, f1 0.8710, precision 0.8702, recall 0.8718, auc 0.8709
epoch 8801, loss 0.1932, train acc 87.17%, f1 0.8716, precision 0.8722, recall 0.8709, auc 0.8717
epoch 8901, loss 0.2573, train acc 87.17%, f1 0.8720, precision 0.8697, recall 0.8743, auc 0.8717
epoch 9001, loss 0.3221, train acc 87.23%, f1 0.8723, precision 0.8725, recall 0.8721, auc 0.8723
epoch 9101, loss 0.2406, train acc 87.27%, f1 0.8728, precision 0.8720, recall 0.8736, auc 0.8727
epoch 9201, loss 0.3256, train acc 87.32%, f1 0.8732, precision 0.8727, recall 0.8738, auc 0.8732
epoch 9301, loss 0.2601, train acc 87.33%, f1 0.8736, precision 0.8712, recall 0.8761, auc 0.8733
epoch 9401, loss 0.2818, train acc 87.36%, f1 0.8735, precision 0.8743, recall 0.8726, auc 0.8736
epoch 9501, loss 0.2593, train acc 87.40%, f1 0.8740, precision 0.8735, recall 0.8745, auc 0.8740
epoch 9601, loss 0.3076, train acc 87.45%, f1 0.8745, precision 0.8739, recall 0.8751, auc 0.8745
epoch 9701, loss 0.2352, train acc 87.54%, f1 0.8754, precision 0.8751, recall 0.8756, auc 0.8754
epoch 9801, loss 0.2114, train acc 87.57%, f1 0.8759, precision 0.8744, recall 0.8774, auc 0.8757
epoch 9901, loss 0.3267, train acc 87.66%, f1 0.8769, precision 0.8743, recall 0.8796, auc 0.8766
epoch 10001, loss 0.2524, train acc 87.64%, f1 0.8762, precision 0.8774, recall 0.8749, auc 0.8764
epoch 10101, loss 0.3349, train acc 87.69%, f1 0.8767, precision 0.8779, recall 0.8755, auc 0.8769
epoch 10201, loss 0.2587, train acc 87.76%, f1 0.8780, precision 0.8752, recall 0.8808, auc 0.8776
epoch 10301, loss 0.3017, train acc 87.78%, f1 0.8781, precision 0.8760, recall 0.8802, auc 0.8778
epoch 10401, loss 0.1696, train acc 87.81%, f1 0.8783, precision 0.8769, recall 0.8797, auc 0.8781
epoch 10501, loss 0.2872, train acc 87.86%, f1 0.8787, precision 0.8778, recall 0.8795, auc 0.8786
epoch 10601, loss 0.2931, train acc 87.96%, f1 0.8797, precision 0.8789, recall 0.8804, auc 0.8796
epoch 10701, loss 0.1733, train acc 88.00%, f1 0.8799, precision 0.8805, recall 0.8793, auc 0.8800
epoch 10801, loss 0.1999, train acc 88.09%, f1 0.8813, precision 0.8780, recall 0.8847, auc 0.8809
epoch 10901, loss 0.2581, train acc 88.08%, f1 0.8807, precision 0.8812, recall 0.8802, auc 0.8808
epoch 11001, loss 0.2670, train acc 88.14%, f1 0.8812, precision 0.8822, recall 0.8803, auc 0.8814
epoch 11101, loss 0.2563, train acc 88.17%, f1 0.8818, precision 0.8808, recall 0.8827, auc 0.8817
epoch 11201, loss 0.3083, train acc 88.13%, f1 0.8815, precision 0.8795, recall 0.8836, auc 0.8813
epoch 11301, loss 0.1944, train acc 88.25%, f1 0.8822, precision 0.8842, recall 0.8802, auc 0.8825
epoch 11401, loss 0.2477, train acc 88.33%, f1 0.8833, precision 0.8830, recall 0.8836, auc 0.8833
epoch 11501, loss 0.2407, train acc 88.40%, f1 0.8840, precision 0.8834, recall 0.8847, auc 0.8840
epoch 11601, loss 0.1738, train acc 88.38%, f1 0.8838, precision 0.8835, recall 0.8841, auc 0.8838
epoch 11701, loss 0.2194, train acc 88.53%, f1 0.8853, precision 0.8851, recall 0.8855, auc 0.8853
epoch 11801, loss 0.3476, train acc 88.58%, f1 0.8859, precision 0.8845, recall 0.8873, auc 0.8858
epoch 11901, loss 0.2643, train acc 88.58%, f1 0.8860, precision 0.8841, recall 0.8879, auc 0.8858
epoch 12001, loss 0.2769, train acc 88.63%, f1 0.8861, precision 0.8876, recall 0.8845, auc 0.8863
epoch 12101, loss 0.3149, train acc 88.63%, f1 0.8864, precision 0.8859, recall 0.8869, auc 0.8863
epoch 12201, loss 0.2899, train acc 88.76%, f1 0.8879, precision 0.8853, recall 0.8906, auc 0.8876
epoch 12301, loss 0.2384, train acc 88.75%, f1 0.8874, precision 0.8886, recall 0.8861, auc 0.8875
epoch 12401, loss 0.2596, train acc 88.85%, f1 0.8885, precision 0.8886, recall 0.8883, auc 0.8885
epoch 12501, loss 0.3030, train acc 88.86%, f1 0.8884, precision 0.8899, recall 0.8870, auc 0.8886
epoch 12601, loss 0.1962, train acc 88.97%, f1 0.8897, precision 0.8899, recall 0.8894, auc 0.8897
epoch 12701, loss 0.2672, train acc 89.01%, f1 0.8903, precision 0.8889, recall 0.8916, auc 0.8901
epoch 12801, loss 0.2447, train acc 89.07%, f1 0.8907, precision 0.8900, recall 0.8915, auc 0.8907
epoch 12901, loss 0.3232, train acc 89.01%, f1 0.8899, precision 0.8913, recall 0.8886, auc 0.8901
epoch 13001, loss 0.3158, train acc 89.12%, f1 0.8918, precision 0.8868, recall 0.8969, auc 0.8912
epoch 13101, loss 0.3553, train acc 89.13%, f1 0.8914, precision 0.8902, recall 0.8926, auc 0.8913
epoch 13201, loss 0.2371, train acc 89.18%, f1 0.8920, precision 0.8898, recall 0.8943, auc 0.8918
epoch 13301, loss 0.2355, train acc 89.37%, f1 0.8936, precision 0.8939, recall 0.8933, auc 0.8937
epoch 13401, loss 0.2128, train acc 89.29%, f1 0.8932, precision 0.8905, recall 0.8960, auc 0.8929
epoch 13501, loss 0.2648, train acc 89.41%, f1 0.8939, precision 0.8950, recall 0.8929, auc 0.8941
epoch 13601, loss 0.2681, train acc 89.39%, f1 0.8937, precision 0.8957, recall 0.8918, auc 0.8939
epoch 13701, loss 0.2117, train acc 89.36%, f1 0.8934, precision 0.8945, recall 0.8923, auc 0.8936
epoch 13801, loss 0.2084, train acc 89.45%, f1 0.8943, precision 0.8951, recall 0.8935, auc 0.8945
epoch 13901, loss 0.2293, train acc 89.48%, f1 0.8948, precision 0.8941, recall 0.8956, auc 0.8948
epoch 14001, loss 0.4198, train acc 89.48%, f1 0.8946, precision 0.8965, recall 0.8926, auc 0.8948
epoch 14101, loss 0.3668, train acc 89.61%, f1 0.8957, precision 0.8987, recall 0.8928, auc 0.8961
epoch 14201, loss 0.1804, train acc 89.65%, f1 0.8964, precision 0.8972, recall 0.8957, auc 0.8965
epoch 14301, loss 0.2419, train acc 89.63%, f1 0.8963, precision 0.8961, recall 0.8965, auc 0.8963
epoch 14401, loss 0.3163, train acc 89.70%, f1 0.8967, precision 0.8992, recall 0.8943, auc 0.8970
epoch 14501, loss 0.1947, train acc 89.67%, f1 0.8967, precision 0.8960, recall 0.8975, auc 0.8967
epoch 14601, loss 0.2253, train acc 89.79%, f1 0.8980, precision 0.8972, recall 0.8987, auc 0.8979
epoch 14701, loss 0.2824, train acc 89.87%, f1 0.8989, precision 0.8966, recall 0.9012, auc 0.8987
epoch 14801, loss 0.2438, train acc 89.89%, f1 0.8991, precision 0.8976, recall 0.9005, auc 0.8989
epoch 14901, loss 0.2492, train acc 89.94%, f1 0.8994, precision 0.8995, recall 0.8993, auc 0.8994
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_3
./test_pima/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6387037037037038

the Fscore is 0.5903614457831325

the precision is 0.4375

the recall is 0.9074074074074074

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_3
----------------------



epoch 1, loss 0.6934, train acc 51.27%, f1 0.0568, precision 0.9338, recall 0.0293, auc 0.5136
epoch 101, loss 0.5785, train acc 77.61%, f1 0.7759, precision 0.7783, recall 0.7735, auc 0.7761
epoch 201, loss 0.4128, train acc 81.12%, f1 0.8122, precision 0.8095, recall 0.8149, auc 0.8112
epoch 301, loss 0.4386, train acc 82.52%, f1 0.8250, precision 0.8272, recall 0.8228, auc 0.8252
epoch 401, loss 0.3212, train acc 82.89%, f1 0.8290, precision 0.8301, recall 0.8278, auc 0.8289
epoch 501, loss 0.3746, train acc 83.04%, f1 0.8310, precision 0.8299, recall 0.8321, auc 0.8304
epoch 601, loss 0.4403, train acc 83.08%, f1 0.8311, precision 0.8313, recall 0.8308, auc 0.8308
epoch 701, loss 0.3473, train acc 83.07%, f1 0.8311, precision 0.8308, recall 0.8314, auc 0.8307
epoch 801, loss 0.4599, train acc 83.09%, f1 0.8311, precision 0.8315, recall 0.8307, auc 0.8309
epoch 901, loss 0.3671, train acc 83.14%, f1 0.8317, precision 0.8319, recall 0.8315, auc 0.8314
epoch 1001, loss 0.4034, train acc 83.17%, f1 0.8315, precision 0.8337, recall 0.8294, auc 0.8317
epoch 1101, loss 0.4495, train acc 83.08%, f1 0.8312, precision 0.8306, recall 0.8318, auc 0.8308
epoch 1201, loss 0.3846, train acc 83.07%, f1 0.8304, precision 0.8333, recall 0.8275, auc 0.8307
epoch 1301, loss 0.3463, train acc 83.17%, f1 0.8317, precision 0.8332, recall 0.8302, auc 0.8317
epoch 1401, loss 0.3233, train acc 83.12%, f1 0.8312, precision 0.8328, recall 0.8297, auc 0.8313
epoch 1501, loss 0.2769, train acc 83.14%, f1 0.8320, precision 0.8308, recall 0.8331, auc 0.8314
epoch 1601, loss 0.2941, train acc 83.19%, f1 0.8317, precision 0.8342, recall 0.8292, auc 0.8319
epoch 1701, loss 0.3328, train acc 83.20%, f1 0.8322, precision 0.8326, recall 0.8318, auc 0.8320
epoch 1801, loss 0.3905, train acc 83.13%, f1 0.8313, precision 0.8330, recall 0.8295, auc 0.8313
epoch 1901, loss 0.4087, train acc 83.12%, f1 0.8313, precision 0.8324, recall 0.8303, auc 0.8312
epoch 2001, loss 0.3823, train acc 83.12%, f1 0.8316, precision 0.8310, recall 0.8323, auc 0.8312
epoch 2101, loss 0.3324, train acc 83.12%, f1 0.8309, precision 0.8340, recall 0.8278, auc 0.8312
epoch 2201, loss 0.4042, train acc 83.13%, f1 0.8314, precision 0.8326, recall 0.8302, auc 0.8313
epoch 2301, loss 0.4721, train acc 83.14%, f1 0.8315, precision 0.8328, recall 0.8302, auc 0.8314
epoch 2401, loss 0.4491, train acc 83.14%, f1 0.8316, precision 0.8321, recall 0.8312, auc 0.8314
epoch 2501, loss 0.2737, train acc 83.17%, f1 0.8326, precision 0.8297, recall 0.8354, auc 0.8317
epoch 2601, loss 0.3703, train acc 83.17%, f1 0.8321, precision 0.8317, recall 0.8326, auc 0.8317
epoch 2701, loss 0.4019, train acc 83.18%, f1 0.8318, precision 0.8333, recall 0.8303, auc 0.8318
epoch 2801, loss 0.4381, train acc 83.20%, f1 0.8323, precision 0.8325, recall 0.8322, auc 0.8320
epoch 2901, loss 0.3034, train acc 83.26%, f1 0.8325, precision 0.8342, recall 0.8309, auc 0.8326
epoch 3001, loss 0.3569, train acc 83.27%, f1 0.8324, precision 0.8352, recall 0.8297, auc 0.8327
epoch 3101, loss 0.4468, train acc 83.32%, f1 0.8336, precision 0.8334, recall 0.8338, auc 0.8332
epoch 3201, loss 0.2121, train acc 83.34%, f1 0.8337, precision 0.8341, recall 0.8332, auc 0.8334
epoch 3301, loss 0.5154, train acc 83.38%, f1 0.8342, precision 0.8336, recall 0.8348, auc 0.8337
epoch 3401, loss 0.3326, train acc 83.50%, f1 0.8355, precision 0.8342, recall 0.8369, auc 0.8349
epoch 3501, loss 0.4377, train acc 83.57%, f1 0.8358, precision 0.8365, recall 0.8351, auc 0.8357
epoch 3601, loss 0.3197, train acc 83.71%, f1 0.8366, precision 0.8408, recall 0.8324, auc 0.8371
epoch 3701, loss 0.3526, train acc 83.77%, f1 0.8379, precision 0.8387, recall 0.8371, auc 0.8377
epoch 3801, loss 0.3838, train acc 83.94%, f1 0.8392, precision 0.8417, recall 0.8367, auc 0.8394
epoch 3901, loss 0.3620, train acc 84.13%, f1 0.8413, precision 0.8430, recall 0.8396, auc 0.8413
epoch 4001, loss 0.4109, train acc 84.21%, f1 0.8430, precision 0.8400, recall 0.8461, auc 0.8421
epoch 4101, loss 0.3400, train acc 84.31%, f1 0.8438, precision 0.8414, recall 0.8462, auc 0.8431
epoch 4201, loss 0.3161, train acc 84.42%, f1 0.8441, precision 0.8460, recall 0.8422, auc 0.8442
epoch 4301, loss 0.3524, train acc 84.60%, f1 0.8465, precision 0.8452, recall 0.8478, auc 0.8460
epoch 4401, loss 0.2301, train acc 84.79%, f1 0.8481, precision 0.8483, recall 0.8479, auc 0.8479
epoch 4501, loss 0.4123, train acc 84.82%, f1 0.8491, precision 0.8455, recall 0.8526, auc 0.8481
epoch 4601, loss 0.2658, train acc 84.96%, f1 0.8501, precision 0.8486, recall 0.8516, auc 0.8496
epoch 4701, loss 0.2828, train acc 85.07%, f1 0.8513, precision 0.8495, recall 0.8531, auc 0.8507
epoch 4801, loss 0.3707, train acc 85.18%, f1 0.8522, precision 0.8513, recall 0.8531, auc 0.8518
epoch 4901, loss 0.2484, train acc 85.40%, f1 0.8540, precision 0.8561, recall 0.8518, auc 0.8540
epoch 5001, loss 0.2898, train acc 85.46%, f1 0.8549, precision 0.8543, recall 0.8556, auc 0.8546
epoch 5101, loss 0.3127, train acc 85.57%, f1 0.8556, precision 0.8580, recall 0.8531, auc 0.8557
epoch 5201, loss 0.3797, train acc 85.69%, f1 0.8572, precision 0.8572, recall 0.8571, auc 0.8569
epoch 5301, loss 0.3792, train acc 85.75%, f1 0.8579, precision 0.8573, recall 0.8585, auc 0.8575
epoch 5401, loss 0.2187, train acc 85.78%, f1 0.8581, precision 0.8578, recall 0.8585, auc 0.8578
epoch 5501, loss 0.3789, train acc 85.99%, f1 0.8599, precision 0.8615, recall 0.8584, auc 0.8599
epoch 5601, loss 0.2856, train acc 86.06%, f1 0.8607, precision 0.8616, recall 0.8598, auc 0.8606
epoch 5701, loss 0.2221, train acc 86.09%, f1 0.8611, precision 0.8616, recall 0.8606, auc 0.8609
epoch 5801, loss 0.2794, train acc 86.16%, f1 0.8617, precision 0.8626, recall 0.8608, auc 0.8616
epoch 5901, loss 0.3877, train acc 86.21%, f1 0.8624, precision 0.8624, recall 0.8624, auc 0.8621
epoch 6001, loss 0.3496, train acc 86.21%, f1 0.8620, precision 0.8639, recall 0.8602, auc 0.8621
epoch 6101, loss 0.2414, train acc 86.32%, f1 0.8632, precision 0.8650, recall 0.8614, auc 0.8632
epoch 6201, loss 0.3264, train acc 86.39%, f1 0.8637, precision 0.8670, recall 0.8604, auc 0.8639
epoch 6301, loss 0.2909, train acc 86.38%, f1 0.8642, precision 0.8637, recall 0.8646, auc 0.8638
epoch 6401, loss 0.3388, train acc 86.50%, f1 0.8648, precision 0.8682, recall 0.8614, auc 0.8650
epoch 6501, loss 0.3021, train acc 86.60%, f1 0.8655, precision 0.8703, recall 0.8607, auc 0.8660
epoch 6601, loss 0.3566, train acc 86.55%, f1 0.8660, precision 0.8644, recall 0.8676, auc 0.8655
epoch 6701, loss 0.2820, train acc 86.59%, f1 0.8662, precision 0.8664, recall 0.8659, auc 0.8659
epoch 6801, loss 0.1785, train acc 86.67%, f1 0.8665, precision 0.8694, recall 0.8635, auc 0.8667
epoch 6901, loss 0.2865, train acc 86.74%, f1 0.8681, precision 0.8654, recall 0.8708, auc 0.8674
epoch 7001, loss 0.3619, train acc 86.74%, f1 0.8673, precision 0.8696, recall 0.8650, auc 0.8674
epoch 7101, loss 0.2844, train acc 86.82%, f1 0.8678, precision 0.8723, recall 0.8634, auc 0.8682
epoch 7201, loss 0.1882, train acc 86.79%, f1 0.8676, precision 0.8710, recall 0.8642, auc 0.8679
epoch 7301, loss 0.2943, train acc 86.89%, f1 0.8688, precision 0.8715, recall 0.8661, auc 0.8689
epoch 7401, loss 0.2340, train acc 86.87%, f1 0.8685, precision 0.8715, recall 0.8655, auc 0.8687
epoch 7501, loss 0.3575, train acc 86.91%, f1 0.8690, precision 0.8714, recall 0.8666, auc 0.8691
epoch 7601, loss 0.3535, train acc 86.97%, f1 0.8702, precision 0.8683, recall 0.8721, auc 0.8697
epoch 7701, loss 0.3011, train acc 87.01%, f1 0.8701, precision 0.8718, recall 0.8685, auc 0.8701
epoch 7801, loss 0.2786, train acc 87.07%, f1 0.8711, precision 0.8703, recall 0.8718, auc 0.8707
epoch 7901, loss 0.2978, train acc 87.12%, f1 0.8716, precision 0.8708, recall 0.8723, auc 0.8712
epoch 8001, loss 0.2783, train acc 87.12%, f1 0.8711, precision 0.8736, recall 0.8687, auc 0.8713
epoch 8101, loss 0.3245, train acc 87.18%, f1 0.8720, precision 0.8726, recall 0.8714, auc 0.8718
epoch 8201, loss 0.2926, train acc 87.28%, f1 0.8728, precision 0.8744, recall 0.8711, auc 0.8728/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2874, train acc 87.24%, f1 0.8728, precision 0.8713, recall 0.8744, auc 0.8724
epoch 8401, loss 0.3219, train acc 87.34%, f1 0.8737, precision 0.8735, recall 0.8738, auc 0.8734
epoch 8501, loss 0.2897, train acc 87.30%, f1 0.8727, precision 0.8764, recall 0.8690, auc 0.8730
epoch 8601, loss 0.2488, train acc 87.36%, f1 0.8739, precision 0.8731, recall 0.8747, auc 0.8736
epoch 8701, loss 0.2786, train acc 87.37%, f1 0.8738, precision 0.8748, recall 0.8728, auc 0.8737
epoch 8801, loss 0.2671, train acc 87.46%, f1 0.8742, precision 0.8785, recall 0.8700, auc 0.8746
epoch 8901, loss 0.1778, train acc 87.43%, f1 0.8748, precision 0.8734, recall 0.8761, auc 0.8743
epoch 9001, loss 0.3205, train acc 87.53%, f1 0.8755, precision 0.8760, recall 0.8750, auc 0.8753
epoch 9101, loss 0.2292, train acc 87.54%, f1 0.8757, precision 0.8754, recall 0.8759, auc 0.8754
epoch 9201, loss 0.2765, train acc 87.52%, f1 0.8753, precision 0.8759, recall 0.8747, auc 0.8752
epoch 9301, loss 0.2308, train acc 87.60%, f1 0.8755, precision 0.8805, recall 0.8705, auc 0.8760
epoch 9401, loss 0.2729, train acc 87.64%, f1 0.8765, precision 0.8776, recall 0.8753, auc 0.8764
epoch 9501, loss 0.3950, train acc 87.63%, f1 0.8764, precision 0.8770, recall 0.8759, auc 0.8763
epoch 9601, loss 0.2471, train acc 87.69%, f1 0.8774, precision 0.8754, recall 0.8794, auc 0.8769
epoch 9701, loss 0.2108, train acc 87.78%, f1 0.8776, precision 0.8803, recall 0.8750, auc 0.8778
epoch 9801, loss 0.2558, train acc 87.84%, f1 0.8782, precision 0.8810, recall 0.8754, auc 0.8784
epoch 9901, loss 0.4269, train acc 87.87%, f1 0.8794, precision 0.8759, recall 0.8830, auc 0.8787
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_3
./test_pima/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6307407407407408

the Fscore is 0.5921787709497206

the precision is 0.424

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_3
----------------------



epoch 1, loss 0.6932, train acc 53.47%, f1 0.1681, precision 0.8040, recall 0.0939, auc 0.5355
epoch 101, loss 0.5882, train acc 77.96%, f1 0.7839, precision 0.7703, recall 0.7980, auc 0.7796
epoch 201, loss 0.5535, train acc 80.96%, f1 0.8100, precision 0.8097, recall 0.8103, auc 0.8096
epoch 301, loss 0.3883, train acc 82.44%, f1 0.8244, precision 0.8260, recall 0.8228, auc 0.8244
epoch 401, loss 0.3324, train acc 82.90%, f1 0.8288, precision 0.8312, recall 0.8265, auc 0.8290
epoch 501, loss 0.4454, train acc 83.04%, f1 0.8303, precision 0.8325, recall 0.8280, auc 0.8304
epoch 601, loss 0.4297, train acc 83.04%, f1 0.8299, precision 0.8335, recall 0.8264, auc 0.8304
epoch 701, loss 0.2077, train acc 83.09%, f1 0.8307, precision 0.8330, recall 0.8284, auc 0.8309
epoch 801, loss 0.3417, train acc 83.18%, f1 0.8315, precision 0.8344, recall 0.8286, auc 0.8318
epoch 901, loss 0.3757, train acc 83.17%, f1 0.8316, precision 0.8335, recall 0.8296, auc 0.8317
epoch 1001, loss 0.3977, train acc 83.12%, f1 0.8310, precision 0.8332, recall 0.8288, auc 0.8312
epoch 1101, loss 0.3446, train acc 83.12%, f1 0.8311, precision 0.8328, recall 0.8294, auc 0.8312
epoch 1201, loss 0.3761, train acc 83.12%, f1 0.8312, precision 0.8326, recall 0.8299, auc 0.8312
epoch 1301, loss 0.2990, train acc 83.12%, f1 0.8307, precision 0.8343, recall 0.8271, auc 0.8312
epoch 1401, loss 0.5268, train acc 83.11%, f1 0.8311, precision 0.8327, recall 0.8294, auc 0.8311
epoch 1501, loss 0.3910, train acc 83.13%, f1 0.8316, precision 0.8315, recall 0.8318, auc 0.8313
epoch 1601, loss 0.4803, train acc 83.12%, f1 0.8312, precision 0.8330, recall 0.8293, auc 0.8313
epoch 1701, loss 0.3766, train acc 83.11%, f1 0.8306, precision 0.8344, recall 0.8269, auc 0.8311
epoch 1801, loss 0.3138, train acc 83.09%, f1 0.8311, precision 0.8319, recall 0.8302, auc 0.8309
epoch 1901, loss 0.4640, train acc 83.15%, f1 0.8308, precision 0.8353, recall 0.8265, auc 0.8315
epoch 2001, loss 0.3335, train acc 83.12%, f1 0.8315, precision 0.8314, recall 0.8317, auc 0.8312
epoch 2101, loss 0.4155, train acc 83.11%, f1 0.8307, precision 0.8341, recall 0.8274, auc 0.8312
epoch 2201, loss 0.4969, train acc 83.14%, f1 0.8315, precision 0.8324, recall 0.8307, auc 0.8314
epoch 2301, loss 0.3804, train acc 83.12%, f1 0.8312, precision 0.8326, recall 0.8299, auc 0.8312
epoch 2401, loss 0.4192, train acc 83.10%, f1 0.8307, precision 0.8335, recall 0.8280, auc 0.8310
epoch 2501, loss 0.3915, train acc 83.13%, f1 0.8310, precision 0.8339, recall 0.8280, auc 0.8313
epoch 2601, loss 0.3608, train acc 83.11%, f1 0.8306, precision 0.8344, recall 0.8269, auc 0.8311
epoch 2701, loss 0.5007, train acc 83.11%, f1 0.8311, precision 0.8325, recall 0.8297, auc 0.8311
epoch 2801, loss 0.4236, train acc 83.09%, f1 0.8302, precision 0.8352, recall 0.8252, auc 0.8309
epoch 2901, loss 0.3143, train acc 83.16%, f1 0.8318, precision 0.8323, recall 0.8314, auc 0.8316
epoch 3001, loss 0.3627, train acc 83.23%, f1 0.8328, precision 0.8317, recall 0.8340, auc 0.8323
epoch 3101, loss 0.4146, train acc 83.26%, f1 0.8330, precision 0.8322, recall 0.8338, auc 0.8326
epoch 3201, loss 0.3892, train acc 83.27%, f1 0.8330, precision 0.8328, recall 0.8331, auc 0.8327
epoch 3301, loss 0.3504, train acc 83.30%, f1 0.8327, precision 0.8358, recall 0.8295, auc 0.8330
epoch 3401, loss 0.4325, train acc 83.39%, f1 0.8340, precision 0.8345, recall 0.8336, auc 0.8339
epoch 3501, loss 0.4003, train acc 83.41%, f1 0.8337, precision 0.8370, recall 0.8304, auc 0.8341
epoch 3601, loss 0.3155, train acc 83.46%, f1 0.8342, precision 0.8376, recall 0.8309, auc 0.8346
epoch 3701, loss 0.4570, train acc 83.63%, f1 0.8366, precision 0.8366, recall 0.8367, auc 0.8363
epoch 3801, loss 0.3831, train acc 83.70%, f1 0.8369, precision 0.8387, recall 0.8351, auc 0.8370
epoch 3901, loss 0.5103, train acc 83.80%, f1 0.8381, precision 0.8388, recall 0.8374, auc 0.8380
epoch 4001, loss 0.3340, train acc 83.89%, f1 0.8394, precision 0.8380, recall 0.8409, auc 0.8389
epoch 4101, loss 0.2661, train acc 84.09%, f1 0.8405, precision 0.8441, recall 0.8370, auc 0.8410
epoch 4201, loss 0.3479, train acc 84.16%, f1 0.8412, precision 0.8447, recall 0.8376, auc 0.8416
epoch 4301, loss 0.3396, train acc 84.34%, f1 0.8439, precision 0.8430, recall 0.8448, auc 0.8434
epoch 4401, loss 0.3560, train acc 84.45%, f1 0.8447, precision 0.8450, recall 0.8444, auc 0.8445
epoch 4501, loss 0.3701, train acc 84.57%, f1 0.8460, precision 0.8462, recall 0.8457, auc 0.8457
epoch 4601, loss 0.3979, train acc 84.72%, f1 0.8468, precision 0.8502, recall 0.8435, auc 0.8472
epoch 4701, loss 0.3022, train acc 84.85%, f1 0.8485, precision 0.8500, recall 0.8470, auc 0.8485
epoch 4801, loss 0.2885, train acc 84.95%, f1 0.8505, precision 0.8467, recall 0.8543, auc 0.8495
epoch 4901, loss 0.2707, train acc 85.04%, f1 0.8507, precision 0.8503, recall 0.8511, auc 0.8504
epoch 5001, loss 0.3724, train acc 85.14%, f1 0.8515, precision 0.8525, recall 0.8505, auc 0.8514
epoch 5101, loss 0.3368, train acc 85.30%, f1 0.8530, precision 0.8547, recall 0.8512, auc 0.8530
epoch 5201, loss 0.3239, train acc 85.48%, f1 0.8551, precision 0.8551, recall 0.8551, auc 0.8548
epoch 5301, loss 0.3424, train acc 85.48%, f1 0.8547, precision 0.8569, recall 0.8525, auc 0.8548
epoch 5401, loss 0.3061, train acc 85.66%, f1 0.8557, precision 0.8624, recall 0.8491, auc 0.8566
epoch 5501, loss 0.3101, train acc 85.62%, f1 0.8561, precision 0.8584, recall 0.8539, auc 0.8563
epoch 5601, loss 0.3179, train acc 85.78%, f1 0.8573, precision 0.8618, recall 0.8528, auc 0.8578
epoch 5701, loss 0.3815, train acc 85.88%, f1 0.8588, precision 0.8604, recall 0.8573, auc 0.8588
epoch 5801, loss 0.2660, train acc 85.95%, f1 0.8596, precision 0.8604, recall 0.8588, auc 0.8595
epoch 5901, loss 0.3361, train acc 86.00%, f1 0.8597, precision 0.8625, recall 0.8570, auc 0.8600
epoch 6001, loss 0.2714, train acc 86.07%, f1 0.8613, precision 0.8590, recall 0.8636, auc 0.8607
epoch 6101, loss 0.3127, train acc 86.20%, f1 0.8626, precision 0.8606, recall 0.8646, auc 0.8620
epoch 6201, loss 0.3168, train acc 86.23%, f1 0.8627, precision 0.8618, recall 0.8636, auc 0.8623
epoch 6301, loss 0.2778, train acc 86.26%, f1 0.8625, precision 0.8649, recall 0.8601, auc 0.8626
epoch 6401, loss 0.3470, train acc 86.40%, f1 0.8642, precision 0.8645, recall 0.8638, auc 0.8640
epoch 6501, loss 0.3744, train acc 86.44%, f1 0.8644, precision 0.8654, recall 0.8634, auc 0.8644
epoch 6601, loss 0.3213, train acc 86.37%, f1 0.8635, precision 0.8666, recall 0.8604, auc 0.8637
epoch 6701, loss 0.2668, train acc 86.51%, f1 0.8654, precision 0.8648, recall 0.8661, auc 0.8651
epoch 6801, loss 0.3116, train acc 86.52%, f1 0.8658, precision 0.8634, recall 0.8681, auc 0.8652
epoch 6901, loss 0.3532, train acc 86.60%, f1 0.8660, precision 0.8673, recall 0.8646, auc 0.8660
epoch 7001, loss 0.2132, train acc 86.60%, f1 0.8655, precision 0.8702, recall 0.8608, auc 0.8660
epoch 7101, loss 0.2992, train acc 86.64%, f1 0.8665, precision 0.8674, recall 0.8657, auc 0.8664
epoch 7201, loss 0.3680, train acc 86.66%, f1 0.8670, precision 0.8653, recall 0.8688, auc 0.8665
epoch 7301, loss 0.3526, train acc 86.77%, f1 0.8679, precision 0.8681, recall 0.8677, auc 0.8677
epoch 7401, loss 0.3725, train acc 86.82%, f1 0.8681, precision 0.8702, recall 0.8660, auc 0.8682
epoch 7501, loss 0.2665, train acc 86.77%, f1 0.8676, precision 0.8696, recall 0.8657, auc 0.8677
epoch 7601, loss 0.2625, train acc 86.89%, f1 0.8688, precision 0.8713, recall 0.8662, auc 0.8689
epoch 7701, loss 0.3677, train acc 86.88%, f1 0.8692, precision 0.8683, recall 0.8701, auc 0.8688
epoch 7801, loss 0.3699, train acc 86.96%, f1 0.8700, precision 0.8684, recall 0.8716, auc 0.8696
epoch 7901, loss 0.3174, train acc 87.01%, f1 0.8703, precision 0.8702, recall 0.8703, auc 0.8701
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_3
./test_pima/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6599999999999999

the Fscore is 0.6136363636363636

the precision is 0.4426229508196721

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_3
----------------------



epoch 1, loss 0.6939, train acc 50.09%, f1 0.6675, precision 0.5009, recall 1.0000, auc 0.5000
epoch 101, loss 0.5867, train acc 78.17%, f1 0.7834, precision 0.7787, recall 0.7881, auc 0.7817
epoch 201, loss 0.3465, train acc 80.96%, f1 0.8104, precision 0.8084, recall 0.8123, auc 0.8096
epoch 301, loss 0.3475, train acc 82.51%, f1 0.8257, precision 0.8245, recall 0.8270, auc 0.8251
epoch 401, loss 0.4648, train acc 82.85%, f1 0.8290, precision 0.8284, recall 0.8296, auc 0.8285
epoch 501, loss 0.3645, train acc 83.02%, f1 0.8302, precision 0.8313, recall 0.8292, auc 0.8302
epoch 601, loss 0.4346, train acc 83.11%, f1 0.8308, precision 0.8336, recall 0.8281, auc 0.8311
epoch 701, loss 0.3648, train acc 83.12%, f1 0.8314, precision 0.8319, recall 0.8309, auc 0.8312
epoch 801, loss 0.4051, train acc 83.09%, f1 0.8308, precision 0.8331, recall 0.8285, auc 0.8310
epoch 901, loss 0.3462, train acc 83.09%, f1 0.8310, precision 0.8318, recall 0.8303, auc 0.8309
epoch 1001, loss 0.3988, train acc 83.08%, f1 0.8313, precision 0.8303, recall 0.8322, auc 0.8308
epoch 1101, loss 0.3572, train acc 83.12%, f1 0.8313, precision 0.8324, recall 0.8303, auc 0.8312
epoch 1201, loss 0.3926, train acc 83.10%, f1 0.8310, precision 0.8327, recall 0.8293, auc 0.8310
epoch 1301, loss 0.4798, train acc 83.05%, f1 0.8308, precision 0.8311, recall 0.8305, auc 0.8305
epoch 1401, loss 0.4267, train acc 83.14%, f1 0.8317, precision 0.8321, recall 0.8313, auc 0.8314
epoch 1501, loss 0.3587, train acc 83.18%, f1 0.8321, precision 0.8320, recall 0.8322, auc 0.8318
epoch 1601, loss 0.3291, train acc 83.16%, f1 0.8312, precision 0.8344, recall 0.8281, auc 0.8316
epoch 1701, loss 0.4282, train acc 83.09%, f1 0.8312, precision 0.8314, recall 0.8311, auc 0.8309
epoch 1801, loss 0.4500, train acc 83.14%, f1 0.8322, precision 0.8295, recall 0.8349, auc 0.8313
epoch 1901, loss 0.4561, train acc 83.14%, f1 0.8324, precision 0.8293, recall 0.8355, auc 0.8314
epoch 2001, loss 0.4120, train acc 83.13%, f1 0.8310, precision 0.8340, recall 0.8281, auc 0.8313
epoch 2101, loss 0.4642, train acc 83.14%, f1 0.8324, precision 0.8294, recall 0.8354, auc 0.8314
epoch 2201, loss 0.3278, train acc 83.15%, f1 0.8323, precision 0.8298, recall 0.8348, auc 0.8315
epoch 2301, loss 0.3824, train acc 83.10%, f1 0.8313, precision 0.8314, recall 0.8312, auc 0.8310
epoch 2401, loss 0.3699, train acc 83.12%, f1 0.8313, precision 0.8325, recall 0.8301, auc 0.8312
epoch 2501, loss 0.3178, train acc 83.13%, f1 0.8322, precision 0.8295, recall 0.8349, auc 0.8313
epoch 2601, loss 0.3510, train acc 83.15%, f1 0.8310, precision 0.8347, recall 0.8274, auc 0.8315
epoch 2701, loss 0.2898, train acc 83.20%, f1 0.8324, precision 0.8322, recall 0.8325, auc 0.8320
epoch 2801, loss 0.4344, train acc 83.25%, f1 0.8330, precision 0.8323, recall 0.8337, auc 0.8325
epoch 2901, loss 0.3593, train acc 83.22%, f1 0.8328, precision 0.8318, recall 0.8337, auc 0.8322
epoch 3001, loss 0.3256, train acc 83.33%, f1 0.8338, precision 0.8329, recall 0.8347, auc 0.8333
epoch 3101, loss 0.3326, train acc 83.31%, f1 0.8337, precision 0.8324, recall 0.8350, auc 0.8331
epoch 3201, loss 0.3600, train acc 83.37%, f1 0.8345, precision 0.8323, recall 0.8366, auc 0.8337
epoch 3301, loss 0.4353, train acc 83.48%, f1 0.8345, precision 0.8374, recall 0.8316, auc 0.8348
epoch 3401, loss 0.3639, train acc 83.56%, f1 0.8361, precision 0.8354, recall 0.8368, auc 0.8356
epoch 3501, loss 0.4749, train acc 83.70%, f1 0.8373, precision 0.8373, recall 0.8372, auc 0.8370
epoch 3601, loss 0.2809, train acc 83.83%, f1 0.8381, precision 0.8404, recall 0.8359, auc 0.8383
epoch 3701, loss 0.3090, train acc 83.97%, f1 0.8395, precision 0.8420, recall 0.8370, auc 0.8397
epoch 3801, loss 0.3681, train acc 84.11%, f1 0.8414, precision 0.8413, recall 0.8414, auc 0.8411
epoch 3901, loss 0.3423, train acc 84.18%, f1 0.8414, precision 0.8449, recall 0.8379, auc 0.8418
epoch 4001, loss 0.4244, train acc 84.39%, f1 0.8437, precision 0.8461, recall 0.8414, auc 0.8439
epoch 4101, loss 0.4106, train acc 84.45%, f1 0.8449, precision 0.8445, recall 0.8453, auc 0.8445
epoch 4201, loss 0.4505, train acc 84.58%, f1 0.8458, precision 0.8472, recall 0.8445, auc 0.8458
epoch 4301, loss 0.2738, train acc 84.65%, f1 0.8468, precision 0.8467, recall 0.8470, auc 0.8465
epoch 4401, loss 0.3903, train acc 84.84%, f1 0.8487, precision 0.8487, recall 0.8487, auc 0.8484
epoch 4501, loss 0.2808, train acc 84.93%, f1 0.8494, precision 0.8505, recall 0.8483, auc 0.8493
epoch 4601, loss 0.4325, train acc 85.06%, f1 0.8514, precision 0.8483, recall 0.8545, auc 0.8506
epoch 4701, loss 0.4416, train acc 85.16%, f1 0.8512, precision 0.8553, recall 0.8472, auc 0.8517
epoch 4801, loss 0.2337, train acc 85.18%, f1 0.8519, precision 0.8527, recall 0.8512, auc 0.8518
epoch 4901, loss 0.3713, train acc 85.41%, f1 0.8543, precision 0.8548, recall 0.8539, auc 0.8541
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_3
./test_pima/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.625

the Fscore is 0.5901639344262295

the precision is 0.4186046511627907

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_3
----------------------



epoch 1, loss 0.6929, train acc 49.97%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5858, train acc 77.42%, f1 0.7720, precision 0.7800, recall 0.7641, auc 0.7742
epoch 201, loss 0.4053, train acc 81.16%, f1 0.8112, precision 0.8136, recall 0.8088, auc 0.8116
epoch 301, loss 0.3782, train acc 82.51%, f1 0.8250, precision 0.8261, recall 0.8239, auc 0.8251
epoch 401, loss 0.3763, train acc 82.87%, f1 0.8295, precision 0.8260, recall 0.8331, auc 0.8287
epoch 501, loss 0.3969, train acc 82.99%, f1 0.8307, precision 0.8273, recall 0.8341, auc 0.8299
epoch 601, loss 0.2698, train acc 83.10%, f1 0.8308, precision 0.8320, recall 0.8297, auc 0.8310
epoch 701, loss 0.2735, train acc 83.14%, f1 0.8317, precision 0.8307, recall 0.8327, auc 0.8314
epoch 801, loss 0.4073, train acc 83.12%, f1 0.8314, precision 0.8310, recall 0.8317, auc 0.8312
epoch 901, loss 0.3895, train acc 83.15%, f1 0.8314, precision 0.8327, recall 0.8301, auc 0.8315
epoch 1001, loss 0.5163, train acc 83.11%, f1 0.8309, precision 0.8326, recall 0.8291, auc 0.8311
epoch 1101, loss 0.4435, train acc 83.16%, f1 0.8314, precision 0.8329, recall 0.8299, auc 0.8316
epoch 1201, loss 0.3892, train acc 83.06%, f1 0.8307, precision 0.8306, recall 0.8309, auc 0.8306
epoch 1301, loss 0.4093, train acc 83.09%, f1 0.8314, precision 0.8294, recall 0.8335, auc 0.8309
epoch 1401, loss 0.4935, train acc 83.09%, f1 0.8309, precision 0.8313, recall 0.8305, auc 0.8309
epoch 1501, loss 0.4477, train acc 83.12%, f1 0.8310, precision 0.8326, recall 0.8295, auc 0.8313
epoch 1601, loss 0.3463, train acc 83.12%, f1 0.8307, precision 0.8333, recall 0.8281, auc 0.8312
epoch 1701, loss 0.4340, train acc 83.13%, f1 0.8312, precision 0.8324, recall 0.8299, auc 0.8313
epoch 1801, loss 0.4714, train acc 83.11%, f1 0.8315, precision 0.8300, recall 0.8331, auc 0.8311
epoch 1901, loss 0.3271, train acc 83.20%, f1 0.8321, precision 0.8323, recall 0.8320, auc 0.8320
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_3
./test_pima/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.565

the Fscore is 0.5538461538461539

the precision is 0.3829787234042553

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_3
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6157, train acc 77.07%, f1 0.7682, precision 0.7765, recall 0.7602, auc 0.7707
epoch 201, loss 0.5482, train acc 79.84%, f1 0.7981, precision 0.7992, recall 0.7970, auc 0.7984
epoch 301, loss 0.4998, train acc 81.48%, f1 0.8149, precision 0.8146, recall 0.8151, auc 0.8148
epoch 401, loss 0.3752, train acc 82.37%, f1 0.8238, precision 0.8234, recall 0.8242, auc 0.8237
epoch 501, loss 0.5025, train acc 82.83%, f1 0.8285, precision 0.8277, recall 0.8292, auc 0.8283
epoch 601, loss 0.3748, train acc 83.02%, f1 0.8303, precision 0.8298, recall 0.8309, auc 0.8302
epoch 701, loss 0.4999, train acc 83.11%, f1 0.8312, precision 0.8305, recall 0.8319, auc 0.8311
epoch 801, loss 0.3334, train acc 83.14%, f1 0.8314, precision 0.8312, recall 0.8317, auc 0.8314
epoch 901, loss 0.3951, train acc 83.11%, f1 0.8312, precision 0.8307, recall 0.8316, auc 0.8311
epoch 1001, loss 0.4675, train acc 83.12%, f1 0.8312, precision 0.8310, recall 0.8314, auc 0.8312
epoch 1101, loss 0.3176, train acc 83.13%, f1 0.8314, precision 0.8312, recall 0.8315, auc 0.8313
epoch 1201, loss 0.3092, train acc 83.11%, f1 0.8311, precision 0.8310, recall 0.8313, auc 0.8311
epoch 1301, loss 0.3879, train acc 83.13%, f1 0.8314, precision 0.8312, recall 0.8315, auc 0.8313
epoch 1401, loss 0.3311, train acc 83.13%, f1 0.8313, precision 0.8313, recall 0.8313, auc 0.8313
epoch 1501, loss 0.3615, train acc 83.14%, f1 0.8314, precision 0.8314, recall 0.8314, auc 0.8314
epoch 1601, loss 0.3578, train acc 83.10%, f1 0.8310, precision 0.8309, recall 0.8312, auc 0.8310
epoch 1701, loss 0.2730, train acc 83.14%, f1 0.8314, precision 0.8315, recall 0.8313, auc 0.8314
epoch 1801, loss 0.3651, train acc 83.12%, f1 0.8312, precision 0.8313, recall 0.8312, auc 0.8312
epoch 1901, loss 0.4332, train acc 83.17%, f1 0.8317, precision 0.8317, recall 0.8318, auc 0.8317
epoch 2001, loss 0.3146, train acc 83.16%, f1 0.8316, precision 0.8315, recall 0.8317, auc 0.8316
epoch 2101, loss 0.4289, train acc 83.15%, f1 0.8315, precision 0.8315, recall 0.8314, auc 0.8315
epoch 2201, loss 0.3080, train acc 83.14%, f1 0.8314, precision 0.8313, recall 0.8315, auc 0.8314
epoch 2301, loss 0.2971, train acc 83.18%, f1 0.8318, precision 0.8319, recall 0.8318, auc 0.8318
epoch 2401, loss 0.3309, train acc 83.13%, f1 0.8313, precision 0.8316, recall 0.8309, auc 0.8313
epoch 2501, loss 0.4481, train acc 83.15%, f1 0.8315, precision 0.8315, recall 0.8314, auc 0.8315
epoch 2601, loss 0.5198, train acc 83.14%, f1 0.8314, precision 0.8312, recall 0.8316, auc 0.8314
epoch 2701, loss 0.3932, train acc 83.11%, f1 0.8311, precision 0.8312, recall 0.8310, auc 0.8311
epoch 2801, loss 0.3979, train acc 83.12%, f1 0.8312, precision 0.8310, recall 0.8314, auc 0.8312
epoch 2901, loss 0.3141, train acc 83.11%, f1 0.8312, precision 0.8310, recall 0.8313, auc 0.8311
epoch 3001, loss 0.3426, train acc 83.15%, f1 0.8315, precision 0.8314, recall 0.8317, auc 0.8315
epoch 3101, loss 0.2372, train acc 83.14%, f1 0.8314, precision 0.8312, recall 0.8316, auc 0.8314
epoch 3201, loss 0.3657, train acc 83.14%, f1 0.8314, precision 0.8313, recall 0.8316, auc 0.8314
epoch 3301, loss 0.3380, train acc 83.10%, f1 0.8311, precision 0.8310, recall 0.8312, auc 0.8310
epoch 3401, loss 0.4269, train acc 83.07%, f1 0.8307, precision 0.8308, recall 0.8307, auc 0.8307
epoch 3501, loss 0.4104, train acc 83.15%, f1 0.8315, precision 0.8317, recall 0.8313, auc 0.8315
epoch 3601, loss 0.3338, train acc 83.12%, f1 0.8312, precision 0.8314, recall 0.8310, auc 0.8312
epoch 3701, loss 0.3830, train acc 83.13%, f1 0.8314, precision 0.8311, recall 0.8316, auc 0.8313
epoch 3801, loss 0.2856, train acc 83.21%, f1 0.8321, precision 0.8321, recall 0.8320, auc 0.8321
epoch 3901, loss 0.4218, train acc 83.20%, f1 0.8320, precision 0.8320, recall 0.8320, auc 0.8320
epoch 4001, loss 0.3776, train acc 83.21%, f1 0.8320, precision 0.8322, recall 0.8318, auc 0.8321
epoch 4101, loss 0.3766, train acc 83.25%, f1 0.8324, precision 0.8327, recall 0.8322, auc 0.8325
epoch 4201, loss 0.3602, train acc 83.19%, f1 0.8319, precision 0.8320, recall 0.8318, auc 0.8319
epoch 4301, loss 0.3036, train acc 83.22%, f1 0.8322, precision 0.8321, recall 0.8322, auc 0.8322
epoch 4401, loss 0.3126, train acc 83.22%, f1 0.8322, precision 0.8323, recall 0.8320, auc 0.8322
epoch 4501, loss 0.3723, train acc 83.18%, f1 0.8317, precision 0.8319, recall 0.8315, auc 0.8318
epoch 4601, loss 0.3144, train acc 83.23%, f1 0.8323, precision 0.8322, recall 0.8324, auc 0.8323
epoch 4701, loss 0.4646, train acc 83.26%, f1 0.8327, precision 0.8326, recall 0.8327, auc 0.8326
epoch 4801, loss 0.4598, train acc 83.28%, f1 0.8328, precision 0.8329, recall 0.8326, auc 0.8328
epoch 4901, loss 0.3113, train acc 83.29%, f1 0.8329, precision 0.8330, recall 0.8328, auc 0.8329
epoch 5001, loss 0.3090, train acc 83.30%, f1 0.8330, precision 0.8331, recall 0.8329, auc 0.8330
epoch 5101, loss 0.2272, train acc 83.32%, f1 0.8332, precision 0.8334, recall 0.8330, auc 0.8332
epoch 5201, loss 0.4122, train acc 83.34%, f1 0.8334, precision 0.8336, recall 0.8332, auc 0.8334
epoch 5301, loss 0.4418, train acc 83.31%, f1 0.8331, precision 0.8334, recall 0.8328, auc 0.8331
epoch 5401, loss 0.3535, train acc 83.33%, f1 0.8333, precision 0.8334, recall 0.8332, auc 0.8333
epoch 5501, loss 0.3617, train acc 83.35%, f1 0.8335, precision 0.8336, recall 0.8335, auc 0.8335
epoch 5601, loss 0.2751, train acc 83.38%, f1 0.8338, precision 0.8339, recall 0.8337, auc 0.8338
epoch 5701, loss 0.3918, train acc 83.43%, f1 0.8343, precision 0.8344, recall 0.8342, auc 0.8343
epoch 5801, loss 0.3703, train acc 83.44%, f1 0.8344, precision 0.8346, recall 0.8342, auc 0.8344
epoch 5901, loss 0.3379, train acc 83.42%, f1 0.8341, precision 0.8344, recall 0.8338, auc 0.8342
epoch 6001, loss 0.4494, train acc 83.45%, f1 0.8345, precision 0.8348, recall 0.8342, auc 0.8345
epoch 6101, loss 0.4490, train acc 83.49%, f1 0.8349, precision 0.8350, recall 0.8347, auc 0.8349
epoch 6201, loss 0.3353, train acc 83.45%, f1 0.8345, precision 0.8347, recall 0.8343, auc 0.8345
epoch 6301, loss 0.3821, train acc 83.54%, f1 0.8353, precision 0.8356, recall 0.8351, auc 0.8354
epoch 6401, loss 0.3332, train acc 83.52%, f1 0.8352, precision 0.8353, recall 0.8350, auc 0.8352
epoch 6501, loss 0.2811, train acc 83.53%, f1 0.8353, precision 0.8354, recall 0.8352, auc 0.8353
epoch 6601, loss 0.3398, train acc 83.61%, f1 0.8360, precision 0.8363, recall 0.8357, auc 0.8361
epoch 6701, loss 0.4927, train acc 83.66%, f1 0.8366, precision 0.8367, recall 0.8366, auc 0.8366
epoch 6801, loss 0.4648, train acc 83.65%, f1 0.8364, precision 0.8366, recall 0.8363, auc 0.8365
epoch 6901, loss 0.3758, train acc 83.64%, f1 0.8364, precision 0.8366, recall 0.8362, auc 0.8364
epoch 7001, loss 0.3959, train acc 83.66%, f1 0.8366, precision 0.8366, recall 0.8365, auc 0.8366
epoch 7101, loss 0.3711, train acc 83.72%, f1 0.8372, precision 0.8371, recall 0.8372, auc 0.8372
epoch 7201, loss 0.3900, train acc 83.76%, f1 0.8376, precision 0.8377, recall 0.8375, auc 0.8376
epoch 7301, loss 0.3422, train acc 83.79%, f1 0.8379, precision 0.8380, recall 0.8378, auc 0.8379
epoch 7401, loss 0.3039, train acc 83.85%, f1 0.8385, precision 0.8385, recall 0.8385, auc 0.8385
epoch 7501, loss 0.3974, train acc 83.86%, f1 0.8386, precision 0.8386, recall 0.8386, auc 0.8386
epoch 7601, loss 0.4008, train acc 83.87%, f1 0.8387, precision 0.8388, recall 0.8385, auc 0.8387
epoch 7701, loss 0.3433, train acc 83.86%, f1 0.8385, precision 0.8386, recall 0.8385, auc 0.8386
epoch 7801, loss 0.3206, train acc 83.95%, f1 0.8395, precision 0.8395, recall 0.8395, auc 0.8395
epoch 7901, loss 0.3815, train acc 83.93%, f1 0.8393, precision 0.8393, recall 0.8393, auc 0.8393
epoch 8001, loss 0.3418, train acc 83.95%, f1 0.8395, precision 0.8395, recall 0.8395, auc 0.8395
epoch 8101, loss 0.2989, train acc 83.96%, f1 0.8396, precision 0.8398, recall 0.8395, auc 0.8396
epoch 8201, loss 0.3392, train acc 84.00%, f1 0.8400, precision 0.8400, recall 0.8400, auc 0.8400
epoch 8301, loss 0.2668, train acc 84.07%, f1 0.8407, precision 0.8407, recall 0.8406, auc 0.8407
epoch 8401, loss 0.3242, train acc 84.09%, f1 0.8408, precision 0.8411, recall 0.8406, auc 0.8409
epoch 8501, loss 0.3625, train acc 84.12%, f1 0.8412, precision 0.8413, recall 0.8411, auc 0.8412
epoch 8601, loss 0.3047, train acc 84.10%, f1 0.8410, precision 0.8411, recall 0.8408, auc 0.8410
epoch 8701, loss 0.4281, train acc 84.15%, f1 0.8415, precision 0.8414, recall 0.8416, auc 0.8415
epoch 8801, loss 0.3707, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 8901, loss 0.3701, train acc 84.20%, f1 0.8420, precision 0.8420, recall 0.8420, auc 0.8420
epoch 9001, loss 0.3051, train acc 84.22%, f1 0.8422, precision 0.8423, recall 0.8422, auc 0.8422
epoch 9101, loss 0.3934, train acc 84.25%, f1 0.8425, precision 0.8425, recall 0.8426, auc 0.8425
epoch 9201, loss 0.3122, train acc 84.23%, f1 0.8423, precision 0.8423, recall 0.8423, auc 0.8423
epoch 9301, loss 0.3907, train acc 84.22%, f1 0.8422, precision 0.8420, recall 0.8424, auc 0.8422
epoch 9401, loss 0.2939, train acc 84.26%, f1 0.8425, precision 0.8426, recall 0.8424, auc 0.8426
epoch 9501, loss 0.3190, train acc 84.22%, f1 0.8422, precision 0.8424, recall 0.8420, auc 0.8422
epoch 9601, loss 0.2175, train acc 84.33%, f1 0.8433, precision 0.8433, recall 0.8433, auc 0.8433
epoch 9701, loss 0.2975, train acc 84.41%, f1 0.8441, precision 0.8440, recall 0.8442, auc 0.8441
epoch 9801, loss 0.3529, train acc 84.36%, f1 0.8436, precision 0.8435, recall 0.8437, auc 0.8436
epoch 9901, loss 0.3046, train acc 84.35%, f1 0.8435, precision 0.8435, recall 0.8436, auc 0.8435
epoch 10001, loss 0.4555, train acc 84.47%, f1 0.8447, precision 0.8447, recall 0.8447, auc 0.8447
epoch 10101, loss 0.3223, train acc 84.43%, f1 0.8443, precision 0.8443, recall 0.8442, auc 0.8443
epoch 10201, loss 0.3565, train acc 84.49%, f1 0.8449, precision 0.8448, recall 0.8450, auc 0.8449
epoch 10301, loss 0.3560, train acc 84.41%, f1 0.8441, precision 0.8441, recall 0.8441, auc 0.8441
epoch 10401, loss 0.3402, train acc 84.53%, f1 0.8452, precision 0.8454, recall 0.8451, auc 0.8453
epoch 10501, loss 0.3353, train acc 84.55%, f1 0.8455, precision 0.8455, recall 0.8455, auc 0.8455
epoch 10601, loss 0.3197, train acc 84.55%, f1 0.8455, precision 0.8455, recall 0.8455, auc 0.8455
epoch 10701, loss 0.2635, train acc 84.57%, f1 0.8457, precision 0.8458, recall 0.8455, auc 0.8457
epoch 10801, loss 0.2854, train acc 84.59%, f1 0.8458, precision 0.8459, recall 0.8458, auc 0.8459
epoch 10901, loss 0.3819, train acc 84.54%, f1 0.8454, precision 0.8454, recall 0.8454, auc 0.8454
epoch 11001, loss 0.3268, train acc 84.62%, f1 0.8461, precision 0.8462, recall 0.8461, auc 0.8462
epoch 11101, loss 0.2923, train acc 84.59%, f1 0.8459, precision 0.8459, recall 0.8458, auc 0.8459
epoch 11201, loss 0.4886, train acc 84.62%, f1 0.8461, precision 0.8464, recall 0.8458, auc 0.8462
epoch 11301, loss 0.3203, train acc 84.64%, f1 0.8464, precision 0.8464, recall 0.8464, auc 0.8464
epoch 11401, loss 0.2827, train acc 84.68%, f1 0.8468, precision 0.8467, recall 0.8468, auc 0.8468
epoch 11501, loss 0.3499, train acc 84.73%, f1 0.8474, precision 0.8473, recall 0.8474, auc 0.8473
epoch 11601, loss 0.2500, train acc 84.70%, f1 0.8470, precision 0.8471, recall 0.8469, auc 0.8470
epoch 11701, loss 0.3023, train acc 84.75%, f1 0.8475, precision 0.8474, recall 0.8475, auc 0.8475
epoch 11801, loss 0.2975, train acc 84.73%, f1 0.8473, precision 0.8472, recall 0.8473, auc 0.8473
epoch 11901, loss 0.3293, train acc 84.71%, f1 0.8471, precision 0.8472, recall 0.8470, auc 0.8471
epoch 12001, loss 0.3404, train acc 84.79%, f1 0.8480, precision 0.8478, recall 0.8481, auc 0.8479
epoch 12101, loss 0.4172, train acc 84.77%, f1 0.8477, precision 0.8478, recall 0.8477, auc 0.8477
epoch 12201, loss 0.3567, train acc 84.82%, f1 0.8482, precision 0.8482, recall 0.8481, auc 0.8482
epoch 12301, loss 0.2897, train acc 84.83%, f1 0.8483, precision 0.8483, recall 0.8482, auc 0.8483
epoch 12401, loss 0.3151, train acc 84.82%, f1 0.8482, precision 0.8483, recall 0.8482, auc 0.8482
epoch 12501, loss 0.3901, train acc 84.83%, f1 0.8483, precision 0.8483, recall 0.8483, auc 0.8483
epoch 12601, loss 0.2785, train acc 84.86%, f1 0.8486, precision 0.8487, recall 0.8486, auc 0.8486
epoch 12701, loss 0.2854, train acc 84.85%, f1 0.8485, precision 0.8485, recall 0.8485, auc 0.8485
epoch 12801, loss 0.2766, train acc 84.83%, f1 0.8483, precision 0.8483, recall 0.8483, auc 0.8483
epoch 12901, loss 0.4614, train acc 84.86%, f1 0.8486, precision 0.8485, recall 0.8486, auc 0.8486
epoch 13001, loss 0.3562, train acc 84.88%, f1 0.8488, precision 0.8488, recall 0.8487, auc 0.8488
epoch 13101, loss 0.2791, train acc 84.88%, f1 0.8488, precision 0.8488, recall 0.8489, auc 0.8488
epoch 13201, loss 0.3820, train acc 84.92%, f1 0.8492, precision 0.8491, recall 0.8492, auc 0.8492
epoch 13301, loss 0.2855, train acc 84.86%, f1 0.8486, precision 0.8486, recall 0.8485, auc 0.8486
epoch 13401, loss 0.2936, train acc 84.98%, f1 0.8498, precision 0.8498, recall 0.8498, auc 0.8498
epoch 13501, loss 0.3534, train acc 84.99%, f1 0.8499, precision 0.8499, recall 0.8499, auc 0.8499
epoch 13601, loss 0.3581, train acc 84.92%, f1 0.8492, precision 0.8492, recall 0.8492, auc 0.8492
epoch 13701, loss 0.3940, train acc 84.94%, f1 0.8494, precision 0.8494, recall 0.8494, auc 0.8494
epoch 13801, loss 0.3421, train acc 84.99%, f1 0.8499, precision 0.8499, recall 0.8499, auc 0.8499
epoch 13901, loss 0.3243, train acc 84.99%, f1 0.8499, precision 0.8499, recall 0.8498, auc 0.8499
epoch 14001, loss 0.3534, train acc 85.02%, f1 0.8502, precision 0.8503, recall 0.8500, auc 0.8502
epoch 14101, loss 0.3204, train acc 85.02%, f1 0.8502, precision 0.8502, recall 0.8501, auc 0.8502
epoch 14201, loss 0.3002, train acc 85.06%, f1 0.8507, precision 0.8506, recall 0.8508, auc 0.8506
epoch 14301, loss 0.2600, train acc 85.03%, f1 0.8503, precision 0.8502, recall 0.8503, auc 0.8503
epoch 14401, loss 0.3225, train acc 85.04%, f1 0.8504, precision 0.8503, recall 0.8504, auc 0.8504
epoch 14501, loss 0.3905, train acc 85.14%, f1 0.8514, precision 0.8513, recall 0.8515, auc 0.8514
epoch 14601, loss 0.3225, train acc 85.07%, f1 0.8507, precision 0.8508, recall 0.8506, auc 0.8507
epoch 14701, loss 0.2796, train acc 85.06%, f1 0.8505, precision 0.8506, recall 0.8504, auc 0.8506
epoch 14801, loss 0.3033, train acc 85.00%, f1 0.8500, precision 0.8500, recall 0.8499, auc 0.8500
epoch 14901, loss 0.3785, train acc 84.97%, f1 0.8497, precision 0.8498, recall 0.8497, auc 0.8497
epoch 15001, loss 0.4150, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8504, auc 0.8505
epoch 15101, loss 0.3117, train acc 85.04%, f1 0.8504, precision 0.8503, recall 0.8505, auc 0.8504
epoch 15201, loss 0.3336, train acc 85.13%, f1 0.8514, precision 0.8513, recall 0.8515, auc 0.8513
epoch 15301, loss 0.3830, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 15401, loss 0.3500, train acc 85.06%, f1 0.8506, precision 0.8506, recall 0.8506, auc 0.8506
epoch 15501, loss 0.3570, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8507, auc 0.8506
epoch 15601, loss 0.2855, train acc 85.14%, f1 0.8514, precision 0.8515, recall 0.8514, auc 0.8514
epoch 15701, loss 0.3344, train acc 85.10%, f1 0.8510, precision 0.8510, recall 0.8511, auc 0.8510
epoch 15801, loss 0.3267, train acc 85.10%, f1 0.8510, precision 0.8510, recall 0.8511, auc 0.8510
epoch 15901, loss 0.4319, train acc 85.10%, f1 0.8510, precision 0.8509, recall 0.8510, auc 0.8510
epoch 16001, loss 0.4236, train acc 85.17%, f1 0.8517, precision 0.8516, recall 0.8518, auc 0.8517
epoch 16101, loss 0.3552, train acc 85.15%, f1 0.8516, precision 0.8515, recall 0.8516, auc 0.8515
epoch 16201, loss 0.3169, train acc 85.10%, f1 0.8510, precision 0.8510, recall 0.8510, auc 0.8510
epoch 16301, loss 0.2653, train acc 85.12%, f1 0.8511, precision 0.8513, recall 0.8510, auc 0.8512
epoch 16401, loss 0.2402, train acc 85.13%, f1 0.8513, precision 0.8513, recall 0.8513, auc 0.8513
epoch 16501, loss 0.4131, train acc 85.11%, f1 0.8511, precision 0.8510, recall 0.8512, auc 0.8511/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.2208, train acc 85.15%, f1 0.8515, precision 0.8515, recall 0.8515, auc 0.8515
epoch 16701, loss 0.3224, train acc 85.19%, f1 0.8519, precision 0.8519, recall 0.8520, auc 0.8519
epoch 16801, loss 0.2740, train acc 85.20%, f1 0.8520, precision 0.8520, recall 0.8520, auc 0.8520
epoch 16901, loss 0.2937, train acc 85.20%, f1 0.8520, precision 0.8521, recall 0.8520, auc 0.8520
epoch 17001, loss 0.3614, train acc 85.15%, f1 0.8515, precision 0.8515, recall 0.8515, auc 0.8515
epoch 17101, loss 0.2900, train acc 85.21%, f1 0.8521, precision 0.8520, recall 0.8521, auc 0.8521
epoch 17201, loss 0.3773, train acc 85.23%, f1 0.8524, precision 0.8523, recall 0.8525, auc 0.8523
epoch 17301, loss 0.3437, train acc 85.27%, f1 0.8527, precision 0.8527, recall 0.8527, auc 0.8527
epoch 17401, loss 0.4197, train acc 85.17%, f1 0.8517, precision 0.8518, recall 0.8516, auc 0.8517
epoch 17501, loss 0.3620, train acc 85.17%, f1 0.8518, precision 0.8517, recall 0.8518, auc 0.8517
epoch 17601, loss 0.3569, train acc 85.17%, f1 0.8517, precision 0.8518, recall 0.8516, auc 0.8517
epoch 17701, loss 0.2979, train acc 85.25%, f1 0.8525, precision 0.8525, recall 0.8525, auc 0.8525
epoch 17801, loss 0.3655, train acc 85.19%, f1 0.8519, precision 0.8519, recall 0.8520, auc 0.8519
epoch 17901, loss 0.3905, train acc 85.17%, f1 0.8517, precision 0.8518, recall 0.8517, auc 0.8517
epoch 18001, loss 0.3051, train acc 85.17%, f1 0.8517, precision 0.8517, recall 0.8517, auc 0.8517
epoch 18101, loss 0.2838, train acc 85.28%, f1 0.8528, precision 0.8529, recall 0.8527, auc 0.8528
epoch 18201, loss 0.3241, train acc 85.26%, f1 0.8526, precision 0.8525, recall 0.8527, auc 0.8526
epoch 18301, loss 0.3207, train acc 85.22%, f1 0.8522, precision 0.8522, recall 0.8523, auc 0.8522
epoch 18401, loss 0.3909, train acc 85.22%, f1 0.8523, precision 0.8522, recall 0.8523, auc 0.8522
epoch 18501, loss 0.2951, train acc 85.29%, f1 0.8529, precision 0.8528, recall 0.8531, auc 0.8529
epoch 18601, loss 0.3041, train acc 85.24%, f1 0.8524, precision 0.8524, recall 0.8525, auc 0.8524
epoch 18701, loss 0.3230, train acc 85.23%, f1 0.8523, precision 0.8524, recall 0.8522, auc 0.8523
epoch 18801, loss 0.2818, train acc 85.23%, f1 0.8523, precision 0.8522, recall 0.8523, auc 0.8523
epoch 18901, loss 0.2756, train acc 85.26%, f1 0.8527, precision 0.8526, recall 0.8527, auc 0.8526
epoch 19001, loss 0.4536, train acc 85.23%, f1 0.8524, precision 0.8523, recall 0.8524, auc 0.8523
epoch 19101, loss 0.2541, train acc 85.22%, f1 0.8521, precision 0.8522, recall 0.8521, auc 0.8522
epoch 19201, loss 0.3924, train acc 85.24%, f1 0.8524, precision 0.8524, recall 0.8525, auc 0.8524
epoch 19301, loss 0.3336, train acc 85.24%, f1 0.8524, precision 0.8526, recall 0.8522, auc 0.8524
epoch 19401, loss 0.3113, train acc 85.24%, f1 0.8524, precision 0.8524, recall 0.8525, auc 0.8524
epoch 19501, loss 0.3431, train acc 85.35%, f1 0.8535, precision 0.8536, recall 0.8534, auc 0.8535
epoch 19601, loss 0.4203, train acc 85.29%, f1 0.8529, precision 0.8529, recall 0.8530, auc 0.8529
epoch 19701, loss 0.2548, train acc 85.29%, f1 0.8529, precision 0.8529, recall 0.8529, auc 0.8529
epoch 19801, loss 0.3371, train acc 85.24%, f1 0.8524, precision 0.8524, recall 0.8525, auc 0.8524
epoch 19901, loss 0.4314, train acc 85.22%, f1 0.8522, precision 0.8522, recall 0.8522, auc 0.8522
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_3
./test_pima/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6699999999999999

the Fscore is 0.6206896551724138

the precision is 0.45

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_3
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6111, train acc 77.45%, f1 0.7745, precision 0.7746, recall 0.7745, auc 0.7745
epoch 201, loss 0.5122, train acc 79.96%, f1 0.7996, precision 0.7996, recall 0.7996, auc 0.7996
epoch 301, loss 0.4254, train acc 81.37%, f1 0.8137, precision 0.8137, recall 0.8137, auc 0.8137
epoch 401, loss 0.4430, train acc 82.45%, f1 0.8245, precision 0.8245, recall 0.8245, auc 0.8245
epoch 501, loss 0.4320, train acc 82.85%, f1 0.8285, precision 0.8285, recall 0.8285, auc 0.8285
epoch 601, loss 0.4266, train acc 82.95%, f1 0.8295, precision 0.8295, recall 0.8295, auc 0.8295
epoch 701, loss 0.3633, train acc 83.04%, f1 0.8304, precision 0.8304, recall 0.8304, auc 0.8304
epoch 801, loss 0.4374, train acc 83.04%, f1 0.8304, precision 0.8304, recall 0.8304, auc 0.8304
epoch 901, loss 0.3803, train acc 83.10%, f1 0.8310, precision 0.8310, recall 0.8310, auc 0.8310
epoch 1001, loss 0.2984, train acc 83.09%, f1 0.8309, precision 0.8309, recall 0.8309, auc 0.8309
epoch 1101, loss 0.3796, train acc 83.09%, f1 0.8309, precision 0.8309, recall 0.8309, auc 0.8309
epoch 1201, loss 0.4868, train acc 83.10%, f1 0.8310, precision 0.8310, recall 0.8310, auc 0.8310
epoch 1301, loss 0.4550, train acc 83.12%, f1 0.8312, precision 0.8312, recall 0.8312, auc 0.8312
epoch 1401, loss 0.4964, train acc 83.16%, f1 0.8316, precision 0.8316, recall 0.8316, auc 0.8316
epoch 1501, loss 0.3677, train acc 83.17%, f1 0.8317, precision 0.8317, recall 0.8317, auc 0.8317
epoch 1601, loss 0.3225, train acc 83.17%, f1 0.8317, precision 0.8317, recall 0.8317, auc 0.8317
epoch 1701, loss 0.4078, train acc 83.22%, f1 0.8322, precision 0.8322, recall 0.8322, auc 0.8322
epoch 1801, loss 0.3810, train acc 83.19%, f1 0.8319, precision 0.8319, recall 0.8319, auc 0.8319
epoch 1901, loss 0.3189, train acc 83.19%, f1 0.8319, precision 0.8319, recall 0.8318, auc 0.8319
epoch 2001, loss 0.4351, train acc 83.15%, f1 0.8315, precision 0.8315, recall 0.8314, auc 0.8315
epoch 2101, loss 0.3933, train acc 83.15%, f1 0.8315, precision 0.8315, recall 0.8316, auc 0.8315
epoch 2201, loss 0.3649, train acc 83.11%, f1 0.8311, precision 0.8310, recall 0.8312, auc 0.8311
epoch 2301, loss 0.3884, train acc 83.14%, f1 0.8314, precision 0.8314, recall 0.8314, auc 0.8314
epoch 2401, loss 0.4559, train acc 83.16%, f1 0.8316, precision 0.8316, recall 0.8316, auc 0.8316
epoch 2501, loss 0.3559, train acc 83.11%, f1 0.8311, precision 0.8311, recall 0.8311, auc 0.8311
epoch 2601, loss 0.2434, train acc 83.14%, f1 0.8314, precision 0.8313, recall 0.8315, auc 0.8314
epoch 2701, loss 0.3483, train acc 83.18%, f1 0.8318, precision 0.8318, recall 0.8317, auc 0.8318
epoch 2801, loss 0.4131, train acc 83.18%, f1 0.8318, precision 0.8319, recall 0.8317, auc 0.8318
epoch 2901, loss 0.4076, train acc 83.20%, f1 0.8320, precision 0.8320, recall 0.8320, auc 0.8320
epoch 3001, loss 0.4035, train acc 83.17%, f1 0.8317, precision 0.8316, recall 0.8317, auc 0.8317
epoch 3101, loss 0.3131, train acc 83.16%, f1 0.8316, precision 0.8314, recall 0.8318, auc 0.8316
epoch 3201, loss 0.3423, train acc 83.18%, f1 0.8319, precision 0.8314, recall 0.8323, auc 0.8318
epoch 3301, loss 0.3459, train acc 83.08%, f1 0.8308, precision 0.8308, recall 0.8309, auc 0.8308
epoch 3401, loss 0.4175, train acc 83.13%, f1 0.8314, precision 0.8311, recall 0.8318, auc 0.8313
epoch 3501, loss 0.4045, train acc 83.16%, f1 0.8316, precision 0.8315, recall 0.8317, auc 0.8316
epoch 3601, loss 0.2990, train acc 83.12%, f1 0.8312, precision 0.8312, recall 0.8312, auc 0.8312
epoch 3701, loss 0.2988, train acc 83.15%, f1 0.8316, precision 0.8311, recall 0.8320, auc 0.8315
epoch 3801, loss 0.3044, train acc 83.17%, f1 0.8317, precision 0.8315, recall 0.8320, auc 0.8317
epoch 3901, loss 0.3050, train acc 83.13%, f1 0.8313, precision 0.8313, recall 0.8312, auc 0.8313
epoch 4001, loss 0.3404, train acc 83.15%, f1 0.8315, precision 0.8316, recall 0.8314, auc 0.8315
epoch 4101, loss 0.3765, train acc 83.11%, f1 0.8312, precision 0.8309, recall 0.8314, auc 0.8311
epoch 4201, loss 0.4450, train acc 83.14%, f1 0.8315, precision 0.8310, recall 0.8320, auc 0.8314
epoch 4301, loss 0.3262, train acc 83.24%, f1 0.8324, precision 0.8324, recall 0.8324, auc 0.8324
epoch 4401, loss 0.3144, train acc 83.22%, f1 0.8323, precision 0.8319, recall 0.8326, auc 0.8322
epoch 4501, loss 0.3459, train acc 83.23%, f1 0.8324, precision 0.8322, recall 0.8326, auc 0.8323
epoch 4601, loss 0.4300, train acc 83.25%, f1 0.8325, precision 0.8325, recall 0.8324, auc 0.8325
epoch 4701, loss 0.3535, train acc 83.27%, f1 0.8327, precision 0.8324, recall 0.8330, auc 0.8327
epoch 4801, loss 0.2774, train acc 83.30%, f1 0.8330, precision 0.8328, recall 0.8333, auc 0.8330
epoch 4901, loss 0.3882, train acc 83.30%, f1 0.8331, precision 0.8326, recall 0.8335, auc 0.8330
epoch 5001, loss 0.3543, train acc 83.32%, f1 0.8333, precision 0.8331, recall 0.8335, auc 0.8332
epoch 5101, loss 0.4016, train acc 83.27%, f1 0.8328, precision 0.8324, recall 0.8332, auc 0.8327
epoch 5201, loss 0.3677, train acc 83.30%, f1 0.8330, precision 0.8328, recall 0.8332, auc 0.8330
epoch 5301, loss 0.3938, train acc 83.33%, f1 0.8332, precision 0.8336, recall 0.8327, auc 0.8333
epoch 5401, loss 0.3024, train acc 83.34%, f1 0.8333, precision 0.8341, recall 0.8325, auc 0.8334
epoch 5501, loss 0.4005, train acc 83.28%, f1 0.8331, precision 0.8317, recall 0.8346, auc 0.8328
epoch 5601, loss 0.4164, train acc 83.40%, f1 0.8341, precision 0.8334, recall 0.8349, auc 0.8340
epoch 5701, loss 0.3106, train acc 83.39%, f1 0.8340, precision 0.8336, recall 0.8345, auc 0.8339
epoch 5801, loss 0.2862, train acc 83.45%, f1 0.8346, precision 0.8342, recall 0.8350, auc 0.8345
epoch 5901, loss 0.3909, train acc 83.47%, f1 0.8347, precision 0.8348, recall 0.8346, auc 0.8347
epoch 6001, loss 0.4138, train acc 83.43%, f1 0.8344, precision 0.8340, recall 0.8349, auc 0.8343
epoch 6101, loss 0.2852, train acc 83.49%, f1 0.8350, precision 0.8344, recall 0.8356, auc 0.8349
epoch 6201, loss 0.3583, train acc 83.51%, f1 0.8353, precision 0.8346, recall 0.8360, auc 0.8351
epoch 6301, loss 0.3270, train acc 83.54%, f1 0.8355, precision 0.8349, recall 0.8360, auc 0.8354
epoch 6401, loss 0.3571, train acc 83.51%, f1 0.8352, precision 0.8347, recall 0.8356, auc 0.8351
epoch 6501, loss 0.3309, train acc 83.56%, f1 0.8357, precision 0.8353, recall 0.8361, auc 0.8356
epoch 6601, loss 0.4325, train acc 83.62%, f1 0.8363, precision 0.8358, recall 0.8368, auc 0.8362
epoch 6701, loss 0.3695, train acc 83.64%, f1 0.8363, precision 0.8365, recall 0.8362, auc 0.8364
epoch 6801, loss 0.3055, train acc 83.64%, f1 0.8364, precision 0.8366, recall 0.8361, auc 0.8364
epoch 6901, loss 0.3483, train acc 83.64%, f1 0.8366, precision 0.8352, recall 0.8381, auc 0.8364
epoch 7001, loss 0.5499, train acc 83.67%, f1 0.8367, precision 0.8366, recall 0.8368, auc 0.8367
epoch 7101, loss 0.5010, train acc 83.71%, f1 0.8373, precision 0.8363, recall 0.8383, auc 0.8371
epoch 7201, loss 0.2943, train acc 83.77%, f1 0.8378, precision 0.8373, recall 0.8384, auc 0.8377
epoch 7301, loss 0.3585, train acc 83.76%, f1 0.8376, precision 0.8379, recall 0.8372, auc 0.8376
epoch 7401, loss 0.3113, train acc 83.77%, f1 0.8377, precision 0.8378, recall 0.8377, auc 0.8377
epoch 7501, loss 0.3588, train acc 83.80%, f1 0.8380, precision 0.8380, recall 0.8381, auc 0.8380
epoch 7601, loss 0.2780, train acc 83.86%, f1 0.8387, precision 0.8384, recall 0.8390, auc 0.8386
epoch 7701, loss 0.3854, train acc 83.81%, f1 0.8383, precision 0.8373, recall 0.8393, auc 0.8381
epoch 7801, loss 0.2777, train acc 83.88%, f1 0.8390, precision 0.8383, recall 0.8396, auc 0.8388
epoch 7901, loss 0.3780, train acc 83.93%, f1 0.8393, precision 0.8394, recall 0.8392, auc 0.8393
epoch 8001, loss 0.3276, train acc 83.99%, f1 0.8399, precision 0.8397, recall 0.8401, auc 0.8399
epoch 8101, loss 0.3888, train acc 83.99%, f1 0.8400, precision 0.8395, recall 0.8405, auc 0.8399
epoch 8201, loss 0.3499, train acc 83.98%, f1 0.8398, precision 0.8398, recall 0.8397, auc 0.8398/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.3862, train acc 84.02%, f1 0.8403, precision 0.8396, recall 0.8411, auc 0.8402
epoch 8401, loss 0.3036, train acc 84.07%, f1 0.8407, precision 0.8406, recall 0.8408, auc 0.8407
epoch 8501, loss 0.3256, train acc 84.10%, f1 0.8411, precision 0.8408, recall 0.8414, auc 0.8410
epoch 8601, loss 0.3438, train acc 84.13%, f1 0.8414, precision 0.8411, recall 0.8416, auc 0.8413
epoch 8701, loss 0.4038, train acc 84.14%, f1 0.8413, precision 0.8417, recall 0.8409, auc 0.8414
epoch 8801, loss 0.2596, train acc 84.12%, f1 0.8412, precision 0.8413, recall 0.8410, auc 0.8412
epoch 8901, loss 0.3556, train acc 84.17%, f1 0.8417, precision 0.8415, recall 0.8420, auc 0.8417
epoch 9001, loss 0.2649, train acc 84.20%, f1 0.8419, precision 0.8421, recall 0.8418, auc 0.8420
epoch 9101, loss 0.4179, train acc 84.20%, f1 0.8421, precision 0.8417, recall 0.8424, auc 0.8420
epoch 9201, loss 0.4005, train acc 84.24%, f1 0.8425, precision 0.8419, recall 0.8430, auc 0.8424
epoch 9301, loss 0.3209, train acc 84.22%, f1 0.8421, precision 0.8425, recall 0.8417, auc 0.8422
epoch 9401, loss 0.3288, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8421, auc 0.8423
epoch 9501, loss 0.4670, train acc 84.28%, f1 0.8428, precision 0.8428, recall 0.8428, auc 0.8428
epoch 9601, loss 0.3639, train acc 84.36%, f1 0.8436, precision 0.8436, recall 0.8435, auc 0.8436
epoch 9701, loss 0.4042, train acc 84.41%, f1 0.8441, precision 0.8443, recall 0.8438, auc 0.8441
epoch 9801, loss 0.3959, train acc 84.34%, f1 0.8434, precision 0.8432, recall 0.8436, auc 0.8434
epoch 9901, loss 0.2418, train acc 84.40%, f1 0.8440, precision 0.8439, recall 0.8442, auc 0.8440
epoch 10001, loss 0.3577, train acc 84.35%, f1 0.8434, precision 0.8439, recall 0.8430, auc 0.8435
epoch 10101, loss 0.3575, train acc 84.42%, f1 0.8442, precision 0.8442, recall 0.8442, auc 0.8442
epoch 10201, loss 0.3783, train acc 84.45%, f1 0.8444, precision 0.8449, recall 0.8439, auc 0.8445
epoch 10301, loss 0.3274, train acc 84.46%, f1 0.8445, precision 0.8450, recall 0.8440, auc 0.8446
epoch 10401, loss 0.3589, train acc 84.47%, f1 0.8448, precision 0.8444, recall 0.8452, auc 0.8447
epoch 10501, loss 0.3337, train acc 84.50%, f1 0.8450, precision 0.8449, recall 0.8452, auc 0.8450
epoch 10601, loss 0.3344, train acc 84.60%, f1 0.8460, precision 0.8459, recall 0.8461, auc 0.8460
epoch 10701, loss 0.3418, train acc 84.49%, f1 0.8449, precision 0.8448, recall 0.8449, auc 0.8449
epoch 10801, loss 0.2707, train acc 84.55%, f1 0.8455, precision 0.8453, recall 0.8457, auc 0.8455
epoch 10901, loss 0.4615, train acc 84.52%, f1 0.8453, precision 0.8451, recall 0.8454, auc 0.8452
epoch 11001, loss 0.3708, train acc 84.60%, f1 0.8461, precision 0.8453, recall 0.8470, auc 0.8460
epoch 11101, loss 0.3421, train acc 84.59%, f1 0.8459, precision 0.8462, recall 0.8455, auc 0.8459
epoch 11201, loss 0.3875, train acc 84.59%, f1 0.8459, precision 0.8458, recall 0.8459, auc 0.8459
epoch 11301, loss 0.3662, train acc 84.68%, f1 0.8467, precision 0.8469, recall 0.8466, auc 0.8468
epoch 11401, loss 0.3320, train acc 84.70%, f1 0.8470, precision 0.8467, recall 0.8474, auc 0.8470
epoch 11501, loss 0.2750, train acc 84.71%, f1 0.8471, precision 0.8471, recall 0.8472, auc 0.8471
epoch 11601, loss 0.3211, train acc 84.68%, f1 0.8469, precision 0.8465, recall 0.8473, auc 0.8468
epoch 11701, loss 0.2524, train acc 84.77%, f1 0.8476, precision 0.8479, recall 0.8474, auc 0.8477
epoch 11801, loss 0.3311, train acc 84.83%, f1 0.8484, precision 0.8480, recall 0.8487, auc 0.8483
epoch 11901, loss 0.3051, train acc 84.77%, f1 0.8477, precision 0.8476, recall 0.8477, auc 0.8477
epoch 12001, loss 0.3860, train acc 84.89%, f1 0.8489, precision 0.8492, recall 0.8486, auc 0.8489
epoch 12101, loss 0.3379, train acc 84.85%, f1 0.8484, precision 0.8486, recall 0.8483, auc 0.8485
epoch 12201, loss 0.3216, train acc 84.80%, f1 0.8480, precision 0.8480, recall 0.8480, auc 0.8480
epoch 12301, loss 0.3856, train acc 84.79%, f1 0.8479, precision 0.8479, recall 0.8478, auc 0.8479
epoch 12401, loss 0.3707, train acc 84.79%, f1 0.8479, precision 0.8477, recall 0.8482, auc 0.8479
epoch 12501, loss 0.3893, train acc 84.88%, f1 0.8487, precision 0.8490, recall 0.8484, auc 0.8488
epoch 12601, loss 0.2781, train acc 84.85%, f1 0.8485, precision 0.8485, recall 0.8485, auc 0.8485
epoch 12701, loss 0.4047, train acc 84.89%, f1 0.8489, precision 0.8487, recall 0.8491, auc 0.8489
epoch 12801, loss 0.2602, train acc 84.83%, f1 0.8484, precision 0.8480, recall 0.8488, auc 0.8483
epoch 12901, loss 0.3262, train acc 84.89%, f1 0.8489, precision 0.8486, recall 0.8493, auc 0.8489
epoch 13001, loss 0.4197, train acc 84.95%, f1 0.8495, precision 0.8495, recall 0.8495, auc 0.8495
epoch 13101, loss 0.3822, train acc 84.92%, f1 0.8493, precision 0.8489, recall 0.8497, auc 0.8492
epoch 13201, loss 0.4397, train acc 84.97%, f1 0.8497, precision 0.8497, recall 0.8497, auc 0.8497
epoch 13301, loss 0.2740, train acc 84.91%, f1 0.8491, precision 0.8491, recall 0.8491, auc 0.8491
epoch 13401, loss 0.2568, train acc 84.95%, f1 0.8495, precision 0.8494, recall 0.8496, auc 0.8495
epoch 13501, loss 0.2222, train acc 84.98%, f1 0.8498, precision 0.8496, recall 0.8501, auc 0.8498
epoch 13601, loss 0.3517, train acc 85.00%, f1 0.8500, precision 0.8498, recall 0.8502, auc 0.8500
epoch 13701, loss 0.3858, train acc 84.97%, f1 0.8497, precision 0.8497, recall 0.8497, auc 0.8497
epoch 13801, loss 0.3570, train acc 85.07%, f1 0.8508, precision 0.8507, recall 0.8508, auc 0.8507
epoch 13901, loss 0.4222, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8505
epoch 14001, loss 0.3389, train acc 85.03%, f1 0.8503, precision 0.8502, recall 0.8505, auc 0.8503
epoch 14101, loss 0.3394, train acc 84.99%, f1 0.8499, precision 0.8497, recall 0.8502, auc 0.8499
epoch 14201, loss 0.4594, train acc 84.95%, f1 0.8495, precision 0.8498, recall 0.8492, auc 0.8495
epoch 14301, loss 0.3850, train acc 85.04%, f1 0.8504, precision 0.8504, recall 0.8505, auc 0.8504
epoch 14401, loss 0.3667, train acc 85.08%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8508
epoch 14501, loss 0.3487, train acc 85.09%, f1 0.8509, precision 0.8505, recall 0.8514, auc 0.8509
epoch 14601, loss 0.2689, train acc 85.07%, f1 0.8507, precision 0.8506, recall 0.8508, auc 0.8507
epoch 14701, loss 0.4847, train acc 85.07%, f1 0.8507, precision 0.8506, recall 0.8508, auc 0.8507
epoch 14801, loss 0.3194, train acc 85.08%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8508
epoch 14901, loss 0.3338, train acc 85.15%, f1 0.8516, precision 0.8514, recall 0.8518, auc 0.8515
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_3
./test_pima/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.62

the Fscore is 0.5869565217391305

the precision is 0.4153846153846154

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_3
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6181, train acc 77.53%, f1 0.7631, precision 0.8068, recall 0.7238, auc 0.7753
epoch 201, loss 0.5245, train acc 79.87%, f1 0.7974, precision 0.8025, recall 0.7924, auc 0.7987
epoch 301, loss 0.4039, train acc 81.68%, f1 0.8169, precision 0.8166, recall 0.8172, auc 0.8168
epoch 401, loss 0.3744, train acc 82.50%, f1 0.8257, precision 0.8222, recall 0.8293, auc 0.8250
epoch 501, loss 0.3906, train acc 82.85%, f1 0.8294, precision 0.8253, recall 0.8335, auc 0.8285
epoch 601, loss 0.3229, train acc 82.97%, f1 0.8307, precision 0.8258, recall 0.8357, auc 0.8297
epoch 701, loss 0.3770, train acc 83.04%, f1 0.8315, precision 0.8266, recall 0.8364, auc 0.8304
epoch 801, loss 0.2626, train acc 83.11%, f1 0.8318, precision 0.8281, recall 0.8356, auc 0.8311
epoch 901, loss 0.4097, train acc 83.15%, f1 0.8323, precision 0.8284, recall 0.8363, auc 0.8315
epoch 1001, loss 0.3237, train acc 83.16%, f1 0.8322, precision 0.8295, recall 0.8349, auc 0.8316
epoch 1101, loss 0.3604, train acc 83.11%, f1 0.8316, precision 0.8291, recall 0.8341, auc 0.8311
epoch 1201, loss 0.4774, train acc 83.11%, f1 0.8318, precision 0.8283, recall 0.8353, auc 0.8311
epoch 1301, loss 0.4600, train acc 83.16%, f1 0.8321, precision 0.8295, recall 0.8348, auc 0.8316
epoch 1401, loss 0.3146, train acc 83.15%, f1 0.8318, precision 0.8305, recall 0.8331, auc 0.8315
epoch 1501, loss 0.4818, train acc 83.15%, f1 0.8319, precision 0.8297, recall 0.8342, auc 0.8315
epoch 1601, loss 0.3880, train acc 83.19%, f1 0.8320, precision 0.8312, recall 0.8329, auc 0.8319
epoch 1701, loss 0.3888, train acc 83.13%, f1 0.8317, precision 0.8299, recall 0.8335, auc 0.8313
epoch 1801, loss 0.5560, train acc 83.07%, f1 0.8308, precision 0.8304, recall 0.8312, auc 0.8307
epoch 1901, loss 0.4330, train acc 83.07%, f1 0.8308, precision 0.8301, recall 0.8315, auc 0.8307
epoch 2001, loss 0.5071, train acc 83.16%, f1 0.8318, precision 0.8309, recall 0.8327, auc 0.8316
epoch 2101, loss 0.5070, train acc 83.16%, f1 0.8317, precision 0.8313, recall 0.8320, auc 0.8316
epoch 2201, loss 0.3902, train acc 83.15%, f1 0.8315, precision 0.8315, recall 0.8315, auc 0.8315
epoch 2301, loss 0.4224, train acc 83.14%, f1 0.8315, precision 0.8312, recall 0.8318, auc 0.8314
epoch 2401, loss 0.4945, train acc 83.13%, f1 0.8313, precision 0.8311, recall 0.8315, auc 0.8313
epoch 2501, loss 0.4580, train acc 83.17%, f1 0.8318, precision 0.8312, recall 0.8325, auc 0.8317
epoch 2601, loss 0.3238, train acc 83.17%, f1 0.8317, precision 0.8316, recall 0.8318, auc 0.8317
epoch 2701, loss 0.3649, train acc 83.15%, f1 0.8317, precision 0.8309, recall 0.8324, auc 0.8315
epoch 2801, loss 0.4179, train acc 83.09%, f1 0.8309, precision 0.8308, recall 0.8311, auc 0.8309
epoch 2901, loss 0.3560, train acc 83.12%, f1 0.8311, precision 0.8315, recall 0.8308, auc 0.8312
epoch 3001, loss 0.3554, train acc 83.10%, f1 0.8310, precision 0.8311, recall 0.8310, auc 0.8310
epoch 3101, loss 0.3840, train acc 83.12%, f1 0.8312, precision 0.8314, recall 0.8309, auc 0.8312
epoch 3201, loss 0.4357, train acc 83.15%, f1 0.8315, precision 0.8313, recall 0.8316, auc 0.8315
epoch 3301, loss 0.4031, train acc 83.12%, f1 0.8312, precision 0.8311, recall 0.8313, auc 0.8312
epoch 3401, loss 0.3783, train acc 83.21%, f1 0.8323, precision 0.8316, recall 0.8329, auc 0.8321
epoch 3501, loss 0.4821, train acc 83.15%, f1 0.8315, precision 0.8314, recall 0.8317, auc 0.8315
epoch 3601, loss 0.4484, train acc 83.14%, f1 0.8315, precision 0.8311, recall 0.8319, auc 0.8314
epoch 3701, loss 0.4236, train acc 83.11%, f1 0.8311, precision 0.8311, recall 0.8312, auc 0.8311
epoch 3801, loss 0.4022, train acc 83.12%, f1 0.8312, precision 0.8311, recall 0.8314, auc 0.8312
epoch 3901, loss 0.2809, train acc 83.14%, f1 0.8314, precision 0.8312, recall 0.8316, auc 0.8314
epoch 4001, loss 0.3070, train acc 83.16%, f1 0.8317, precision 0.8314, recall 0.8319, auc 0.8316
epoch 4101, loss 0.5666, train acc 83.16%, f1 0.8317, precision 0.8312, recall 0.8322, auc 0.8316
epoch 4201, loss 0.3455, train acc 83.17%, f1 0.8317, precision 0.8314, recall 0.8320, auc 0.8317
epoch 4301, loss 0.4073, train acc 83.18%, f1 0.8318, precision 0.8318, recall 0.8319, auc 0.8318
epoch 4401, loss 0.4555, train acc 83.20%, f1 0.8321, precision 0.8317, recall 0.8324, auc 0.8320
epoch 4501, loss 0.3680, train acc 83.20%, f1 0.8321, precision 0.8315, recall 0.8327, auc 0.8320
epoch 4601, loss 0.4264, train acc 83.31%, f1 0.8331, precision 0.8328, recall 0.8335, auc 0.8331
epoch 4701, loss 0.3517, train acc 83.27%, f1 0.8327, precision 0.8326, recall 0.8329, auc 0.8327
epoch 4801, loss 0.3928, train acc 83.36%, f1 0.8337, precision 0.8333, recall 0.8341, auc 0.8336
epoch 4901, loss 0.4267, train acc 83.35%, f1 0.8336, precision 0.8331, recall 0.8340, auc 0.8335
epoch 5001, loss 0.3128, train acc 83.37%, f1 0.8338, precision 0.8335, recall 0.8341, auc 0.8337
epoch 5101, loss 0.3649, train acc 83.37%, f1 0.8338, precision 0.8335, recall 0.8341, auc 0.8337
epoch 5201, loss 0.4108, train acc 83.38%, f1 0.8338, precision 0.8338, recall 0.8338, auc 0.8338
epoch 5301, loss 0.4792, train acc 83.42%, f1 0.8342, precision 0.8342, recall 0.8342, auc 0.8342
epoch 5401, loss 0.3420, train acc 83.41%, f1 0.8341, precision 0.8338, recall 0.8344, auc 0.8341
epoch 5501, loss 0.3637, train acc 83.39%, f1 0.8339, precision 0.8340, recall 0.8337, auc 0.8339
epoch 5601, loss 0.3081, train acc 83.43%, f1 0.8343, precision 0.8343, recall 0.8344, auc 0.8343
epoch 5701, loss 0.2902, train acc 83.45%, f1 0.8345, precision 0.8346, recall 0.8344, auc 0.8345
epoch 5801, loss 0.3373, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 5901, loss 0.3377, train acc 83.49%, f1 0.8349, precision 0.8350, recall 0.8348, auc 0.8349
epoch 6001, loss 0.3796, train acc 83.48%, f1 0.8348, precision 0.8349, recall 0.8347, auc 0.8348
epoch 6101, loss 0.3476, train acc 83.53%, f1 0.8353, precision 0.8353, recall 0.8352, auc 0.8353
epoch 6201, loss 0.3641, train acc 83.52%, f1 0.8352, precision 0.8353, recall 0.8351, auc 0.8352
epoch 6301, loss 0.3954, train acc 83.56%, f1 0.8356, precision 0.8356, recall 0.8357, auc 0.8356
epoch 6401, loss 0.4370, train acc 83.59%, f1 0.8359, precision 0.8359, recall 0.8359, auc 0.8359
epoch 6501, loss 0.3019, train acc 83.61%, f1 0.8361, precision 0.8362, recall 0.8359, auc 0.8361
epoch 6601, loss 0.3691, train acc 83.62%, f1 0.8362, precision 0.8362, recall 0.8362, auc 0.8362
epoch 6701, loss 0.3222, train acc 83.67%, f1 0.8367, precision 0.8368, recall 0.8366, auc 0.8367
epoch 6801, loss 0.4328, train acc 83.65%, f1 0.8366, precision 0.8365, recall 0.8366, auc 0.8365
epoch 6901, loss 0.4199, train acc 83.72%, f1 0.8372, precision 0.8372, recall 0.8372, auc 0.8372
epoch 7001, loss 0.3917, train acc 83.76%, f1 0.8376, precision 0.8378, recall 0.8374, auc 0.8376
epoch 7101, loss 0.3635, train acc 83.76%, f1 0.8376, precision 0.8378, recall 0.8374, auc 0.8376
epoch 7201, loss 0.2686, train acc 83.74%, f1 0.8374, precision 0.8374, recall 0.8374, auc 0.8374
epoch 7301, loss 0.4156, train acc 83.84%, f1 0.8384, precision 0.8383, recall 0.8385, auc 0.8384
epoch 7401, loss 0.3836, train acc 83.88%, f1 0.8388, precision 0.8388, recall 0.8388, auc 0.8388
epoch 7501, loss 0.3560, train acc 83.91%, f1 0.8391, precision 0.8390, recall 0.8392, auc 0.8391
epoch 7601, loss 0.3085, train acc 83.90%, f1 0.8390, precision 0.8392, recall 0.8388, auc 0.8390
epoch 7701, loss 0.4062, train acc 83.95%, f1 0.8395, precision 0.8396, recall 0.8395, auc 0.8395
epoch 7801, loss 0.4408, train acc 83.91%, f1 0.8391, precision 0.8392, recall 0.8390, auc 0.8391
epoch 7901, loss 0.5688, train acc 83.98%, f1 0.8398, precision 0.8398, recall 0.8399, auc 0.8398
epoch 8001, loss 0.4308, train acc 83.99%, f1 0.8399, precision 0.8400, recall 0.8399, auc 0.8399
epoch 8101, loss 0.4497, train acc 84.07%, f1 0.8407, precision 0.8406, recall 0.8408, auc 0.8407
epoch 8201, loss 0.3510, train acc 84.07%, f1 0.8407, precision 0.8409, recall 0.8405, auc 0.8407/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.3376, train acc 84.05%, f1 0.8405, precision 0.8405, recall 0.8405, auc 0.8405
epoch 8401, loss 0.3497, train acc 84.11%, f1 0.8411, precision 0.8411, recall 0.8412, auc 0.8411
epoch 8501, loss 0.2962, train acc 84.15%, f1 0.8415, precision 0.8415, recall 0.8414, auc 0.8415
epoch 8601, loss 0.4247, train acc 84.25%, f1 0.8425, precision 0.8426, recall 0.8424, auc 0.8425
epoch 8701, loss 0.3491, train acc 84.24%, f1 0.8425, precision 0.8424, recall 0.8425, auc 0.8424
epoch 8801, loss 0.4379, train acc 84.22%, f1 0.8422, precision 0.8421, recall 0.8422, auc 0.8422
epoch 8901, loss 0.3782, train acc 84.25%, f1 0.8425, precision 0.8425, recall 0.8424, auc 0.8425
epoch 9001, loss 0.4018, train acc 84.30%, f1 0.8430, precision 0.8429, recall 0.8430, auc 0.8430
epoch 9101, loss 0.3730, train acc 84.33%, f1 0.8433, precision 0.8434, recall 0.8433, auc 0.8433
epoch 9201, loss 0.3465, train acc 84.37%, f1 0.8436, precision 0.8437, recall 0.8435, auc 0.8437
epoch 9301, loss 0.2861, train acc 84.41%, f1 0.8441, precision 0.8441, recall 0.8441, auc 0.8441
epoch 9401, loss 0.4382, train acc 84.33%, f1 0.8433, precision 0.8434, recall 0.8433, auc 0.8433
epoch 9501, loss 0.4077, train acc 84.37%, f1 0.8436, precision 0.8437, recall 0.8436, auc 0.8437
epoch 9601, loss 0.3639, train acc 84.44%, f1 0.8444, precision 0.8444, recall 0.8444, auc 0.8444
epoch 9701, loss 0.4353, train acc 84.46%, f1 0.8446, precision 0.8446, recall 0.8446, auc 0.8446
epoch 9801, loss 0.3681, train acc 84.42%, f1 0.8442, precision 0.8442, recall 0.8441, auc 0.8442
epoch 9901, loss 0.2524, train acc 84.53%, f1 0.8454, precision 0.8453, recall 0.8454, auc 0.8453
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_3
./test_pima/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.61

the Fscore is 0.5806451612903226

the precision is 0.4090909090909091

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_3
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5925, train acc 77.25%, f1 0.7743, precision 0.7682, recall 0.7805, auc 0.7725
epoch 201, loss 0.5087, train acc 80.04%, f1 0.8007, precision 0.7994, recall 0.8021, auc 0.8004
epoch 301, loss 0.3881, train acc 81.61%, f1 0.8161, precision 0.8159, recall 0.8163, auc 0.8161
epoch 401, loss 0.3952, train acc 82.51%, f1 0.8250, precision 0.8252, recall 0.8249, auc 0.8251
epoch 501, loss 0.4037, train acc 82.80%, f1 0.8279, precision 0.8283, recall 0.8274, auc 0.8280
epoch 601, loss 0.3425, train acc 82.91%, f1 0.8290, precision 0.8294, recall 0.8287, auc 0.8291
epoch 701, loss 0.3861, train acc 83.04%, f1 0.8303, precision 0.8306, recall 0.8300, auc 0.8304
epoch 801, loss 0.4268, train acc 83.09%, f1 0.8308, precision 0.8313, recall 0.8302, auc 0.8309
epoch 901, loss 0.4403, train acc 83.10%, f1 0.8309, precision 0.8313, recall 0.8304, auc 0.8310
epoch 1001, loss 0.3668, train acc 83.12%, f1 0.8312, precision 0.8313, recall 0.8311, auc 0.8312
epoch 1101, loss 0.3701, train acc 83.17%, f1 0.8317, precision 0.8318, recall 0.8315, auc 0.8317
epoch 1201, loss 0.4572, train acc 83.11%, f1 0.8311, precision 0.8312, recall 0.8311, auc 0.8311
epoch 1301, loss 0.3485, train acc 83.08%, f1 0.8308, precision 0.8308, recall 0.8308, auc 0.8308
epoch 1401, loss 0.3730, train acc 83.10%, f1 0.8310, precision 0.8312, recall 0.8309, auc 0.8310
epoch 1501, loss 0.4414, train acc 83.12%, f1 0.8312, precision 0.8313, recall 0.8312, auc 0.8312
epoch 1601, loss 0.2455, train acc 83.20%, f1 0.8320, precision 0.8321, recall 0.8318, auc 0.8320
epoch 1701, loss 0.3299, train acc 83.18%, f1 0.8317, precision 0.8318, recall 0.8317, auc 0.8318
epoch 1801, loss 0.4743, train acc 83.15%, f1 0.8315, precision 0.8316, recall 0.8314, auc 0.8315
epoch 1901, loss 0.4517, train acc 83.13%, f1 0.8314, precision 0.8313, recall 0.8314, auc 0.8313
epoch 2001, loss 0.4001, train acc 83.13%, f1 0.8313, precision 0.8314, recall 0.8311, auc 0.8313
epoch 2101, loss 0.3449, train acc 83.13%, f1 0.8313, precision 0.8313, recall 0.8313, auc 0.8313
epoch 2201, loss 0.3523, train acc 83.12%, f1 0.8312, precision 0.8311, recall 0.8314, auc 0.8312
epoch 2301, loss 0.4283, train acc 83.11%, f1 0.8311, precision 0.8311, recall 0.8311, auc 0.8311
epoch 2401, loss 0.3895, train acc 83.14%, f1 0.8314, precision 0.8314, recall 0.8315, auc 0.8314
epoch 2501, loss 0.3603, train acc 83.12%, f1 0.8312, precision 0.8312, recall 0.8313, auc 0.8312
epoch 2601, loss 0.4070, train acc 83.15%, f1 0.8315, precision 0.8316, recall 0.8313, auc 0.8315
epoch 2701, loss 0.3418, train acc 83.13%, f1 0.8313, precision 0.8314, recall 0.8311, auc 0.8313
epoch 2801, loss 0.4658, train acc 83.15%, f1 0.8314, precision 0.8315, recall 0.8313, auc 0.8315
epoch 2901, loss 0.3034, train acc 83.12%, f1 0.8312, precision 0.8312, recall 0.8311, auc 0.8312
epoch 3001, loss 0.4104, train acc 83.16%, f1 0.8316, precision 0.8317, recall 0.8315, auc 0.8316
epoch 3101, loss 0.5175, train acc 83.18%, f1 0.8318, precision 0.8317, recall 0.8319, auc 0.8318
epoch 3201, loss 0.3889, train acc 83.19%, f1 0.8319, precision 0.8319, recall 0.8319, auc 0.8319
epoch 3301, loss 0.2489, train acc 83.18%, f1 0.8318, precision 0.8319, recall 0.8317, auc 0.8318
epoch 3401, loss 0.4102, train acc 83.10%, f1 0.8309, precision 0.8312, recall 0.8307, auc 0.8310
epoch 3501, loss 0.4101, train acc 83.19%, f1 0.8318, precision 0.8321, recall 0.8315, auc 0.8319
epoch 3601, loss 0.2937, train acc 83.12%, f1 0.8312, precision 0.8312, recall 0.8311, auc 0.8312
epoch 3701, loss 0.4701, train acc 83.13%, f1 0.8313, precision 0.8313, recall 0.8313, auc 0.8313
epoch 3801, loss 0.2572, train acc 83.08%, f1 0.8308, precision 0.8309, recall 0.8308, auc 0.8308
epoch 3901, loss 0.3858, train acc 83.12%, f1 0.8312, precision 0.8310, recall 0.8314, auc 0.8312
epoch 4001, loss 0.4228, train acc 83.08%, f1 0.8308, precision 0.8309, recall 0.8308, auc 0.8308
epoch 4101, loss 0.3638, train acc 83.14%, f1 0.8314, precision 0.8312, recall 0.8317, auc 0.8314
epoch 4201, loss 0.3472, train acc 83.24%, f1 0.8323, precision 0.8325, recall 0.8321, auc 0.8324
epoch 4301, loss 0.3336, train acc 83.19%, f1 0.8319, precision 0.8317, recall 0.8322, auc 0.8319
epoch 4401, loss 0.4868, train acc 83.25%, f1 0.8325, precision 0.8324, recall 0.8326, auc 0.8325
epoch 4501, loss 0.3438, train acc 83.28%, f1 0.8328, precision 0.8328, recall 0.8328, auc 0.8328
epoch 4601, loss 0.2930, train acc 83.23%, f1 0.8323, precision 0.8321, recall 0.8326, auc 0.8323
epoch 4701, loss 0.3551, train acc 83.21%, f1 0.8321, precision 0.8323, recall 0.8320, auc 0.8321
epoch 4801, loss 0.3314, train acc 83.18%, f1 0.8319, precision 0.8316, recall 0.8322, auc 0.8318
epoch 4901, loss 0.3751, train acc 83.23%, f1 0.8324, precision 0.8321, recall 0.8326, auc 0.8323
epoch 5001, loss 0.4031, train acc 83.29%, f1 0.8329, precision 0.8328, recall 0.8329, auc 0.8329
epoch 5101, loss 0.2835, train acc 83.31%, f1 0.8332, precision 0.8330, recall 0.8334, auc 0.8331
epoch 5201, loss 0.4191, train acc 83.31%, f1 0.8332, precision 0.8329, recall 0.8334, auc 0.8331
epoch 5301, loss 0.3711, train acc 83.32%, f1 0.8333, precision 0.8330, recall 0.8336, auc 0.8332
epoch 5401, loss 0.3837, train acc 83.36%, f1 0.8336, precision 0.8334, recall 0.8338, auc 0.8336
epoch 5501, loss 0.4581, train acc 83.36%, f1 0.8336, precision 0.8334, recall 0.8337, auc 0.8336
epoch 5601, loss 0.4547, train acc 83.38%, f1 0.8338, precision 0.8338, recall 0.8339, auc 0.8338
epoch 5701, loss 0.3601, train acc 83.37%, f1 0.8337, precision 0.8337, recall 0.8338, auc 0.8337
epoch 5801, loss 0.3910, train acc 83.35%, f1 0.8336, precision 0.8333, recall 0.8339, auc 0.8335
epoch 5901, loss 0.2268, train acc 83.38%, f1 0.8338, precision 0.8338, recall 0.8339, auc 0.8338
epoch 6001, loss 0.3825, train acc 83.35%, f1 0.8335, precision 0.8337, recall 0.8332, auc 0.8335
epoch 6101, loss 0.4732, train acc 83.41%, f1 0.8341, precision 0.8341, recall 0.8340, auc 0.8341
epoch 6201, loss 0.3136, train acc 83.42%, f1 0.8343, precision 0.8341, recall 0.8344, auc 0.8342
epoch 6301, loss 0.3120, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8350, auc 0.8349
epoch 6401, loss 0.2956, train acc 83.54%, f1 0.8354, precision 0.8355, recall 0.8354, auc 0.8354
epoch 6501, loss 0.3416, train acc 83.57%, f1 0.8357, precision 0.8356, recall 0.8357, auc 0.8357
epoch 6601, loss 0.2963, train acc 83.59%, f1 0.8360, precision 0.8358, recall 0.8362, auc 0.8359
epoch 6701, loss 0.3443, train acc 83.60%, f1 0.8360, precision 0.8360, recall 0.8360, auc 0.8360
epoch 6801, loss 0.4475, train acc 83.65%, f1 0.8365, precision 0.8364, recall 0.8367, auc 0.8365
epoch 6901, loss 0.3736, train acc 83.67%, f1 0.8366, precision 0.8368, recall 0.8365, auc 0.8367
epoch 7001, loss 0.2703, train acc 83.69%, f1 0.8370, precision 0.8368, recall 0.8372, auc 0.8369
epoch 7101, loss 0.4241, train acc 83.73%, f1 0.8374, precision 0.8372, recall 0.8375, auc 0.8373
epoch 7201, loss 0.4126, train acc 83.79%, f1 0.8379, precision 0.8380, recall 0.8377, auc 0.8379
epoch 7301, loss 0.4467, train acc 83.79%, f1 0.8379, precision 0.8379, recall 0.8378, auc 0.8379
epoch 7401, loss 0.2757, train acc 83.86%, f1 0.8386, precision 0.8385, recall 0.8386, auc 0.8386
epoch 7501, loss 0.4002, train acc 83.83%, f1 0.8383, precision 0.8383, recall 0.8382, auc 0.8383
epoch 7601, loss 0.3207, train acc 83.89%, f1 0.8389, precision 0.8389, recall 0.8388, auc 0.8389
epoch 7701, loss 0.3225, train acc 83.94%, f1 0.8395, precision 0.8393, recall 0.8396, auc 0.8394
epoch 7801, loss 0.3684, train acc 83.96%, f1 0.8396, precision 0.8396, recall 0.8395, auc 0.8396
epoch 7901, loss 0.3901, train acc 83.98%, f1 0.8398, precision 0.8399, recall 0.8397, auc 0.8398
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_3
./test_pima/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.615

the Fscore is 0.5837837837837838

the precision is 0.4122137404580153

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_3
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5952, train acc 76.85%, f1 0.7786, precision 0.7461, recall 0.8141, auc 0.7685
epoch 201, loss 0.4669, train acc 80.03%, f1 0.8017, precision 0.7961, recall 0.8074, auc 0.8003
epoch 301, loss 0.5149, train acc 81.48%, f1 0.8143, precision 0.8166, recall 0.8120, auc 0.8148
epoch 401, loss 0.3276, train acc 82.50%, f1 0.8247, precision 0.8264, recall 0.8229, auc 0.8250
epoch 501, loss 0.3686, train acc 82.83%, f1 0.8274, precision 0.8321, recall 0.8227, auc 0.8283
epoch 601, loss 0.5107, train acc 82.98%, f1 0.8290, precision 0.8328, recall 0.8252, auc 0.8298
epoch 701, loss 0.5963, train acc 83.08%, f1 0.8301, precision 0.8337, recall 0.8265, auc 0.8308
epoch 801, loss 0.3354, train acc 83.05%, f1 0.8299, precision 0.8333, recall 0.8265, auc 0.8305
epoch 901, loss 0.4127, train acc 83.08%, f1 0.8302, precision 0.8330, recall 0.8275, auc 0.8308
epoch 1001, loss 0.4155, train acc 83.06%, f1 0.8300, precision 0.8327, recall 0.8274, auc 0.8306
epoch 1101, loss 0.3720, train acc 83.11%, f1 0.8306, precision 0.8331, recall 0.8282, auc 0.8311
epoch 1201, loss 0.3535, train acc 83.14%, f1 0.8310, precision 0.8329, recall 0.8290, auc 0.8314
epoch 1301, loss 0.3003, train acc 83.13%, f1 0.8311, precision 0.8322, recall 0.8300, auc 0.8313
epoch 1401, loss 0.3993, train acc 83.14%, f1 0.8310, precision 0.8326, recall 0.8295, auc 0.8314
epoch 1501, loss 0.3636, train acc 83.19%, f1 0.8316, precision 0.8328, recall 0.8305, auc 0.8319
epoch 1601, loss 0.3061, train acc 83.18%, f1 0.8316, precision 0.8328, recall 0.8304, auc 0.8318
epoch 1701, loss 0.3984, train acc 83.16%, f1 0.8315, precision 0.8318, recall 0.8312, auc 0.8316
epoch 1801, loss 0.4702, train acc 83.10%, f1 0.8309, precision 0.8315, recall 0.8303, auc 0.8310
epoch 1901, loss 0.4752, train acc 83.11%, f1 0.8308, precision 0.8321, recall 0.8295, auc 0.8311
epoch 2001, loss 0.3959, train acc 83.12%, f1 0.8310, precision 0.8316, recall 0.8305, auc 0.8312
epoch 2101, loss 0.3218, train acc 83.14%, f1 0.8313, precision 0.8318, recall 0.8307, auc 0.8314
epoch 2201, loss 0.5010, train acc 83.12%, f1 0.8311, precision 0.8315, recall 0.8308, auc 0.8312
epoch 2301, loss 0.2898, train acc 83.13%, f1 0.8313, precision 0.8311, recall 0.8315, auc 0.8313
epoch 2401, loss 0.3772, train acc 83.15%, f1 0.8314, precision 0.8321, recall 0.8307, auc 0.8315
epoch 2501, loss 0.3935, train acc 83.12%, f1 0.8312, precision 0.8311, recall 0.8314, auc 0.8312
epoch 2601, loss 0.3689, train acc 83.14%, f1 0.8314, precision 0.8315, recall 0.8313, auc 0.8314
epoch 2701, loss 0.4410, train acc 83.12%, f1 0.8312, precision 0.8313, recall 0.8310, auc 0.8312
epoch 2801, loss 0.4142, train acc 83.12%, f1 0.8312, precision 0.8312, recall 0.8311, auc 0.8312
epoch 2901, loss 0.4474, train acc 83.12%, f1 0.8311, precision 0.8315, recall 0.8308, auc 0.8312
epoch 3001, loss 0.3168, train acc 83.14%, f1 0.8314, precision 0.8317, recall 0.8311, auc 0.8314
epoch 3101, loss 0.3092, train acc 83.17%, f1 0.8318, precision 0.8314, recall 0.8321, auc 0.8317
epoch 3201, loss 0.3905, train acc 83.18%, f1 0.8318, precision 0.8322, recall 0.8314, auc 0.8318
epoch 3301, loss 0.3271, train acc 83.17%, f1 0.8317, precision 0.8319, recall 0.8314, auc 0.8317
epoch 3401, loss 0.3554, train acc 83.23%, f1 0.8322, precision 0.8323, recall 0.8322, auc 0.8323
epoch 3501, loss 0.3816, train acc 83.23%, f1 0.8322, precision 0.8327, recall 0.8318, auc 0.8323
epoch 3601, loss 0.3272, train acc 83.20%, f1 0.8319, precision 0.8325, recall 0.8313, auc 0.8320
epoch 3701, loss 0.5002, train acc 83.14%, f1 0.8313, precision 0.8317, recall 0.8309, auc 0.8314
epoch 3801, loss 0.3478, train acc 83.17%, f1 0.8317, precision 0.8317, recall 0.8316, auc 0.8317
epoch 3901, loss 0.4312, train acc 83.18%, f1 0.8317, precision 0.8321, recall 0.8314, auc 0.8318
epoch 4001, loss 0.3592, train acc 83.16%, f1 0.8315, precision 0.8318, recall 0.8312, auc 0.8316
epoch 4101, loss 0.3712, train acc 83.21%, f1 0.8320, precision 0.8326, recall 0.8314, auc 0.8321
epoch 4201, loss 0.4240, train acc 83.18%, f1 0.8318, precision 0.8318, recall 0.8318, auc 0.8318
epoch 4301, loss 0.4059, train acc 83.24%, f1 0.8324, precision 0.8325, recall 0.8324, auc 0.8324
epoch 4401, loss 0.4387, train acc 83.25%, f1 0.8325, precision 0.8328, recall 0.8322, auc 0.8325
epoch 4501, loss 0.3667, train acc 83.31%, f1 0.8331, precision 0.8330, recall 0.8332, auc 0.8331
epoch 4601, loss 0.3153, train acc 83.28%, f1 0.8328, precision 0.8327, recall 0.8330, auc 0.8328
epoch 4701, loss 0.3161, train acc 83.27%, f1 0.8327, precision 0.8329, recall 0.8325, auc 0.8327
epoch 4801, loss 0.3908, train acc 83.29%, f1 0.8328, precision 0.8334, recall 0.8321, auc 0.8329
epoch 4901, loss 0.2804, train acc 83.30%, f1 0.8329, precision 0.8334, recall 0.8323, auc 0.8330
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_3
./test_pima/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.575

the Fscore is 0.5595854922279793

the precision is 0.38848920863309355

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_3
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6244, train acc 77.97%, f1 0.7779, precision 0.7843, recall 0.7715, auc 0.7797
epoch 201, loss 0.4434, train acc 79.99%, f1 0.7998, precision 0.8003, recall 0.7994, auc 0.7999
epoch 301, loss 0.4186, train acc 81.79%, f1 0.8179, precision 0.8179, recall 0.8178, auc 0.8179
epoch 401, loss 0.3868, train acc 82.51%, f1 0.8252, precision 0.8250, recall 0.8253, auc 0.8251
epoch 501, loss 0.4650, train acc 82.81%, f1 0.8281, precision 0.8279, recall 0.8284, auc 0.8281
epoch 601, loss 0.3329, train acc 82.95%, f1 0.8296, precision 0.8292, recall 0.8300, auc 0.8295
epoch 701, loss 0.5226, train acc 83.01%, f1 0.8302, precision 0.8299, recall 0.8304, auc 0.8301
epoch 801, loss 0.4505, train acc 83.07%, f1 0.8308, precision 0.8303, recall 0.8313, auc 0.8307
epoch 901, loss 0.3633, train acc 83.12%, f1 0.8312, precision 0.8312, recall 0.8313, auc 0.8312
epoch 1001, loss 0.3685, train acc 83.12%, f1 0.8313, precision 0.8311, recall 0.8314, auc 0.8312
epoch 1101, loss 0.3225, train acc 83.13%, f1 0.8313, precision 0.8313, recall 0.8314, auc 0.8313
epoch 1201, loss 0.4253, train acc 83.07%, f1 0.8307, precision 0.8306, recall 0.8308, auc 0.8307
epoch 1301, loss 0.4589, train acc 83.10%, f1 0.8310, precision 0.8310, recall 0.8311, auc 0.8310
epoch 1401, loss 0.3613, train acc 83.17%, f1 0.8316, precision 0.8317, recall 0.8315, auc 0.8317
epoch 1501, loss 0.4314, train acc 83.12%, f1 0.8312, precision 0.8311, recall 0.8313, auc 0.8312
epoch 1601, loss 0.4252, train acc 83.20%, f1 0.8321, precision 0.8320, recall 0.8321, auc 0.8320
epoch 1701, loss 0.2885, train acc 83.15%, f1 0.8315, precision 0.8314, recall 0.8315, auc 0.8315
epoch 1801, loss 0.3995, train acc 83.09%, f1 0.8309, precision 0.8309, recall 0.8310, auc 0.8309
epoch 1901, loss 0.3729, train acc 83.10%, f1 0.8310, precision 0.8309, recall 0.8311, auc 0.8310
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_3
./test_pima/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.5857407407407407

the Fscore is 0.5638297872340425

the precision is 0.39552238805970147

the recall is 0.9814814814814815

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_3
----------------------



epoch 1, loss 0.6933, train acc 49.69%, f1 0.6639, precision 0.4969, recall 1.0000, auc 0.5000
epoch 101, loss 0.6287, train acc 73.96%, f1 0.7735, precision 0.6811, recall 0.8949, auc 0.7406
epoch 201, loss 0.4792, train acc 79.53%, f1 0.8003, precision 0.7763, recall 0.8259, auc 0.7954
epoch 301, loss 0.4580, train acc 81.40%, f1 0.8141, precision 0.8088, recall 0.8195, auc 0.8141
epoch 401, loss 0.3100, train acc 82.40%, f1 0.8226, precision 0.8238, recall 0.8215, auc 0.8240
epoch 501, loss 0.4510, train acc 82.71%, f1 0.8245, precision 0.8320, recall 0.8171, auc 0.8271
epoch 601, loss 0.3765, train acc 82.96%, f1 0.8269, precision 0.8349, recall 0.8190, auc 0.8295
epoch 701, loss 0.3605, train acc 83.04%, f1 0.8272, precision 0.8379, recall 0.8167, auc 0.8304
epoch 801, loss 0.4627, train acc 83.10%, f1 0.8276, precision 0.8390, recall 0.8165, auc 0.8309
epoch 901, loss 0.3679, train acc 83.12%, f1 0.8283, precision 0.8376, recall 0.8191, auc 0.8312
epoch 1001, loss 0.3110, train acc 83.12%, f1 0.8284, precision 0.8370, recall 0.8201, auc 0.8312
epoch 1101, loss 0.3937, train acc 83.11%, f1 0.8284, precision 0.8363, recall 0.8207, auc 0.8310
epoch 1201, loss 0.3978, train acc 83.15%, f1 0.8289, precision 0.8362, recall 0.8219, auc 0.8314
epoch 1301, loss 0.3045, train acc 83.11%, f1 0.8286, precision 0.8356, recall 0.8218, auc 0.8310
epoch 1401, loss 0.4023, train acc 83.15%, f1 0.8291, precision 0.8358, recall 0.8224, auc 0.8315
epoch 1501, loss 0.4475, train acc 83.15%, f1 0.8286, precision 0.8378, recall 0.8197, auc 0.8315
epoch 1601, loss 0.3578, train acc 83.19%, f1 0.8295, precision 0.8361, recall 0.8231, auc 0.8319
epoch 1701, loss 0.3413, train acc 83.19%, f1 0.8299, precision 0.8348, recall 0.8250, auc 0.8319
epoch 1801, loss 0.3448, train acc 83.17%, f1 0.8295, precision 0.8353, recall 0.8237, auc 0.8317
epoch 1901, loss 0.4296, train acc 83.17%, f1 0.8297, precision 0.8347, recall 0.8247, auc 0.8317
epoch 2001, loss 0.4713, train acc 83.19%, f1 0.8301, precision 0.8338, recall 0.8265, auc 0.8319
epoch 2101, loss 0.4724, train acc 83.11%, f1 0.8291, precision 0.8336, recall 0.8246, auc 0.8310
epoch 2201, loss 0.4430, train acc 83.10%, f1 0.8293, precision 0.8325, recall 0.8262, auc 0.8310
epoch 2301, loss 0.4352, train acc 83.13%, f1 0.8297, precision 0.8325, recall 0.8269, auc 0.8313
epoch 2401, loss 0.3800, train acc 83.16%, f1 0.8295, precision 0.8343, recall 0.8248, auc 0.8315
epoch 2501, loss 0.3309, train acc 83.07%, f1 0.8291, precision 0.8319, recall 0.8263, auc 0.8307
epoch 2601, loss 0.4147, train acc 83.11%, f1 0.8298, precision 0.8309, recall 0.8287, auc 0.8311
epoch 2701, loss 0.4209, train acc 83.13%, f1 0.8296, precision 0.8329, recall 0.8262, auc 0.8313
epoch 2801, loss 0.3688, train acc 83.14%, f1 0.8296, precision 0.8330, recall 0.8263, auc 0.8313
epoch 2901, loss 0.2895, train acc 83.08%, f1 0.8291, precision 0.8323, recall 0.8260, auc 0.8308
epoch 3001, loss 0.4153, train acc 83.07%, f1 0.8293, precision 0.8310, recall 0.8276, auc 0.8306
epoch 3101, loss 0.3232, train acc 83.13%, f1 0.8298, precision 0.8322, recall 0.8273, auc 0.8313
epoch 3201, loss 0.4277, train acc 83.06%, f1 0.8290, precision 0.8317, recall 0.8262, auc 0.8306
epoch 3301, loss 0.3769, train acc 83.19%, f1 0.8304, precision 0.8324, recall 0.8283, auc 0.8318
epoch 3401, loss 0.2944, train acc 83.10%, f1 0.8302, precision 0.8286, recall 0.8319, auc 0.8310
epoch 3501, loss 0.5092, train acc 83.12%, f1 0.8299, precision 0.8310, recall 0.8288, auc 0.8312
epoch 3601, loss 0.4436, train acc 83.16%, f1 0.8305, precision 0.8309, recall 0.8300, auc 0.8316
epoch 3701, loss 0.4195, train acc 83.14%, f1 0.8299, precision 0.8321, recall 0.8278, auc 0.8314
epoch 3801, loss 0.3414, train acc 83.15%, f1 0.8298, precision 0.8332, recall 0.8263, auc 0.8315
epoch 3901, loss 0.3711, train acc 83.21%, f1 0.8304, precision 0.8339, recall 0.8269, auc 0.8321
epoch 4001, loss 0.5087, train acc 83.22%, f1 0.8309, precision 0.8323, recall 0.8295, auc 0.8322
epoch 4101, loss 0.3472, train acc 83.21%, f1 0.8312, precision 0.8305, recall 0.8319, auc 0.8321
epoch 4201, loss 0.4123, train acc 83.20%, f1 0.8309, precision 0.8314, recall 0.8303, auc 0.8320
epoch 4301, loss 0.3931, train acc 83.22%, f1 0.8306, precision 0.8335, recall 0.8277, auc 0.8322
epoch 4401, loss 0.5332, train acc 83.24%, f1 0.8312, precision 0.8318, recall 0.8307, auc 0.8324
epoch 4501, loss 0.2744, train acc 83.23%, f1 0.8307, precision 0.8337, recall 0.8277, auc 0.8323
epoch 4601, loss 0.5243, train acc 83.25%, f1 0.8316, precision 0.8308, recall 0.8323, auc 0.8325
epoch 4701, loss 0.3820, train acc 83.23%, f1 0.8313, precision 0.8308, recall 0.8318, auc 0.8323
epoch 4801, loss 0.5010, train acc 83.20%, f1 0.8304, precision 0.8332, recall 0.8277, auc 0.8320
epoch 4901, loss 0.3366, train acc 83.26%, f1 0.8309, precision 0.8340, recall 0.8279, auc 0.8326
epoch 5001, loss 0.4247, train acc 83.24%, f1 0.8312, precision 0.8318, recall 0.8306, auc 0.8324
epoch 5101, loss 0.2678, train acc 83.29%, f1 0.8316, precision 0.8332, recall 0.8299, auc 0.8329
epoch 5201, loss 0.2502, train acc 83.26%, f1 0.8309, precision 0.8338, recall 0.8281, auc 0.8325
epoch 5301, loss 0.3457, train acc 83.33%, f1 0.8322, precision 0.8324, recall 0.8320, auc 0.8333
epoch 5401, loss 0.3782, train acc 83.34%, f1 0.8323, precision 0.8322, recall 0.8325, auc 0.8333
epoch 5501, loss 0.2978, train acc 83.34%, f1 0.8317, precision 0.8350, recall 0.8285, auc 0.8334
epoch 5601, loss 0.4139, train acc 83.31%, f1 0.8316, precision 0.8341, recall 0.8291, auc 0.8331
epoch 5701, loss 0.3411, train acc 83.38%, f1 0.8324, precision 0.8342, recall 0.8307, auc 0.8338
epoch 5801, loss 0.3085, train acc 83.42%, f1 0.8327, precision 0.8350, recall 0.8303, auc 0.8341
epoch 5901, loss 0.3289, train acc 83.37%, f1 0.8322, precision 0.8341, recall 0.8304, auc 0.8336
epoch 6001, loss 0.2726, train acc 83.39%, f1 0.8330, precision 0.8322, recall 0.8338, auc 0.8339
epoch 6101, loss 0.3470, train acc 83.40%, f1 0.8326, precision 0.8346, recall 0.8307, auc 0.8340
epoch 6201, loss 0.3961, train acc 83.45%, f1 0.8333, precision 0.8339, recall 0.8328, auc 0.8345
epoch 6301, loss 0.3396, train acc 83.48%, f1 0.8331, precision 0.8366, recall 0.8295, auc 0.8348
epoch 6401, loss 0.2821, train acc 83.50%, f1 0.8336, precision 0.8355, recall 0.8318, auc 0.8350
epoch 6501, loss 0.2609, train acc 83.49%, f1 0.8335, precision 0.8351, recall 0.8320, auc 0.8349
epoch 6601, loss 0.3530, train acc 83.49%, f1 0.8337, precision 0.8342, recall 0.8333, auc 0.8349
epoch 6701, loss 0.2861, train acc 83.49%, f1 0.8335, precision 0.8356, recall 0.8313, auc 0.8349
epoch 6801, loss 0.4155, train acc 83.54%, f1 0.8339, precision 0.8360, recall 0.8319, auc 0.8354
epoch 6901, loss 0.4268, train acc 83.57%, f1 0.8344, precision 0.8362, recall 0.8326, auc 0.8357
epoch 7001, loss 0.2484, train acc 83.62%, f1 0.8348, precision 0.8368, recall 0.8328, auc 0.8362
epoch 7101, loss 0.3134, train acc 83.62%, f1 0.8350, precision 0.8361, recall 0.8339, auc 0.8362
epoch 7201, loss 0.3853, train acc 83.64%, f1 0.8352, precision 0.8364, recall 0.8340, auc 0.8364
epoch 7301, loss 0.4061, train acc 83.67%, f1 0.8353, precision 0.8374, recall 0.8332, auc 0.8367
epoch 7401, loss 0.4596, train acc 83.65%, f1 0.8356, precision 0.8353, recall 0.8358, auc 0.8365
epoch 7501, loss 0.4980, train acc 83.70%, f1 0.8355, precision 0.8380, recall 0.8329, auc 0.8369
epoch 7601, loss 0.2775, train acc 83.73%, f1 0.8365, precision 0.8356, recall 0.8374, auc 0.8373
epoch 7701, loss 0.4158, train acc 83.80%, f1 0.8364, precision 0.8395, recall 0.8333, auc 0.8380
epoch 7801, loss 0.4309, train acc 83.82%, f1 0.8371, precision 0.8374, recall 0.8367, auc 0.8381
epoch 7901, loss 0.3333, train acc 83.84%, f1 0.8373, precision 0.8381, recall 0.8364, auc 0.8384
epoch 8001, loss 0.4102, train acc 83.88%, f1 0.8379, precision 0.8374, recall 0.8383, auc 0.8388
epoch 8101, loss 0.4049, train acc 83.90%, f1 0.8372, precision 0.8413, recall 0.8331, auc 0.8390
epoch 8201, loss 0.2555, train acc 83.94%, f1 0.8385, precision 0.8379, recall 0.8391, auc 0.8394
epoch 8301, loss 0.4279, train acc 83.95%, f1 0.8385, precision 0.8385, recall 0.8386, auc 0.8395
epoch 8401, loss 0.4247, train acc 83.95%, f1 0.8385, precision 0.8383, recall 0.8388, auc 0.8395
epoch 8501, loss 0.3650, train acc 83.99%, f1 0.8391, precision 0.8378, recall 0.8404, auc 0.8399
epoch 8601, loss 0.3927, train acc 84.02%, f1 0.8387, precision 0.8410, recall 0.8364, auc 0.8401
epoch 8701, loss 0.2914, train acc 84.04%, f1 0.8393, precision 0.8398, recall 0.8388, auc 0.8404
epoch 8801, loss 0.3462, train acc 84.09%, f1 0.8397, precision 0.8408, recall 0.8386, auc 0.8409
epoch 8901, loss 0.3425, train acc 84.09%, f1 0.8394, precision 0.8421, recall 0.8367, auc 0.8409
epoch 9001, loss 0.2944, train acc 84.15%, f1 0.8401, precision 0.8422, recall 0.8381, auc 0.8415
epoch 9101, loss 0.4237, train acc 84.18%, f1 0.8405, precision 0.8421, recall 0.8389, auc 0.8418
epoch 9201, loss 0.4208, train acc 84.13%, f1 0.8407, precision 0.8387, recall 0.8426, auc 0.8413
epoch 9301, loss 0.4901, train acc 84.16%, f1 0.8408, precision 0.8394, recall 0.8423, auc 0.8416
epoch 9401, loss 0.4101, train acc 84.25%, f1 0.8411, precision 0.8436, recall 0.8386, auc 0.8425
epoch 9501, loss 0.3524, train acc 84.27%, f1 0.8418, precision 0.8413, recall 0.8423, auc 0.8427
epoch 9601, loss 0.3141, train acc 84.32%, f1 0.8419, precision 0.8435, recall 0.8402, auc 0.8432
epoch 9701, loss 0.4201, train acc 84.28%, f1 0.8416, precision 0.8427, recall 0.8405, auc 0.8428
epoch 9801, loss 0.3199, train acc 84.33%, f1 0.8422, precision 0.8431, recall 0.8412, auc 0.8433
epoch 9901, loss 0.3355, train acc 84.36%, f1 0.8427, precision 0.8426, recall 0.8428, auc 0.8436
epoch 10001, loss 0.2682, train acc 84.38%, f1 0.8428, precision 0.8429, recall 0.8426, auc 0.8438
epoch 10101, loss 0.5314, train acc 84.40%, f1 0.8431, precision 0.8426, recall 0.8436, auc 0.8440
epoch 10201, loss 0.3559, train acc 84.38%, f1 0.8422, precision 0.8454, recall 0.8390, auc 0.8437
epoch 10301, loss 0.2929, train acc 84.41%, f1 0.8435, precision 0.8418, recall 0.8452, auc 0.8442
epoch 10401, loss 0.4020, train acc 84.42%, f1 0.8431, precision 0.8437, recall 0.8424, auc 0.8442
epoch 10501, loss 0.4466, train acc 84.46%, f1 0.8439, precision 0.8426, recall 0.8452, auc 0.8446
epoch 10601, loss 0.4184, train acc 84.43%, f1 0.8430, precision 0.8447, recall 0.8413, auc 0.8443
epoch 10701, loss 0.1940, train acc 84.51%, f1 0.8442, precision 0.8440, recall 0.8444, auc 0.8451
epoch 10801, loss 0.3387, train acc 84.49%, f1 0.8443, precision 0.8425, recall 0.8461, auc 0.8449
epoch 10901, loss 0.3754, train acc 84.57%, f1 0.8448, precision 0.8447, recall 0.8448, auc 0.8457
epoch 11001, loss 0.3235, train acc 84.56%, f1 0.8448, precision 0.8437, recall 0.8460, auc 0.8456
epoch 11101, loss 0.3873, train acc 84.58%, f1 0.8446, precision 0.8456, recall 0.8437, auc 0.8458
epoch 11201, loss 0.3836, train acc 84.60%, f1 0.8452, precision 0.8445, recall 0.8459, auc 0.8460
epoch 11301, loss 0.3291, train acc 84.63%, f1 0.8455, precision 0.8442, recall 0.8469, auc 0.8463
epoch 11401, loss 0.3284, train acc 84.62%, f1 0.8452, precision 0.8453, recall 0.8451, auc 0.8462
epoch 11501, loss 0.4088, train acc 84.65%, f1 0.8453, precision 0.8464, recall 0.8443, auc 0.8465
epoch 11601, loss 0.3814, train acc 84.70%, f1 0.8460, precision 0.8462, recall 0.8458, auc 0.8470
epoch 11701, loss 0.3852, train acc 84.69%, f1 0.8462, precision 0.8447, recall 0.8477, auc 0.8469
epoch 11801, loss 0.3163, train acc 84.64%, f1 0.8451, precision 0.8470, recall 0.8433, auc 0.8464
epoch 11901, loss 0.3121, train acc 84.71%, f1 0.8459, precision 0.8471, recall 0.8448, auc 0.8471
epoch 12001, loss 0.3973, train acc 84.68%, f1 0.8458, precision 0.8462, recall 0.8453, auc 0.8468
epoch 12101, loss 0.5575, train acc 84.62%, f1 0.8450, precision 0.8463, recall 0.8437, auc 0.8462
epoch 12201, loss 0.3708, train acc 84.76%, f1 0.8467, precision 0.8464, recall 0.8470, auc 0.8476
epoch 12301, loss 0.3505, train acc 84.77%, f1 0.8471, precision 0.8453, recall 0.8489, auc 0.8477
epoch 12401, loss 0.3503, train acc 84.81%, f1 0.8472, precision 0.8469, recall 0.8476, auc 0.8481
epoch 12501, loss 0.2593, train acc 84.81%, f1 0.8470, precision 0.8479, recall 0.8462, auc 0.8481
epoch 12601, loss 0.3207, train acc 84.91%, f1 0.8483, precision 0.8472, recall 0.8495, auc 0.8491
epoch 12701, loss 0.4148, train acc 84.84%, f1 0.8479, precision 0.8455, recall 0.8502, auc 0.8484
epoch 12801, loss 0.3133, train acc 84.85%, f1 0.8475, precision 0.8475, recall 0.8476, auc 0.8485
epoch 12901, loss 0.3025, train acc 84.85%, f1 0.8477, precision 0.8469, recall 0.8484, auc 0.8485
epoch 13001, loss 0.2488, train acc 84.86%, f1 0.8477, precision 0.8478, recall 0.8475, auc 0.8486
epoch 13101, loss 0.2959, train acc 84.87%, f1 0.8477, precision 0.8480, recall 0.8474, auc 0.8487
epoch 13201, loss 0.3104, train acc 84.91%, f1 0.8482, precision 0.8480, recall 0.8483, auc 0.8491
epoch 13301, loss 0.3926, train acc 84.89%, f1 0.8482, precision 0.8469, recall 0.8496, auc 0.8490
epoch 13401, loss 0.3179, train acc 84.84%, f1 0.8481, precision 0.8449, recall 0.8513, auc 0.8485
epoch 13501, loss 0.4079, train acc 84.86%, f1 0.8477, precision 0.8477, recall 0.8477, auc 0.8486
epoch 13601, loss 0.2974, train acc 84.96%, f1 0.8489, precision 0.8474, recall 0.8504, auc 0.8496
epoch 13701, loss 0.3696, train acc 84.93%, f1 0.8483, precision 0.8488, recall 0.8478, auc 0.8493
epoch 13801, loss 0.3659, train acc 84.94%, f1 0.8483, precision 0.8488, recall 0.8479, auc 0.8494
epoch 13901, loss 0.3709, train acc 85.00%, f1 0.8488, precision 0.8499, recall 0.8478, auc 0.8500
epoch 14001, loss 0.3331, train acc 84.93%, f1 0.8484, precision 0.8480, recall 0.8488, auc 0.8493
epoch 14101, loss 0.4325, train acc 85.01%, f1 0.8488, precision 0.8507, recall 0.8470, auc 0.8501
epoch 14201, loss 0.2884, train acc 84.98%, f1 0.8487, precision 0.8491, recall 0.8484, auc 0.8497
epoch 14301, loss 0.2918, train acc 85.01%, f1 0.8486, precision 0.8518, recall 0.8455, auc 0.8501
epoch 14401, loss 0.2283, train acc 85.01%, f1 0.8491, precision 0.8495, recall 0.8487, auc 0.8501
epoch 14501, loss 0.2728, train acc 84.98%, f1 0.8485, precision 0.8507, recall 0.8462, auc 0.8498
epoch 14601, loss 0.3462, train acc 85.03%, f1 0.8491, precision 0.8505, recall 0.8477, auc 0.8502
epoch 14701, loss 0.2009, train acc 85.03%, f1 0.8493, precision 0.8498, recall 0.8488, auc 0.8503
epoch 14801, loss 0.3833, train acc 85.02%, f1 0.8492, precision 0.8496, recall 0.8489, auc 0.8502
epoch 14901, loss 0.2826, train acc 85.01%, f1 0.8494, precision 0.8478, recall 0.8511, auc 0.8501
epoch 15001, loss 0.3325, train acc 85.03%, f1 0.8494, precision 0.8490, recall 0.8498, auc 0.8503
epoch 15101, loss 0.3530, train acc 85.07%, f1 0.8496, precision 0.8504, recall 0.8487, auc 0.8506
epoch 15201, loss 0.3357, train acc 85.10%, f1 0.8498, precision 0.8513, recall 0.8482, auc 0.8510
epoch 15301, loss 0.3293, train acc 85.09%, f1 0.8497, precision 0.8512, recall 0.8483, auc 0.8509
epoch 15401, loss 0.3300, train acc 85.12%, f1 0.8504, precision 0.8496, recall 0.8512, auc 0.8512
epoch 15501, loss 0.3108, train acc 85.05%, f1 0.8491, precision 0.8518, recall 0.8465, auc 0.8505
epoch 15601, loss 0.2688, train acc 85.05%, f1 0.8495, precision 0.8502, recall 0.8488, auc 0.8505
epoch 15701, loss 0.5265, train acc 85.00%, f1 0.8491, precision 0.8486, recall 0.8496, auc 0.8500
epoch 15801, loss 0.3600, train acc 85.07%, f1 0.8492, precision 0.8519, recall 0.8466, auc 0.8506
epoch 15901, loss 0.3936, train acc 85.05%, f1 0.8494, precision 0.8501, recall 0.8488, auc 0.8505
epoch 16001, loss 0.3200, train acc 85.09%, f1 0.8497, precision 0.8510, recall 0.8484, auc 0.8509
epoch 16101, loss 0.3132, train acc 85.13%, f1 0.8502, precision 0.8512, recall 0.8493, auc 0.8513
epoch 16201, loss 0.2979, train acc 85.13%, f1 0.8502, precision 0.8512, recall 0.8492, auc 0.8513
epoch 16301, loss 0.3104, train acc 85.19%, f1 0.8508, precision 0.8520, recall 0.8496, auc 0.8519
epoch 16401, loss 0.3850, train acc 85.15%, f1 0.8505, precision 0.8505, recall 0.8506, auc 0.8515
epoch 16501, loss 0.4061, train acc 85.09%, f1 0.8499, precision 0.8498, recall 0.8501, auc 0.8508/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.3617, train acc 85.15%, f1 0.8504, precision 0.8513, recall 0.8495, auc 0.8515
epoch 16701, loss 0.2950, train acc 85.11%, f1 0.8503, precision 0.8496, recall 0.8510, auc 0.8511
epoch 16801, loss 0.2997, train acc 85.16%, f1 0.8506, precision 0.8508, recall 0.8505, auc 0.8516
epoch 16901, loss 0.3880, train acc 85.25%, f1 0.8521, precision 0.8495, recall 0.8547, auc 0.8526
epoch 17001, loss 0.2849, train acc 85.18%, f1 0.8509, precision 0.8506, recall 0.8513, auc 0.8518
epoch 17101, loss 0.2686, train acc 85.11%, f1 0.8503, precision 0.8498, recall 0.8508, auc 0.8511
epoch 17201, loss 0.4530, train acc 85.18%, f1 0.8511, precision 0.8494, recall 0.8528, auc 0.8518
epoch 17301, loss 0.3707, train acc 85.16%, f1 0.8507, precision 0.8505, recall 0.8508, auc 0.8516
epoch 17401, loss 0.3041, train acc 85.19%, f1 0.8511, precision 0.8504, recall 0.8517, auc 0.8519
epoch 17501, loss 0.3500, train acc 85.22%, f1 0.8515, precision 0.8505, recall 0.8524, auc 0.8522
epoch 17601, loss 0.3705, train acc 85.21%, f1 0.8513, precision 0.8506, recall 0.8520, auc 0.8521
epoch 17701, loss 0.4066, train acc 85.19%, f1 0.8510, precision 0.8509, recall 0.8511, auc 0.8519
epoch 17801, loss 0.3354, train acc 85.16%, f1 0.8506, precision 0.8512, recall 0.8500, auc 0.8516
epoch 17901, loss 0.2701, train acc 85.17%, f1 0.8507, precision 0.8514, recall 0.8500, auc 0.8517
epoch 18001, loss 0.3370, train acc 85.17%, f1 0.8508, precision 0.8503, recall 0.8514, auc 0.8517
epoch 18101, loss 0.3724, train acc 85.23%, f1 0.8509, precision 0.8539, recall 0.8479, auc 0.8523
epoch 18201, loss 0.4422, train acc 85.21%, f1 0.8511, precision 0.8516, recall 0.8506, auc 0.8521
epoch 18301, loss 0.4453, train acc 85.29%, f1 0.8517, precision 0.8531, recall 0.8503, auc 0.8529
epoch 18401, loss 0.3751, train acc 85.20%, f1 0.8510, precision 0.8513, recall 0.8507, auc 0.8520
epoch 18501, loss 0.3339, train acc 85.17%, f1 0.8504, precision 0.8522, recall 0.8486, auc 0.8516
epoch 18601, loss 0.3307, train acc 85.19%, f1 0.8513, precision 0.8496, recall 0.8530, auc 0.8519
epoch 18701, loss 0.3168, train acc 85.23%, f1 0.8514, precision 0.8512, recall 0.8515, auc 0.8523
epoch 18801, loss 0.2792, train acc 85.19%, f1 0.8511, precision 0.8502, recall 0.8520, auc 0.8519
epoch 18901, loss 0.3117, train acc 85.25%, f1 0.8512, precision 0.8529, recall 0.8496, auc 0.8524
epoch 19001, loss 0.2758, train acc 85.23%, f1 0.8516, precision 0.8506, recall 0.8526, auc 0.8523
epoch 19101, loss 0.3153, train acc 85.27%, f1 0.8515, precision 0.8529, recall 0.8501, auc 0.8527
epoch 19201, loss 0.2890, train acc 85.24%, f1 0.8515, precision 0.8517, recall 0.8513, auc 0.8524
epoch 19301, loss 0.4709, train acc 85.25%, f1 0.8516, precision 0.8516, recall 0.8516, auc 0.8525
epoch 19401, loss 0.2835, train acc 85.23%, f1 0.8509, precision 0.8536, recall 0.8483, auc 0.8523
epoch 19501, loss 0.3358, train acc 85.32%, f1 0.8523, precision 0.8521, recall 0.8526, auc 0.8532
epoch 19601, loss 0.3567, train acc 85.24%, f1 0.8514, precision 0.8515, recall 0.8514, auc 0.8524
epoch 19701, loss 0.3960, train acc 85.30%, f1 0.8520, precision 0.8523, recall 0.8517, auc 0.8529
epoch 19801, loss 0.3361, train acc 85.35%, f1 0.8522, precision 0.8546, recall 0.8498, auc 0.8535
epoch 19901, loss 0.3393, train acc 85.30%, f1 0.8519, precision 0.8530, recall 0.8508, auc 0.8530
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_3
./test_pima/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.615

the Fscore is 0.5837837837837838

the precision is 0.4122137404580153

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_3
----------------------



epoch 1, loss 0.6927, train acc 50.11%, f1 0.6677, precision 0.5011, recall 1.0000, auc 0.5000
epoch 101, loss 0.6236, train acc 76.81%, f1 0.7776, precision 0.7486, recall 0.8089, auc 0.7680
epoch 201, loss 0.4881, train acc 79.80%, f1 0.7990, precision 0.7970, recall 0.8009, auc 0.7980
epoch 301, loss 0.4592, train acc 81.35%, f1 0.8132, precision 0.8161, recall 0.8104, auc 0.8135
epoch 401, loss 0.3571, train acc 82.41%, f1 0.8238, precision 0.8270, recall 0.8206, auc 0.8241
epoch 501, loss 0.4467, train acc 82.82%, f1 0.8279, precision 0.8312, recall 0.8247, auc 0.8282
epoch 601, loss 0.3335, train acc 83.02%, f1 0.8305, precision 0.8307, recall 0.8304, auc 0.8302
epoch 701, loss 0.2932, train acc 83.08%, f1 0.8305, precision 0.8335, recall 0.8276, auc 0.8308
epoch 801, loss 0.3509, train acc 83.10%, f1 0.8308, precision 0.8334, recall 0.8283, auc 0.8310
epoch 901, loss 0.3258, train acc 83.17%, f1 0.8314, precision 0.8346, recall 0.8284, auc 0.8317
epoch 1001, loss 0.4110, train acc 83.16%, f1 0.8309, precision 0.8363, recall 0.8255, auc 0.8316
epoch 1101, loss 0.3868, train acc 83.14%, f1 0.8309, precision 0.8356, recall 0.8262, auc 0.8315
epoch 1201, loss 0.3857, train acc 83.14%, f1 0.8309, precision 0.8354, recall 0.8264, auc 0.8314
epoch 1301, loss 0.4087, train acc 83.16%, f1 0.8313, precision 0.8346, recall 0.8280, auc 0.8316
epoch 1401, loss 0.4227, train acc 83.17%, f1 0.8315, precision 0.8344, recall 0.8285, auc 0.8317
epoch 1501, loss 0.2881, train acc 83.19%, f1 0.8317, precision 0.8346, recall 0.8288, auc 0.8319
epoch 1601, loss 0.4649, train acc 83.15%, f1 0.8313, precision 0.8342, recall 0.8284, auc 0.8315
epoch 1701, loss 0.3525, train acc 83.10%, f1 0.8307, precision 0.8340, recall 0.8274, auc 0.8310
epoch 1801, loss 0.3720, train acc 83.15%, f1 0.8319, precision 0.8319, recall 0.8319, auc 0.8315
epoch 1901, loss 0.4529, train acc 83.16%, f1 0.8316, precision 0.8333, recall 0.8299, auc 0.8316
epoch 2001, loss 0.3716, train acc 83.17%, f1 0.8317, precision 0.8335, recall 0.8300, auc 0.8317
epoch 2101, loss 0.3975, train acc 83.15%, f1 0.8313, precision 0.8343, recall 0.8283, auc 0.8315
epoch 2201, loss 0.3517, train acc 83.17%, f1 0.8314, precision 0.8349, recall 0.8279, auc 0.8317
epoch 2301, loss 0.3679, train acc 83.15%, f1 0.8315, precision 0.8335, recall 0.8295, auc 0.8315
epoch 2401, loss 0.4712, train acc 83.14%, f1 0.8308, precision 0.8356, recall 0.8261, auc 0.8314
epoch 2501, loss 0.4664, train acc 83.12%, f1 0.8307, precision 0.8350, recall 0.8264, auc 0.8312
epoch 2601, loss 0.4703, train acc 83.15%, f1 0.8312, precision 0.8348, recall 0.8276, auc 0.8315
epoch 2701, loss 0.3289, train acc 83.13%, f1 0.8312, precision 0.8337, recall 0.8288, auc 0.8313
epoch 2801, loss 0.4362, train acc 83.15%, f1 0.8315, precision 0.8332, recall 0.8298, auc 0.8315
epoch 2901, loss 0.4166, train acc 83.20%, f1 0.8319, precision 0.8340, recall 0.8299, auc 0.8320
epoch 3001, loss 0.4025, train acc 83.13%, f1 0.8317, precision 0.8318, recall 0.8315, auc 0.8313
epoch 3101, loss 0.3684, train acc 83.19%, f1 0.8318, precision 0.8342, recall 0.8294, auc 0.8319
epoch 3201, loss 0.2596, train acc 83.19%, f1 0.8319, precision 0.8340, recall 0.8298, auc 0.8319
epoch 3301, loss 0.4250, train acc 83.20%, f1 0.8323, precision 0.8330, recall 0.8316, auc 0.8320
epoch 3401, loss 0.3575, train acc 83.12%, f1 0.8313, precision 0.8325, recall 0.8301, auc 0.8312
epoch 3501, loss 0.2879, train acc 83.13%, f1 0.8312, precision 0.8334, recall 0.8291, auc 0.8313
epoch 3601, loss 0.2996, train acc 83.19%, f1 0.8318, precision 0.8340, recall 0.8296, auc 0.8319
epoch 3701, loss 0.3634, train acc 83.24%, f1 0.8325, precision 0.8338, recall 0.8311, auc 0.8324
epoch 3801, loss 0.3117, train acc 83.18%, f1 0.8313, precision 0.8355, recall 0.8272, auc 0.8318
epoch 3901, loss 0.3346, train acc 83.14%, f1 0.8314, precision 0.8336, recall 0.8292, auc 0.8315
epoch 4001, loss 0.3051, train acc 83.18%, f1 0.8325, precision 0.8307, recall 0.8344, auc 0.8318
epoch 4101, loss 0.2788, train acc 83.18%, f1 0.8321, precision 0.8325, recall 0.8317, auc 0.8318
epoch 4201, loss 0.4101, train acc 83.23%, f1 0.8319, precision 0.8355, recall 0.8284, auc 0.8323
epoch 4301, loss 0.2555, train acc 83.24%, f1 0.8327, precision 0.8331, recall 0.8323, auc 0.8324
epoch 4401, loss 0.3837, train acc 83.25%, f1 0.8328, precision 0.8333, recall 0.8323, auc 0.8325
epoch 4501, loss 0.3137, train acc 83.27%, f1 0.8328, precision 0.8344, recall 0.8311, auc 0.8327
epoch 4601, loss 0.4463, train acc 83.29%, f1 0.8332, precision 0.8335, recall 0.8330, auc 0.8329
epoch 4701, loss 0.3393, train acc 83.27%, f1 0.8323, precision 0.8364, recall 0.8282, auc 0.8327
epoch 4801, loss 0.3680, train acc 83.32%, f1 0.8334, precision 0.8345, recall 0.8323, auc 0.8332
epoch 4901, loss 0.4902, train acc 83.30%, f1 0.8330, precision 0.8351, recall 0.8310, auc 0.8331
epoch 5001, loss 0.3835, train acc 83.35%, f1 0.8333, precision 0.8361, recall 0.8305, auc 0.8335
epoch 5101, loss 0.5122, train acc 83.32%, f1 0.8330, precision 0.8360, recall 0.8300, auc 0.8332
epoch 5201, loss 0.3892, train acc 83.29%, f1 0.8331, precision 0.8342, recall 0.8320, auc 0.8329
epoch 5301, loss 0.3158, train acc 83.32%, f1 0.8332, precision 0.8354, recall 0.8309, auc 0.8332
epoch 5401, loss 0.3928, train acc 83.32%, f1 0.8339, precision 0.8321, recall 0.8357, auc 0.8332
epoch 5501, loss 0.3992, train acc 83.37%, f1 0.8341, precision 0.8339, recall 0.8342, auc 0.8337
epoch 5601, loss 0.4249, train acc 83.38%, f1 0.8339, precision 0.8350, recall 0.8329, auc 0.8338
epoch 5701, loss 0.2426, train acc 83.39%, f1 0.8344, precision 0.8341, recall 0.8346, auc 0.8339
epoch 5801, loss 0.4329, train acc 83.39%, f1 0.8338, precision 0.8360, recall 0.8317, auc 0.8339
epoch 5901, loss 0.3045, train acc 83.35%, f1 0.8332, precision 0.8369, recall 0.8295, auc 0.8335
epoch 6001, loss 0.4141, train acc 83.42%, f1 0.8341, precision 0.8363, recall 0.8320, auc 0.8342
epoch 6101, loss 0.4117, train acc 83.43%, f1 0.8340, precision 0.8376, recall 0.8304, auc 0.8343
epoch 6201, loss 0.3966, train acc 83.51%, f1 0.8350, precision 0.8375, recall 0.8325, auc 0.8351
epoch 6301, loss 0.4905, train acc 83.52%, f1 0.8353, precision 0.8368, recall 0.8338, auc 0.8352
epoch 6401, loss 0.3275, train acc 83.53%, f1 0.8348, precision 0.8392, recall 0.8303, auc 0.8353
epoch 6501, loss 0.5123, train acc 83.59%, f1 0.8360, precision 0.8374, recall 0.8346, auc 0.8359
epoch 6601, loss 0.3440, train acc 83.57%, f1 0.8356, precision 0.8377, recall 0.8336, auc 0.8357
epoch 6701, loss 0.4003, train acc 83.58%, f1 0.8362, precision 0.8359, recall 0.8365, auc 0.8358
epoch 6801, loss 0.3358, train acc 83.65%, f1 0.8365, precision 0.8384, recall 0.8347, auc 0.8365
epoch 6901, loss 0.3924, train acc 83.64%, f1 0.8364, precision 0.8383, recall 0.8345, auc 0.8364
epoch 7001, loss 0.3049, train acc 83.69%, f1 0.8366, precision 0.8397, recall 0.8336, auc 0.8369
epoch 7101, loss 0.2627, train acc 83.75%, f1 0.8373, precision 0.8402, recall 0.8343, auc 0.8375
epoch 7201, loss 0.3195, train acc 83.73%, f1 0.8374, precision 0.8392, recall 0.8355, auc 0.8374
epoch 7301, loss 0.4187, train acc 83.83%, f1 0.8387, precision 0.8383, recall 0.8392, auc 0.8383
epoch 7401, loss 0.3064, train acc 83.79%, f1 0.8378, precision 0.8403, recall 0.8354, auc 0.8379
epoch 7501, loss 0.3146, train acc 83.80%, f1 0.8378, precision 0.8410, recall 0.8346, auc 0.8381
epoch 7601, loss 0.3643, train acc 83.87%, f1 0.8387, precision 0.8408, recall 0.8365, auc 0.8387
epoch 7701, loss 0.2558, train acc 83.91%, f1 0.8388, precision 0.8423, recall 0.8354, auc 0.8392
epoch 7801, loss 0.2855, train acc 83.89%, f1 0.8382, precision 0.8440, recall 0.8324, auc 0.8390
epoch 7901, loss 0.3309, train acc 83.92%, f1 0.8392, precision 0.8411, recall 0.8374, auc 0.8392
epoch 8001, loss 0.4744, train acc 83.96%, f1 0.8397, precision 0.8412, recall 0.8383, auc 0.8396
epoch 8101, loss 0.3070, train acc 84.01%, f1 0.8394, precision 0.8451, recall 0.8338, auc 0.8401
epoch 8201, loss 0.3763, train acc 84.01%, f1 0.8400, precision 0.8422, recall 0.8379, auc 0.8401/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.3475, train acc 84.05%, f1 0.8405, precision 0.8427, recall 0.8382, auc 0.8405
epoch 8401, loss 0.3355, train acc 84.14%, f1 0.8414, precision 0.8431, recall 0.8397, auc 0.8414
epoch 8501, loss 0.3249, train acc 84.09%, f1 0.8409, precision 0.8429, recall 0.8389, auc 0.8409
epoch 8601, loss 0.3046, train acc 84.12%, f1 0.8411, precision 0.8435, recall 0.8387, auc 0.8412
epoch 8701, loss 0.3981, train acc 84.16%, f1 0.8415, precision 0.8440, recall 0.8390, auc 0.8416
epoch 8801, loss 0.2806, train acc 84.23%, f1 0.8426, precision 0.8432, recall 0.8419, auc 0.8423
epoch 8901, loss 0.3975, train acc 84.20%, f1 0.8415, precision 0.8461, recall 0.8368, auc 0.8420
epoch 9001, loss 0.4641, train acc 84.23%, f1 0.8422, precision 0.8444, recall 0.8401, auc 0.8423
epoch 9101, loss 0.3361, train acc 84.23%, f1 0.8418, precision 0.8466, recall 0.8371, auc 0.8423
epoch 9201, loss 0.3787, train acc 84.28%, f1 0.8429, precision 0.8439, recall 0.8419, auc 0.8428
epoch 9301, loss 0.5047, train acc 84.22%, f1 0.8419, precision 0.8455, recall 0.8384, auc 0.8422
epoch 9401, loss 0.4000, train acc 84.27%, f1 0.8430, precision 0.8435, recall 0.8425, auc 0.8427
epoch 9501, loss 0.3272, train acc 84.32%, f1 0.8430, precision 0.8461, recall 0.8399, auc 0.8433
epoch 9601, loss 0.2946, train acc 84.38%, f1 0.8438, precision 0.8454, recall 0.8422, auc 0.8438
epoch 9701, loss 0.3024, train acc 84.36%, f1 0.8436, precision 0.8457, recall 0.8415, auc 0.8436
epoch 9801, loss 0.2938, train acc 84.38%, f1 0.8438, precision 0.8455, recall 0.8421, auc 0.8438
epoch 9901, loss 0.4046, train acc 84.44%, f1 0.8440, precision 0.8480, recall 0.8401, auc 0.8444
epoch 10001, loss 0.2981, train acc 84.51%, f1 0.8452, precision 0.8467, recall 0.8437, auc 0.8451
epoch 10101, loss 0.3483, train acc 84.47%, f1 0.8447, precision 0.8469, recall 0.8425, auc 0.8447
epoch 10201, loss 0.3889, train acc 84.44%, f1 0.8444, precision 0.8465, recall 0.8423, auc 0.8444
epoch 10301, loss 0.3605, train acc 84.53%, f1 0.8454, precision 0.8464, recall 0.8444, auc 0.8453
epoch 10401, loss 0.3310, train acc 84.53%, f1 0.8454, precision 0.8466, recall 0.8442, auc 0.8453
epoch 10501, loss 0.4053, train acc 84.54%, f1 0.8454, precision 0.8474, recall 0.8435, auc 0.8454
epoch 10601, loss 0.2756, train acc 84.54%, f1 0.8452, precision 0.8482, recall 0.8422, auc 0.8454
epoch 10701, loss 0.3544, train acc 84.59%, f1 0.8460, precision 0.8475, recall 0.8444, auc 0.8459
epoch 10801, loss 0.4414, train acc 84.58%, f1 0.8458, precision 0.8474, recall 0.8443, auc 0.8458
epoch 10901, loss 0.4381, train acc 84.62%, f1 0.8462, precision 0.8478, recall 0.8446, auc 0.8462
epoch 11001, loss 0.3607, train acc 84.65%, f1 0.8470, precision 0.8464, recall 0.8476, auc 0.8465
epoch 11101, loss 0.3854, train acc 84.62%, f1 0.8458, precision 0.8500, recall 0.8417, auc 0.8462
epoch 11201, loss 0.3498, train acc 84.63%, f1 0.8463, precision 0.8482, recall 0.8444, auc 0.8463
epoch 11301, loss 0.3538, train acc 84.65%, f1 0.8464, precision 0.8491, recall 0.8437, auc 0.8465
epoch 11401, loss 0.2723, train acc 84.70%, f1 0.8469, precision 0.8492, recall 0.8446, auc 0.8470
epoch 11501, loss 0.3913, train acc 84.75%, f1 0.8475, precision 0.8495, recall 0.8456, auc 0.8475
epoch 11601, loss 0.3526, train acc 84.74%, f1 0.8471, precision 0.8508, recall 0.8434, auc 0.8474
epoch 11701, loss 0.3024, train acc 84.77%, f1 0.8478, precision 0.8493, recall 0.8462, auc 0.8477
epoch 11801, loss 0.3501, train acc 84.82%, f1 0.8481, precision 0.8504, recall 0.8458, auc 0.8482
epoch 11901, loss 0.3471, train acc 84.79%, f1 0.8484, precision 0.8476, recall 0.8493, auc 0.8479
epoch 12001, loss 0.3505, train acc 84.78%, f1 0.8478, precision 0.8496, recall 0.8459, auc 0.8478
epoch 12101, loss 0.3438, train acc 84.83%, f1 0.8484, precision 0.8500, recall 0.8468, auc 0.8483
epoch 12201, loss 0.3520, train acc 84.82%, f1 0.8485, precision 0.8488, recall 0.8483, auc 0.8482
epoch 12301, loss 0.4019, train acc 84.84%, f1 0.8480, precision 0.8523, recall 0.8438, auc 0.8485
epoch 12401, loss 0.3320, train acc 84.84%, f1 0.8480, precision 0.8523, recall 0.8438, auc 0.8484
epoch 12501, loss 0.4407, train acc 84.87%, f1 0.8488, precision 0.8503, recall 0.8473, auc 0.8487
epoch 12601, loss 0.4079, train acc 84.85%, f1 0.8485, precision 0.8504, recall 0.8466, auc 0.8485
epoch 12701, loss 0.4290, train acc 84.95%, f1 0.8494, precision 0.8514, recall 0.8475, auc 0.8495
epoch 12801, loss 0.2764, train acc 84.96%, f1 0.8494, precision 0.8523, recall 0.8464, auc 0.8496
epoch 12901, loss 0.2476, train acc 84.92%, f1 0.8491, precision 0.8518, recall 0.8465, auc 0.8492
epoch 13001, loss 0.5471, train acc 84.91%, f1 0.8491, precision 0.8512, recall 0.8470, auc 0.8491
epoch 13101, loss 0.3532, train acc 84.92%, f1 0.8493, precision 0.8505, recall 0.8481, auc 0.8492
epoch 13201, loss 0.3515, train acc 84.98%, f1 0.8499, precision 0.8513, recall 0.8486, auc 0.8498
epoch 13301, loss 0.3033, train acc 85.00%, f1 0.8497, precision 0.8532, recall 0.8463, auc 0.8500
epoch 13401, loss 0.3496, train acc 84.94%, f1 0.8494, precision 0.8512, recall 0.8476, auc 0.8494
epoch 13501, loss 0.2802, train acc 84.98%, f1 0.8494, precision 0.8534, recall 0.8454, auc 0.8498
epoch 13601, loss 0.4832, train acc 84.95%, f1 0.8494, precision 0.8521, recall 0.8466, auc 0.8495
epoch 13701, loss 0.3039, train acc 85.02%, f1 0.8505, precision 0.8508, recall 0.8502, auc 0.8502
epoch 13801, loss 0.4035, train acc 85.05%, f1 0.8505, precision 0.8527, recall 0.8483, auc 0.8505
epoch 13901, loss 0.4736, train acc 85.06%, f1 0.8508, precision 0.8518, recall 0.8498, auc 0.8506
epoch 14001, loss 0.2857, train acc 85.04%, f1 0.8502, precision 0.8532, recall 0.8471, auc 0.8504
epoch 14101, loss 0.3884, train acc 85.03%, f1 0.8504, precision 0.8518, recall 0.8490, auc 0.8503
epoch 14201, loss 0.3075, train acc 85.06%, f1 0.8505, precision 0.8530, recall 0.8481, auc 0.8506
epoch 14301, loss 0.4124, train acc 85.00%, f1 0.8499, precision 0.8525, recall 0.8473, auc 0.8500
epoch 14401, loss 0.3188, train acc 84.99%, f1 0.8501, precision 0.8511, recall 0.8491, auc 0.8499
epoch 14501, loss 0.2541, train acc 85.12%, f1 0.8513, precision 0.8525, recall 0.8500, auc 0.8512
epoch 14601, loss 0.4524, train acc 85.07%, f1 0.8507, precision 0.8530, recall 0.8484, auc 0.8507
epoch 14701, loss 0.3665, train acc 85.11%, f1 0.8509, precision 0.8539, recall 0.8478, auc 0.8511
epoch 14801, loss 0.4164, train acc 85.03%, f1 0.8501, precision 0.8533, recall 0.8469, auc 0.8503
epoch 14901, loss 0.2884, train acc 85.13%, f1 0.8515, precision 0.8519, recall 0.8512, auc 0.8513
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_3
./test_pima/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.63

the Fscore is 0.5934065934065934

the precision is 0.421875

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_3
----------------------



epoch 1, loss 0.6932, train acc 50.14%, f1 0.6679, precision 0.5014, recall 1.0000, auc 0.5000
epoch 101, loss 0.6116, train acc 77.11%, f1 0.7710, precision 0.7733, recall 0.7687, auc 0.7711
epoch 201, loss 0.5151, train acc 79.47%, f1 0.7956, precision 0.7944, recall 0.7967, auc 0.7947
epoch 301, loss 0.4343, train acc 81.49%, f1 0.8153, precision 0.8161, recall 0.8144, auc 0.8149
epoch 401, loss 0.4239, train acc 82.52%, f1 0.8259, precision 0.8249, recall 0.8270, auc 0.8252
epoch 501, loss 0.4348, train acc 82.95%, f1 0.8297, precision 0.8309, recall 0.8285, auc 0.8295
epoch 601, loss 0.3423, train acc 83.02%, f1 0.8306, precision 0.8311, recall 0.8302, auc 0.8302
epoch 701, loss 0.4496, train acc 83.09%, f1 0.8308, precision 0.8332, recall 0.8284, auc 0.8309
epoch 801, loss 0.3644, train acc 83.12%, f1 0.8316, precision 0.8320, recall 0.8311, auc 0.8312
epoch 901, loss 0.4781, train acc 83.10%, f1 0.8312, precision 0.8323, recall 0.8302, auc 0.8310
epoch 1001, loss 0.3161, train acc 83.13%, f1 0.8321, precision 0.8304, recall 0.8338, auc 0.8313
epoch 1101, loss 0.3187, train acc 83.16%, f1 0.8318, precision 0.8332, recall 0.8304, auc 0.8316
epoch 1201, loss 0.2628, train acc 83.16%, f1 0.8320, precision 0.8322, recall 0.8318, auc 0.8316
epoch 1301, loss 0.3705, train acc 83.16%, f1 0.8318, precision 0.8332, recall 0.8304, auc 0.8316
epoch 1401, loss 0.3499, train acc 83.17%, f1 0.8319, precision 0.8336, recall 0.8301, auc 0.8317
epoch 1501, loss 0.3766, train acc 83.18%, f1 0.8321, precision 0.8327, recall 0.8315, auc 0.8318
epoch 1601, loss 0.3743, train acc 83.19%, f1 0.8323, precision 0.8325, recall 0.8322, auc 0.8319
epoch 1701, loss 0.3331, train acc 83.14%, f1 0.8320, precision 0.8316, recall 0.8324, auc 0.8314
epoch 1801, loss 0.3439, train acc 83.17%, f1 0.8323, precision 0.8318, recall 0.8328, auc 0.8317
epoch 1901, loss 0.4110, train acc 83.19%, f1 0.8322, precision 0.8332, recall 0.8312, auc 0.8319
epoch 2001, loss 0.3780, train acc 83.16%, f1 0.8315, precision 0.8340, recall 0.8291, auc 0.8316
epoch 2101, loss 0.3371, train acc 83.14%, f1 0.8317, precision 0.8324, recall 0.8310, auc 0.8314
epoch 2201, loss 0.4332, train acc 83.15%, f1 0.8316, precision 0.8332, recall 0.8300, auc 0.8315
epoch 2301, loss 0.3147, train acc 83.20%, f1 0.8323, precision 0.8329, recall 0.8318, auc 0.8320
epoch 2401, loss 0.5575, train acc 83.11%, f1 0.8318, precision 0.8307, recall 0.8329, auc 0.8311
epoch 2501, loss 0.3772, train acc 83.15%, f1 0.8319, precision 0.8323, recall 0.8315, auc 0.8315
epoch 2601, loss 0.3620, train acc 83.13%, f1 0.8312, precision 0.8337, recall 0.8287, auc 0.8313
epoch 2701, loss 0.3370, train acc 83.11%, f1 0.8316, precision 0.8314, recall 0.8318, auc 0.8311
epoch 2801, loss 0.3242, train acc 83.10%, f1 0.8311, precision 0.8329, recall 0.8292, auc 0.8310
epoch 2901, loss 0.3176, train acc 83.11%, f1 0.8314, precision 0.8320, recall 0.8309, auc 0.8311
epoch 3001, loss 0.4087, train acc 83.13%, f1 0.8315, precision 0.8331, recall 0.8299, auc 0.8313
epoch 3101, loss 0.3202, train acc 83.11%, f1 0.8312, precision 0.8331, recall 0.8293, auc 0.8311
epoch 3201, loss 0.3463, train acc 83.16%, f1 0.8314, precision 0.8349, recall 0.8279, auc 0.8316
epoch 3301, loss 0.5070, train acc 83.13%, f1 0.8313, precision 0.8335, recall 0.8292, auc 0.8313
epoch 3401, loss 0.4528, train acc 83.13%, f1 0.8312, precision 0.8338, recall 0.8287, auc 0.8313
epoch 3501, loss 0.3598, train acc 83.16%, f1 0.8319, precision 0.8330, recall 0.8308, auc 0.8316
epoch 3601, loss 0.4454, train acc 83.19%, f1 0.8323, precision 0.8327, recall 0.8318, auc 0.8319
epoch 3701, loss 0.3508, train acc 83.14%, f1 0.8320, precision 0.8312, recall 0.8329, auc 0.8314
epoch 3801, loss 0.2494, train acc 83.11%, f1 0.8316, precision 0.8317, recall 0.8314, auc 0.8311
epoch 3901, loss 0.3100, train acc 83.13%, f1 0.8319, precision 0.8312, recall 0.8326, auc 0.8313
epoch 4001, loss 0.3449, train acc 83.14%, f1 0.8317, precision 0.8327, recall 0.8307, auc 0.8314
epoch 4101, loss 0.3131, train acc 83.13%, f1 0.8318, precision 0.8317, recall 0.8319, auc 0.8313
epoch 4201, loss 0.4292, train acc 83.19%, f1 0.8325, precision 0.8318, recall 0.8332, auc 0.8319
epoch 4301, loss 0.5721, train acc 83.21%, f1 0.8321, precision 0.8343, recall 0.8300, auc 0.8321
epoch 4401, loss 0.2886, train acc 83.22%, f1 0.8322, precision 0.8345, recall 0.8299, auc 0.8322
epoch 4501, loss 0.2683, train acc 83.26%, f1 0.8325, precision 0.8351, recall 0.8299, auc 0.8326
epoch 4601, loss 0.3958, train acc 83.26%, f1 0.8328, precision 0.8340, recall 0.8316, auc 0.8326
epoch 4701, loss 0.3338, train acc 83.26%, f1 0.8328, precision 0.8343, recall 0.8313, auc 0.8326
epoch 4801, loss 0.3186, train acc 83.32%, f1 0.8330, precision 0.8361, recall 0.8301, auc 0.8332
epoch 4901, loss 0.3636, train acc 83.29%, f1 0.8335, precision 0.8330, recall 0.8340, auc 0.8329
epoch 5001, loss 0.3325, train acc 83.31%, f1 0.8336, precision 0.8335, recall 0.8337, auc 0.8331
epoch 5101, loss 0.4045, train acc 83.31%, f1 0.8333, precision 0.8346, recall 0.8320, auc 0.8331
epoch 5201, loss 0.4092, train acc 83.30%, f1 0.8331, precision 0.8348, recall 0.8314, auc 0.8330
epoch 5301, loss 0.3193, train acc 83.38%, f1 0.8344, precision 0.8334, recall 0.8355, auc 0.8338
epoch 5401, loss 0.3430, train acc 83.36%, f1 0.8335, precision 0.8361, recall 0.8309, auc 0.8336
epoch 5501, loss 0.3430, train acc 83.38%, f1 0.8340, precision 0.8354, recall 0.8327, auc 0.8338
epoch 5601, loss 0.3754, train acc 83.43%, f1 0.8346, precision 0.8354, recall 0.8338, auc 0.8343
epoch 5701, loss 0.3282, train acc 83.46%, f1 0.8348, precision 0.8364, recall 0.8332, auc 0.8346
epoch 5801, loss 0.2360, train acc 83.37%, f1 0.8342, precision 0.8337, recall 0.8348, auc 0.8337
epoch 5901, loss 0.2701, train acc 83.42%, f1 0.8348, precision 0.8340, recall 0.8357, auc 0.8342
epoch 6001, loss 0.3719, train acc 83.50%, f1 0.8354, precision 0.8361, recall 0.8346, auc 0.8350
epoch 6101, loss 0.3789, train acc 83.49%, f1 0.8354, precision 0.8355, recall 0.8352, auc 0.8349
epoch 6201, loss 0.4115, train acc 83.55%, f1 0.8351, precision 0.8393, recall 0.8310, auc 0.8355
epoch 6301, loss 0.3231, train acc 83.57%, f1 0.8359, precision 0.8375, recall 0.8342, auc 0.8357
epoch 6401, loss 0.2968, train acc 83.60%, f1 0.8362, precision 0.8375, recall 0.8350, auc 0.8360
epoch 6501, loss 0.3147, train acc 83.60%, f1 0.8362, precision 0.8375, recall 0.8349, auc 0.8360
epoch 6601, loss 0.4084, train acc 83.69%, f1 0.8372, precision 0.8382, recall 0.8362, auc 0.8369
epoch 6701, loss 0.3934, train acc 83.72%, f1 0.8371, precision 0.8397, recall 0.8345, auc 0.8372
epoch 6801, loss 0.5044, train acc 83.72%, f1 0.8375, precision 0.8381, recall 0.8370, auc 0.8372
epoch 6901, loss 0.4057, train acc 83.73%, f1 0.8377, precision 0.8378, recall 0.8377, auc 0.8373
epoch 7001, loss 0.3246, train acc 83.78%, f1 0.8384, precision 0.8376, recall 0.8392, auc 0.8378
epoch 7101, loss 0.3638, train acc 83.86%, f1 0.8384, precision 0.8418, recall 0.8350, auc 0.8386
epoch 7201, loss 0.4038, train acc 83.86%, f1 0.8388, precision 0.8399, recall 0.8377, auc 0.8386
epoch 7301, loss 0.3146, train acc 83.90%, f1 0.8397, precision 0.8382, recall 0.8412, auc 0.8390
epoch 7401, loss 0.3073, train acc 83.86%, f1 0.8392, precision 0.8382, recall 0.8402, auc 0.8386
epoch 7501, loss 0.3899, train acc 84.01%, f1 0.8403, precision 0.8418, recall 0.8388, auc 0.8401
epoch 7601, loss 0.3151, train acc 84.01%, f1 0.8406, precision 0.8405, recall 0.8407, auc 0.8401
epoch 7701, loss 0.3821, train acc 84.05%, f1 0.8406, precision 0.8421, recall 0.8392, auc 0.8405
epoch 7801, loss 0.3861, train acc 83.95%, f1 0.8398, precision 0.8406, recall 0.8390, auc 0.8395
epoch 7901, loss 0.2766, train acc 84.04%, f1 0.8404, precision 0.8430, recall 0.8378, auc 0.8404
epoch 8001, loss 0.4081, train acc 84.02%, f1 0.8401, precision 0.8430, recall 0.8372, auc 0.8402
epoch 8101, loss 0.3665, train acc 84.05%, f1 0.8409, precision 0.8414, recall 0.8404, auc 0.8405
epoch 8201, loss 0.3352, train acc 84.12%, f1 0.8417, precision 0.8412, recall 0.8423, auc 0.8412/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2410, train acc 84.09%, f1 0.8410, precision 0.8426, recall 0.8395, auc 0.8409
epoch 8401, loss 0.4329, train acc 84.21%, f1 0.8424, precision 0.8433, recall 0.8415, auc 0.8421
epoch 8501, loss 0.3421, train acc 84.15%, f1 0.8414, precision 0.8443, recall 0.8385, auc 0.8415
epoch 8601, loss 0.2455, train acc 84.17%, f1 0.8417, precision 0.8437, recall 0.8398, auc 0.8417
epoch 8701, loss 0.4436, train acc 84.20%, f1 0.8421, precision 0.8436, recall 0.8407, auc 0.8420
epoch 8801, loss 0.3727, train acc 84.24%, f1 0.8424, precision 0.8444, recall 0.8405, auc 0.8424
epoch 8901, loss 0.3466, train acc 84.29%, f1 0.8434, precision 0.8427, recall 0.8441, auc 0.8429
epoch 9001, loss 0.3074, train acc 84.25%, f1 0.8425, precision 0.8446, recall 0.8405, auc 0.8425
epoch 9101, loss 0.3208, train acc 84.29%, f1 0.8429, precision 0.8453, recall 0.8406, auc 0.8429
epoch 9201, loss 0.4034, train acc 84.31%, f1 0.8436, precision 0.8432, recall 0.8441, auc 0.8431
epoch 9301, loss 0.3913, train acc 84.39%, f1 0.8442, precision 0.8453, recall 0.8430, auc 0.8440
epoch 9401, loss 0.4699, train acc 84.38%, f1 0.8443, precision 0.8437, recall 0.8448, auc 0.8438
epoch 9501, loss 0.4528, train acc 84.35%, f1 0.8438, precision 0.8444, recall 0.8432, auc 0.8435
epoch 9601, loss 0.3448, train acc 84.41%, f1 0.8445, precision 0.8446, recall 0.8443, auc 0.8441
epoch 9701, loss 0.2880, train acc 84.41%, f1 0.8446, precision 0.8445, recall 0.8447, auc 0.8441
epoch 9801, loss 0.3846, train acc 84.42%, f1 0.8441, precision 0.8469, recall 0.8414, auc 0.8442
epoch 9901, loss 0.2562, train acc 84.48%, f1 0.8453, precision 0.8448, recall 0.8458, auc 0.8448
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_3
./test_pima/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6157407407407407

the Fscore is 0.5824175824175825

the precision is 0.4140625

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_3
----------------------



epoch 1, loss 0.6932, train acc 49.99%, f1 0.6666, precision 0.4999, recall 1.0000, auc 0.5000
epoch 101, loss 0.6197, train acc 77.54%, f1 0.7682, precision 0.7936, recall 0.7444, auc 0.7754
epoch 201, loss 0.4651, train acc 80.03%, f1 0.7997, precision 0.8020, recall 0.7975, auc 0.8003
epoch 301, loss 0.4767, train acc 81.48%, f1 0.8151, precision 0.8137, recall 0.8165, auc 0.8148
epoch 401, loss 0.3302, train acc 82.38%, f1 0.8250, precision 0.8192, recall 0.8310, auc 0.8238
epoch 501, loss 0.3419, train acc 82.79%, f1 0.8289, precision 0.8238, recall 0.8341, auc 0.8279
epoch 601, loss 0.3721, train acc 82.93%, f1 0.8302, precision 0.8254, recall 0.8351, auc 0.8293
epoch 701, loss 0.3871, train acc 83.05%, f1 0.8314, precision 0.8273, recall 0.8354, auc 0.8305
epoch 801, loss 0.3162, train acc 83.08%, f1 0.8323, precision 0.8250, recall 0.8397, auc 0.8308
epoch 901, loss 0.3554, train acc 83.11%, f1 0.8319, precision 0.8280, recall 0.8358, auc 0.8311
epoch 1001, loss 0.2750, train acc 83.12%, f1 0.8321, precision 0.8279, recall 0.8363, auc 0.8313
epoch 1101, loss 0.3707, train acc 83.15%, f1 0.8325, precision 0.8274, recall 0.8377, auc 0.8315
epoch 1201, loss 0.2851, train acc 83.16%, f1 0.8321, precision 0.8294, recall 0.8348, auc 0.8316
epoch 1301, loss 0.4016, train acc 83.14%, f1 0.8325, precision 0.8268, recall 0.8383, auc 0.8314
epoch 1401, loss 0.4420, train acc 83.16%, f1 0.8323, precision 0.8287, recall 0.8360, auc 0.8316
epoch 1501, loss 0.2986, train acc 83.13%, f1 0.8325, precision 0.8267, recall 0.8384, auc 0.8313
epoch 1601, loss 0.5815, train acc 83.17%, f1 0.8327, precision 0.8276, recall 0.8379, auc 0.8317
epoch 1701, loss 0.2574, train acc 83.16%, f1 0.8326, precision 0.8275, recall 0.8378, auc 0.8316
epoch 1801, loss 0.4410, train acc 83.16%, f1 0.8319, precision 0.8300, recall 0.8339, auc 0.8316
epoch 1901, loss 0.3982, train acc 83.16%, f1 0.8324, precision 0.8285, recall 0.8363, auc 0.8316
epoch 2001, loss 0.3632, train acc 83.14%, f1 0.8321, precision 0.8284, recall 0.8358, auc 0.8314
epoch 2101, loss 0.3654, train acc 83.11%, f1 0.8317, precision 0.8286, recall 0.8348, auc 0.8311
epoch 2201, loss 0.5527, train acc 83.11%, f1 0.8318, precision 0.8283, recall 0.8353, auc 0.8311
epoch 2301, loss 0.4632, train acc 83.14%, f1 0.8319, precision 0.8295, recall 0.8343, auc 0.8314
epoch 2401, loss 0.4510, train acc 83.13%, f1 0.8324, precision 0.8270, recall 0.8379, auc 0.8313
epoch 2501, loss 0.4207, train acc 83.16%, f1 0.8320, precision 0.8299, recall 0.8341, auc 0.8316
epoch 2601, loss 0.3560, train acc 83.10%, f1 0.8311, precision 0.8306, recall 0.8316, auc 0.8310
epoch 2701, loss 0.4778, train acc 83.10%, f1 0.8314, precision 0.8297, recall 0.8331, auc 0.8310
epoch 2801, loss 0.3113, train acc 83.14%, f1 0.8315, precision 0.8307, recall 0.8323, auc 0.8314
epoch 2901, loss 0.3687, train acc 83.09%, f1 0.8314, precision 0.8286, recall 0.8342, auc 0.8309
epoch 3001, loss 0.3229, train acc 83.11%, f1 0.8321, precision 0.8274, recall 0.8368, auc 0.8311
epoch 3101, loss 0.4064, train acc 83.12%, f1 0.8322, precision 0.8269, recall 0.8377, auc 0.8312
epoch 3201, loss 0.4466, train acc 83.21%, f1 0.8328, precision 0.8292, recall 0.8365, auc 0.8321
epoch 3301, loss 0.3008, train acc 83.12%, f1 0.8318, precision 0.8287, recall 0.8350, auc 0.8312
epoch 3401, loss 0.4991, train acc 83.07%, f1 0.8307, precision 0.8304, recall 0.8311, auc 0.8307
epoch 3501, loss 0.3164, train acc 83.09%, f1 0.8313, precision 0.8293, recall 0.8334, auc 0.8309
epoch 3601, loss 0.3483, train acc 83.11%, f1 0.8316, precision 0.8291, recall 0.8340, auc 0.8311
epoch 3701, loss 0.3549, train acc 83.11%, f1 0.8318, precision 0.8282, recall 0.8354, auc 0.8311
epoch 3801, loss 0.3592, train acc 83.13%, f1 0.8319, precision 0.8291, recall 0.8347, auc 0.8313
epoch 3901, loss 0.3359, train acc 83.19%, f1 0.8329, precision 0.8280, recall 0.8379, auc 0.8319
epoch 4001, loss 0.4439, train acc 83.18%, f1 0.8315, precision 0.8329, recall 0.8302, auc 0.8318
epoch 4101, loss 0.4158, train acc 83.18%, f1 0.8323, precision 0.8294, recall 0.8352, auc 0.8318
epoch 4201, loss 0.3197, train acc 83.19%, f1 0.8321, precision 0.8307, recall 0.8336, auc 0.8319
epoch 4301, loss 0.4044, train acc 83.18%, f1 0.8319, precision 0.8312, recall 0.8326, auc 0.8318
epoch 4401, loss 0.2599, train acc 83.17%, f1 0.8323, precision 0.8295, recall 0.8351, auc 0.8317
epoch 4501, loss 0.4658, train acc 83.23%, f1 0.8325, precision 0.8317, recall 0.8333, auc 0.8323
epoch 4601, loss 0.3727, train acc 83.23%, f1 0.8325, precision 0.8315, recall 0.8334, auc 0.8323
epoch 4701, loss 0.3392, train acc 83.30%, f1 0.8334, precision 0.8312, recall 0.8357, auc 0.8330
epoch 4801, loss 0.4247, train acc 83.28%, f1 0.8340, precision 0.8279, recall 0.8402, auc 0.8328
epoch 4901, loss 0.3358, train acc 83.33%, f1 0.8334, precision 0.8325, recall 0.8344, auc 0.8333
epoch 5001, loss 0.2922, train acc 83.27%, f1 0.8332, precision 0.8309, recall 0.8355, auc 0.8327
epoch 5101, loss 0.3706, train acc 83.29%, f1 0.8333, precision 0.8311, recall 0.8355, auc 0.8329
epoch 5201, loss 0.3843, train acc 83.30%, f1 0.8333, precision 0.8317, recall 0.8349, auc 0.8330
epoch 5301, loss 0.4164, train acc 83.27%, f1 0.8334, precision 0.8296, recall 0.8372, auc 0.8327
epoch 5401, loss 0.3940, train acc 83.28%, f1 0.8331, precision 0.8316, recall 0.8346, auc 0.8328
epoch 5501, loss 0.4331, train acc 83.30%, f1 0.8339, precision 0.8293, recall 0.8385, auc 0.8330
epoch 5601, loss 0.3724, train acc 83.33%, f1 0.8338, precision 0.8312, recall 0.8364, auc 0.8333
epoch 5701, loss 0.3286, train acc 83.37%, f1 0.8341, precision 0.8321, recall 0.8361, auc 0.8337
epoch 5801, loss 0.3028, train acc 83.36%, f1 0.8339, precision 0.8325, recall 0.8353, auc 0.8336
epoch 5901, loss 0.3778, train acc 83.38%, f1 0.8342, precision 0.8321, recall 0.8362, auc 0.8338
epoch 6001, loss 0.3459, train acc 83.42%, f1 0.8350, precision 0.8307, recall 0.8394, auc 0.8342
epoch 6101, loss 0.3467, train acc 83.41%, f1 0.8348, precision 0.8313, recall 0.8383, auc 0.8341
epoch 6201, loss 0.3506, train acc 83.44%, f1 0.8350, precision 0.8320, recall 0.8380, auc 0.8344
epoch 6301, loss 0.3359, train acc 83.48%, f1 0.8356, precision 0.8312, recall 0.8401, auc 0.8348
epoch 6401, loss 0.3768, train acc 83.49%, f1 0.8355, precision 0.8324, recall 0.8386, auc 0.8349
epoch 6501, loss 0.4409, train acc 83.48%, f1 0.8352, precision 0.8333, recall 0.8370, auc 0.8348
epoch 6601, loss 0.3471, train acc 83.47%, f1 0.8353, precision 0.8321, recall 0.8385, auc 0.8347
epoch 6701, loss 0.2284, train acc 83.55%, f1 0.8359, precision 0.8336, recall 0.8382, auc 0.8355
epoch 6801, loss 0.2349, train acc 83.53%, f1 0.8359, precision 0.8329, recall 0.8390, auc 0.8353
epoch 6901, loss 0.3233, train acc 83.62%, f1 0.8364, precision 0.8353, recall 0.8375, auc 0.8362
epoch 7001, loss 0.3723, train acc 83.59%, f1 0.8365, precision 0.8336, recall 0.8395, auc 0.8359
epoch 7101, loss 0.3504, train acc 83.53%, f1 0.8359, precision 0.8327, recall 0.8391, auc 0.8353
epoch 7201, loss 0.3628, train acc 83.56%, f1 0.8361, precision 0.8334, recall 0.8389, auc 0.8356
epoch 7301, loss 0.3964, train acc 83.58%, f1 0.8363, precision 0.8338, recall 0.8387, auc 0.8358
epoch 7401, loss 0.3745, train acc 83.64%, f1 0.8367, precision 0.8355, recall 0.8379, auc 0.8364
epoch 7501, loss 0.2486, train acc 83.62%, f1 0.8373, precision 0.8320, recall 0.8425, auc 0.8363
epoch 7601, loss 0.3790, train acc 83.70%, f1 0.8376, precision 0.8344, recall 0.8407, auc 0.8370
epoch 7701, loss 0.2576, train acc 83.69%, f1 0.8368, precision 0.8375, recall 0.8361, auc 0.8369
epoch 7801, loss 0.2986, train acc 83.76%, f1 0.8381, precision 0.8355, recall 0.8408, auc 0.8376
epoch 7901, loss 0.4360, train acc 83.79%, f1 0.8382, precision 0.8364, recall 0.8401, auc 0.8379
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_3
./test_pima/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6

the Fscore is 0.5744680851063829

the precision is 0.40298507462686567

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_3
----------------------



epoch 1, loss 0.6927, train acc 50.16%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6327, train acc 74.39%, f1 0.6954, precision 0.8539, recall 0.5866, auc 0.7434
epoch 201, loss 0.4980, train acc 79.64%, f1 0.7875, precision 0.8206, recall 0.7571, auc 0.7963
epoch 301, loss 0.3144, train acc 81.51%, f1 0.8131, precision 0.8191, recall 0.8072, auc 0.8151
epoch 401, loss 0.5066, train acc 82.34%, f1 0.8236, precision 0.8201, recall 0.8271, auc 0.8234
epoch 501, loss 0.4555, train acc 82.82%, f1 0.8292, precision 0.8219, recall 0.8366, auc 0.8282
epoch 601, loss 0.4094, train acc 83.01%, f1 0.8309, precision 0.8245, recall 0.8373, auc 0.8302
epoch 701, loss 0.3164, train acc 83.08%, f1 0.8319, precision 0.8238, recall 0.8402, auc 0.8308
epoch 801, loss 0.3584, train acc 83.09%, f1 0.8321, precision 0.8235, recall 0.8409, auc 0.8309
epoch 901, loss 0.3654, train acc 83.07%, f1 0.8318, precision 0.8237, recall 0.8400, auc 0.8307
epoch 1001, loss 0.3703, train acc 83.09%, f1 0.8321, precision 0.8240, recall 0.8402, auc 0.8310
epoch 1101, loss 0.4363, train acc 83.14%, f1 0.8320, precision 0.8263, recall 0.8377, auc 0.8314
epoch 1201, loss 0.4685, train acc 83.12%, f1 0.8321, precision 0.8253, recall 0.8389, auc 0.8313
epoch 1301, loss 0.3320, train acc 83.10%, f1 0.8322, precision 0.8240, recall 0.8406, auc 0.8311
epoch 1401, loss 0.2591, train acc 83.11%, f1 0.8319, precision 0.8257, recall 0.8381, auc 0.8312
epoch 1501, loss 0.3224, train acc 83.14%, f1 0.8316, precision 0.8278, recall 0.8354, auc 0.8314
epoch 1601, loss 0.4728, train acc 83.12%, f1 0.8320, precision 0.8258, recall 0.8382, auc 0.8313
epoch 1701, loss 0.4501, train acc 83.06%, f1 0.8309, precision 0.8268, recall 0.8351, auc 0.8306
epoch 1801, loss 0.3964, train acc 83.09%, f1 0.8310, precision 0.8279, recall 0.8341, auc 0.8309
epoch 1901, loss 0.3939, train acc 83.14%, f1 0.8320, precision 0.8265, recall 0.8375, auc 0.8314
epoch 2001, loss 0.4040, train acc 83.13%, f1 0.8320, precision 0.8262, recall 0.8378, auc 0.8314
epoch 2101, loss 0.3247, train acc 83.22%, f1 0.8319, precision 0.8307, recall 0.8331, auc 0.8322
epoch 2201, loss 0.3684, train acc 83.23%, f1 0.8322, precision 0.8298, recall 0.8347, auc 0.8323
epoch 2301, loss 0.3800, train acc 83.16%, f1 0.8316, precision 0.8288, recall 0.8344, auc 0.8316
epoch 2401, loss 0.3182, train acc 83.21%, f1 0.8322, precision 0.8289, recall 0.8356, auc 0.8321
epoch 2501, loss 0.4139, train acc 83.16%, f1 0.8319, precision 0.8276, recall 0.8363, auc 0.8316
epoch 2601, loss 0.3080, train acc 83.10%, f1 0.8312, precision 0.8275, recall 0.8350, auc 0.8310
epoch 2701, loss 0.2902, train acc 83.12%, f1 0.8310, precision 0.8290, recall 0.8331, auc 0.8312
epoch 2801, loss 0.4664, train acc 83.13%, f1 0.8311, precision 0.8295, recall 0.8327, auc 0.8313
epoch 2901, loss 0.3253, train acc 83.17%, f1 0.8312, precision 0.8307, recall 0.8318, auc 0.8317
epoch 3001, loss 0.3971, train acc 83.15%, f1 0.8308, precision 0.8316, recall 0.8301, auc 0.8315
epoch 3101, loss 0.3327, train acc 83.16%, f1 0.8307, precision 0.8321, recall 0.8293, auc 0.8316
epoch 3201, loss 0.3888, train acc 83.12%, f1 0.8314, precision 0.8281, recall 0.8347, auc 0.8312
epoch 3301, loss 0.4353, train acc 83.16%, f1 0.8310, precision 0.8310, recall 0.8310, auc 0.8316
epoch 3401, loss 0.4747, train acc 83.09%, f1 0.8306, precision 0.8294, recall 0.8318, auc 0.8309
epoch 3501, loss 0.3591, train acc 83.08%, f1 0.8305, precision 0.8295, recall 0.8314, auc 0.8308
epoch 3601, loss 0.4855, train acc 83.11%, f1 0.8308, precision 0.8294, recall 0.8323, auc 0.8311
epoch 3701, loss 0.3385, train acc 83.13%, f1 0.8309, precision 0.8302, recall 0.8316, auc 0.8313
epoch 3801, loss 0.3579, train acc 83.15%, f1 0.8312, precision 0.8299, recall 0.8325, auc 0.8315
epoch 3901, loss 0.3084, train acc 83.22%, f1 0.8316, precision 0.8318, recall 0.8314, auc 0.8322
epoch 4001, loss 0.3618, train acc 83.26%, f1 0.8322, precision 0.8315, recall 0.8329, auc 0.8326
epoch 4101, loss 0.2251, train acc 83.26%, f1 0.8319, precision 0.8326, recall 0.8313, auc 0.8326
epoch 4201, loss 0.3796, train acc 83.16%, f1 0.8305, precision 0.8333, recall 0.8277, auc 0.8316
epoch 4301, loss 0.2990, train acc 83.13%, f1 0.8307, precision 0.8308, recall 0.8307, auc 0.8313
epoch 4401, loss 0.4135, train acc 83.15%, f1 0.8312, precision 0.8300, recall 0.8324, auc 0.8315
epoch 4501, loss 0.2960, train acc 83.15%, f1 0.8308, precision 0.8314, recall 0.8302, auc 0.8315
epoch 4601, loss 0.2707, train acc 83.29%, f1 0.8323, precision 0.8326, recall 0.8319, auc 0.8329
epoch 4701, loss 0.3080, train acc 83.32%, f1 0.8331, precision 0.8309, recall 0.8353, auc 0.8332
epoch 4801, loss 0.3171, train acc 83.33%, f1 0.8332, precision 0.8307, recall 0.8358, auc 0.8333
epoch 4901, loss 0.3538, train acc 83.33%, f1 0.8330, precision 0.8316, recall 0.8344, auc 0.8333
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_3
./test_pima/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.5700000000000001

the Fscore is 0.5567010309278351

the precision is 0.38571428571428573

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_3
----------------------



epoch 1, loss 0.6935, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6164, train acc 77.75%, f1 0.7763, precision 0.7804, recall 0.7723, auc 0.7775
epoch 201, loss 0.4352, train acc 79.84%, f1 0.7978, precision 0.8002, recall 0.7954, auc 0.7984
epoch 301, loss 0.4049, train acc 81.69%, f1 0.8171, precision 0.8162, recall 0.8181, auc 0.8169
epoch 401, loss 0.4208, train acc 82.43%, f1 0.8244, precision 0.8239, recall 0.8248, auc 0.8243
epoch 501, loss 0.3265, train acc 82.81%, f1 0.8285, precision 0.8266, recall 0.8304, auc 0.8281
epoch 601, loss 0.4111, train acc 82.97%, f1 0.8298, precision 0.8294, recall 0.8302, auc 0.8297
epoch 701, loss 0.4019, train acc 83.02%, f1 0.8308, precision 0.8281, recall 0.8334, auc 0.8302
epoch 801, loss 0.3886, train acc 83.12%, f1 0.8311, precision 0.8312, recall 0.8310, auc 0.8312
epoch 901, loss 0.2103, train acc 83.12%, f1 0.8310, precision 0.8321, recall 0.8300, auc 0.8312
epoch 1001, loss 0.3156, train acc 83.13%, f1 0.8319, precision 0.8290, recall 0.8348, auc 0.8313
epoch 1101, loss 0.3553, train acc 83.11%, f1 0.8311, precision 0.8309, recall 0.8313, auc 0.8311
epoch 1201, loss 0.4193, train acc 83.15%, f1 0.8320, precision 0.8294, recall 0.8347, auc 0.8315
epoch 1301, loss 0.5443, train acc 83.08%, f1 0.8309, precision 0.8302, recall 0.8316, auc 0.8308
epoch 1401, loss 0.4090, train acc 83.11%, f1 0.8316, precision 0.8294, recall 0.8338, auc 0.8311
epoch 1501, loss 0.4659, train acc 83.17%, f1 0.8320, precision 0.8303, recall 0.8338, auc 0.8317
epoch 1601, loss 0.3054, train acc 83.12%, f1 0.8318, precision 0.8291, recall 0.8344, auc 0.8312
epoch 1701, loss 0.3598, train acc 83.11%, f1 0.8318, precision 0.8287, recall 0.8348, auc 0.8311
epoch 1801, loss 0.4424, train acc 83.12%, f1 0.8320, precision 0.8283, recall 0.8357, auc 0.8312
epoch 1901, loss 0.3336, train acc 83.16%, f1 0.8320, precision 0.8302, recall 0.8339, auc 0.8316
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_3
./test_pima/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.595

the Fscore is 0.5714285714285715

the precision is 0.4

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_3
----------------------



epoch 1, loss 0.6895, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6237, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5983, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5736, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5514, train acc 66.78%, f1 0.1356, precision 0.7273, recall 0.0748, auc 0.5299
epoch 501, loss 0.5336, train acc 70.36%, f1 0.3209, precision 0.7963, recall 0.2009, auc 0.5867
epoch 601, loss 0.5200, train acc 73.94%, f1 0.4839, precision 0.7812, recall 0.3505, auc 0.6490
epoch 701, loss 0.5098, train acc 74.59%, f1 0.5412, precision 0.7302, recall 0.4299, auc 0.6725
epoch 801, loss 0.5023, train acc 75.57%, f1 0.5879, precision 0.7133, recall 0.5000, auc 0.6962
epoch 901, loss 0.4967, train acc 76.71%, f1 0.6227, precision 0.7152, recall 0.5514, auc 0.7170
epoch 1001, loss 0.4927, train acc 75.90%, f1 0.6146, precision 0.6941, recall 0.5514, auc 0.7107
epoch 1101, loss 0.4894, train acc 76.22%, f1 0.6256, precision 0.6932, recall 0.5701, auc 0.7175
epoch 1201, loss 0.4865, train acc 76.87%, f1 0.6396, precision 0.7000, recall 0.5888, auc 0.7269
epoch 1301, loss 0.4830, train acc 76.71%, f1 0.6361, precision 0.6983, recall 0.5841, auc 0.7246
epoch 1401, loss 0.4792, train acc 76.38%, f1 0.6272, precision 0.6971, recall 0.5701, auc 0.7188
epoch 1501, loss 0.4750, train acc 77.36%, f1 0.6408, precision 0.7168, recall 0.5794, auc 0.7285
epoch 1601, loss 0.4707, train acc 77.36%, f1 0.6390, precision 0.7193, recall 0.5748, auc 0.7274
epoch 1701, loss 0.4666, train acc 77.52%, f1 0.6480, precision 0.7135, recall 0.5935, auc 0.7330
epoch 1801, loss 0.4628, train acc 77.69%, f1 0.6532, precision 0.7127, recall 0.6028, auc 0.7364
epoch 1901, loss 0.4591, train acc 77.69%, f1 0.6549, precision 0.7104, recall 0.6075, auc 0.7375
epoch 2001, loss 0.4556, train acc 78.34%, f1 0.6683, precision 0.7166, recall 0.6262, auc 0.7468
epoch 2101, loss 0.4525, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 2201, loss 0.4496, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 2301, loss 0.4469, train acc 78.01%, f1 0.6633, precision 0.7112, recall 0.6215, auc 0.7432
epoch 2401, loss 0.4443, train acc 78.50%, f1 0.6700, precision 0.7204, recall 0.6262, auc 0.7481
epoch 2501, loss 0.4418, train acc 79.15%, f1 0.6800, precision 0.7312, recall 0.6355, auc 0.7553
epoch 2601, loss 0.4391, train acc 79.32%, f1 0.6817, precision 0.7351, recall 0.6355, auc 0.7565
epoch 2701, loss 0.4365, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 2801, loss 0.4339, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 2901, loss 0.4315, train acc 79.80%, f1 0.6931, precision 0.7368, recall 0.6542, auc 0.7646
epoch 3001, loss 0.4294, train acc 80.46%, f1 0.7044, precision 0.7448, recall 0.6682, auc 0.7729
epoch 3101, loss 0.4273, train acc 80.62%, f1 0.7090, precision 0.7436, recall 0.6776, auc 0.7763
epoch 3201, loss 0.4254, train acc 80.29%, f1 0.7056, precision 0.7360, recall 0.6776, auc 0.7738
epoch 3301, loss 0.4236, train acc 80.29%, f1 0.7056, precision 0.7360, recall 0.6776, auc 0.7738
epoch 3401, loss 0.4217, train acc 80.78%, f1 0.7136, precision 0.7424, recall 0.6869, auc 0.7797
epoch 3501, loss 0.4199, train acc 81.43%, f1 0.7260, precision 0.7475, recall 0.7056, auc 0.7891
epoch 3601, loss 0.4181, train acc 81.43%, f1 0.7260, precision 0.7475, recall 0.7056, auc 0.7891
epoch 3701, loss 0.4162, train acc 81.60%, f1 0.7277, precision 0.7512, recall 0.7056, auc 0.7903
epoch 3801, loss 0.4143, train acc 81.60%, f1 0.7277, precision 0.7512, recall 0.7056, auc 0.7903
epoch 3901, loss 0.4122, train acc 81.92%, f1 0.7338, precision 0.7537, recall 0.7150, auc 0.7950
epoch 4001, loss 0.4099, train acc 82.41%, f1 0.7416, precision 0.7598, recall 0.7243, auc 0.8009
epoch 4101, loss 0.4070, train acc 82.41%, f1 0.7404, precision 0.7624, recall 0.7196, auc 0.7998
epoch 4201, loss 0.4037, train acc 82.74%, f1 0.7440, precision 0.7700, recall 0.7196, auc 0.8023
epoch 4301, loss 0.4000, train acc 83.06%, f1 0.7488, precision 0.7750, recall 0.7243, auc 0.8059
epoch 4401, loss 0.3966, train acc 83.06%, f1 0.7488, precision 0.7750, recall 0.7243, auc 0.8059
epoch 4501, loss 0.3935, train acc 83.39%, f1 0.7548, precision 0.7772, recall 0.7336, auc 0.8106
epoch 4601, loss 0.3905, train acc 83.88%, f1 0.7637, precision 0.7805, recall 0.7477, auc 0.8176
epoch 4701, loss 0.3874, train acc 84.20%, f1 0.7685, precision 0.7854, recall 0.7523, auc 0.8212
epoch 4801, loss 0.3843, train acc 84.36%, f1 0.7692, precision 0.7921, recall 0.7477, auc 0.8213
epoch 4901, loss 0.3812, train acc 84.36%, f1 0.7692, precision 0.7921, recall 0.7477, auc 0.8213
epoch 5001, loss 0.3781, train acc 84.36%, f1 0.7692, precision 0.7921, recall 0.7477, auc 0.8213
epoch 5101, loss 0.3747, train acc 84.85%, f1 0.7770, precision 0.7980, recall 0.7570, auc 0.8273
epoch 5201, loss 0.3708, train acc 84.53%, f1 0.7733, precision 0.7902, recall 0.7570, auc 0.8248
epoch 5301, loss 0.3667, train acc 84.36%, f1 0.7714, precision 0.7864, recall 0.7570, auc 0.8235
epoch 5401, loss 0.3631, train acc 84.69%, f1 0.7762, precision 0.7913, recall 0.7617, auc 0.8271
epoch 5501, loss 0.3593, train acc 84.85%, f1 0.7791, precision 0.7923, recall 0.7664, auc 0.8294
epoch 5601, loss 0.3558, train acc 85.34%, f1 0.7867, precision 0.7981, recall 0.7757, auc 0.8354
epoch 5701, loss 0.3530, train acc 85.02%, f1 0.7830, precision 0.7905, recall 0.7757, auc 0.8329
epoch 5801, loss 0.3504, train acc 85.02%, f1 0.7830, precision 0.7905, recall 0.7757, auc 0.8329
epoch 5901, loss 0.3480, train acc 85.18%, f1 0.7849, precision 0.7943, recall 0.7757, auc 0.8341
epoch 6001, loss 0.3456, train acc 85.50%, f1 0.7896, precision 0.7990, recall 0.7804, auc 0.8377
epoch 6101, loss 0.3433, train acc 85.67%, f1 0.7915, precision 0.8029, recall 0.7804, auc 0.8389
epoch 6201, loss 0.3410, train acc 85.83%, f1 0.7933, precision 0.8068, recall 0.7804, auc 0.8402
epoch 6301, loss 0.3387, train acc 85.99%, f1 0.7952, precision 0.8107, recall 0.7804, auc 0.8414
epoch 6401, loss 0.3365, train acc 85.99%, f1 0.7952, precision 0.8107, recall 0.7804, auc 0.8414
epoch 6501, loss 0.3344, train acc 86.32%, f1 0.7990, precision 0.8186, recall 0.7804, auc 0.8439
epoch 6601, loss 0.3322, train acc 86.32%, f1 0.8000, precision 0.8155, recall 0.7850, auc 0.8450
epoch 6701, loss 0.3302, train acc 86.64%, f1 0.8048, precision 0.8204, recall 0.7897, auc 0.8486
epoch 6801, loss 0.3283, train acc 86.64%, f1 0.8048, precision 0.8204, recall 0.7897, auc 0.8486
epoch 6901, loss 0.3265, train acc 86.97%, f1 0.8086, precision 0.8284, recall 0.7897, auc 0.8511
epoch 7001, loss 0.3246, train acc 87.13%, f1 0.8115, precision 0.8293, recall 0.7944, auc 0.8534
epoch 7101, loss 0.3225, train acc 87.30%, f1 0.8143, precision 0.8301, recall 0.7991, auc 0.8558
epoch 7201, loss 0.3191, train acc 87.30%, f1 0.8152, precision 0.8269, recall 0.8037, auc 0.8569
epoch 7301, loss 0.3153, train acc 87.30%, f1 0.8143, precision 0.8301, recall 0.7991, auc 0.8558
epoch 7401, loss 0.3115, train acc 87.30%, f1 0.8143, precision 0.8301, recall 0.7991, auc 0.8558
epoch 7501, loss 0.3081, train acc 87.30%, f1 0.8143, precision 0.8301, recall 0.7991, auc 0.8558
epoch 7601, loss 0.3050, train acc 86.97%, f1 0.8086, precision 0.8284, recall 0.7897, auc 0.8511
epoch 7701, loss 0.3014, train acc 86.97%, f1 0.8086, precision 0.8284, recall 0.7897, auc 0.8511
epoch 7801, loss 0.2967, train acc 87.62%, f1 0.8182, precision 0.8382, recall 0.7991, auc 0.8583
epoch 7901, loss 0.2929, train acc 87.62%, f1 0.8182, precision 0.8382, recall 0.7991, auc 0.8583
epoch 8001, loss 0.2898, train acc 87.62%, f1 0.8182, precision 0.8382, recall 0.7991, auc 0.8583
epoch 8101, loss 0.2873, train acc 87.79%, f1 0.8193, precision 0.8458, recall 0.7944, auc 0.8584
epoch 8201, loss 0.2850, train acc 88.27%, f1 0.8278, precision 0.8480, recall 0.8084, auc 0.8655
epoch 8301, loss 0.2828, train acc 88.44%, f1 0.8297, precision 0.8522, recall 0.8084, auc 0.8667
epoch 8401, loss 0.2807, train acc 88.76%, f1 0.8345, precision 0.8571, recall 0.8131, auc 0.8703
epoch 8501, loss 0.2787, train acc 88.93%, f1 0.8373, precision 0.8578, recall 0.8178, auc 0.8726
epoch 8601, loss 0.2769, train acc 89.25%, f1 0.8429, precision 0.8592, recall 0.8271, auc 0.8773
epoch 8701, loss 0.2751, train acc 89.09%, f1 0.8409, precision 0.8551, recall 0.8271, auc 0.8761
epoch 8801, loss 0.2734, train acc 89.09%, f1 0.8409, precision 0.8551, recall 0.8271, auc 0.8761
epoch 8901, loss 0.2710, train acc 89.09%, f1 0.8409, precision 0.8551, recall 0.8271, auc 0.8761
epoch 9001, loss 0.2674, train acc 89.09%, f1 0.8416, precision 0.8517, recall 0.8318, auc 0.8771
epoch 9101, loss 0.2636, train acc 89.09%, f1 0.8416, precision 0.8517, recall 0.8318, auc 0.8771
epoch 9201, loss 0.2602, train acc 89.58%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8852
epoch 9301, loss 0.2574, train acc 89.58%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8852
epoch 9401, loss 0.2548, train acc 89.41%, f1 0.8478, precision 0.8498, recall 0.8458, auc 0.8829
epoch 9501, loss 0.2522, train acc 89.90%, f1 0.8551, precision 0.8551, recall 0.8551, auc 0.8888
epoch 9601, loss 0.2494, train acc 90.07%, f1 0.8578, precision 0.8558, recall 0.8598, auc 0.8912
epoch 9701, loss 0.2464, train acc 90.23%, f1 0.8592, precision 0.8632, recall 0.8551, auc 0.8913
epoch 9801, loss 0.2440, train acc 90.39%, f1 0.8612, precision 0.8673, recall 0.8551, auc 0.8926
epoch 9901, loss 0.2420, train acc 90.23%, f1 0.8585, precision 0.8667, recall 0.8505, auc 0.8902
epoch 10001, loss 0.2401, train acc 90.55%, f1 0.8632, precision 0.8714, recall 0.8551, auc 0.8938
epoch 10101, loss 0.2382, train acc 90.72%, f1 0.8652, precision 0.8756, recall 0.8551, auc 0.8951
epoch 10201, loss 0.2364, train acc 90.88%, f1 0.8667, precision 0.8835, recall 0.8505, auc 0.8952
epoch 10301, loss 0.2345, train acc 90.72%, f1 0.8640, precision 0.8829, recall 0.8458, auc 0.8929
epoch 10401, loss 0.2327, train acc 90.72%, f1 0.8640, precision 0.8829, recall 0.8458, auc 0.8929
epoch 10501, loss 0.2309, train acc 90.72%, f1 0.8640, precision 0.8829, recall 0.8458, auc 0.8929
epoch 10601, loss 0.2289, train acc 90.39%, f1 0.8585, precision 0.8818, recall 0.8364, auc 0.8882
epoch 10701, loss 0.2269, train acc 90.72%, f1 0.8640, precision 0.8829, recall 0.8458, auc 0.8929
epoch 10801, loss 0.2250, train acc 91.04%, f1 0.8687, precision 0.8878, recall 0.8505, auc 0.8965
epoch 10901, loss 0.2234, train acc 91.21%, f1 0.8708, precision 0.8922, recall 0.8505, auc 0.8977
epoch 11001, loss 0.2219, train acc 91.21%, f1 0.8708, precision 0.8922, recall 0.8505, auc 0.8977
epoch 11101, loss 0.2205, train acc 91.21%, f1 0.8708, precision 0.8922, recall 0.8505, auc 0.8977
epoch 11201, loss 0.2191, train acc 91.21%, f1 0.8708, precision 0.8922, recall 0.8505, auc 0.8977
epoch 11301, loss 0.2178, train acc 91.37%, f1 0.8729, precision 0.8966, recall 0.8505, auc 0.8990
epoch 11401, loss 0.2165, train acc 91.37%, f1 0.8729, precision 0.8966, recall 0.8505, auc 0.8990
epoch 11501, loss 0.2152, train acc 91.37%, f1 0.8729, precision 0.8966, recall 0.8505, auc 0.8990
epoch 11601, loss 0.2140, train acc 91.37%, f1 0.8729, precision 0.8966, recall 0.8505, auc 0.8990
epoch 11701, loss 0.2128, train acc 91.86%, f1 0.8810, precision 0.8981, recall 0.8645, auc 0.9060
epoch 11801, loss 0.2116, train acc 91.86%, f1 0.8810, precision 0.8981, recall 0.8645, auc 0.9060
epoch 11901, loss 0.2105, train acc 92.35%, f1 0.8889, precision 0.8995, recall 0.8785, auc 0.9130
epoch 12001, loss 0.2094, train acc 92.35%, f1 0.8889, precision 0.8995, recall 0.8785, auc 0.9130
epoch 12101, loss 0.2083, train acc 92.51%, f1 0.8915, precision 0.9000, recall 0.8832, auc 0.9153
epoch 12201, loss 0.2072, train acc 92.67%, f1 0.8936, precision 0.9043, recall 0.8832, auc 0.9166
epoch 12301, loss 0.2062, train acc 92.67%, f1 0.8936, precision 0.9043, recall 0.8832, auc 0.9166
epoch 12401, loss 0.2052, train acc 92.67%, f1 0.8936, precision 0.9043, recall 0.8832, auc 0.9166
epoch 12501, loss 0.2042, train acc 92.67%, f1 0.8936, precision 0.9043, recall 0.8832, auc 0.9166
epoch 12601, loss 0.2032, train acc 92.67%, f1 0.8936, precision 0.9043, recall 0.8832, auc 0.9166
epoch 12701, loss 0.2022, train acc 92.67%, f1 0.8936, precision 0.9043, recall 0.8832, auc 0.9166
epoch 12801, loss 0.2013, train acc 92.67%, f1 0.8936, precision 0.9043, recall 0.8832, auc 0.9166
epoch 12901, loss 0.2004, train acc 92.83%, f1 0.8962, precision 0.9048, recall 0.8879, auc 0.9189
epoch 13001, loss 0.1994, train acc 92.83%, f1 0.8962, precision 0.9048, recall 0.8879, auc 0.9189
epoch 13101, loss 0.1985, train acc 92.83%, f1 0.8962, precision 0.9048, recall 0.8879, auc 0.9189
epoch 13201, loss 0.1977, train acc 93.00%, f1 0.8983, precision 0.9091, recall 0.8879, auc 0.9202
epoch 13301, loss 0.1968, train acc 93.16%, f1 0.9009, precision 0.9095, recall 0.8925, auc 0.9225
epoch 13401, loss 0.1959, train acc 93.16%, f1 0.9009, precision 0.9095, recall 0.8925, auc 0.9225
epoch 13501, loss 0.1951, train acc 93.16%, f1 0.9009, precision 0.9095, recall 0.8925, auc 0.9225
epoch 13601, loss 0.1943, train acc 93.16%, f1 0.9009, precision 0.9095, recall 0.8925, auc 0.9225
epoch 13701, loss 0.1935, train acc 93.32%, f1 0.9035, precision 0.9100, recall 0.8972, auc 0.9248
epoch 13801, loss 0.1927, train acc 93.32%, f1 0.9035, precision 0.9100, recall 0.8972, auc 0.9248
epoch 13901, loss 0.1919, train acc 93.32%, f1 0.9035, precision 0.9100, recall 0.8972, auc 0.9248
epoch 14001, loss 0.1911, train acc 93.32%, f1 0.9035, precision 0.9100, recall 0.8972, auc 0.9248
epoch 14101, loss 0.1904, train acc 93.32%, f1 0.9035, precision 0.9100, recall 0.8972, auc 0.9248
epoch 14201, loss 0.1896, train acc 93.32%, f1 0.9035, precision 0.9100, recall 0.8972, auc 0.9248
epoch 14301, loss 0.1889, train acc 93.49%, f1 0.9057, precision 0.9143, recall 0.8972, auc 0.9261
epoch 14401, loss 0.1882, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 14501, loss 0.1874, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 14601, loss 0.1867, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 14701, loss 0.1861, train acc 93.49%, f1 0.9061, precision 0.9104, recall 0.9019, auc 0.9272
epoch 14801, loss 0.1854, train acc 93.49%, f1 0.9061, precision 0.9104, recall 0.9019, auc 0.9272
epoch 14901, loss 0.1847, train acc 93.32%, f1 0.9035, precision 0.9100, recall 0.8972, auc 0.9248
epoch 15001, loss 0.1841, train acc 93.49%, f1 0.9057, precision 0.9143, recall 0.8972, auc 0.9261
epoch 15101, loss 0.1834, train acc 93.49%, f1 0.9057, precision 0.9143, recall 0.8972, auc 0.9261
epoch 15201, loss 0.1828, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 15301, loss 0.1821, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 15401, loss 0.1814, train acc 93.49%, f1 0.9057, precision 0.9143, recall 0.8972, auc 0.9261
epoch 15501, loss 0.1794, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 15601, loss 0.1784, train acc 93.49%, f1 0.9057, precision 0.9143, recall 0.8972, auc 0.9261
epoch 15701, loss 0.1775, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 15801, loss 0.1766, train acc 93.81%, f1 0.9108, precision 0.9151, recall 0.9065, auc 0.9308
epoch 15901, loss 0.1757, train acc 93.81%, f1 0.9108, precision 0.9151, recall 0.9065, auc 0.9308
epoch 16001, loss 0.1746, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 16101, loss 0.1736, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 16201, loss 0.1727, train acc 93.49%, f1 0.9061, precision 0.9104, recall 0.9019, auc 0.9272
epoch 16301, loss 0.1718, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 16401, loss 0.1710, train acc 93.81%, f1 0.9108, precision 0.9151, recall 0.9065, auc 0.9308
epoch 16501, loss 0.1703, train acc 93.81%, f1 0.9108, precision 0.9151, recall 0.9065, auc 0.9308/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.1695, train acc 93.97%, f1 0.9133, precision 0.9155, recall 0.9112, auc 0.9331
epoch 16701, loss 0.1686, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 16801, loss 0.1679, train acc 93.49%, f1 0.9057, precision 0.9143, recall 0.8972, auc 0.9261
epoch 16901, loss 0.1671, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17001, loss 0.1663, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17101, loss 0.1656, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17201, loss 0.1650, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17301, loss 0.1644, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17401, loss 0.1638, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17501, loss 0.1632, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17601, loss 0.1626, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17701, loss 0.1621, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17801, loss 0.1615, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17901, loss 0.1610, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 18001, loss 0.1605, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 18101, loss 0.1600, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 18201, loss 0.1595, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 18301, loss 0.1590, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 18401, loss 0.1585, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 18501, loss 0.1580, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18601, loss 0.1575, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18701, loss 0.1570, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18801, loss 0.1566, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18901, loss 0.1561, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19001, loss 0.1557, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19101, loss 0.1552, train acc 94.30%, f1 0.9180, precision 0.9202, recall 0.9159, auc 0.9367
epoch 19201, loss 0.1548, train acc 94.30%, f1 0.9180, precision 0.9202, recall 0.9159, auc 0.9367
epoch 19301, loss 0.1543, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19401, loss 0.1539, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19501, loss 0.1535, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19601, loss 0.1531, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19701, loss 0.1527, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19801, loss 0.1523, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19901, loss 0.1519, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_normal_20000
normal
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_3
./test_pima/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.7105555555555556

the Fscore is 0.6226415094339622

the precision is 0.6346153846153846

the recall is 0.6111111111111112

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_3
----------------------



epoch 1, loss 0.6918, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6231, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5976, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5732, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5513, train acc 66.61%, f1 0.1277, precision 0.7143, recall 0.0701, auc 0.5275
epoch 501, loss 0.5337, train acc 70.36%, f1 0.3209, precision 0.7963, recall 0.2009, auc 0.5867
epoch 601, loss 0.5201, train acc 73.94%, f1 0.4839, precision 0.7812, recall 0.3505, auc 0.6490
epoch 701, loss 0.5100, train acc 74.76%, f1 0.5428, precision 0.7360, recall 0.4299, auc 0.6737
epoch 801, loss 0.5025, train acc 75.57%, f1 0.5879, precision 0.7133, recall 0.5000, auc 0.6962
epoch 901, loss 0.4970, train acc 76.71%, f1 0.6227, precision 0.7152, recall 0.5514, auc 0.7170
epoch 1001, loss 0.4929, train acc 75.90%, f1 0.6146, precision 0.6941, recall 0.5514, auc 0.7107
epoch 1101, loss 0.4898, train acc 76.22%, f1 0.6256, precision 0.6932, recall 0.5701, auc 0.7175
epoch 1201, loss 0.4872, train acc 76.22%, f1 0.6313, precision 0.6868, recall 0.5841, auc 0.7208
epoch 1301, loss 0.4846, train acc 76.38%, f1 0.6366, precision 0.6865, recall 0.5935, auc 0.7242
epoch 1401, loss 0.4814, train acc 76.38%, f1 0.6329, precision 0.6906, recall 0.5841, auc 0.7221
epoch 1501, loss 0.4775, train acc 77.04%, f1 0.6394, precision 0.7062, recall 0.5841, auc 0.7271
epoch 1601, loss 0.4730, train acc 77.52%, f1 0.6443, precision 0.7184, recall 0.5841, auc 0.7308
epoch 1701, loss 0.4683, train acc 77.36%, f1 0.6408, precision 0.7168, recall 0.5794, auc 0.7285
epoch 1801, loss 0.4636, train acc 77.52%, f1 0.6462, precision 0.7159, recall 0.5888, auc 0.7319
epoch 1901, loss 0.4593, train acc 77.69%, f1 0.6496, precision 0.7175, recall 0.5935, auc 0.7342
epoch 2001, loss 0.4552, train acc 77.69%, f1 0.6532, precision 0.7127, recall 0.6028, auc 0.7364
epoch 2101, loss 0.4515, train acc 78.01%, f1 0.6582, precision 0.7182, recall 0.6075, auc 0.7400
epoch 2201, loss 0.4480, train acc 78.34%, f1 0.6633, precision 0.7238, recall 0.6121, auc 0.7436
epoch 2301, loss 0.4449, train acc 78.50%, f1 0.6650, precision 0.7278, recall 0.6121, auc 0.7448
epoch 2401, loss 0.4421, train acc 78.83%, f1 0.6734, precision 0.7283, recall 0.6262, auc 0.7506
epoch 2501, loss 0.4395, train acc 79.15%, f1 0.6784, precision 0.7337, recall 0.6308, auc 0.7542
epoch 2601, loss 0.4371, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 2701, loss 0.4350, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 2801, loss 0.4329, train acc 79.48%, f1 0.6866, precision 0.7340, recall 0.6449, auc 0.7599
epoch 2901, loss 0.4310, train acc 79.48%, f1 0.6897, precision 0.7292, recall 0.6542, auc 0.7621
epoch 3001, loss 0.4292, train acc 79.64%, f1 0.6929, precision 0.7306, recall 0.6589, auc 0.7644
epoch 3101, loss 0.4275, train acc 79.64%, f1 0.6929, precision 0.7306, recall 0.6589, auc 0.7644
epoch 3201, loss 0.4259, train acc 79.97%, f1 0.6993, precision 0.7333, recall 0.6682, auc 0.7691
epoch 3301, loss 0.4244, train acc 79.97%, f1 0.6993, precision 0.7333, recall 0.6682, auc 0.7691
epoch 3401, loss 0.4229, train acc 79.80%, f1 0.6961, precision 0.7320, recall 0.6636, auc 0.7668
epoch 3501, loss 0.4215, train acc 80.13%, f1 0.7010, precision 0.7371, recall 0.6682, auc 0.7704
epoch 3601, loss 0.4201, train acc 79.97%, f1 0.6993, precision 0.7333, recall 0.6682, auc 0.7691
epoch 3701, loss 0.4186, train acc 80.13%, f1 0.7024, precision 0.7347, recall 0.6729, auc 0.7714
epoch 3801, loss 0.4172, train acc 80.13%, f1 0.7024, precision 0.7347, recall 0.6729, auc 0.7714
epoch 3901, loss 0.4155, train acc 80.13%, f1 0.7024, precision 0.7347, recall 0.6729, auc 0.7714
epoch 4001, loss 0.4136, train acc 80.62%, f1 0.7090, precision 0.7436, recall 0.6776, auc 0.7763
epoch 4101, loss 0.4111, train acc 80.94%, f1 0.7139, precision 0.7487, recall 0.6822, auc 0.7799
epoch 4201, loss 0.4080, train acc 81.11%, f1 0.7171, precision 0.7500, recall 0.6869, auc 0.7822
epoch 4301, loss 0.4047, train acc 81.43%, f1 0.7233, precision 0.7525, recall 0.6963, auc 0.7869
epoch 4401, loss 0.4011, train acc 81.92%, f1 0.7299, precision 0.7614, recall 0.7009, auc 0.7917
epoch 4501, loss 0.3970, train acc 82.25%, f1 0.7361, precision 0.7638, recall 0.7103, auc 0.7964
epoch 4601, loss 0.3928, train acc 82.74%, f1 0.7415, precision 0.7755, recall 0.7103, auc 0.8001
epoch 4701, loss 0.3889, train acc 82.57%, f1 0.7409, precision 0.7688, recall 0.7150, auc 0.8000
epoch 4801, loss 0.3851, train acc 82.57%, f1 0.7397, precision 0.7716, recall 0.7103, auc 0.7989
epoch 4901, loss 0.3812, train acc 83.06%, f1 0.7451, precision 0.7835, recall 0.7103, auc 0.8026
epoch 5001, loss 0.3774, train acc 83.22%, f1 0.7469, precision 0.7876, recall 0.7103, auc 0.8039
epoch 5101, loss 0.3742, train acc 83.06%, f1 0.7451, precision 0.7835, recall 0.7103, auc 0.8026
epoch 5201, loss 0.3711, train acc 83.06%, f1 0.7463, precision 0.7806, recall 0.7150, auc 0.8037
epoch 5301, loss 0.3676, train acc 83.88%, f1 0.7614, precision 0.7861, recall 0.7383, auc 0.8154
epoch 5401, loss 0.3637, train acc 83.55%, f1 0.7543, precision 0.7868, recall 0.7243, auc 0.8096
epoch 5501, loss 0.3589, train acc 83.88%, f1 0.7591, precision 0.7919, recall 0.7290, auc 0.8132
epoch 5601, loss 0.3534, train acc 84.36%, f1 0.7659, precision 0.8010, recall 0.7336, auc 0.8181
epoch 5701, loss 0.3472, train acc 85.02%, f1 0.7745, precision 0.8144, recall 0.7383, auc 0.8242
epoch 5801, loss 0.3414, train acc 85.02%, f1 0.7745, precision 0.8144, recall 0.7383, auc 0.8242
epoch 5901, loss 0.3363, train acc 85.34%, f1 0.7805, precision 0.8163, recall 0.7477, auc 0.8288
epoch 6001, loss 0.3318, train acc 85.50%, f1 0.7835, precision 0.8173, recall 0.7523, auc 0.8312
epoch 6101, loss 0.3277, train acc 86.16%, f1 0.7952, precision 0.8209, recall 0.7710, auc 0.8405
epoch 6201, loss 0.3239, train acc 85.99%, f1 0.7933, precision 0.8168, recall 0.7710, auc 0.8393
epoch 6301, loss 0.3205, train acc 86.64%, f1 0.8038, precision 0.8235, recall 0.7850, auc 0.8475
epoch 6401, loss 0.3172, train acc 86.64%, f1 0.8038, precision 0.8235, recall 0.7850, auc 0.8475
epoch 6501, loss 0.3141, train acc 86.81%, f1 0.8058, precision 0.8276, recall 0.7850, auc 0.8488
epoch 6601, loss 0.3111, train acc 86.81%, f1 0.8067, precision 0.8244, recall 0.7897, auc 0.8499
epoch 6701, loss 0.3078, train acc 86.81%, f1 0.8058, precision 0.8276, recall 0.7850, auc 0.8488
epoch 6801, loss 0.3037, train acc 87.46%, f1 0.8145, precision 0.8408, recall 0.7897, auc 0.8549
epoch 6901, loss 0.2995, train acc 87.62%, f1 0.8182, precision 0.8382, recall 0.7991, auc 0.8583
epoch 7001, loss 0.2954, train acc 87.95%, f1 0.8238, precision 0.8398, recall 0.8084, auc 0.8630
epoch 7101, loss 0.2921, train acc 88.27%, f1 0.8294, precision 0.8413, recall 0.8178, auc 0.8676
epoch 7201, loss 0.2892, train acc 88.11%, f1 0.8274, precision 0.8373, recall 0.8178, auc 0.8664
epoch 7301, loss 0.2866, train acc 88.27%, f1 0.8302, precision 0.8381, recall 0.8224, auc 0.8687
epoch 7401, loss 0.2841, train acc 87.95%, f1 0.8255, precision 0.8333, recall 0.8178, auc 0.8651
epoch 7501, loss 0.2817, train acc 87.79%, f1 0.8227, precision 0.8325, recall 0.8131, auc 0.8628
epoch 7601, loss 0.2794, train acc 87.79%, f1 0.8227, precision 0.8325, recall 0.8131, auc 0.8628
epoch 7701, loss 0.2771, train acc 87.95%, f1 0.8246, precision 0.8365, recall 0.8131, auc 0.8640
epoch 7801, loss 0.2748, train acc 88.27%, f1 0.8286, precision 0.8447, recall 0.8131, auc 0.8665
epoch 7901, loss 0.2724, train acc 88.44%, f1 0.8305, precision 0.8488, recall 0.8131, auc 0.8678
epoch 8001, loss 0.2698, train acc 88.76%, f1 0.8361, precision 0.8502, recall 0.8224, auc 0.8725
epoch 8101, loss 0.2671, train acc 88.76%, f1 0.8353, precision 0.8537, recall 0.8178, auc 0.8714
epoch 8201, loss 0.2645, train acc 88.93%, f1 0.8357, precision 0.8650, recall 0.8084, auc 0.8705/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2620, train acc 89.25%, f1 0.8413, precision 0.8663, recall 0.8178, auc 0.8751
epoch 8401, loss 0.2597, train acc 89.41%, f1 0.8434, precision 0.8706, recall 0.8178, auc 0.8764
epoch 8501, loss 0.2575, train acc 89.58%, f1 0.8462, precision 0.8713, recall 0.8224, auc 0.8787
epoch 8601, loss 0.2554, train acc 89.90%, f1 0.8517, precision 0.8725, recall 0.8318, auc 0.8834
epoch 8701, loss 0.2534, train acc 90.07%, f1 0.8537, precision 0.8768, recall 0.8318, auc 0.8846
epoch 8801, loss 0.2512, train acc 90.07%, f1 0.8544, precision 0.8732, recall 0.8364, auc 0.8857
epoch 8901, loss 0.2491, train acc 90.23%, f1 0.8571, precision 0.8738, recall 0.8411, auc 0.8881
epoch 9001, loss 0.2470, train acc 89.90%, f1 0.8524, precision 0.8689, recall 0.8364, auc 0.8845
epoch 9101, loss 0.2450, train acc 90.23%, f1 0.8571, precision 0.8738, recall 0.8411, auc 0.8881
epoch 9201, loss 0.2431, train acc 90.39%, f1 0.8599, precision 0.8744, recall 0.8458, auc 0.8904
epoch 9301, loss 0.2411, train acc 90.39%, f1 0.8599, precision 0.8744, recall 0.8458, auc 0.8904
epoch 9401, loss 0.2390, train acc 90.55%, f1 0.8626, precision 0.8750, recall 0.8505, auc 0.8927
epoch 9501, loss 0.2370, train acc 91.04%, f1 0.8694, precision 0.8841, recall 0.8551, auc 0.8976
epoch 9601, loss 0.2350, train acc 91.04%, f1 0.8700, precision 0.8804, recall 0.8598, auc 0.8987
epoch 9701, loss 0.2332, train acc 90.88%, f1 0.8679, precision 0.8762, recall 0.8598, auc 0.8974
epoch 9801, loss 0.2316, train acc 90.72%, f1 0.8652, precision 0.8756, recall 0.8551, auc 0.8951
epoch 9901, loss 0.2300, train acc 90.72%, f1 0.8652, precision 0.8756, recall 0.8551, auc 0.8951
epoch 10001, loss 0.2285, train acc 90.88%, f1 0.8679, precision 0.8762, recall 0.8598, auc 0.8974
epoch 10101, loss 0.2270, train acc 90.88%, f1 0.8679, precision 0.8762, recall 0.8598, auc 0.8974
epoch 10201, loss 0.2256, train acc 91.04%, f1 0.8706, precision 0.8768, recall 0.8645, auc 0.8997
epoch 10301, loss 0.2243, train acc 91.04%, f1 0.8706, precision 0.8768, recall 0.8645, auc 0.8997
epoch 10401, loss 0.2230, train acc 91.04%, f1 0.8706, precision 0.8768, recall 0.8645, auc 0.8997
epoch 10501, loss 0.2218, train acc 91.04%, f1 0.8706, precision 0.8768, recall 0.8645, auc 0.8997
epoch 10601, loss 0.2207, train acc 91.37%, f1 0.8759, precision 0.8779, recall 0.8738, auc 0.9044
epoch 10701, loss 0.2196, train acc 91.53%, f1 0.8779, precision 0.8821, recall 0.8738, auc 0.9057
epoch 10801, loss 0.2185, train acc 91.69%, f1 0.8806, precision 0.8826, recall 0.8785, auc 0.9080
epoch 10901, loss 0.2175, train acc 91.86%, f1 0.8826, precision 0.8868, recall 0.8785, auc 0.9093
epoch 11001, loss 0.2164, train acc 91.86%, f1 0.8826, precision 0.8868, recall 0.8785, auc 0.9093
epoch 11101, loss 0.2155, train acc 91.86%, f1 0.8826, precision 0.8868, recall 0.8785, auc 0.9093
epoch 11201, loss 0.2145, train acc 91.86%, f1 0.8826, precision 0.8868, recall 0.8785, auc 0.9093
epoch 11301, loss 0.2136, train acc 92.02%, f1 0.8852, precision 0.8873, recall 0.8832, auc 0.9116
epoch 11401, loss 0.2127, train acc 92.18%, f1 0.8879, precision 0.8879, recall 0.8879, auc 0.9139
epoch 11501, loss 0.2118, train acc 92.18%, f1 0.8879, precision 0.8879, recall 0.8879, auc 0.9139
epoch 11601, loss 0.2109, train acc 92.18%, f1 0.8879, precision 0.8879, recall 0.8879, auc 0.9139
epoch 11701, loss 0.2100, train acc 92.18%, f1 0.8879, precision 0.8879, recall 0.8879, auc 0.9139
epoch 11801, loss 0.2092, train acc 92.18%, f1 0.8879, precision 0.8879, recall 0.8879, auc 0.9139
epoch 11901, loss 0.2084, train acc 92.35%, f1 0.8904, precision 0.8884, recall 0.8925, auc 0.9163
epoch 12001, loss 0.2076, train acc 92.51%, f1 0.8925, precision 0.8925, recall 0.8925, auc 0.9175
epoch 12101, loss 0.2068, train acc 92.83%, f1 0.8967, precision 0.9009, recall 0.8925, auc 0.9200
epoch 12201, loss 0.2060, train acc 92.83%, f1 0.8967, precision 0.9009, recall 0.8925, auc 0.9200
epoch 12301, loss 0.2053, train acc 93.00%, f1 0.8993, precision 0.9014, recall 0.8972, auc 0.9223
epoch 12401, loss 0.2046, train acc 93.00%, f1 0.8993, precision 0.9014, recall 0.8972, auc 0.9223
epoch 12501, loss 0.2039, train acc 93.16%, f1 0.9019, precision 0.9019, recall 0.9019, auc 0.9247
epoch 12601, loss 0.2030, train acc 93.81%, f1 0.9112, precision 0.9112, recall 0.9112, auc 0.9319
epoch 12701, loss 0.2023, train acc 93.97%, f1 0.9133, precision 0.9155, recall 0.9112, auc 0.9331
epoch 12801, loss 0.2016, train acc 93.97%, f1 0.9133, precision 0.9155, recall 0.9112, auc 0.9331
epoch 12901, loss 0.2009, train acc 94.14%, f1 0.9159, precision 0.9159, recall 0.9159, auc 0.9354
epoch 13001, loss 0.2003, train acc 94.14%, f1 0.9159, precision 0.9159, recall 0.9159, auc 0.9354
epoch 13101, loss 0.1996, train acc 94.14%, f1 0.9159, precision 0.9159, recall 0.9159, auc 0.9354
epoch 13201, loss 0.1990, train acc 94.14%, f1 0.9159, precision 0.9159, recall 0.9159, auc 0.9354
epoch 13301, loss 0.1984, train acc 94.14%, f1 0.9159, precision 0.9159, recall 0.9159, auc 0.9354
epoch 13401, loss 0.1979, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 13501, loss 0.1973, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 13601, loss 0.1967, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 13701, loss 0.1962, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 13801, loss 0.1956, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 13901, loss 0.1951, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 14001, loss 0.1946, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 14101, loss 0.1941, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 14201, loss 0.1936, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 14301, loss 0.1932, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 14401, loss 0.1927, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 14501, loss 0.1922, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 14601, loss 0.1918, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 14701, loss 0.1913, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 14801, loss 0.1909, train acc 94.14%, f1 0.9159, precision 0.9159, recall 0.9159, auc 0.9354
epoch 14901, loss 0.1905, train acc 94.30%, f1 0.9180, precision 0.9202, recall 0.9159, auc 0.9367
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_normal_15000
normal
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_3
./test_pima/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.655

the Fscore is 0.54

the precision is 0.5869565217391305

the recall is 0.5

Done
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_3
----------------------



epoch 1, loss 0.6934, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6239, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5983, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5740, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5521, train acc 66.45%, f1 0.1121, precision 0.7222, recall 0.0607, auc 0.5241
epoch 501, loss 0.5344, train acc 70.36%, f1 0.3209, precision 0.7963, recall 0.2009, auc 0.5867
epoch 601, loss 0.5207, train acc 73.78%, f1 0.4790, precision 0.7789, recall 0.3458, auc 0.6466
epoch 701, loss 0.5105, train acc 74.59%, f1 0.5357, precision 0.7377, recall 0.4206, auc 0.6703
epoch 801, loss 0.5029, train acc 75.41%, f1 0.5840, precision 0.7114, recall 0.4953, auc 0.6939
epoch 901, loss 0.4973, train acc 76.38%, f1 0.6154, precision 0.7117, recall 0.5421, auc 0.7123
epoch 1001, loss 0.4932, train acc 76.22%, f1 0.6178, precision 0.7024, recall 0.5514, auc 0.7132
epoch 1101, loss 0.4899, train acc 75.90%, f1 0.6186, precision 0.6897, recall 0.5607, auc 0.7129
epoch 1201, loss 0.4870, train acc 76.71%, f1 0.6380, precision 0.6961, recall 0.5888, auc 0.7256
epoch 1301, loss 0.4837, train acc 76.87%, f1 0.6378, precision 0.7022, recall 0.5841, auc 0.7258
epoch 1401, loss 0.4797, train acc 76.38%, f1 0.6272, precision 0.6971, recall 0.5701, auc 0.7188
epoch 1501, loss 0.4754, train acc 77.36%, f1 0.6390, precision 0.7193, recall 0.5748, auc 0.7274
epoch 1601, loss 0.4711, train acc 77.69%, f1 0.6442, precision 0.7251, recall 0.5794, auc 0.7310
epoch 1701, loss 0.4668, train acc 77.69%, f1 0.6496, precision 0.7175, recall 0.5935, auc 0.7342
epoch 1801, loss 0.4627, train acc 77.69%, f1 0.6532, precision 0.7127, recall 0.6028, auc 0.7364
epoch 1901, loss 0.4588, train acc 77.85%, f1 0.6566, precision 0.7143, recall 0.6075, auc 0.7387
epoch 2001, loss 0.4553, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 2101, loss 0.4521, train acc 77.85%, f1 0.6583, precision 0.7120, recall 0.6121, auc 0.7398
epoch 2201, loss 0.4493, train acc 78.18%, f1 0.6616, precision 0.7198, recall 0.6121, auc 0.7423
epoch 2301, loss 0.4467, train acc 77.85%, f1 0.6600, precision 0.7097, recall 0.6168, auc 0.7409
epoch 2401, loss 0.4443, train acc 78.66%, f1 0.6733, precision 0.7219, recall 0.6308, auc 0.7504
epoch 2501, loss 0.4420, train acc 78.83%, f1 0.6750, precision 0.7258, recall 0.6308, auc 0.7517
epoch 2601, loss 0.4397, train acc 79.15%, f1 0.6800, precision 0.7312, recall 0.6355, auc 0.7553
epoch 2701, loss 0.4372, train acc 79.15%, f1 0.6816, precision 0.7287, recall 0.6402, auc 0.7563
epoch 2801, loss 0.4348, train acc 79.64%, f1 0.6898, precision 0.7354, recall 0.6495, auc 0.7623
epoch 2901, loss 0.4325, train acc 79.80%, f1 0.6946, precision 0.7344, recall 0.6589, auc 0.7657
epoch 3001, loss 0.4304, train acc 79.97%, f1 0.6963, precision 0.7382, recall 0.6589, auc 0.7669
epoch 3101, loss 0.4286, train acc 79.97%, f1 0.6978, precision 0.7358, recall 0.6636, auc 0.7680
epoch 3201, loss 0.4269, train acc 80.62%, f1 0.7090, precision 0.7436, recall 0.6776, auc 0.7763
epoch 3301, loss 0.4254, train acc 80.78%, f1 0.7108, precision 0.7474, recall 0.6776, auc 0.7775
epoch 3401, loss 0.4240, train acc 80.94%, f1 0.7139, precision 0.7487, recall 0.6822, auc 0.7799
epoch 3501, loss 0.4226, train acc 81.11%, f1 0.7171, precision 0.7500, recall 0.6869, auc 0.7822
epoch 3601, loss 0.4212, train acc 81.11%, f1 0.7171, precision 0.7500, recall 0.6869, auc 0.7822
epoch 3701, loss 0.4197, train acc 81.27%, f1 0.7202, precision 0.7513, recall 0.6916, auc 0.7845
epoch 3801, loss 0.4181, train acc 81.27%, f1 0.7188, precision 0.7538, recall 0.6869, auc 0.7835
epoch 3901, loss 0.4162, train acc 81.27%, f1 0.7174, precision 0.7565, recall 0.6822, auc 0.7824
epoch 4001, loss 0.4137, train acc 81.60%, f1 0.7210, precision 0.7644, recall 0.6822, auc 0.7849
epoch 4101, loss 0.4108, train acc 81.43%, f1 0.7192, precision 0.7604, recall 0.6822, auc 0.7836
epoch 4201, loss 0.4073, train acc 82.08%, f1 0.7277, precision 0.7737, recall 0.6869, auc 0.7897
epoch 4301, loss 0.4035, train acc 82.57%, f1 0.7384, precision 0.7744, recall 0.7056, auc 0.7978
epoch 4401, loss 0.3996, train acc 82.90%, f1 0.7433, precision 0.7795, recall 0.7103, auc 0.8014
epoch 4501, loss 0.3956, train acc 83.06%, f1 0.7451, precision 0.7835, recall 0.7103, auc 0.8026
epoch 4601, loss 0.3915, train acc 83.55%, f1 0.7531, precision 0.7897, recall 0.7196, auc 0.8086
epoch 4701, loss 0.3875, train acc 83.55%, f1 0.7518, precision 0.7927, recall 0.7150, auc 0.8075
epoch 4801, loss 0.3839, train acc 84.04%, f1 0.7598, precision 0.7990, recall 0.7243, auc 0.8134
epoch 4901, loss 0.3805, train acc 83.88%, f1 0.7579, precision 0.7949, recall 0.7243, auc 0.8121
epoch 5001, loss 0.3775, train acc 84.20%, f1 0.7640, precision 0.7970, recall 0.7336, auc 0.8168
epoch 5101, loss 0.3747, train acc 84.04%, f1 0.7621, precision 0.7929, recall 0.7336, auc 0.8156
epoch 5201, loss 0.3719, train acc 84.36%, f1 0.7681, precision 0.7950, recall 0.7430, auc 0.8202
epoch 5301, loss 0.3690, train acc 83.88%, f1 0.7614, precision 0.7861, recall 0.7383, auc 0.8154
epoch 5401, loss 0.3664, train acc 84.04%, f1 0.7644, precision 0.7871, recall 0.7430, auc 0.8177
epoch 5501, loss 0.3639, train acc 83.88%, f1 0.7614, precision 0.7861, recall 0.7383, auc 0.8154
epoch 5601, loss 0.3615, train acc 83.88%, f1 0.7614, precision 0.7861, recall 0.7383, auc 0.8154
epoch 5701, loss 0.3592, train acc 84.04%, f1 0.7633, precision 0.7900, recall 0.7383, auc 0.8167
epoch 5801, loss 0.3567, train acc 84.04%, f1 0.7633, precision 0.7900, recall 0.7383, auc 0.8167
epoch 5901, loss 0.3541, train acc 84.36%, f1 0.7681, precision 0.7950, recall 0.7430, auc 0.8202
epoch 6001, loss 0.3517, train acc 84.36%, f1 0.7681, precision 0.7950, recall 0.7430, auc 0.8202
epoch 6101, loss 0.3492, train acc 84.53%, f1 0.7711, precision 0.7960, recall 0.7477, auc 0.8226
epoch 6201, loss 0.3468, train acc 84.85%, f1 0.7759, precision 0.8010, recall 0.7523, auc 0.8262
epoch 6301, loss 0.3447, train acc 84.85%, f1 0.7759, precision 0.8010, recall 0.7523, auc 0.8262
epoch 6401, loss 0.3426, train acc 85.02%, f1 0.7778, precision 0.8050, recall 0.7523, auc 0.8274
epoch 6501, loss 0.3404, train acc 85.34%, f1 0.7816, precision 0.8131, recall 0.7523, auc 0.8299
epoch 6601, loss 0.3382, train acc 85.34%, f1 0.7805, precision 0.8163, recall 0.7477, auc 0.8288
epoch 6701, loss 0.3361, train acc 85.67%, f1 0.7864, precision 0.8182, recall 0.7570, auc 0.8335
epoch 6801, loss 0.3342, train acc 85.99%, f1 0.7913, precision 0.8232, recall 0.7617, auc 0.8371
epoch 6901, loss 0.3324, train acc 86.48%, f1 0.8000, precision 0.8259, recall 0.7757, auc 0.8441
epoch 7001, loss 0.3307, train acc 86.64%, f1 0.8029, precision 0.8267, recall 0.7804, auc 0.8464
epoch 7101, loss 0.3286, train acc 86.97%, f1 0.8077, precision 0.8317, recall 0.7850, auc 0.8500
epoch 7201, loss 0.3261, train acc 87.30%, f1 0.8125, precision 0.8366, recall 0.7897, auc 0.8536
epoch 7301, loss 0.3234, train acc 87.30%, f1 0.8125, precision 0.8366, recall 0.7897, auc 0.8536
epoch 7401, loss 0.3202, train acc 87.30%, f1 0.8134, precision 0.8333, recall 0.7944, auc 0.8547
epoch 7501, loss 0.3173, train acc 87.13%, f1 0.8115, precision 0.8293, recall 0.7944, auc 0.8534
epoch 7601, loss 0.3145, train acc 86.97%, f1 0.8086, precision 0.8284, recall 0.7897, auc 0.8511
epoch 7701, loss 0.3116, train acc 86.97%, f1 0.8086, precision 0.8284, recall 0.7897, auc 0.8511
epoch 7801, loss 0.3082, train acc 86.81%, f1 0.8067, precision 0.8244, recall 0.7897, auc 0.8499
epoch 7901, loss 0.3052, train acc 86.97%, f1 0.8077, precision 0.8317, recall 0.7850, auc 0.8500
epoch 8001, loss 0.3028, train acc 87.30%, f1 0.8116, precision 0.8400, recall 0.7850, auc 0.8525
epoch 8101, loss 0.3006, train acc 87.13%, f1 0.8096, precision 0.8358, recall 0.7850, auc 0.8513
epoch 8201, loss 0.2984, train acc 87.30%, f1 0.8116, precision 0.8400, recall 0.7850, auc 0.8525/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2964, train acc 87.79%, f1 0.8201, precision 0.8424, recall 0.7991, auc 0.8595
epoch 8401, loss 0.2945, train acc 87.95%, f1 0.8221, precision 0.8465, recall 0.7991, auc 0.8608
epoch 8501, loss 0.2926, train acc 88.11%, f1 0.8249, precision 0.8473, recall 0.8037, auc 0.8631
epoch 8601, loss 0.2909, train acc 88.27%, f1 0.8269, precision 0.8515, recall 0.8037, auc 0.8644
epoch 8701, loss 0.2891, train acc 88.27%, f1 0.8269, precision 0.8515, recall 0.8037, auc 0.8644
epoch 8801, loss 0.2875, train acc 88.44%, f1 0.8297, precision 0.8522, recall 0.8084, auc 0.8667
epoch 8901, loss 0.2859, train acc 88.27%, f1 0.8278, precision 0.8480, recall 0.8084, auc 0.8655
epoch 9001, loss 0.2844, train acc 88.44%, f1 0.8305, precision 0.8488, recall 0.8131, auc 0.8678
epoch 9101, loss 0.2828, train acc 88.44%, f1 0.8305, precision 0.8488, recall 0.8131, auc 0.8678
epoch 9201, loss 0.2812, train acc 88.60%, f1 0.8325, precision 0.8529, recall 0.8131, auc 0.8690
epoch 9301, loss 0.2798, train acc 88.60%, f1 0.8325, precision 0.8529, recall 0.8131, auc 0.8690
epoch 9401, loss 0.2784, train acc 88.76%, f1 0.8353, precision 0.8537, recall 0.8178, auc 0.8714
epoch 9501, loss 0.2770, train acc 89.09%, f1 0.8409, precision 0.8551, recall 0.8271, auc 0.8761
epoch 9601, loss 0.2756, train acc 89.09%, f1 0.8409, precision 0.8551, recall 0.8271, auc 0.8761
epoch 9701, loss 0.2739, train acc 89.41%, f1 0.8456, precision 0.8599, recall 0.8318, auc 0.8796
epoch 9801, loss 0.2716, train acc 90.07%, f1 0.8544, precision 0.8732, recall 0.8364, auc 0.8857
epoch 9901, loss 0.2678, train acc 89.90%, f1 0.8517, precision 0.8725, recall 0.8318, auc 0.8834
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_normal_10000
normal
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_3
./test_pima/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.6492592592592593

the Fscore is 0.5384615384615384

the precision is 0.56

the recall is 0.5185185185185185

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_3
----------------------



epoch 1, loss 0.6984, train acc 34.85%, f1 0.5169, precision 0.3485, recall 1.0000, auc 0.5000
epoch 101, loss 0.6254, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5994, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5752, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5514, train acc 67.26%, f1 0.1590, precision 0.7600, recall 0.0888, auc 0.5369
epoch 501, loss 0.5271, train acc 72.31%, f1 0.4257, precision 0.7683, recall 0.2944, auc 0.6234
epoch 601, loss 0.5086, train acc 74.59%, f1 0.5543, precision 0.7132, recall 0.4533, auc 0.6779
epoch 701, loss 0.4962, train acc 75.24%, f1 0.6021, precision 0.6845, recall 0.5374, auc 0.7024
epoch 801, loss 0.4879, train acc 76.38%, f1 0.6253, precision 0.6994, recall 0.5654, auc 0.7177
epoch 901, loss 0.4825, train acc 76.71%, f1 0.6380, precision 0.6961, recall 0.5888, auc 0.7256
epoch 1001, loss 0.4789, train acc 76.87%, f1 0.6414, precision 0.6978, recall 0.5935, auc 0.7280
epoch 1101, loss 0.4763, train acc 77.20%, f1 0.6517, precision 0.6968, recall 0.6121, auc 0.7348
epoch 1201, loss 0.4737, train acc 77.36%, f1 0.6516, precision 0.7027, recall 0.6075, auc 0.7350
epoch 1301, loss 0.4706, train acc 77.36%, f1 0.6516, precision 0.7027, recall 0.6075, auc 0.7350
epoch 1401, loss 0.4668, train acc 78.18%, f1 0.6616, precision 0.7198, recall 0.6121, auc 0.7423
epoch 1501, loss 0.4623, train acc 78.34%, f1 0.6616, precision 0.7263, recall 0.6075, auc 0.7425
epoch 1601, loss 0.4573, train acc 77.85%, f1 0.6531, precision 0.7191, recall 0.5981, auc 0.7366
epoch 1701, loss 0.4522, train acc 78.34%, f1 0.6598, precision 0.7288, recall 0.6028, auc 0.7414
epoch 1801, loss 0.4479, train acc 78.01%, f1 0.6547, precision 0.7232, recall 0.5981, auc 0.7378
epoch 1901, loss 0.4445, train acc 78.01%, f1 0.6547, precision 0.7232, recall 0.5981, auc 0.7378
epoch 2001, loss 0.4416, train acc 78.18%, f1 0.6582, precision 0.7247, recall 0.6028, auc 0.7402
epoch 2101, loss 0.4391, train acc 78.83%, f1 0.6701, precision 0.7333, recall 0.6168, auc 0.7484
epoch 2201, loss 0.4368, train acc 79.64%, f1 0.6867, precision 0.7405, recall 0.6402, auc 0.7601
epoch 2301, loss 0.4348, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 2401, loss 0.4330, train acc 80.13%, f1 0.6965, precision 0.7447, recall 0.6542, auc 0.7671
epoch 2501, loss 0.4313, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 2601, loss 0.4297, train acc 79.97%, f1 0.6948, precision 0.7407, recall 0.6542, auc 0.7659
epoch 2701, loss 0.4281, train acc 80.29%, f1 0.6983, precision 0.7487, recall 0.6542, auc 0.7684
epoch 2801, loss 0.4265, train acc 80.46%, f1 0.7015, precision 0.7500, recall 0.6589, auc 0.7707
epoch 2901, loss 0.4247, train acc 80.94%, f1 0.7082, precision 0.7594, recall 0.6636, auc 0.7755
epoch 3001, loss 0.4229, train acc 81.27%, f1 0.7146, precision 0.7619, recall 0.6729, auc 0.7802
epoch 3101, loss 0.4210, train acc 81.60%, f1 0.7210, precision 0.7644, recall 0.6822, auc 0.7849
epoch 3201, loss 0.4189, train acc 81.11%, f1 0.7143, precision 0.7552, recall 0.6776, auc 0.7800
epoch 3301, loss 0.4167, train acc 81.60%, f1 0.7210, precision 0.7644, recall 0.6822, auc 0.7849
epoch 3401, loss 0.4144, train acc 82.08%, f1 0.7291, precision 0.7708, recall 0.6916, auc 0.7908
epoch 3501, loss 0.4121, train acc 81.92%, f1 0.7273, precision 0.7668, recall 0.6916, auc 0.7895
epoch 3601, loss 0.4098, train acc 81.60%, f1 0.7210, precision 0.7644, recall 0.6822, auc 0.7849
epoch 3701, loss 0.4075, train acc 81.60%, f1 0.7210, precision 0.7644, recall 0.6822, auc 0.7849
epoch 3801, loss 0.4051, train acc 81.76%, f1 0.7228, precision 0.7684, recall 0.6822, auc 0.7861
epoch 3901, loss 0.4028, train acc 82.08%, f1 0.7291, precision 0.7708, recall 0.6916, auc 0.7908
epoch 4001, loss 0.4005, train acc 82.57%, f1 0.7371, precision 0.7772, recall 0.7009, auc 0.7967
epoch 4101, loss 0.3982, train acc 82.90%, f1 0.7420, precision 0.7824, recall 0.7056, auc 0.8003
epoch 4201, loss 0.3960, train acc 83.06%, f1 0.7438, precision 0.7865, recall 0.7056, auc 0.8016
epoch 4301, loss 0.3939, train acc 83.22%, f1 0.7469, precision 0.7876, recall 0.7103, auc 0.8039
epoch 4401, loss 0.3919, train acc 83.22%, f1 0.7469, precision 0.7876, recall 0.7103, auc 0.8039
epoch 4501, loss 0.3898, train acc 83.71%, f1 0.7549, precision 0.7938, recall 0.7196, auc 0.8098
epoch 4601, loss 0.3875, train acc 83.71%, f1 0.7549, precision 0.7938, recall 0.7196, auc 0.8098
epoch 4701, loss 0.3844, train acc 83.88%, f1 0.7579, precision 0.7949, recall 0.7243, auc 0.8121
epoch 4801, loss 0.3810, train acc 83.88%, f1 0.7579, precision 0.7949, recall 0.7243, auc 0.8121
epoch 4901, loss 0.3777, train acc 84.69%, f1 0.7685, precision 0.8125, recall 0.7290, auc 0.8195
epoch 5001, loss 0.3745, train acc 85.02%, f1 0.7734, precision 0.8177, recall 0.7336, auc 0.8231
epoch 5101, loss 0.3710, train acc 85.02%, f1 0.7734, precision 0.8177, recall 0.7336, auc 0.8231
epoch 5201, loss 0.3675, train acc 85.18%, f1 0.7764, precision 0.8187, recall 0.7383, auc 0.8254
epoch 5301, loss 0.3640, train acc 85.34%, f1 0.7783, precision 0.8229, recall 0.7383, auc 0.8267
epoch 5401, loss 0.3596, train acc 85.50%, f1 0.7813, precision 0.8238, recall 0.7430, auc 0.8290
epoch 5501, loss 0.3544, train acc 85.67%, f1 0.7854, precision 0.8214, recall 0.7523, auc 0.8324
epoch 5601, loss 0.3496, train acc 85.83%, f1 0.7883, precision 0.8223, recall 0.7570, auc 0.8348
epoch 5701, loss 0.3449, train acc 86.16%, f1 0.7942, precision 0.8241, recall 0.7664, auc 0.8394
epoch 5801, loss 0.3406, train acc 86.32%, f1 0.7961, precision 0.8283, recall 0.7664, auc 0.8407
epoch 5901, loss 0.3369, train acc 86.32%, f1 0.7961, precision 0.8283, recall 0.7664, auc 0.8407
epoch 6001, loss 0.3335, train acc 86.48%, f1 0.7990, precision 0.8291, recall 0.7710, auc 0.8430
epoch 6101, loss 0.3302, train acc 86.64%, f1 0.8010, precision 0.8333, recall 0.7710, auc 0.8443
epoch 6201, loss 0.3270, train acc 87.13%, f1 0.8078, precision 0.8426, recall 0.7757, auc 0.8491
epoch 6301, loss 0.3236, train acc 87.30%, f1 0.8098, precision 0.8469, recall 0.7757, auc 0.8504
epoch 6401, loss 0.3200, train acc 87.13%, f1 0.8078, precision 0.8426, recall 0.7757, auc 0.8491
epoch 6501, loss 0.3162, train acc 87.13%, f1 0.8078, precision 0.8426, recall 0.7757, auc 0.8491
epoch 6601, loss 0.3121, train acc 87.13%, f1 0.8087, precision 0.8392, recall 0.7804, auc 0.8502
epoch 6701, loss 0.3078, train acc 87.13%, f1 0.8068, precision 0.8462, recall 0.7710, auc 0.8480
epoch 6801, loss 0.3037, train acc 87.62%, f1 0.8146, precision 0.8520, recall 0.7804, auc 0.8539
epoch 6901, loss 0.2995, train acc 87.30%, f1 0.8098, precision 0.8469, recall 0.7757, auc 0.8504
epoch 7001, loss 0.2956, train acc 87.79%, f1 0.8184, precision 0.8492, recall 0.7897, auc 0.8574
epoch 7101, loss 0.2918, train acc 88.27%, f1 0.8252, precision 0.8586, recall 0.7944, auc 0.8622
epoch 7201, loss 0.2882, train acc 88.93%, f1 0.8341, precision 0.8724, recall 0.7991, auc 0.8683
epoch 7301, loss 0.2852, train acc 88.76%, f1 0.8313, precision 0.8718, recall 0.7944, auc 0.8659
epoch 7401, loss 0.2824, train acc 88.44%, f1 0.8264, precision 0.8667, recall 0.7897, auc 0.8624
epoch 7501, loss 0.2795, train acc 88.27%, f1 0.8252, precision 0.8586, recall 0.7944, auc 0.8622
epoch 7601, loss 0.2760, train acc 87.95%, f1 0.8195, precision 0.8571, recall 0.7850, auc 0.8575
epoch 7701, loss 0.2727, train acc 87.95%, f1 0.8204, precision 0.8535, recall 0.7897, auc 0.8586
epoch 7801, loss 0.2696, train acc 88.11%, f1 0.8241, precision 0.8507, recall 0.7991, auc 0.8620
epoch 7901, loss 0.2669, train acc 88.11%, f1 0.8241, precision 0.8507, recall 0.7991, auc 0.8620
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_normal_8000
normal
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_3
./test_pima/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.6692592592592593

the Fscore is 0.5599999999999999

the precision is 0.6086956521739131

the recall is 0.5185185185185185

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_3
----------------------



epoch 1, loss 0.6964, train acc 34.85%, f1 0.5169, precision 0.3485, recall 1.0000, auc 0.5000
epoch 101, loss 0.6249, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5991, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5750, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5531, train acc 66.61%, f1 0.1126, precision 0.7647, recall 0.0607, auc 0.5254
epoch 501, loss 0.5354, train acc 70.20%, f1 0.3146, precision 0.7925, recall 0.1963, auc 0.5844
epoch 601, loss 0.5216, train acc 73.62%, f1 0.4706, precision 0.7826, recall 0.3364, auc 0.6432
epoch 701, loss 0.5112, train acc 74.76%, f1 0.5373, precision 0.7438, recall 0.4206, auc 0.6715
epoch 801, loss 0.5035, train acc 75.24%, f1 0.5801, precision 0.7095, recall 0.4907, auc 0.6916
epoch 901, loss 0.4978, train acc 76.38%, f1 0.6154, precision 0.7117, recall 0.5421, auc 0.7123
epoch 1001, loss 0.4936, train acc 76.22%, f1 0.6178, precision 0.7024, recall 0.5514, auc 0.7132
epoch 1101, loss 0.4903, train acc 76.06%, f1 0.6202, precision 0.6936, recall 0.5607, auc 0.7141
epoch 1201, loss 0.4876, train acc 76.22%, f1 0.6313, precision 0.6868, recall 0.5841, auc 0.7208
epoch 1301, loss 0.4847, train acc 76.38%, f1 0.6329, precision 0.6906, recall 0.5841, auc 0.7221
epoch 1401, loss 0.4813, train acc 76.55%, f1 0.6345, precision 0.6944, recall 0.5841, auc 0.7233
epoch 1501, loss 0.4773, train acc 76.87%, f1 0.6359, precision 0.7045, recall 0.5794, auc 0.7247
epoch 1601, loss 0.4727, train acc 77.36%, f1 0.6408, precision 0.7168, recall 0.5794, auc 0.7285
epoch 1701, loss 0.4678, train acc 77.36%, f1 0.6408, precision 0.7168, recall 0.5794, auc 0.7285
epoch 1801, loss 0.4630, train acc 77.52%, f1 0.6462, precision 0.7159, recall 0.5888, auc 0.7319
epoch 1901, loss 0.4585, train acc 77.52%, f1 0.6462, precision 0.7159, recall 0.5888, auc 0.7319
epoch 2001, loss 0.4543, train acc 78.01%, f1 0.6547, precision 0.7232, recall 0.5981, auc 0.7378
epoch 2101, loss 0.4506, train acc 77.85%, f1 0.6548, precision 0.7167, recall 0.6028, auc 0.7377
epoch 2201, loss 0.4471, train acc 78.01%, f1 0.6582, precision 0.7182, recall 0.6075, auc 0.7400
epoch 2301, loss 0.4440, train acc 78.34%, f1 0.6650, precision 0.7213, recall 0.6168, auc 0.7447
epoch 2401, loss 0.4412, train acc 78.34%, f1 0.6667, precision 0.7189, recall 0.6215, auc 0.7457
epoch 2501, loss 0.4386, train acc 78.99%, f1 0.6799, precision 0.7249, recall 0.6402, auc 0.7551
epoch 2601, loss 0.4362, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 2701, loss 0.4341, train acc 79.80%, f1 0.6900, precision 0.7419, recall 0.6449, auc 0.7624
epoch 2801, loss 0.4321, train acc 79.64%, f1 0.6898, precision 0.7354, recall 0.6495, auc 0.7623
epoch 2901, loss 0.4302, train acc 79.80%, f1 0.6931, precision 0.7368, recall 0.6542, auc 0.7646
epoch 3001, loss 0.4285, train acc 79.64%, f1 0.6914, precision 0.7330, recall 0.6542, auc 0.7634
epoch 3101, loss 0.4270, train acc 80.62%, f1 0.7090, precision 0.7436, recall 0.6776, auc 0.7763
epoch 3201, loss 0.4255, train acc 80.78%, f1 0.7122, precision 0.7449, recall 0.6822, auc 0.7786
epoch 3301, loss 0.4240, train acc 80.94%, f1 0.7139, precision 0.7487, recall 0.6822, auc 0.7799
epoch 3401, loss 0.4225, train acc 80.94%, f1 0.7139, precision 0.7487, recall 0.6822, auc 0.7799
epoch 3501, loss 0.4210, train acc 80.78%, f1 0.7108, precision 0.7474, recall 0.6776, auc 0.7775
epoch 3601, loss 0.4194, train acc 80.94%, f1 0.7139, precision 0.7487, recall 0.6822, auc 0.7799
epoch 3701, loss 0.4179, train acc 81.27%, f1 0.7174, precision 0.7565, recall 0.6822, auc 0.7824
epoch 3801, loss 0.4163, train acc 81.27%, f1 0.7174, precision 0.7565, recall 0.6822, auc 0.7824
epoch 3901, loss 0.4147, train acc 81.60%, f1 0.7237, precision 0.7590, recall 0.6916, auc 0.7870
epoch 4001, loss 0.4129, train acc 82.08%, f1 0.7317, precision 0.7653, recall 0.7009, auc 0.7930
epoch 4101, loss 0.4107, train acc 81.92%, f1 0.7286, precision 0.7641, recall 0.6963, auc 0.7906
epoch 4201, loss 0.4084, train acc 81.92%, f1 0.7273, precision 0.7668, recall 0.6916, auc 0.7895
epoch 4301, loss 0.4059, train acc 82.08%, f1 0.7304, precision 0.7680, recall 0.6963, auc 0.7919
epoch 4401, loss 0.4036, train acc 82.25%, f1 0.7335, precision 0.7692, recall 0.7009, auc 0.7942
epoch 4501, loss 0.4015, train acc 82.25%, f1 0.7348, precision 0.7665, recall 0.7056, auc 0.7953
epoch 4601, loss 0.3995, train acc 82.57%, f1 0.7397, precision 0.7716, recall 0.7103, auc 0.7989
epoch 4701, loss 0.3973, train acc 82.57%, f1 0.7409, precision 0.7688, recall 0.7150, auc 0.8000
epoch 4801, loss 0.3946, train acc 83.06%, f1 0.7488, precision 0.7750, recall 0.7243, auc 0.8059
epoch 4901, loss 0.3913, train acc 82.90%, f1 0.7445, precision 0.7766, recall 0.7150, auc 0.8025
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_normal_5000
normal
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_3
./test_pima/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.7212962962962962

the Fscore is 0.6336633663366336

the precision is 0.6808510638297872

the recall is 0.5925925925925926

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_3
----------------------



epoch 1, loss 0.6986, train acc 34.85%, f1 0.5169, precision 0.3485, recall 1.0000, auc 0.5000
epoch 101, loss 0.6258, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5999, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5759, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5541, train acc 66.45%, f1 0.1043, precision 0.7500, recall 0.0561, auc 0.5230
epoch 501, loss 0.5362, train acc 70.03%, f1 0.3030, precision 0.8000, recall 0.1869, auc 0.5810
epoch 601, loss 0.5223, train acc 73.62%, f1 0.4671, precision 0.7889, recall 0.3318, auc 0.6421
epoch 701, loss 0.5118, train acc 74.76%, f1 0.5373, precision 0.7438, recall 0.4206, auc 0.6715
epoch 801, loss 0.5039, train acc 75.41%, f1 0.5817, precision 0.7143, recall 0.4907, auc 0.6928
epoch 901, loss 0.4981, train acc 76.38%, f1 0.6154, precision 0.7117, recall 0.5421, auc 0.7123
epoch 1001, loss 0.4937, train acc 76.06%, f1 0.6162, precision 0.6982, recall 0.5514, auc 0.7120
epoch 1101, loss 0.4900, train acc 76.22%, f1 0.6276, precision 0.6910, recall 0.5748, auc 0.7186
epoch 1201, loss 0.4862, train acc 76.71%, f1 0.6434, precision 0.6898, recall 0.6028, auc 0.7289
epoch 1301, loss 0.4810, train acc 76.87%, f1 0.6485, precision 0.6895, recall 0.6121, auc 0.7323
epoch 1401, loss 0.4748, train acc 77.36%, f1 0.6534, precision 0.7005, recall 0.6121, auc 0.7361
epoch 1501, loss 0.4686, train acc 78.18%, f1 0.6616, precision 0.7198, recall 0.6121, auc 0.7423
epoch 1601, loss 0.4626, train acc 78.18%, f1 0.6616, precision 0.7198, recall 0.6121, auc 0.7423
epoch 1701, loss 0.4571, train acc 77.20%, f1 0.6465, precision 0.7033, recall 0.5981, auc 0.7316
epoch 1801, loss 0.4522, train acc 77.69%, f1 0.6549, precision 0.7104, recall 0.6075, auc 0.7375
epoch 1901, loss 0.4480, train acc 78.01%, f1 0.6599, precision 0.7158, recall 0.6121, auc 0.7411
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_normal_2000
normal
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_3
./test_pima/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.7355555555555556

the Fscore is 0.6534653465346535

the precision is 0.7021276595744681

the recall is 0.6111111111111112

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_3
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3213, train acc 87.51%, f1 0.8751, precision 0.8752, recall 0.8749, auc 0.8751
epoch 201, loss 0.1381, train acc 96.63%, f1 0.9663, precision 0.9662, recall 0.9665, auc 0.9663
epoch 301, loss 0.0861, train acc 98.22%, f1 0.9822, precision 0.9822, recall 0.9822, auc 0.9822
epoch 401, loss 0.0435, train acc 98.66%, f1 0.9866, precision 0.9866, recall 0.9866, auc 0.9866
epoch 501, loss 0.0624, train acc 98.87%, f1 0.9887, precision 0.9887, recall 0.9886, auc 0.9887
epoch 601, loss 0.0446, train acc 99.02%, f1 0.9902, precision 0.9902, recall 0.9902, auc 0.9902
epoch 701, loss 0.0395, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9916, auc 0.9916
epoch 801, loss 0.0200, train acc 99.27%, f1 0.9927, precision 0.9927, recall 0.9927, auc 0.9927
epoch 901, loss 0.0267, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9933, auc 0.9934
epoch 1001, loss 0.0326, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1101, loss 0.0105, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9946, auc 0.9947
epoch 1201, loss 0.0114, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 1301, loss 0.0217, train acc 99.52%, f1 0.9952, precision 0.9953, recall 0.9952, auc 0.9952
epoch 1401, loss 0.0151, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1501, loss 0.0103, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1601, loss 0.0118, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1701, loss 0.0079, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1801, loss 0.0131, train acc 99.63%, f1 0.9963, precision 0.9964, recall 0.9963, auc 0.9963
epoch 1901, loss 0.0130, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2001, loss 0.0085, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2101, loss 0.0066, train acc 99.66%, f1 0.9966, precision 0.9967, recall 0.9966, auc 0.9966
epoch 2201, loss 0.0206, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2301, loss 0.0029, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2401, loss 0.0076, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2501, loss 0.0212, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 2601, loss 0.0080, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2701, loss 0.0064, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2801, loss 0.0044, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2901, loss 0.0135, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3001, loss 0.0140, train acc 99.71%, f1 0.9971, precision 0.9972, recall 0.9971, auc 0.9971
epoch 3101, loss 0.0080, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 3201, loss 0.0084, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9973, auc 0.9973
epoch 3301, loss 0.0065, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 3401, loss 0.0069, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 3501, loss 0.0042, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3601, loss 0.0026, train acc 99.74%, f1 0.9974, precision 0.9975, recall 0.9974, auc 0.9974
epoch 3701, loss 0.0062, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3801, loss 0.0035, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 3901, loss 0.0063, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4001, loss 0.0047, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4101, loss 0.0045, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4201, loss 0.0074, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4301, loss 0.0112, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 4401, loss 0.0037, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4501, loss 0.0054, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 4601, loss 0.0060, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4701, loss 0.0073, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4801, loss 0.0072, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 4901, loss 0.0049, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5001, loss 0.0019, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5101, loss 0.0089, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5201, loss 0.0064, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9982
epoch 5301, loss 0.0025, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 5401, loss 0.0048, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 5501, loss 0.0065, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 5601, loss 0.0030, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5701, loss 0.0088, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5801, loss 0.0036, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 5901, loss 0.0053, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6001, loss 0.0026, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6101, loss 0.0059, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 6201, loss 0.0066, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 6301, loss 0.0041, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6401, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6501, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 6601, loss 0.0044, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6701, loss 0.0011, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 6801, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6901, loss 0.0063, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7001, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7101, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7201, loss 0.0024, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 7301, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7401, loss 0.0039, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 7501, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 7601, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7701, loss 0.0049, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7801, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 7901, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 8001, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 8101, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 8201, loss 0.0024, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 8301, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8401, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 8501, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8601, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 8701, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8801, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 8901, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 9001, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 9101, loss 0.0018, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 9201, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9301, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 9401, loss 0.0023, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9501, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 9601, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 9701, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 9801, loss 0.0018, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9901, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10001, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10101, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10201, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10301, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10401, loss 0.0003, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10501, loss 0.0003, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10601, loss 0.0007, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_3
./test_vehicle0/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.975

the Fscore is 0.9743589743589743

the precision is 1.0

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_3
----------------------



epoch 1, loss 0.6932, train acc 67.31%, f1 0.7092, precision 0.6387, recall 0.7970, auc 0.6731
epoch 101, loss 0.3645, train acc 87.05%, f1 0.8709, precision 0.8678, recall 0.8741, auc 0.8705
epoch 201, loss 0.1483, train acc 96.84%, f1 0.9684, precision 0.9684, recall 0.9685, auc 0.9684
epoch 301, loss 0.0718, train acc 98.21%, f1 0.9821, precision 0.9823, recall 0.9819, auc 0.9821
epoch 401, loss 0.0576, train acc 98.66%, f1 0.9866, precision 0.9868, recall 0.9864, auc 0.9866
epoch 501, loss 0.0545, train acc 98.83%, f1 0.9883, precision 0.9886, recall 0.9881, auc 0.9883
epoch 601, loss 0.0483, train acc 99.03%, f1 0.9903, precision 0.9905, recall 0.9902, auc 0.9903
epoch 701, loss 0.0343, train acc 99.17%, f1 0.9917, precision 0.9919, recall 0.9916, auc 0.9917
epoch 801, loss 0.0326, train acc 99.27%, f1 0.9927, precision 0.9928, recall 0.9927, auc 0.9927
epoch 901, loss 0.0097, train acc 99.35%, f1 0.9935, precision 0.9936, recall 0.9934, auc 0.9935
epoch 1001, loss 0.0271, train acc 99.41%, f1 0.9941, precision 0.9943, recall 0.9940, auc 0.9941
epoch 1101, loss 0.0225, train acc 99.47%, f1 0.9947, precision 0.9948, recall 0.9946, auc 0.9947
epoch 1201, loss 0.0207, train acc 99.50%, f1 0.9950, precision 0.9951, recall 0.9949, auc 0.9950
epoch 1301, loss 0.0157, train acc 99.53%, f1 0.9953, precision 0.9954, recall 0.9952, auc 0.9953
epoch 1401, loss 0.0116, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1501, loss 0.0156, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9957, auc 0.9958
epoch 1601, loss 0.0182, train acc 99.59%, f1 0.9959, precision 0.9960, recall 0.9958, auc 0.9959
epoch 1701, loss 0.0099, train acc 99.61%, f1 0.9961, precision 0.9962, recall 0.9961, auc 0.9961
epoch 1801, loss 0.0077, train acc 99.63%, f1 0.9963, precision 0.9964, recall 0.9962, auc 0.9963
epoch 1901, loss 0.0207, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9963, auc 0.9964
epoch 2001, loss 0.0078, train acc 99.66%, f1 0.9966, precision 0.9967, recall 0.9965, auc 0.9966
epoch 2101, loss 0.0120, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2201, loss 0.0124, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9967, auc 0.9968
epoch 2301, loss 0.0064, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9967, auc 0.9968
epoch 2401, loss 0.0146, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2501, loss 0.0062, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2601, loss 0.0148, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2701, loss 0.0071, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2801, loss 0.0049, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9971, auc 0.9972
epoch 2901, loss 0.0090, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9971, auc 0.9972
epoch 3001, loss 0.0071, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 3101, loss 0.0109, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 3201, loss 0.0077, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 3301, loss 0.0066, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 3401, loss 0.0110, train acc 99.74%, f1 0.9974, precision 0.9975, recall 0.9974, auc 0.9974
epoch 3501, loss 0.0061, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3601, loss 0.0046, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9974, auc 0.9975
epoch 3701, loss 0.0045, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9976, auc 0.9977
epoch 3801, loss 0.0043, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3901, loss 0.0082, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9976, auc 0.9977
epoch 4001, loss 0.0041, train acc 99.77%, f1 0.9977, precision 0.9980, recall 0.9975, auc 0.9977
epoch 4101, loss 0.0122, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 4201, loss 0.0057, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 4301, loss 0.0058, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 4401, loss 0.0150, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 4501, loss 0.0052, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9977, auc 0.9979
epoch 4601, loss 0.0040, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9983, auc 0.9980
epoch 4701, loss 0.0033, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 4801, loss 0.0075, train acc 99.80%, f1 0.9980, precision 0.9982, recall 0.9979, auc 0.9980
epoch 4901, loss 0.0092, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9979, auc 0.9981
epoch 5001, loss 0.0025, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 5101, loss 0.0006, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9980, auc 0.9982
epoch 5201, loss 0.0049, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5301, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 5401, loss 0.0046, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9979, auc 0.9982
epoch 5501, loss 0.0025, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9981, auc 0.9983
epoch 5601, loss 0.0047, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 5701, loss 0.0061, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 5801, loss 0.0029, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9981, auc 0.9984
epoch 5901, loss 0.0042, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 6001, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 6101, loss 0.0032, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9983, auc 0.9985
epoch 6201, loss 0.0042, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9983, auc 0.9986
epoch 6301, loss 0.0047, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 6401, loss 0.0075, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6501, loss 0.0045, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9985, auc 0.9987
epoch 6601, loss 0.0017, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6701, loss 0.0034, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9987, auc 0.9989
epoch 6801, loss 0.0049, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 6901, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9988, auc 0.9989
epoch 7001, loss 0.0024, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7101, loss 0.0025, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7201, loss 0.0037, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7301, loss 0.0024, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 7401, loss 0.0024, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7501, loss 0.0029, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 7601, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7701, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7801, loss 0.0033, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 7901, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 8001, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 8101, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9995, recall 0.9992, auc 0.9993
epoch 8201, loss 0.0011, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 8401, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9992, auc 0.9994
epoch 8501, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 8601, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 8701, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 8801, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 8901, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 9001, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 9101, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9201, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9301, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9994, auc 0.9996
epoch 9401, loss 0.0027, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9501, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9994, auc 0.9996
epoch 9601, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9994, auc 0.9996
epoch 9701, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9801, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 9901, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 10001, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10101, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 10201, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10301, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10401, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10501, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9996, auc 0.9998
epoch 10601, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10701, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10801, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10901, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11001, loss 0.0018, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 11101, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11201, loss 0.0014, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11301, loss 0.0015, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 11401, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11501, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11601, loss 0.0006, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 11701, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 11801, loss 0.0021, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 11901, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12001, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12101, loss 0.0005, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12201, loss 0.0003, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12301, loss 0.0002, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12601, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 13001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 13101, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_3
./test_vehicle0/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.875

the Fscore is 0.8571428571428571

the precision is 1.0

the recall is 0.75

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_3
----------------------



epoch 1, loss 0.6940, train acc 51.83%, f1 0.1135, precision 0.7105, recall 0.0617, auc 0.5183
epoch 101, loss 0.3588, train acc 86.91%, f1 0.8690, precision 0.8692, recall 0.8688, auc 0.8691
epoch 201, loss 0.1464, train acc 96.85%, f1 0.9685, precision 0.9686, recall 0.9685, auc 0.9685
epoch 301, loss 0.1006, train acc 98.29%, f1 0.9828, precision 0.9829, recall 0.9828, auc 0.9829
epoch 401, loss 0.0591, train acc 98.65%, f1 0.9865, precision 0.9866, recall 0.9864, auc 0.9865
epoch 501, loss 0.0338, train acc 98.86%, f1 0.9886, precision 0.9886, recall 0.9886, auc 0.9886
epoch 601, loss 0.0274, train acc 99.06%, f1 0.9906, precision 0.9906, recall 0.9905, auc 0.9906
epoch 701, loss 0.0206, train acc 99.17%, f1 0.9917, precision 0.9917, recall 0.9916, auc 0.9917
epoch 801, loss 0.0318, train acc 99.27%, f1 0.9927, precision 0.9927, recall 0.9927, auc 0.9927
epoch 901, loss 0.0331, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9933, auc 0.9933
epoch 1001, loss 0.0213, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1101, loss 0.0207, train acc 99.45%, f1 0.9945, precision 0.9945, recall 0.9945, auc 0.9945
epoch 1201, loss 0.0105, train acc 99.50%, f1 0.9950, precision 0.9950, recall 0.9950, auc 0.9950
epoch 1301, loss 0.0213, train acc 99.52%, f1 0.9952, precision 0.9953, recall 0.9952, auc 0.9952
epoch 1401, loss 0.0211, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9954, auc 0.9955
epoch 1501, loss 0.0138, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1601, loss 0.0187, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 1701, loss 0.0152, train acc 99.60%, f1 0.9960, precision 0.9961, recall 0.9960, auc 0.9960
epoch 1801, loss 0.0068, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1901, loss 0.0116, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9962, auc 0.9963
epoch 2001, loss 0.0102, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 2101, loss 0.0148, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2201, loss 0.0170, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2301, loss 0.0062, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2401, loss 0.0115, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2501, loss 0.0105, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2601, loss 0.0070, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2701, loss 0.0105, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2801, loss 0.0054, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2901, loss 0.0115, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 3001, loss 0.0104, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 3101, loss 0.0030, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 3201, loss 0.0031, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3301, loss 0.0015, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3401, loss 0.0058, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3501, loss 0.0047, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3601, loss 0.0042, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3701, loss 0.0042, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3801, loss 0.0040, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9976, auc 0.9977
epoch 3901, loss 0.0079, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4001, loss 0.0118, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4101, loss 0.0147, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 4201, loss 0.0042, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4301, loss 0.0049, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 4401, loss 0.0032, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 4501, loss 0.0073, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4601, loss 0.0019, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 4701, loss 0.0035, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 4801, loss 0.0043, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4901, loss 0.0052, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 5001, loss 0.0068, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5101, loss 0.0046, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 5201, loss 0.0008, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 5301, loss 0.0124, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5401, loss 0.0023, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 5501, loss 0.0011, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 5601, loss 0.0036, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 5701, loss 0.0038, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5801, loss 0.0017, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 5901, loss 0.0056, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 6001, loss 0.0012, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 6101, loss 0.0037, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6201, loss 0.0079, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 6301, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6401, loss 0.0023, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 6501, loss 0.0007, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6601, loss 0.0020, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6701, loss 0.0061, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6801, loss 0.0010, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6901, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7001, loss 0.0016, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7101, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7201, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7301, loss 0.0016, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7401, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7501, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 7601, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 7701, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 7801, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 7901, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8001, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8101, loss 0.0033, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 8201, loss 0.0029, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8401, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8501, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 8601, loss 0.0029, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 8701, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8801, loss 0.0033, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 8901, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 9001, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9101, loss 0.0029, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9201, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 9301, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9401, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9501, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9601, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9701, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9801, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9901, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_3
./test_vehicle0/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9875

the Fscore is 0.9873417721518987

the precision is 1.0

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_3
----------------------



epoch 1, loss 0.6932, train acc 57.40%, f1 0.2593, precision 0.9922, recall 0.1491, auc 0.5740
epoch 101, loss 0.2847, train acc 87.38%, f1 0.8732, precision 0.8774, recall 0.8690, auc 0.8738
epoch 201, loss 0.1417, train acc 97.03%, f1 0.9703, precision 0.9705, recall 0.9701, auc 0.9703
epoch 301, loss 0.0959, train acc 98.22%, f1 0.9822, precision 0.9819, recall 0.9824, auc 0.9822
epoch 401, loss 0.0540, train acc 98.60%, f1 0.9860, precision 0.9858, recall 0.9863, auc 0.9860
epoch 501, loss 0.0562, train acc 98.90%, f1 0.9890, precision 0.9888, recall 0.9892, auc 0.9890
epoch 601, loss 0.0512, train acc 99.06%, f1 0.9906, precision 0.9905, recall 0.9907, auc 0.9906
epoch 701, loss 0.0361, train acc 99.17%, f1 0.9917, precision 0.9917, recall 0.9918, auc 0.9917
epoch 801, loss 0.0267, train acc 99.27%, f1 0.9927, precision 0.9926, recall 0.9928, auc 0.9927
epoch 901, loss 0.0202, train acc 99.35%, f1 0.9935, precision 0.9934, recall 0.9936, auc 0.9935
epoch 1001, loss 0.0287, train acc 99.42%, f1 0.9942, precision 0.9941, recall 0.9943, auc 0.9942
epoch 1101, loss 0.0225, train acc 99.45%, f1 0.9945, precision 0.9944, recall 0.9946, auc 0.9945
epoch 1201, loss 0.0146, train acc 99.50%, f1 0.9950, precision 0.9950, recall 0.9951, auc 0.9950
epoch 1301, loss 0.0161, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9953, auc 0.9952
epoch 1401, loss 0.0147, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9956, auc 0.9956
epoch 1501, loss 0.0158, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9959, auc 0.9958
epoch 1601, loss 0.0137, train acc 99.60%, f1 0.9960, precision 0.9959, recall 0.9960, auc 0.9960
epoch 1701, loss 0.0208, train acc 99.60%, f1 0.9960, precision 0.9959, recall 0.9961, auc 0.9960
epoch 1801, loss 0.0104, train acc 99.62%, f1 0.9962, precision 0.9961, recall 0.9962, auc 0.9962
epoch 1901, loss 0.0105, train acc 99.64%, f1 0.9964, precision 0.9963, recall 0.9964, auc 0.9964
epoch 2001, loss 0.0221, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9965, auc 0.9965
epoch 2101, loss 0.0064, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 2201, loss 0.0078, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9968, auc 0.9967
epoch 2301, loss 0.0089, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2401, loss 0.0179, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9970, auc 0.9969
epoch 2501, loss 0.0097, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9971, auc 0.9970
epoch 2601, loss 0.0130, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2701, loss 0.0059, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 2801, loss 0.0104, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9972
epoch 2901, loss 0.0039, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9973
epoch 3001, loss 0.0081, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 3101, loss 0.0063, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 3201, loss 0.0061, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 3301, loss 0.0099, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9974
epoch 3401, loss 0.0061, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9977, auc 0.9976
epoch 3501, loss 0.0060, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 3601, loss 0.0099, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 3701, loss 0.0057, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9978, auc 0.9976
epoch 3801, loss 0.0095, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3901, loss 0.0071, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 4001, loss 0.0093, train acc 99.79%, f1 0.9979, precision 0.9976, recall 0.9981, auc 0.9979
epoch 4101, loss 0.0077, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 4201, loss 0.0026, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9980, auc 0.9979
epoch 4301, loss 0.0073, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4401, loss 0.0068, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 4501, loss 0.0095, train acc 99.80%, f1 0.9980, precision 0.9977, recall 0.9983, auc 0.9980
epoch 4601, loss 0.0077, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9982, auc 0.9980
epoch 4701, loss 0.0034, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 4801, loss 0.0047, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9984, auc 0.9981
epoch 4901, loss 0.0016, train acc 99.81%, f1 0.9981, precision 0.9978, recall 0.9985, auc 0.9981
epoch 5001, loss 0.0019, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9983, auc 0.9981
epoch 5101, loss 0.0053, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9983, auc 0.9981
epoch 5201, loss 0.0041, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 5301, loss 0.0023, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9985, auc 0.9983
epoch 5401, loss 0.0059, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 5501, loss 0.0032, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9985, auc 0.9984
epoch 5601, loss 0.0049, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 5701, loss 0.0021, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9982, auc 0.9984
epoch 5801, loss 0.0090, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 5901, loss 0.0042, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 6001, loss 0.0017, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 6101, loss 0.0080, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 6201, loss 0.0025, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 6301, loss 0.0016, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 6401, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9990, auc 0.9987
epoch 6501, loss 0.0064, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 6601, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6701, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 6801, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9991, auc 0.9990
epoch 6901, loss 0.0035, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 7001, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7101, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 7201, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7301, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7401, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7501, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7601, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 7701, loss 0.0045, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 7801, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 7901, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_3
./test_vehicle0/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9961240310077519

the Fscore is 0.9876543209876543

the precision is 0.975609756097561

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_3
----------------------



epoch 1, loss 0.6931, train acc 50.75%, f1 0.6700, precision 0.5038, recall 1.0000, auc 0.5075
epoch 101, loss 0.3528, train acc 86.92%, f1 0.8691, precision 0.8700, recall 0.8681, auc 0.8692
epoch 201, loss 0.1301, train acc 96.73%, f1 0.9673, precision 0.9675, recall 0.9671, auc 0.9673
epoch 301, loss 0.0746, train acc 98.27%, f1 0.9827, precision 0.9826, recall 0.9828, auc 0.9827
epoch 401, loss 0.0539, train acc 98.65%, f1 0.9865, precision 0.9864, recall 0.9866, auc 0.9865
epoch 501, loss 0.0577, train acc 98.90%, f1 0.9890, precision 0.9889, recall 0.9892, auc 0.9890
epoch 601, loss 0.0372, train acc 99.06%, f1 0.9906, precision 0.9906, recall 0.9906, auc 0.9906
epoch 701, loss 0.0389, train acc 99.15%, f1 0.9915, precision 0.9915, recall 0.9916, auc 0.9915
epoch 801, loss 0.0237, train acc 99.28%, f1 0.9928, precision 0.9927, recall 0.9928, auc 0.9928
epoch 901, loss 0.0236, train acc 99.36%, f1 0.9936, precision 0.9936, recall 0.9937, auc 0.9936
epoch 1001, loss 0.0321, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9944, auc 0.9943
epoch 1101, loss 0.0186, train acc 99.47%, f1 0.9947, precision 0.9946, recall 0.9948, auc 0.9947
epoch 1201, loss 0.0202, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9952, auc 0.9951
epoch 1301, loss 0.0136, train acc 99.53%, f1 0.9953, precision 0.9952, recall 0.9953, auc 0.9953
epoch 1401, loss 0.0155, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9957, auc 0.9956
epoch 1501, loss 0.0182, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9959, auc 0.9958
epoch 1601, loss 0.0123, train acc 99.59%, f1 0.9959, precision 0.9958, recall 0.9960, auc 0.9959
epoch 1701, loss 0.0106, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9961, auc 0.9960
epoch 1801, loss 0.0109, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1901, loss 0.0128, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9965, auc 0.9964
epoch 2001, loss 0.0095, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9966, auc 0.9965
epoch 2101, loss 0.0176, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 2201, loss 0.0095, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2301, loss 0.0061, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9968, auc 0.9968
epoch 2401, loss 0.0048, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2501, loss 0.0128, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2601, loss 0.0044, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9970, auc 0.9971
epoch 2701, loss 0.0059, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2801, loss 0.0075, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 2901, loss 0.0122, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 3001, loss 0.0155, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 3101, loss 0.0024, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9973
epoch 3201, loss 0.0049, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9973
epoch 3301, loss 0.0110, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3401, loss 0.0063, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 3501, loss 0.0024, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 3601, loss 0.0063, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 3701, loss 0.0030, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 3801, loss 0.0036, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 3901, loss 0.0069, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4001, loss 0.0067, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4101, loss 0.0051, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9980, auc 0.9978
epoch 4201, loss 0.0054, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 4301, loss 0.0046, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 4401, loss 0.0019, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9982, auc 0.9979
epoch 4501, loss 0.0078, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 4601, loss 0.0081, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 4701, loss 0.0065, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9981, auc 0.9979
epoch 4801, loss 0.0065, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 4901, loss 0.0033, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_3
./test_vehicle0/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9883720930232558

the Fscore is 0.963855421686747

the precision is 0.9302325581395349

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_3
----------------------



epoch 1, loss 0.6937, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.3704, train acc 86.16%, f1 0.8626, precision 0.8564, recall 0.8689, auc 0.8616
epoch 201, loss 0.1593, train acc 96.64%, f1 0.9664, precision 0.9660, recall 0.9668, auc 0.9664
epoch 301, loss 0.0842, train acc 98.18%, f1 0.9818, precision 0.9820, recall 0.9815, auc 0.9818
epoch 401, loss 0.0592, train acc 98.54%, f1 0.9854, precision 0.9857, recall 0.9851, auc 0.9854
epoch 501, loss 0.0496, train acc 98.80%, f1 0.9880, precision 0.9882, recall 0.9878, auc 0.9880
epoch 601, loss 0.0274, train acc 99.02%, f1 0.9902, precision 0.9904, recall 0.9899, auc 0.9902
epoch 701, loss 0.0225, train acc 99.14%, f1 0.9914, precision 0.9915, recall 0.9913, auc 0.9914
epoch 801, loss 0.0324, train acc 99.24%, f1 0.9924, precision 0.9926, recall 0.9923, auc 0.9924
epoch 901, loss 0.0280, train acc 99.32%, f1 0.9932, precision 0.9934, recall 0.9930, auc 0.9932
epoch 1001, loss 0.0336, train acc 99.39%, f1 0.9939, precision 0.9941, recall 0.9938, auc 0.9939
epoch 1101, loss 0.0162, train acc 99.43%, f1 0.9943, precision 0.9944, recall 0.9942, auc 0.9943
epoch 1201, loss 0.0171, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9947, auc 0.9948
epoch 1301, loss 0.0100, train acc 99.51%, f1 0.9951, precision 0.9952, recall 0.9951, auc 0.9951
epoch 1401, loss 0.0116, train acc 99.54%, f1 0.9954, precision 0.9956, recall 0.9953, auc 0.9954
epoch 1501, loss 0.0064, train acc 99.57%, f1 0.9957, precision 0.9958, recall 0.9956, auc 0.9957
epoch 1601, loss 0.0105, train acc 99.59%, f1 0.9959, precision 0.9960, recall 0.9958, auc 0.9959
epoch 1701, loss 0.0120, train acc 99.60%, f1 0.9960, precision 0.9961, recall 0.9959, auc 0.9960
epoch 1801, loss 0.0067, train acc 99.62%, f1 0.9962, precision 0.9963, recall 0.9962, auc 0.9962
epoch 1901, loss 0.0046, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9963, auc 0.9964
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_3
./test_vehicle0/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9556201550387597

the Fscore is 0.9156626506024096

the precision is 0.8837209302325582

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_3
----------------------



epoch 1, loss 0.6937, train acc 50.25%, f1 0.6689, precision 0.5025, recall 1.0000, auc 0.5000
epoch 101, loss 0.3148, train acc 86.71%, f1 0.8679, precision 0.8668, recall 0.8690, auc 0.8671
epoch 201, loss 0.1503, train acc 96.87%, f1 0.9689, precision 0.9699, recall 0.9678, auc 0.9687
epoch 301, loss 0.0778, train acc 98.19%, f1 0.9820, precision 0.9832, recall 0.9808, auc 0.9819
epoch 401, loss 0.0728, train acc 98.66%, f1 0.9867, precision 0.9873, recall 0.9860, auc 0.9866
epoch 501, loss 0.0441, train acc 98.86%, f1 0.9887, precision 0.9895, recall 0.9878, auc 0.9887
epoch 601, loss 0.0330, train acc 99.05%, f1 0.9905, precision 0.9910, recall 0.9901, auc 0.9905
epoch 701, loss 0.0488, train acc 99.17%, f1 0.9917, precision 0.9918, recall 0.9916, auc 0.9917
epoch 801, loss 0.0295, train acc 99.27%, f1 0.9927, precision 0.9930, recall 0.9924, auc 0.9927
epoch 901, loss 0.0188, train acc 99.37%, f1 0.9937, precision 0.9941, recall 0.9933, auc 0.9937
epoch 1001, loss 0.0203, train acc 99.41%, f1 0.9941, precision 0.9948, recall 0.9935, auc 0.9941
epoch 1101, loss 0.0154, train acc 99.47%, f1 0.9947, precision 0.9956, recall 0.9938, auc 0.9947
epoch 1201, loss 0.0248, train acc 99.50%, f1 0.9950, precision 0.9956, recall 0.9945, auc 0.9950
epoch 1301, loss 0.0153, train acc 99.53%, f1 0.9953, precision 0.9960, recall 0.9946, auc 0.9953
epoch 1401, loss 0.0186, train acc 99.54%, f1 0.9954, precision 0.9955, recall 0.9954, auc 0.9954
epoch 1501, loss 0.0171, train acc 99.58%, f1 0.9958, precision 0.9963, recall 0.9954, auc 0.9958
epoch 1601, loss 0.0196, train acc 99.59%, f1 0.9960, precision 0.9962, recall 0.9957, auc 0.9959
epoch 1701, loss 0.0091, train acc 99.61%, f1 0.9961, precision 0.9963, recall 0.9959, auc 0.9961
epoch 1801, loss 0.0119, train acc 99.63%, f1 0.9963, precision 0.9964, recall 0.9962, auc 0.9963
epoch 1901, loss 0.0123, train acc 99.64%, f1 0.9964, precision 0.9966, recall 0.9962, auc 0.9964
epoch 2001, loss 0.0099, train acc 99.64%, f1 0.9964, precision 0.9970, recall 0.9958, auc 0.9964
epoch 2101, loss 0.0162, train acc 99.65%, f1 0.9965, precision 0.9968, recall 0.9963, auc 0.9965
epoch 2201, loss 0.0080, train acc 99.65%, f1 0.9966, precision 0.9972, recall 0.9959, auc 0.9965
epoch 2301, loss 0.0108, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9966, auc 0.9967
epoch 2401, loss 0.0111, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9967, auc 0.9967
epoch 2501, loss 0.0089, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9967, auc 0.9968
epoch 2601, loss 0.0113, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9968, auc 0.9969
epoch 2701, loss 0.0042, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9969, auc 0.9970
epoch 2801, loss 0.0104, train acc 99.70%, f1 0.9971, precision 0.9973, recall 0.9968, auc 0.9971
epoch 2901, loss 0.0103, train acc 99.70%, f1 0.9970, precision 0.9975, recall 0.9966, auc 0.9970
epoch 3001, loss 0.0150, train acc 99.71%, f1 0.9971, precision 0.9973, recall 0.9969, auc 0.9971
epoch 3101, loss 0.0110, train acc 99.71%, f1 0.9971, precision 0.9975, recall 0.9968, auc 0.9971
epoch 3201, loss 0.0100, train acc 99.72%, f1 0.9972, precision 0.9975, recall 0.9969, auc 0.9972
epoch 3301, loss 0.0069, train acc 99.73%, f1 0.9973, precision 0.9977, recall 0.9969, auc 0.9973
epoch 3401, loss 0.0132, train acc 99.73%, f1 0.9973, precision 0.9976, recall 0.9971, auc 0.9973
epoch 3501, loss 0.0118, train acc 99.74%, f1 0.9974, precision 0.9971, recall 0.9977, auc 0.9974
epoch 3601, loss 0.0122, train acc 99.73%, f1 0.9974, precision 0.9973, recall 0.9974, auc 0.9973
epoch 3701, loss 0.0110, train acc 99.74%, f1 0.9974, precision 0.9977, recall 0.9970, auc 0.9974
epoch 3801, loss 0.0054, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 3901, loss 0.0028, train acc 99.75%, f1 0.9975, precision 0.9977, recall 0.9973, auc 0.9975
epoch 4001, loss 0.0054, train acc 99.76%, f1 0.9976, precision 0.9978, recall 0.9973, auc 0.9976
epoch 4101, loss 0.0114, train acc 99.76%, f1 0.9976, precision 0.9978, recall 0.9974, auc 0.9976
epoch 4201, loss 0.0033, train acc 99.77%, f1 0.9977, precision 0.9980, recall 0.9974, auc 0.9977
epoch 4301, loss 0.0098, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 4401, loss 0.0043, train acc 99.77%, f1 0.9977, precision 0.9984, recall 0.9971, auc 0.9977
epoch 4501, loss 0.0028, train acc 99.77%, f1 0.9977, precision 0.9980, recall 0.9975, auc 0.9977
epoch 4601, loss 0.0029, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 4701, loss 0.0072, train acc 99.78%, f1 0.9978, precision 0.9982, recall 0.9975, auc 0.9978
epoch 4801, loss 0.0067, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9979, auc 0.9979
epoch 4901, loss 0.0096, train acc 99.79%, f1 0.9979, precision 0.9982, recall 0.9977, auc 0.9979
epoch 5001, loss 0.0049, train acc 99.79%, f1 0.9979, precision 0.9983, recall 0.9976, auc 0.9979
epoch 5101, loss 0.0056, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 5201, loss 0.0059, train acc 99.80%, f1 0.9980, precision 0.9984, recall 0.9976, auc 0.9980
epoch 5301, loss 0.0029, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 5401, loss 0.0066, train acc 99.81%, f1 0.9981, precision 0.9984, recall 0.9978, auc 0.9981
epoch 5501, loss 0.0050, train acc 99.81%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9981
epoch 5601, loss 0.0019, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9979, auc 0.9982
epoch 5701, loss 0.0022, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9980, auc 0.9982
epoch 5801, loss 0.0045, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9979, auc 0.9982
epoch 5901, loss 0.0034, train acc 99.83%, f1 0.9983, precision 0.9986, recall 0.9980, auc 0.9983
epoch 6001, loss 0.0025, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6101, loss 0.0050, train acc 99.83%, f1 0.9983, precision 0.9989, recall 0.9976, auc 0.9983
epoch 6201, loss 0.0056, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 6301, loss 0.0005, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9985, auc 0.9983
epoch 6401, loss 0.0101, train acc 99.84%, f1 0.9984, precision 0.9989, recall 0.9979, auc 0.9984
epoch 6501, loss 0.0008, train acc 99.85%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9985
epoch 6601, loss 0.0097, train acc 99.84%, f1 0.9984, precision 0.9991, recall 0.9977, auc 0.9984
epoch 6701, loss 0.0072, train acc 99.85%, f1 0.9985, precision 0.9989, recall 0.9982, auc 0.9985
epoch 6801, loss 0.0052, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 6901, loss 0.0053, train acc 99.86%, f1 0.9986, precision 0.9990, recall 0.9981, auc 0.9986
epoch 7001, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9991, recall 0.9981, auc 0.9986
epoch 7101, loss 0.0017, train acc 99.87%, f1 0.9988, precision 0.9990, recall 0.9985, auc 0.9988
epoch 7201, loss 0.0011, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9984, auc 0.9987
epoch 7301, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 7401, loss 0.0041, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 7501, loss 0.0018, train acc 99.88%, f1 0.9988, precision 0.9991, recall 0.9985, auc 0.9988
epoch 7601, loss 0.0066, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 7701, loss 0.0035, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 7801, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9988, auc 0.9990
epoch 7901, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9988, auc 0.9990
epoch 8001, loss 0.0056, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 8101, loss 0.0010, train acc 99.90%, f1 0.9990, precision 0.9993, recall 0.9987, auc 0.9990
epoch 8201, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8301, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 8401, loss 0.0048, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9988, auc 0.9991
epoch 8501, loss 0.0004, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9990, auc 0.9992
epoch 8601, loss 0.0023, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8701, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8801, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 8901, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9995, recall 0.9991, auc 0.9993
epoch 9001, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9101, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 9201, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 9301, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 9401, loss 0.0032, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 9501, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9601, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 9701, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9801, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9901, loss 0.0024, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 10001, loss 0.0026, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 10101, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 10201, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 10301, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 10401, loss 0.0026, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 10501, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 10601, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10701, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 10801, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 10901, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 11001, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11101, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11201, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11301, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11401, loss 0.0023, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 11501, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11601, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 11701, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11801, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11901, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12001, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12101, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12201, loss 0.0005, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12301, loss 0.0007, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12501, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 12601, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12801, loss 0.0006, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 13001, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_3
./test_vehicle0/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9375

the Fscore is 0.9333333333333333

the precision is 1.0

the recall is 0.875

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_3
----------------------



epoch 1, loss 0.6928, train acc 49.75%, f1 0.6645, precision 0.4975, recall 1.0000, auc 0.5000
epoch 101, loss 0.2973, train acc 87.05%, f1 0.8701, precision 0.8688, recall 0.8714, auc 0.8705
epoch 201, loss 0.1496, train acc 96.76%, f1 0.9674, precision 0.9680, recall 0.9667, auc 0.9676
epoch 301, loss 0.0787, train acc 98.18%, f1 0.9818, precision 0.9811, recall 0.9825, auc 0.9819
epoch 401, loss 0.0780, train acc 98.64%, f1 0.9863, precision 0.9857, recall 0.9870, auc 0.9864
epoch 501, loss 0.0672, train acc 98.84%, f1 0.9884, precision 0.9876, recall 0.9892, auc 0.9884
epoch 601, loss 0.0521, train acc 99.05%, f1 0.9904, precision 0.9902, recall 0.9907, auc 0.9905
epoch 701, loss 0.0279, train acc 99.16%, f1 0.9915, precision 0.9918, recall 0.9913, auc 0.9916
epoch 801, loss 0.0198, train acc 99.27%, f1 0.9926, precision 0.9925, recall 0.9927, auc 0.9927
epoch 901, loss 0.0267, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9933, auc 0.9934
epoch 1001, loss 0.0109, train acc 99.40%, f1 0.9940, precision 0.9936, recall 0.9944, auc 0.9940
epoch 1101, loss 0.0284, train acc 99.47%, f1 0.9947, precision 0.9948, recall 0.9946, auc 0.9947
epoch 1201, loss 0.0109, train acc 99.50%, f1 0.9950, precision 0.9943, recall 0.9957, auc 0.9950
epoch 1301, loss 0.0163, train acc 99.52%, f1 0.9952, precision 0.9951, recall 0.9954, auc 0.9952
epoch 1401, loss 0.0140, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1501, loss 0.0102, train acc 99.57%, f1 0.9956, precision 0.9953, recall 0.9960, auc 0.9957
epoch 1601, loss 0.0103, train acc 99.60%, f1 0.9959, precision 0.9959, recall 0.9960, auc 0.9960
epoch 1701, loss 0.0169, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1801, loss 0.0204, train acc 99.62%, f1 0.9962, precision 0.9955, recall 0.9968, auc 0.9962
epoch 1901, loss 0.0107, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9964, auc 0.9963
epoch 2001, loss 0.0219, train acc 99.65%, f1 0.9965, precision 0.9962, recall 0.9967, auc 0.9965
epoch 2101, loss 0.0109, train acc 99.66%, f1 0.9966, precision 0.9961, recall 0.9970, auc 0.9966
epoch 2201, loss 0.0088, train acc 99.67%, f1 0.9967, precision 0.9970, recall 0.9964, auc 0.9967
epoch 2301, loss 0.0152, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2401, loss 0.0094, train acc 99.67%, f1 0.9967, precision 0.9963, recall 0.9971, auc 0.9967
epoch 2501, loss 0.0090, train acc 99.69%, f1 0.9968, precision 0.9963, recall 0.9973, auc 0.9969
epoch 2601, loss 0.0080, train acc 99.70%, f1 0.9970, precision 0.9968, recall 0.9972, auc 0.9970
epoch 2701, loss 0.0100, train acc 99.71%, f1 0.9971, precision 0.9968, recall 0.9974, auc 0.9971
epoch 2801, loss 0.0069, train acc 99.71%, f1 0.9971, precision 0.9968, recall 0.9974, auc 0.9971
epoch 2901, loss 0.0036, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 3001, loss 0.0064, train acc 99.72%, f1 0.9971, precision 0.9971, recall 0.9972, auc 0.9972
epoch 3101, loss 0.0027, train acc 99.71%, f1 0.9971, precision 0.9968, recall 0.9974, auc 0.9971
epoch 3201, loss 0.0075, train acc 99.73%, f1 0.9972, precision 0.9974, recall 0.9971, auc 0.9973
epoch 3301, loss 0.0057, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 3401, loss 0.0183, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9974
epoch 3501, loss 0.0110, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3601, loss 0.0075, train acc 99.75%, f1 0.9975, precision 0.9972, recall 0.9977, auc 0.9975
epoch 3701, loss 0.0051, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9974
epoch 3801, loss 0.0052, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 3901, loss 0.0084, train acc 99.74%, f1 0.9974, precision 0.9970, recall 0.9977, auc 0.9974
epoch 4001, loss 0.0074, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9976, auc 0.9976
epoch 4101, loss 0.0053, train acc 99.75%, f1 0.9975, precision 0.9978, recall 0.9972, auc 0.9975
epoch 4201, loss 0.0065, train acc 99.76%, f1 0.9976, precision 0.9978, recall 0.9974, auc 0.9976
epoch 4301, loss 0.0105, train acc 99.77%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9977
epoch 4401, loss 0.0080, train acc 99.77%, f1 0.9977, precision 0.9973, recall 0.9980, auc 0.9977
epoch 4501, loss 0.0027, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9976, auc 0.9977
epoch 4601, loss 0.0042, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 4701, loss 0.0066, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9977, auc 0.9978
epoch 4801, loss 0.0056, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 4901, loss 0.0017, train acc 99.80%, f1 0.9980, precision 0.9977, recall 0.9982, auc 0.9980
epoch 5001, loss 0.0061, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9978, auc 0.9980
epoch 5101, loss 0.0028, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 5201, loss 0.0067, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 5301, loss 0.0053, train acc 99.81%, f1 0.9980, precision 0.9985, recall 0.9976, auc 0.9981
epoch 5401, loss 0.0127, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 5501, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 5601, loss 0.0022, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 5701, loss 0.0026, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9980, auc 0.9982
epoch 5801, loss 0.0013, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 5901, loss 0.0033, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 6001, loss 0.0029, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9985, auc 0.9983
epoch 6101, loss 0.0085, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 6201, loss 0.0027, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9982, auc 0.9984
epoch 6301, loss 0.0055, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9981, auc 0.9983
epoch 6401, loss 0.0048, train acc 99.85%, f1 0.9985, precision 0.9988, recall 0.9982, auc 0.9985
epoch 6501, loss 0.0040, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9986, auc 0.9985
epoch 6601, loss 0.0006, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6701, loss 0.0086, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 6801, loss 0.0027, train acc 99.85%, f1 0.9985, precision 0.9988, recall 0.9982, auc 0.9985
epoch 6901, loss 0.0077, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 7001, loss 0.0061, train acc 99.86%, f1 0.9986, precision 0.9989, recall 0.9983, auc 0.9986
epoch 7101, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9986, auc 0.9987
epoch 7201, loss 0.0038, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7301, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 7401, loss 0.0024, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 7501, loss 0.0026, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9990, auc 0.9989
epoch 7601, loss 0.0045, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 7701, loss 0.0058, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 7801, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 7901, loss 0.0033, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9992, auc 0.9990
epoch 8001, loss 0.0038, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 8101, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0061, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8301, loss 0.0032, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8401, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9995, auc 0.9992
epoch 8501, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 8601, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 8701, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8801, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 8901, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 9001, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 9101, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 9201, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 9301, loss 0.0033, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 9401, loss 0.0028, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9501, loss 0.0041, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9601, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9995, auc 0.9996
epoch 9701, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 9801, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 9901, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 10001, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 10101, loss 0.0020, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 10201, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9999, auc 0.9997
epoch 10301, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9996, auc 0.9997
epoch 10401, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 10501, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10601, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10701, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 10801, loss 0.0001, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10901, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 11001, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 11101, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11201, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11301, loss 0.0005, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 11401, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11501, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11601, loss 0.0003, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 11701, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11801, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11901, loss 0.0003, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12001, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12101, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12401, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_3
./test_vehicle0/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9

the Fscore is 0.888888888888889

the precision is 1.0

the recall is 0.8

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_3
----------------------



epoch 1, loss 0.6936, train acc 50.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3471, train acc 86.59%, f1 0.8647, precision 0.8683, recall 0.8611, auc 0.8659
epoch 201, loss 0.1548, train acc 96.73%, f1 0.9671, precision 0.9678, recall 0.9664, auc 0.9673
epoch 301, loss 0.0699, train acc 98.27%, f1 0.9826, precision 0.9820, recall 0.9833, auc 0.9827
epoch 401, loss 0.0581, train acc 98.64%, f1 0.9863, precision 0.9861, recall 0.9865, auc 0.9864
epoch 501, loss 0.0350, train acc 98.89%, f1 0.9888, precision 0.9886, recall 0.9891, auc 0.9889
epoch 601, loss 0.0420, train acc 99.00%, f1 0.9899, precision 0.9895, recall 0.9903, auc 0.9900
epoch 701, loss 0.0276, train acc 99.16%, f1 0.9916, precision 0.9911, recall 0.9920, auc 0.9916
epoch 801, loss 0.0145, train acc 99.24%, f1 0.9924, precision 0.9916, recall 0.9932, auc 0.9924
epoch 901, loss 0.0255, train acc 99.32%, f1 0.9932, precision 0.9928, recall 0.9936, auc 0.9932
epoch 1001, loss 0.0255, train acc 99.40%, f1 0.9940, precision 0.9939, recall 0.9941, auc 0.9940
epoch 1101, loss 0.0305, train acc 99.46%, f1 0.9945, precision 0.9940, recall 0.9951, auc 0.9946
epoch 1201, loss 0.0237, train acc 99.49%, f1 0.9949, precision 0.9948, recall 0.9950, auc 0.9949
epoch 1301, loss 0.0182, train acc 99.53%, f1 0.9953, precision 0.9952, recall 0.9954, auc 0.9953
epoch 1401, loss 0.0155, train acc 99.55%, f1 0.9955, precision 0.9950, recall 0.9960, auc 0.9955
epoch 1501, loss 0.0089, train acc 99.57%, f1 0.9957, precision 0.9955, recall 0.9959, auc 0.9957
epoch 1601, loss 0.0157, train acc 99.59%, f1 0.9959, precision 0.9954, recall 0.9963, auc 0.9959
epoch 1701, loss 0.0124, train acc 99.62%, f1 0.9962, precision 0.9963, recall 0.9961, auc 0.9962
epoch 1801, loss 0.0092, train acc 99.63%, f1 0.9963, precision 0.9959, recall 0.9966, auc 0.9963
epoch 1901, loss 0.0059, train acc 99.65%, f1 0.9965, precision 0.9962, recall 0.9969, auc 0.9965
epoch 2001, loss 0.0159, train acc 99.67%, f1 0.9967, precision 0.9963, recall 0.9972, auc 0.9967
epoch 2101, loss 0.0062, train acc 99.69%, f1 0.9969, precision 0.9966, recall 0.9971, auc 0.9969
epoch 2201, loss 0.0102, train acc 99.68%, f1 0.9968, precision 0.9966, recall 0.9970, auc 0.9968
epoch 2301, loss 0.0114, train acc 99.69%, f1 0.9969, precision 0.9965, recall 0.9972, auc 0.9969
epoch 2401, loss 0.0069, train acc 99.71%, f1 0.9971, precision 0.9969, recall 0.9972, auc 0.9971
epoch 2501, loss 0.0133, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9971, auc 0.9970
epoch 2601, loss 0.0065, train acc 99.72%, f1 0.9972, precision 0.9967, recall 0.9977, auc 0.9972
epoch 2701, loss 0.0032, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9973
epoch 2801, loss 0.0084, train acc 99.73%, f1 0.9972, precision 0.9969, recall 0.9976, auc 0.9973
epoch 2901, loss 0.0057, train acc 99.74%, f1 0.9974, precision 0.9972, recall 0.9976, auc 0.9974
epoch 3001, loss 0.0070, train acc 99.74%, f1 0.9974, precision 0.9971, recall 0.9977, auc 0.9974
epoch 3101, loss 0.0048, train acc 99.75%, f1 0.9975, precision 0.9972, recall 0.9977, auc 0.9975
epoch 3201, loss 0.0043, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 3301, loss 0.0144, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 3401, loss 0.0074, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9977, auc 0.9976
epoch 3501, loss 0.0111, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9978, auc 0.9976
epoch 3601, loss 0.0057, train acc 99.78%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9978
epoch 3701, loss 0.0041, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 3801, loss 0.0061, train acc 99.77%, f1 0.9977, precision 0.9974, recall 0.9980, auc 0.9977
epoch 3901, loss 0.0084, train acc 99.77%, f1 0.9977, precision 0.9972, recall 0.9981, auc 0.9977
epoch 4001, loss 0.0074, train acc 99.78%, f1 0.9978, precision 0.9975, recall 0.9980, auc 0.9978
epoch 4101, loss 0.0041, train acc 99.77%, f1 0.9977, precision 0.9973, recall 0.9981, auc 0.9977
epoch 4201, loss 0.0052, train acc 99.79%, f1 0.9979, precision 0.9974, recall 0.9983, auc 0.9979
epoch 4301, loss 0.0049, train acc 99.78%, f1 0.9978, precision 0.9975, recall 0.9982, auc 0.9978
epoch 4401, loss 0.0023, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9982, auc 0.9979
epoch 4501, loss 0.0049, train acc 99.79%, f1 0.9979, precision 0.9974, recall 0.9984, auc 0.9979
epoch 4601, loss 0.0034, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9981, auc 0.9979
epoch 4701, loss 0.0037, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9979, auc 0.9981
epoch 4801, loss 0.0065, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9982, auc 0.9980
epoch 4901, loss 0.0035, train acc 99.82%, f1 0.9982, precision 0.9979, recall 0.9985, auc 0.9982
epoch 5001, loss 0.0048, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 5101, loss 0.0050, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9978, auc 0.9982
epoch 5201, loss 0.0042, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 5301, loss 0.0015, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9985, auc 0.9983
epoch 5401, loss 0.0045, train acc 99.83%, f1 0.9983, precision 0.9978, recall 0.9987, auc 0.9983
epoch 5501, loss 0.0071, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 5601, loss 0.0045, train acc 99.84%, f1 0.9984, precision 0.9979, recall 0.9989, auc 0.9984
epoch 5701, loss 0.0095, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 5801, loss 0.0024, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 5901, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6001, loss 0.0057, train acc 99.86%, f1 0.9986, precision 0.9981, recall 0.9990, auc 0.9986
epoch 6101, loss 0.0025, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9989, auc 0.9986
epoch 6201, loss 0.0045, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9985, auc 0.9987
epoch 6301, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 6401, loss 0.0064, train acc 99.87%, f1 0.9987, precision 0.9983, recall 0.9991, auc 0.9988
epoch 6501, loss 0.0042, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 6601, loss 0.0026, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 6701, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 6801, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9986, recall 0.9993, auc 0.9990
epoch 6901, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7001, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7101, loss 0.0042, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 7201, loss 0.0021, train acc 99.91%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9991
epoch 7301, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 7401, loss 0.0031, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 7501, loss 0.0039, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9994, auc 0.9992
epoch 7601, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7701, loss 0.0060, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 7801, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7901, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9994, auc 0.9992
epoch 8001, loss 0.0030, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9990, auc 0.9992
epoch 8101, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9995, auc 0.9993
epoch 8301, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9996, auc 0.9994
epoch 8401, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8501, loss 0.0005, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8601, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 8701, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 8801, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 8901, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9992, recall 0.9997, auc 0.9995
epoch 9001, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 9101, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9991, recall 0.9998, auc 0.9994
epoch 9201, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9992, recall 0.9998, auc 0.9995
epoch 9301, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9992, recall 0.9998, auc 0.9995
epoch 9401, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 9501, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9993, recall 0.9998, auc 0.9996
epoch 9601, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9998, auc 0.9996
epoch 9701, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 9801, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 9901, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_3
./test_vehicle0/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9961240310077519

the Fscore is 0.9876543209876543

the precision is 0.975609756097561

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_3
----------------------



epoch 1, loss 0.6931, train acc 50.23%, f1 0.6687, precision 0.5023, recall 1.0000, auc 0.5000
epoch 101, loss 0.3254, train acc 86.78%, f1 0.8687, precision 0.8670, recall 0.8703, auc 0.8678
epoch 201, loss 0.1290, train acc 96.77%, f1 0.9678, precision 0.9675, recall 0.9682, auc 0.9677
epoch 301, loss 0.0832, train acc 98.21%, f1 0.9822, precision 0.9816, recall 0.9828, auc 0.9821
epoch 401, loss 0.0670, train acc 98.63%, f1 0.9864, precision 0.9857, recall 0.9871, auc 0.9863
epoch 501, loss 0.0638, train acc 98.88%, f1 0.9888, precision 0.9885, recall 0.9892, auc 0.9888
epoch 601, loss 0.0377, train acc 99.05%, f1 0.9906, precision 0.9902, recall 0.9909, auc 0.9905
epoch 701, loss 0.0466, train acc 99.14%, f1 0.9914, precision 0.9908, recall 0.9920, auc 0.9914
epoch 801, loss 0.0201, train acc 99.27%, f1 0.9927, precision 0.9921, recall 0.9934, auc 0.9927
epoch 901, loss 0.0213, train acc 99.34%, f1 0.9935, precision 0.9932, recall 0.9937, auc 0.9934
epoch 1001, loss 0.0195, train acc 99.40%, f1 0.9940, precision 0.9936, recall 0.9944, auc 0.9940
epoch 1101, loss 0.0116, train acc 99.43%, f1 0.9943, precision 0.9940, recall 0.9946, auc 0.9943
epoch 1201, loss 0.0169, train acc 99.48%, f1 0.9948, precision 0.9945, recall 0.9951, auc 0.9948
epoch 1301, loss 0.0130, train acc 99.52%, f1 0.9952, precision 0.9953, recall 0.9951, auc 0.9952
epoch 1401, loss 0.0089, train acc 99.55%, f1 0.9955, precision 0.9952, recall 0.9958, auc 0.9955
epoch 1501, loss 0.0265, train acc 99.57%, f1 0.9957, precision 0.9956, recall 0.9959, auc 0.9957
epoch 1601, loss 0.0121, train acc 99.58%, f1 0.9959, precision 0.9957, recall 0.9960, auc 0.9958
epoch 1701, loss 0.0133, train acc 99.60%, f1 0.9961, precision 0.9956, recall 0.9965, auc 0.9960
epoch 1801, loss 0.0098, train acc 99.62%, f1 0.9962, precision 0.9961, recall 0.9963, auc 0.9962
epoch 1901, loss 0.0090, train acc 99.64%, f1 0.9964, precision 0.9959, recall 0.9970, auc 0.9964
epoch 2001, loss 0.0189, train acc 99.65%, f1 0.9966, precision 0.9968, recall 0.9963, auc 0.9965
epoch 2101, loss 0.0129, train acc 99.66%, f1 0.9966, precision 0.9959, recall 0.9973, auc 0.9966
epoch 2201, loss 0.0132, train acc 99.67%, f1 0.9967, precision 0.9964, recall 0.9970, auc 0.9967
epoch 2301, loss 0.0099, train acc 99.68%, f1 0.9968, precision 0.9964, recall 0.9973, auc 0.9968
epoch 2401, loss 0.0097, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2501, loss 0.0164, train acc 99.70%, f1 0.9971, precision 0.9967, recall 0.9974, auc 0.9970
epoch 2601, loss 0.0070, train acc 99.70%, f1 0.9970, precision 0.9968, recall 0.9971, auc 0.9970
epoch 2701, loss 0.0142, train acc 99.71%, f1 0.9971, precision 0.9966, recall 0.9976, auc 0.9971
epoch 2801, loss 0.0087, train acc 99.72%, f1 0.9972, precision 0.9969, recall 0.9975, auc 0.9972
epoch 2901, loss 0.0071, train acc 99.73%, f1 0.9974, precision 0.9969, recall 0.9978, auc 0.9973
epoch 3001, loss 0.0130, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9974, auc 0.9973
epoch 3101, loss 0.0054, train acc 99.74%, f1 0.9974, precision 0.9969, recall 0.9979, auc 0.9974
epoch 3201, loss 0.0112, train acc 99.74%, f1 0.9974, precision 0.9971, recall 0.9977, auc 0.9974
epoch 3301, loss 0.0048, train acc 99.75%, f1 0.9975, precision 0.9972, recall 0.9978, auc 0.9975
epoch 3401, loss 0.0112, train acc 99.76%, f1 0.9976, precision 0.9971, recall 0.9980, auc 0.9976
epoch 3501, loss 0.0039, train acc 99.76%, f1 0.9976, precision 0.9973, recall 0.9978, auc 0.9976
epoch 3601, loss 0.0122, train acc 99.76%, f1 0.9976, precision 0.9970, recall 0.9982, auc 0.9976
epoch 3701, loss 0.0095, train acc 99.77%, f1 0.9977, precision 0.9973, recall 0.9981, auc 0.9977
epoch 3801, loss 0.0079, train acc 99.78%, f1 0.9978, precision 0.9975, recall 0.9980, auc 0.9978
epoch 3901, loss 0.0042, train acc 99.78%, f1 0.9978, precision 0.9975, recall 0.9982, auc 0.9978
epoch 4001, loss 0.0089, train acc 99.79%, f1 0.9979, precision 0.9976, recall 0.9981, auc 0.9978
epoch 4101, loss 0.0067, train acc 99.79%, f1 0.9979, precision 0.9976, recall 0.9983, auc 0.9979
epoch 4201, loss 0.0029, train acc 99.79%, f1 0.9979, precision 0.9974, recall 0.9983, auc 0.9978
epoch 4301, loss 0.0034, train acc 99.79%, f1 0.9979, precision 0.9973, recall 0.9985, auc 0.9979
epoch 4401, loss 0.0054, train acc 99.80%, f1 0.9980, precision 0.9976, recall 0.9985, auc 0.9980
epoch 4501, loss 0.0046, train acc 99.82%, f1 0.9982, precision 0.9979, recall 0.9984, auc 0.9982
epoch 4601, loss 0.0024, train acc 99.82%, f1 0.9982, precision 0.9979, recall 0.9985, auc 0.9982
epoch 4701, loss 0.0051, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 4801, loss 0.0027, train acc 99.83%, f1 0.9984, precision 0.9982, recall 0.9985, auc 0.9983
epoch 4901, loss 0.0018, train acc 99.83%, f1 0.9983, precision 0.9979, recall 0.9986, auc 0.9983
epoch 5001, loss 0.0086, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9986, auc 0.9983
epoch 5101, loss 0.0028, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9987, auc 0.9983
epoch 5201, loss 0.0060, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 5301, loss 0.0038, train acc 99.84%, f1 0.9984, precision 0.9981, recall 0.9988, auc 0.9984
epoch 5401, loss 0.0028, train acc 99.83%, f1 0.9984, precision 0.9979, recall 0.9988, auc 0.9983
epoch 5501, loss 0.0029, train acc 99.85%, f1 0.9985, precision 0.9989, recall 0.9982, auc 0.9985
epoch 5601, loss 0.0058, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5701, loss 0.0039, train acc 99.85%, f1 0.9985, precision 0.9981, recall 0.9989, auc 0.9985
epoch 5801, loss 0.0005, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9985, auc 0.9987
epoch 5901, loss 0.0070, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 6001, loss 0.0022, train acc 99.87%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9987
epoch 6101, loss 0.0054, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9990, auc 0.9986
epoch 6201, loss 0.0034, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 6301, loss 0.0009, train acc 99.88%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9988
epoch 6401, loss 0.0029, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6501, loss 0.0014, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6601, loss 0.0033, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6701, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9985, recall 0.9991, auc 0.9988
epoch 6801, loss 0.0069, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 6901, loss 0.0056, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 7001, loss 0.0025, train acc 99.90%, f1 0.9990, precision 0.9987, recall 0.9992, auc 0.9990
epoch 7101, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9992, auc 0.9991
epoch 7201, loss 0.0007, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 7301, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 7401, loss 0.0035, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 7501, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 7601, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7701, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 7801, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 7901, loss 0.0018, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_3
./test_vehicle0/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9961240310077519

the Fscore is 0.9876543209876543

the precision is 0.975609756097561

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_3
----------------------



epoch 1, loss 0.6926, train acc 50.05%, f1 0.6671, precision 0.5005, recall 1.0000, auc 0.5000
epoch 101, loss 0.3511, train acc 86.77%, f1 0.8676, precision 0.8692, recall 0.8660, auc 0.8677
epoch 201, loss 0.1596, train acc 96.62%, f1 0.9662, precision 0.9649, recall 0.9676, auc 0.9662
epoch 301, loss 0.0870, train acc 98.25%, f1 0.9825, precision 0.9821, recall 0.9829, auc 0.9825
epoch 401, loss 0.0510, train acc 98.58%, f1 0.9858, precision 0.9862, recall 0.9854, auc 0.9858
epoch 501, loss 0.0279, train acc 98.80%, f1 0.9880, precision 0.9877, recall 0.9884, auc 0.9880
epoch 601, loss 0.0423, train acc 98.99%, f1 0.9899, precision 0.9899, recall 0.9899, auc 0.9899
epoch 701, loss 0.0146, train acc 99.11%, f1 0.9912, precision 0.9908, recall 0.9916, auc 0.9911
epoch 801, loss 0.0270, train acc 99.23%, f1 0.9923, precision 0.9922, recall 0.9925, auc 0.9923
epoch 901, loss 0.0262, train acc 99.32%, f1 0.9932, precision 0.9930, recall 0.9935, auc 0.9932
epoch 1001, loss 0.0248, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9940, auc 0.9939
epoch 1101, loss 0.0294, train acc 99.45%, f1 0.9945, precision 0.9945, recall 0.9944, auc 0.9945
epoch 1201, loss 0.0157, train acc 99.49%, f1 0.9949, precision 0.9947, recall 0.9951, auc 0.9949
epoch 1301, loss 0.0102, train acc 99.51%, f1 0.9951, precision 0.9952, recall 0.9950, auc 0.9951
epoch 1401, loss 0.0173, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1501, loss 0.0149, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1601, loss 0.0169, train acc 99.58%, f1 0.9958, precision 0.9954, recall 0.9962, auc 0.9958
epoch 1701, loss 0.0116, train acc 99.60%, f1 0.9960, precision 0.9962, recall 0.9958, auc 0.9960
epoch 1801, loss 0.0104, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1901, loss 0.0065, train acc 99.62%, f1 0.9962, precision 0.9963, recall 0.9961, auc 0.9962
epoch 2001, loss 0.0060, train acc 99.64%, f1 0.9964, precision 0.9960, recall 0.9968, auc 0.9964
epoch 2101, loss 0.0086, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9967, auc 0.9966
epoch 2201, loss 0.0148, train acc 99.66%, f1 0.9966, precision 0.9968, recall 0.9964, auc 0.9966
epoch 2301, loss 0.0146, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2401, loss 0.0110, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9966, auc 0.9967
epoch 2501, loss 0.0078, train acc 99.67%, f1 0.9967, precision 0.9965, recall 0.9970, auc 0.9967
epoch 2601, loss 0.0111, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 2701, loss 0.0105, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9970, auc 0.9969
epoch 2801, loss 0.0062, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 2901, loss 0.0042, train acc 99.70%, f1 0.9970, precision 0.9968, recall 0.9972, auc 0.9970
epoch 3001, loss 0.0124, train acc 99.72%, f1 0.9972, precision 0.9975, recall 0.9969, auc 0.9972
epoch 3101, loss 0.0094, train acc 99.73%, f1 0.9973, precision 0.9975, recall 0.9971, auc 0.9973
epoch 3201, loss 0.0073, train acc 99.74%, f1 0.9974, precision 0.9977, recall 0.9970, auc 0.9974
epoch 3301, loss 0.0079, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 3401, loss 0.0070, train acc 99.74%, f1 0.9974, precision 0.9977, recall 0.9970, auc 0.9974
epoch 3501, loss 0.0040, train acc 99.74%, f1 0.9974, precision 0.9976, recall 0.9972, auc 0.9974
epoch 3601, loss 0.0099, train acc 99.74%, f1 0.9974, precision 0.9971, recall 0.9978, auc 0.9974
epoch 3701, loss 0.0076, train acc 99.76%, f1 0.9976, precision 0.9978, recall 0.9973, auc 0.9976
epoch 3801, loss 0.0054, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 3901, loss 0.0035, train acc 99.75%, f1 0.9975, precision 0.9977, recall 0.9974, auc 0.9975
epoch 4001, loss 0.0029, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4101, loss 0.0041, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9976, auc 0.9977
epoch 4201, loss 0.0046, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 4301, loss 0.0101, train acc 99.77%, f1 0.9977, precision 0.9974, recall 0.9981, auc 0.9977
epoch 4401, loss 0.0062, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9980, auc 0.9978
epoch 4501, loss 0.0046, train acc 99.77%, f1 0.9977, precision 0.9981, recall 0.9973, auc 0.9977
epoch 4601, loss 0.0065, train acc 99.79%, f1 0.9979, precision 0.9982, recall 0.9977, auc 0.9979
epoch 4701, loss 0.0035, train acc 99.79%, f1 0.9979, precision 0.9982, recall 0.9976, auc 0.9979
epoch 4801, loss 0.0052, train acc 99.80%, f1 0.9980, precision 0.9977, recall 0.9983, auc 0.9980
epoch 4901, loss 0.0059, train acc 99.80%, f1 0.9980, precision 0.9982, recall 0.9978, auc 0.9980
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_3
./test_vehicle0/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9922480620155039

the Fscore is 0.975609756097561

the precision is 0.9523809523809523

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_3
----------------------



epoch 1, loss 0.6932, train acc 50.21%, f1 0.6685, precision 0.5021, recall 1.0000, auc 0.5000
epoch 101, loss 0.3358, train acc 86.84%, f1 0.8690, precision 0.8684, recall 0.8695, auc 0.8684
epoch 201, loss 0.1689, train acc 96.69%, f1 0.9670, precision 0.9671, recall 0.9670, auc 0.9669
epoch 301, loss 0.0999, train acc 98.20%, f1 0.9821, precision 0.9820, recall 0.9821, auc 0.9820
epoch 401, loss 0.0391, train acc 98.61%, f1 0.9862, precision 0.9857, recall 0.9867, auc 0.9861
epoch 501, loss 0.0447, train acc 98.88%, f1 0.9888, precision 0.9887, recall 0.9889, auc 0.9888
epoch 601, loss 0.0465, train acc 99.05%, f1 0.9905, precision 0.9903, recall 0.9907, auc 0.9905
epoch 701, loss 0.0281, train acc 99.15%, f1 0.9916, precision 0.9912, recall 0.9920, auc 0.9915
epoch 801, loss 0.0284, train acc 99.26%, f1 0.9927, precision 0.9921, recall 0.9933, auc 0.9926
epoch 901, loss 0.0177, train acc 99.34%, f1 0.9934, precision 0.9933, recall 0.9935, auc 0.9934
epoch 1001, loss 0.0289, train acc 99.41%, f1 0.9941, precision 0.9936, recall 0.9947, auc 0.9941
epoch 1101, loss 0.0191, train acc 99.45%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9945
epoch 1201, loss 0.0135, train acc 99.49%, f1 0.9949, precision 0.9950, recall 0.9949, auc 0.9949
epoch 1301, loss 0.0314, train acc 99.54%, f1 0.9954, precision 0.9950, recall 0.9958, auc 0.9954
epoch 1401, loss 0.0146, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1501, loss 0.0186, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9958, auc 0.9957
epoch 1601, loss 0.0281, train acc 99.59%, f1 0.9959, precision 0.9957, recall 0.9961, auc 0.9959
epoch 1701, loss 0.0116, train acc 99.61%, f1 0.9961, precision 0.9957, recall 0.9966, auc 0.9961
epoch 1801, loss 0.0065, train acc 99.62%, f1 0.9962, precision 0.9956, recall 0.9968, auc 0.9962
epoch 1901, loss 0.0109, train acc 99.63%, f1 0.9963, precision 0.9969, recall 0.9958, auc 0.9963
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_3
./test_vehicle0/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9603682170542637

the Fscore is 0.9069767441860466

the precision is 0.8478260869565217

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_3
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.3795, train acc 82.83%, f1 0.8283, precision 0.8284, recall 0.8283, auc 0.8283
epoch 201, loss 0.2049, train acc 94.35%, f1 0.9435, precision 0.9435, recall 0.9435, auc 0.9435
epoch 301, loss 0.1418, train acc 97.37%, f1 0.9737, precision 0.9737, recall 0.9737, auc 0.9737
epoch 401, loss 0.0980, train acc 98.15%, f1 0.9815, precision 0.9815, recall 0.9815, auc 0.9815
epoch 501, loss 0.0584, train acc 98.54%, f1 0.9854, precision 0.9854, recall 0.9854, auc 0.9854
epoch 601, loss 0.0567, train acc 98.72%, f1 0.9872, precision 0.9872, recall 0.9872, auc 0.9872
epoch 701, loss 0.0463, train acc 98.87%, f1 0.9887, precision 0.9887, recall 0.9887, auc 0.9887
epoch 801, loss 0.0468, train acc 99.03%, f1 0.9903, precision 0.9903, recall 0.9903, auc 0.9903
epoch 901, loss 0.0253, train acc 99.10%, f1 0.9910, precision 0.9910, recall 0.9910, auc 0.9910
epoch 1001, loss 0.0360, train acc 99.19%, f1 0.9919, precision 0.9919, recall 0.9919, auc 0.9919
epoch 1101, loss 0.0209, train acc 99.26%, f1 0.9926, precision 0.9926, recall 0.9926, auc 0.9926
epoch 1201, loss 0.0210, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9933, auc 0.9933
epoch 1301, loss 0.0239, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9939, auc 0.9939
epoch 1401, loss 0.0127, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9943, auc 0.9943
epoch 1501, loss 0.0162, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9946
epoch 1601, loss 0.0284, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 1701, loss 0.0159, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9951, auc 0.9951
epoch 1801, loss 0.0284, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1901, loss 0.0089, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 2001, loss 0.0165, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 2101, loss 0.0181, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 2201, loss 0.0121, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 2301, loss 0.0156, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2401, loss 0.0137, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2501, loss 0.0090, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 2601, loss 0.0156, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2701, loss 0.0136, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2801, loss 0.0070, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2901, loss 0.0065, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 3001, loss 0.0170, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 3101, loss 0.0033, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 3201, loss 0.0067, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3301, loss 0.0048, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 3401, loss 0.0126, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3501, loss 0.0051, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3601, loss 0.0090, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 3701, loss 0.0084, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3801, loss 0.0130, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 3901, loss 0.0097, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 4001, loss 0.0073, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 4101, loss 0.0042, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 4201, loss 0.0102, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4301, loss 0.0069, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4401, loss 0.0068, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4501, loss 0.0048, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4601, loss 0.0073, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4701, loss 0.0091, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4801, loss 0.0024, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 4901, loss 0.0067, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 5001, loss 0.0077, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 5101, loss 0.0114, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 5201, loss 0.0028, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5301, loss 0.0089, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 5401, loss 0.0050, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5501, loss 0.0061, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5601, loss 0.0090, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5701, loss 0.0050, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 5801, loss 0.0041, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 5901, loss 0.0063, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 6001, loss 0.0082, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 6101, loss 0.0025, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6201, loss 0.0064, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6301, loss 0.0030, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6401, loss 0.0036, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 6501, loss 0.0065, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6601, loss 0.0006, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6701, loss 0.0020, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6801, loss 0.0036, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 6901, loss 0.0033, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7001, loss 0.0045, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7101, loss 0.0019, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7201, loss 0.0050, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7301, loss 0.0055, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7401, loss 0.0029, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7501, loss 0.0027, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7601, loss 0.0067, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7701, loss 0.0044, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7801, loss 0.0055, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7901, loss 0.0033, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 8001, loss 0.0019, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 8101, loss 0.0072, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 8201, loss 0.0031, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 8301, loss 0.0049, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 8401, loss 0.0019, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 8501, loss 0.0016, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 8601, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 8701, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8801, loss 0.0020, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8901, loss 0.0020, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 9001, loss 0.0039, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 9101, loss 0.0048, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 9201, loss 0.0053, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 9301, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 9401, loss 0.0073, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 9501, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9601, loss 0.0065, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9701, loss 0.0026, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9801, loss 0.0030, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 9901, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 10001, loss 0.0015, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 10101, loss 0.0080, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 10201, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 10301, loss 0.0043, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 10401, loss 0.0012, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 10501, loss 0.0010, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 10601, loss 0.0046, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10701, loss 0.0058, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10801, loss 0.0050, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 10901, loss 0.0034, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11001, loss 0.0006, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 11101, loss 0.0064, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11201, loss 0.0007, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11301, loss 0.0016, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11401, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11501, loss 0.0035, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11601, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11701, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11801, loss 0.0043, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11901, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 12001, loss 0.0026, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12101, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12201, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 12301, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 12401, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 12501, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 12601, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 12701, loss 0.0031, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 12801, loss 0.0031, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 12901, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 13001, loss 0.0037, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 13101, loss 0.0007, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 13201, loss 0.0016, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 13301, loss 0.0046, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 13401, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 13501, loss 0.0044, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 13601, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 13701, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 13801, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 13901, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 14001, loss 0.0003, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 14101, loss 0.0020, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 14201, loss 0.0049, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 14301, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 14401, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 14501, loss 0.0043, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14601, loss 0.0034, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14701, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14801, loss 0.0027, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14901, loss 0.0036, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 15001, loss 0.0036, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 15101, loss 0.0005, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 15201, loss 0.0052, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 15301, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 15401, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 15501, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 15601, loss 0.0001, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 15701, loss 0.0003, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 15801, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 15901, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 16001, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 16101, loss 0.0033, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 16201, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 16301, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 16401, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 16501, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0019, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 16701, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 16801, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 16901, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 17001, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 17101, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 17201, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 17301, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 17401, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 17501, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 17601, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 17701, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 17801, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 17901, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 18001, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 18101, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 18201, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 18301, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 18401, loss 0.0028, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 18501, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 18601, loss 0.0003, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 18701, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 18801, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 18901, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 19001, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 19101, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 19201, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 19301, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 19401, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 19501, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 19601, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 19701, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 19801, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 19901, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_3
./test_vehicle0/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9961240310077519

the Fscore is 0.9876543209876543

the precision is 0.975609756097561

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_3
----------------------



epoch 1, loss 0.6927, train acc 67.39%, f1 0.5193, precision 0.9877, recall 0.3522, auc 0.6739
epoch 101, loss 0.4109, train acc 82.97%, f1 0.8299, precision 0.8289, recall 0.8310, auc 0.8297
epoch 201, loss 0.2256, train acc 94.39%, f1 0.9439, precision 0.9438, recall 0.9440, auc 0.9439
epoch 301, loss 0.1224, train acc 97.43%, f1 0.9743, precision 0.9743, recall 0.9743, auc 0.9743
epoch 401, loss 0.0870, train acc 98.20%, f1 0.9820, precision 0.9820, recall 0.9820, auc 0.9820
epoch 501, loss 0.0591, train acc 98.50%, f1 0.9850, precision 0.9851, recall 0.9850, auc 0.9850
epoch 601, loss 0.0387, train acc 98.69%, f1 0.9869, precision 0.9870, recall 0.9869, auc 0.9869
epoch 701, loss 0.0440, train acc 98.90%, f1 0.9890, precision 0.9891, recall 0.9890, auc 0.9890
epoch 801, loss 0.0258, train acc 99.01%, f1 0.9901, precision 0.9902, recall 0.9901, auc 0.9901
epoch 901, loss 0.0366, train acc 99.11%, f1 0.9911, precision 0.9911, recall 0.9910, auc 0.9911
epoch 1001, loss 0.0208, train acc 99.19%, f1 0.9919, precision 0.9919, recall 0.9919, auc 0.9919
epoch 1101, loss 0.0158, train acc 99.26%, f1 0.9926, precision 0.9926, recall 0.9925, auc 0.9926
epoch 1201, loss 0.0154, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9933, auc 0.9933
epoch 1301, loss 0.0146, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9938, auc 0.9939
epoch 1401, loss 0.0239, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9943, auc 0.9943
epoch 1501, loss 0.0143, train acc 99.46%, f1 0.9946, precision 0.9947, recall 0.9946, auc 0.9946
epoch 1601, loss 0.0120, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 1701, loss 0.0078, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1801, loss 0.0162, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1901, loss 0.0176, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 2001, loss 0.0202, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 2101, loss 0.0101, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9959, auc 0.9960
epoch 2201, loss 0.0130, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2301, loss 0.0093, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 2401, loss 0.0079, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9964, auc 0.9964
epoch 2501, loss 0.0070, train acc 99.66%, f1 0.9966, precision 0.9967, recall 0.9966, auc 0.9966
epoch 2601, loss 0.0102, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9967, auc 0.9967
epoch 2701, loss 0.0109, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2801, loss 0.0064, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9969, auc 0.9970
epoch 2901, loss 0.0160, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3001, loss 0.0051, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 3101, loss 0.0107, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 3201, loss 0.0100, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 3301, loss 0.0082, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 3401, loss 0.0070, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 3501, loss 0.0100, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 3601, loss 0.0078, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 3701, loss 0.0059, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3801, loss 0.0061, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3901, loss 0.0106, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4001, loss 0.0077, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4101, loss 0.0102, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4201, loss 0.0058, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4301, loss 0.0086, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4401, loss 0.0098, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4501, loss 0.0113, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 4601, loss 0.0104, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 4701, loss 0.0060, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4801, loss 0.0061, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4901, loss 0.0023, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 5001, loss 0.0117, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 5101, loss 0.0084, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 5201, loss 0.0077, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5301, loss 0.0079, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5401, loss 0.0025, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5501, loss 0.0085, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5601, loss 0.0029, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 5701, loss 0.0056, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5801, loss 0.0046, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5901, loss 0.0031, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 6001, loss 0.0040, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 6101, loss 0.0018, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6201, loss 0.0048, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6301, loss 0.0030, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6401, loss 0.0028, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6501, loss 0.0032, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6601, loss 0.0039, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6701, loss 0.0054, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6801, loss 0.0033, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6901, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7001, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7101, loss 0.0047, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7201, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7301, loss 0.0051, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7401, loss 0.0025, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7501, loss 0.0026, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7601, loss 0.0072, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7701, loss 0.0005, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7801, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7901, loss 0.0045, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8001, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8101, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8201, loss 0.0008, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0010, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8401, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8501, loss 0.0030, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 8601, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 8701, loss 0.0029, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8801, loss 0.0005, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 8901, loss 0.0004, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9001, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9101, loss 0.0035, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9201, loss 0.0031, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9301, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9401, loss 0.0047, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9501, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9601, loss 0.0007, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9701, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9801, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9901, loss 0.0023, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10001, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10101, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10201, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10301, loss 0.0012, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10401, loss 0.0037, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10501, loss 0.0008, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10601, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10701, loss 0.0004, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10801, loss 0.0023, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10901, loss 0.0003, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11001, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11101, loss 0.0045, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11201, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11301, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11401, loss 0.0008, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11501, loss 0.0006, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11601, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11701, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11801, loss 0.0032, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11901, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12001, loss 0.0005, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12101, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12201, loss 0.0009, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12301, loss 0.0016, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12401, loss 0.0030, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12501, loss 0.0003, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12601, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12701, loss 0.0027, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12801, loss 0.0029, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12901, loss 0.0033, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 13001, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13101, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13201, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13301, loss 0.0009, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 13401, loss 0.0046, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 13501, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13601, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13701, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13801, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13901, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14001, loss 0.0024, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14101, loss 0.0023, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14201, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14301, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14401, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14501, loss 0.0034, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14601, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14701, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14801, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14901, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_3
./test_vehicle0/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 1.0

the Fscore is 1.0

the precision is 1.0

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_3
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.4000, train acc 82.99%, f1 0.8307, precision 0.8269, recall 0.8346, auc 0.8299
epoch 201, loss 0.2398, train acc 94.34%, f1 0.9435, precision 0.9429, recall 0.9440, auc 0.9434
epoch 301, loss 0.1381, train acc 97.56%, f1 0.9756, precision 0.9756, recall 0.9756, auc 0.9756
epoch 401, loss 0.0728, train acc 98.24%, f1 0.9824, precision 0.9826, recall 0.9822, auc 0.9824
epoch 501, loss 0.0544, train acc 98.63%, f1 0.9863, precision 0.9864, recall 0.9862, auc 0.9863
epoch 601, loss 0.0695, train acc 98.82%, f1 0.9882, precision 0.9883, recall 0.9881, auc 0.9882
epoch 701, loss 0.0447, train acc 98.94%, f1 0.9894, precision 0.9896, recall 0.9893, auc 0.9894
epoch 801, loss 0.0418, train acc 99.04%, f1 0.9904, precision 0.9906, recall 0.9903, auc 0.9904
epoch 901, loss 0.0314, train acc 99.14%, f1 0.9914, precision 0.9915, recall 0.9913, auc 0.9914
epoch 1001, loss 0.0381, train acc 99.22%, f1 0.9922, precision 0.9923, recall 0.9921, auc 0.9922
epoch 1101, loss 0.0225, train acc 99.29%, f1 0.9929, precision 0.9929, recall 0.9928, auc 0.9929
epoch 1201, loss 0.0204, train acc 99.34%, f1 0.9934, precision 0.9935, recall 0.9932, auc 0.9934
epoch 1301, loss 0.0269, train acc 99.41%, f1 0.9941, precision 0.9942, recall 0.9940, auc 0.9941
epoch 1401, loss 0.0206, train acc 99.45%, f1 0.9945, precision 0.9945, recall 0.9944, auc 0.9945
epoch 1501, loss 0.0136, train acc 99.47%, f1 0.9947, precision 0.9948, recall 0.9946, auc 0.9947
epoch 1601, loss 0.0210, train acc 99.51%, f1 0.9951, precision 0.9952, recall 0.9949, auc 0.9951
epoch 1701, loss 0.0125, train acc 99.53%, f1 0.9953, precision 0.9954, recall 0.9952, auc 0.9953
epoch 1801, loss 0.0138, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9954, auc 0.9955
epoch 1901, loss 0.0117, train acc 99.56%, f1 0.9956, precision 0.9957, recall 0.9956, auc 0.9956
epoch 2001, loss 0.0176, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9957, auc 0.9958
epoch 2101, loss 0.0126, train acc 99.60%, f1 0.9960, precision 0.9961, recall 0.9960, auc 0.9960
epoch 2201, loss 0.0141, train acc 99.60%, f1 0.9960, precision 0.9961, recall 0.9960, auc 0.9960
epoch 2301, loss 0.0232, train acc 99.62%, f1 0.9962, precision 0.9963, recall 0.9962, auc 0.9962
epoch 2401, loss 0.0181, train acc 99.63%, f1 0.9963, precision 0.9964, recall 0.9963, auc 0.9963
epoch 2501, loss 0.0060, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9964, auc 0.9964
epoch 2601, loss 0.0074, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9965, auc 0.9966
epoch 2701, loss 0.0113, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9966, auc 0.9967
epoch 2801, loss 0.0061, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9967, auc 0.9968
epoch 2901, loss 0.0171, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9968, auc 0.9969
epoch 3001, loss 0.0078, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9968, auc 0.9969
epoch 3101, loss 0.0099, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3201, loss 0.0034, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9969, auc 0.9969
epoch 3301, loss 0.0112, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9969, auc 0.9969
epoch 3401, loss 0.0164, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9970, auc 0.9971
epoch 3501, loss 0.0086, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 3601, loss 0.0117, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3701, loss 0.0044, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 3801, loss 0.0046, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 3901, loss 0.0141, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4001, loss 0.0065, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4101, loss 0.0070, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 4201, loss 0.0068, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 4301, loss 0.0044, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4401, loss 0.0067, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 4501, loss 0.0097, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 4601, loss 0.0056, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 4701, loss 0.0019, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 4801, loss 0.0071, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4901, loss 0.0098, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 5001, loss 0.0029, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 5101, loss 0.0044, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 5201, loss 0.0105, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9976, auc 0.9977
epoch 5301, loss 0.0059, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9977, auc 0.9977
epoch 5401, loss 0.0039, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5501, loss 0.0047, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5601, loss 0.0085, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5701, loss 0.0032, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5801, loss 0.0035, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 5901, loss 0.0024, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6001, loss 0.0070, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 6101, loss 0.0066, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6201, loss 0.0049, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6301, loss 0.0005, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6401, loss 0.0023, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6501, loss 0.0047, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6601, loss 0.0040, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6701, loss 0.0049, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6801, loss 0.0007, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6901, loss 0.0090, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7001, loss 0.0036, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7101, loss 0.0059, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7201, loss 0.0028, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7301, loss 0.0060, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7401, loss 0.0032, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7501, loss 0.0051, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7601, loss 0.0063, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7701, loss 0.0034, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7801, loss 0.0042, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7901, loss 0.0035, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 8001, loss 0.0060, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 8101, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 8201, loss 0.0080, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 8401, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 8501, loss 0.0031, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 8601, loss 0.0035, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 8701, loss 0.0050, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8801, loss 0.0072, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8901, loss 0.0048, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 9001, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 9101, loss 0.0047, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 9201, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 9301, loss 0.0004, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9401, loss 0.0009, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9501, loss 0.0026, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9601, loss 0.0014, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9701, loss 0.0009, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 9801, loss 0.0025, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9901, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_3
./test_vehicle0/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9961240310077519

the Fscore is 0.9876543209876543

the precision is 0.975609756097561

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_3
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3994, train acc 82.54%, f1 0.8247, precision 0.8281, recall 0.8212, auc 0.8254
epoch 201, loss 0.2504, train acc 94.53%, f1 0.9453, precision 0.9461, recall 0.9445, auc 0.9453
epoch 301, loss 0.1222, train acc 97.31%, f1 0.9731, precision 0.9731, recall 0.9731, auc 0.9731
epoch 401, loss 0.1044, train acc 98.02%, f1 0.9802, precision 0.9801, recall 0.9802, auc 0.9802
epoch 501, loss 0.0573, train acc 98.56%, f1 0.9856, precision 0.9854, recall 0.9857, auc 0.9856
epoch 601, loss 0.0520, train acc 98.77%, f1 0.9877, precision 0.9876, recall 0.9879, auc 0.9877
epoch 701, loss 0.0431, train acc 98.93%, f1 0.9893, precision 0.9892, recall 0.9894, auc 0.9893
epoch 801, loss 0.0351, train acc 99.03%, f1 0.9903, precision 0.9902, recall 0.9904, auc 0.9903
epoch 901, loss 0.0326, train acc 99.13%, f1 0.9913, precision 0.9911, recall 0.9914, auc 0.9913
epoch 1001, loss 0.0193, train acc 99.23%, f1 0.9923, precision 0.9922, recall 0.9924, auc 0.9923
epoch 1101, loss 0.0302, train acc 99.28%, f1 0.9928, precision 0.9927, recall 0.9928, auc 0.9928
epoch 1201, loss 0.0197, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9934, auc 0.9933
epoch 1301, loss 0.0254, train acc 99.39%, f1 0.9939, precision 0.9938, recall 0.9940, auc 0.9939
epoch 1401, loss 0.0208, train acc 99.44%, f1 0.9944, precision 0.9943, recall 0.9945, auc 0.9944
epoch 1501, loss 0.0204, train acc 99.47%, f1 0.9947, precision 0.9946, recall 0.9947, auc 0.9947
epoch 1601, loss 0.0160, train acc 99.50%, f1 0.9950, precision 0.9949, recall 0.9951, auc 0.9950
epoch 1701, loss 0.0194, train acc 99.52%, f1 0.9952, precision 0.9951, recall 0.9953, auc 0.9952
epoch 1801, loss 0.0161, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9955, auc 0.9954
epoch 1901, loss 0.0175, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9957, auc 0.9956
epoch 2001, loss 0.0161, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9958, auc 0.9958
epoch 2101, loss 0.0135, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 2201, loss 0.0133, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9962, auc 0.9961
epoch 2301, loss 0.0130, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9963, auc 0.9962
epoch 2401, loss 0.0124, train acc 99.64%, f1 0.9964, precision 0.9963, recall 0.9964, auc 0.9964
epoch 2501, loss 0.0091, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9965, auc 0.9965
epoch 2601, loss 0.0066, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9966, auc 0.9966
epoch 2701, loss 0.0089, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2801, loss 0.0041, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2901, loss 0.0106, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 3001, loss 0.0042, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 3101, loss 0.0065, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 3201, loss 0.0111, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3301, loss 0.0071, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 3401, loss 0.0103, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3501, loss 0.0057, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9972, auc 0.9971
epoch 3601, loss 0.0054, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9973
epoch 3701, loss 0.0027, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9972
epoch 3801, loss 0.0126, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9973
epoch 3901, loss 0.0051, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4001, loss 0.0052, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4101, loss 0.0056, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4201, loss 0.0118, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4301, loss 0.0087, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4401, loss 0.0116, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4501, loss 0.0031, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 4601, loss 0.0041, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4701, loss 0.0095, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 4801, loss 0.0028, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 4901, loss 0.0095, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 5001, loss 0.0045, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 5101, loss 0.0102, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 5201, loss 0.0112, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5301, loss 0.0078, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5401, loss 0.0071, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 5501, loss 0.0032, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 5601, loss 0.0047, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 5701, loss 0.0042, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 5801, loss 0.0115, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5901, loss 0.0036, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6001, loss 0.0077, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6101, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6201, loss 0.0048, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 6301, loss 0.0047, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 6401, loss 0.0058, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 6501, loss 0.0023, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 6601, loss 0.0062, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6701, loss 0.0048, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6801, loss 0.0057, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6901, loss 0.0006, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7001, loss 0.0024, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7101, loss 0.0044, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7201, loss 0.0056, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 7301, loss 0.0011, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7401, loss 0.0056, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7501, loss 0.0005, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7601, loss 0.0025, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7701, loss 0.0019, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7801, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7901, loss 0.0077, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_3
./test_vehicle0/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9922480620155039

the Fscore is 0.975609756097561

the precision is 0.9523809523809523

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_3
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4151, train acc 83.04%, f1 0.8304, precision 0.8303, recall 0.8304, auc 0.8304
epoch 201, loss 0.2551, train acc 94.23%, f1 0.9423, precision 0.9423, recall 0.9423, auc 0.9423
epoch 301, loss 0.1459, train acc 97.59%, f1 0.9759, precision 0.9759, recall 0.9759, auc 0.9759
epoch 401, loss 0.0989, train acc 98.29%, f1 0.9829, precision 0.9829, recall 0.9829, auc 0.9829
epoch 501, loss 0.0564, train acc 98.47%, f1 0.9847, precision 0.9847, recall 0.9847, auc 0.9847
epoch 601, loss 0.0537, train acc 98.78%, f1 0.9878, precision 0.9878, recall 0.9878, auc 0.9878
epoch 701, loss 0.0283, train acc 98.93%, f1 0.9893, precision 0.9893, recall 0.9893, auc 0.9893
epoch 801, loss 0.0355, train acc 99.05%, f1 0.9905, precision 0.9905, recall 0.9905, auc 0.9905
epoch 901, loss 0.0402, train acc 99.13%, f1 0.9913, precision 0.9913, recall 0.9913, auc 0.9913
epoch 1001, loss 0.0297, train acc 99.22%, f1 0.9922, precision 0.9922, recall 0.9922, auc 0.9922
epoch 1101, loss 0.0259, train acc 99.28%, f1 0.9928, precision 0.9928, recall 0.9928, auc 0.9928
epoch 1201, loss 0.0176, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9933, auc 0.9933
epoch 1301, loss 0.0182, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9939, auc 0.9939
epoch 1401, loss 0.0136, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9943, auc 0.9943
epoch 1501, loss 0.0100, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9946
epoch 1601, loss 0.0099, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9951, auc 0.9951
epoch 1701, loss 0.0235, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1801, loss 0.0240, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1901, loss 0.0113, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 2001, loss 0.0186, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 2101, loss 0.0127, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 2201, loss 0.0274, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 2301, loss 0.0149, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2401, loss 0.0128, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 2501, loss 0.0109, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2601, loss 0.0059, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2701, loss 0.0074, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2801, loss 0.0069, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2901, loss 0.0100, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9968, auc 0.9968
epoch 3001, loss 0.0088, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 3101, loss 0.0128, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3201, loss 0.0116, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 3301, loss 0.0131, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 3401, loss 0.0126, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3501, loss 0.0093, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 3601, loss 0.0151, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 3701, loss 0.0113, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 3801, loss 0.0073, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 3901, loss 0.0100, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 4001, loss 0.0054, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 4101, loss 0.0031, train acc 99.74%, f1 0.9974, precision 0.9975, recall 0.9974, auc 0.9974
epoch 4201, loss 0.0043, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4301, loss 0.0127, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4401, loss 0.0054, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4501, loss 0.0021, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4601, loss 0.0092, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4701, loss 0.0119, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4801, loss 0.0034, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4901, loss 0.0055, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_3
./test_vehicle0/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9844961240310077

the Fscore is 0.9523809523809523

the precision is 0.9090909090909091

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_3
----------------------



epoch 1, loss 0.6928, train acc 54.09%, f1 0.1513, precision 0.9981, recall 0.0819, auc 0.5409
epoch 101, loss 0.4000, train acc 82.87%, f1 0.8287, precision 0.8285, recall 0.8289, auc 0.8287
epoch 201, loss 0.2110, train acc 94.64%, f1 0.9464, precision 0.9463, recall 0.9465, auc 0.9464
epoch 301, loss 0.1246, train acc 97.45%, f1 0.9745, precision 0.9745, recall 0.9746, auc 0.9745
epoch 401, loss 0.1036, train acc 98.24%, f1 0.9824, precision 0.9824, recall 0.9824, auc 0.9824
epoch 501, loss 0.0650, train acc 98.62%, f1 0.9862, precision 0.9863, recall 0.9862, auc 0.9862
epoch 601, loss 0.0613, train acc 98.76%, f1 0.9876, precision 0.9877, recall 0.9876, auc 0.9876
epoch 701, loss 0.0348, train acc 98.90%, f1 0.9890, precision 0.9891, recall 0.9890, auc 0.9890
epoch 801, loss 0.0377, train acc 99.01%, f1 0.9901, precision 0.9901, recall 0.9901, auc 0.9901
epoch 901, loss 0.0379, train acc 99.10%, f1 0.9910, precision 0.9910, recall 0.9909, auc 0.9910
epoch 1001, loss 0.0251, train acc 99.20%, f1 0.9919, precision 0.9920, recall 0.9919, auc 0.9920
epoch 1101, loss 0.0302, train acc 99.28%, f1 0.9928, precision 0.9928, recall 0.9927, auc 0.9928
epoch 1201, loss 0.0299, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9933, auc 0.9933
epoch 1301, loss 0.0207, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9938, auc 0.9939
epoch 1401, loss 0.0222, train acc 99.42%, f1 0.9942, precision 0.9942, recall 0.9942, auc 0.9942
epoch 1501, loss 0.0174, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9946
epoch 1601, loss 0.0158, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 1701, loss 0.0121, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9951, auc 0.9951
epoch 1801, loss 0.0097, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9953, auc 0.9953
epoch 1901, loss 0.0252, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_3
./test_vehicle0/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9681201550387598

the Fscore is 0.9285714285714285

the precision is 0.8863636363636364

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_3
----------------------



epoch 1, loss 0.6932, train acc 49.68%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3988, train acc 82.89%, f1 0.8298, precision 0.8310, recall 0.8285, auc 0.8289
epoch 201, loss 0.2001, train acc 94.24%, f1 0.9428, precision 0.9427, recall 0.9429, auc 0.9424
epoch 301, loss 0.1209, train acc 97.60%, f1 0.9761, precision 0.9769, recall 0.9754, auc 0.9760
epoch 401, loss 0.0781, train acc 98.18%, f1 0.9819, precision 0.9822, recall 0.9817, auc 0.9818
epoch 501, loss 0.0475, train acc 98.53%, f1 0.9854, precision 0.9855, recall 0.9852, auc 0.9853
epoch 601, loss 0.0444, train acc 98.77%, f1 0.9878, precision 0.9879, recall 0.9876, auc 0.9877
epoch 701, loss 0.0390, train acc 98.92%, f1 0.9893, precision 0.9891, recall 0.9895, auc 0.9892
epoch 801, loss 0.0562, train acc 99.03%, f1 0.9904, precision 0.9903, recall 0.9905, auc 0.9903
epoch 901, loss 0.0359, train acc 99.12%, f1 0.9912, precision 0.9912, recall 0.9912, auc 0.9912
epoch 1001, loss 0.0348, train acc 99.21%, f1 0.9922, precision 0.9920, recall 0.9924, auc 0.9921
epoch 1101, loss 0.0284, train acc 99.26%, f1 0.9926, precision 0.9924, recall 0.9928, auc 0.9926
epoch 1201, loss 0.0257, train acc 99.34%, f1 0.9934, precision 0.9931, recall 0.9938, auc 0.9934
epoch 1301, loss 0.0235, train acc 99.39%, f1 0.9939, precision 0.9935, recall 0.9943, auc 0.9939
epoch 1401, loss 0.0157, train acc 99.44%, f1 0.9945, precision 0.9943, recall 0.9946, auc 0.9944
epoch 1501, loss 0.0274, train acc 99.47%, f1 0.9948, precision 0.9944, recall 0.9952, auc 0.9947
epoch 1601, loss 0.0215, train acc 99.49%, f1 0.9950, precision 0.9948, recall 0.9952, auc 0.9949
epoch 1701, loss 0.0065, train acc 99.52%, f1 0.9952, precision 0.9948, recall 0.9957, auc 0.9952
epoch 1801, loss 0.0063, train acc 99.55%, f1 0.9955, precision 0.9952, recall 0.9958, auc 0.9955
epoch 1901, loss 0.0250, train acc 99.56%, f1 0.9956, precision 0.9953, recall 0.9959, auc 0.9956
epoch 2001, loss 0.0180, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9959, auc 0.9958
epoch 2101, loss 0.0145, train acc 99.60%, f1 0.9960, precision 0.9956, recall 0.9965, auc 0.9960
epoch 2201, loss 0.0105, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9964, auc 0.9962
epoch 2301, loss 0.0094, train acc 99.62%, f1 0.9963, precision 0.9959, recall 0.9966, auc 0.9962
epoch 2401, loss 0.0123, train acc 99.64%, f1 0.9964, precision 0.9960, recall 0.9968, auc 0.9964
epoch 2501, loss 0.0116, train acc 99.65%, f1 0.9965, precision 0.9961, recall 0.9969, auc 0.9965
epoch 2601, loss 0.0090, train acc 99.66%, f1 0.9967, precision 0.9965, recall 0.9968, auc 0.9966
epoch 2701, loss 0.0099, train acc 99.67%, f1 0.9968, precision 0.9965, recall 0.9970, auc 0.9967
epoch 2801, loss 0.0166, train acc 99.68%, f1 0.9968, precision 0.9967, recall 0.9969, auc 0.9968
epoch 2901, loss 0.0148, train acc 99.69%, f1 0.9969, precision 0.9967, recall 0.9970, auc 0.9969
epoch 3001, loss 0.0083, train acc 99.69%, f1 0.9969, precision 0.9967, recall 0.9971, auc 0.9969
epoch 3101, loss 0.0143, train acc 99.70%, f1 0.9970, precision 0.9968, recall 0.9972, auc 0.9970
epoch 3201, loss 0.0064, train acc 99.70%, f1 0.9970, precision 0.9967, recall 0.9974, auc 0.9970
epoch 3301, loss 0.0133, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9971, auc 0.9970
epoch 3401, loss 0.0082, train acc 99.71%, f1 0.9972, precision 0.9970, recall 0.9973, auc 0.9971
epoch 3501, loss 0.0094, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 3601, loss 0.0076, train acc 99.71%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9971
epoch 3701, loss 0.0116, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9974, auc 0.9972
epoch 3801, loss 0.0115, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9975, auc 0.9973
epoch 3901, loss 0.0078, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9975, auc 0.9973
epoch 4001, loss 0.0054, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 4101, loss 0.0070, train acc 99.74%, f1 0.9974, precision 0.9972, recall 0.9977, auc 0.9974
epoch 4201, loss 0.0039, train acc 99.75%, f1 0.9975, precision 0.9973, recall 0.9976, auc 0.9974
epoch 4301, loss 0.0078, train acc 99.75%, f1 0.9975, precision 0.9973, recall 0.9977, auc 0.9975
epoch 4401, loss 0.0117, train acc 99.75%, f1 0.9976, precision 0.9974, recall 0.9977, auc 0.9975
epoch 4501, loss 0.0130, train acc 99.75%, f1 0.9976, precision 0.9974, recall 0.9978, auc 0.9975
epoch 4601, loss 0.0101, train acc 99.76%, f1 0.9976, precision 0.9974, recall 0.9978, auc 0.9976
epoch 4701, loss 0.0048, train acc 99.76%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9976
epoch 4801, loss 0.0118, train acc 99.76%, f1 0.9977, precision 0.9974, recall 0.9979, auc 0.9976
epoch 4901, loss 0.0117, train acc 99.77%, f1 0.9977, precision 0.9974, recall 0.9980, auc 0.9977
epoch 5001, loss 0.0062, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 5101, loss 0.0172, train acc 99.77%, f1 0.9978, precision 0.9975, recall 0.9980, auc 0.9977
epoch 5201, loss 0.0074, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9980, auc 0.9978
epoch 5301, loss 0.0100, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 5401, loss 0.0041, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9981, auc 0.9979
epoch 5501, loss 0.0086, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 5601, loss 0.0056, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9981, auc 0.9979
epoch 5701, loss 0.0075, train acc 99.78%, f1 0.9979, precision 0.9976, recall 0.9981, auc 0.9978
epoch 5801, loss 0.0061, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 5901, loss 0.0043, train acc 99.78%, f1 0.9979, precision 0.9976, recall 0.9981, auc 0.9978
epoch 6001, loss 0.0032, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6101, loss 0.0020, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 6201, loss 0.0026, train acc 99.80%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9980
epoch 6301, loss 0.0070, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 6401, loss 0.0049, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 6501, loss 0.0071, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 6601, loss 0.0104, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 6701, loss 0.0056, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9985, auc 0.9982
epoch 6801, loss 0.0042, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 6901, loss 0.0044, train acc 99.83%, f1 0.9984, precision 0.9981, recall 0.9986, auc 0.9983
epoch 7001, loss 0.0076, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9985, auc 0.9984
epoch 7101, loss 0.0031, train acc 99.83%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9983
epoch 7201, loss 0.0108, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9985, auc 0.9984
epoch 7301, loss 0.0024, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 7401, loss 0.0058, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9986, auc 0.9984
epoch 7501, loss 0.0027, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 7601, loss 0.0071, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 7701, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 7801, loss 0.0041, train acc 99.85%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9985
epoch 7901, loss 0.0075, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9987, auc 0.9985
epoch 8001, loss 0.0019, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 8101, loss 0.0023, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 8201, loss 0.0014, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 8301, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 8401, loss 0.0042, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9988, auc 0.9986
epoch 8501, loss 0.0015, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9988, auc 0.9987
epoch 8601, loss 0.0021, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 8701, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 8801, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8901, loss 0.0051, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 9001, loss 0.0055, train acc 99.87%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9987
epoch 9101, loss 0.0060, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 9201, loss 0.0038, train acc 99.87%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9987
epoch 9301, loss 0.0018, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 9401, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 9501, loss 0.0005, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 9601, loss 0.0052, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 9701, loss 0.0039, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9989, auc 0.9987
epoch 9801, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 9901, loss 0.0026, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9990, auc 0.9989
epoch 10001, loss 0.0034, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9990, auc 0.9988
epoch 10101, loss 0.0008, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 10201, loss 0.0013, train acc 99.89%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9989
epoch 10301, loss 0.0020, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 10401, loss 0.0012, train acc 99.89%, f1 0.9990, precision 0.9988, recall 0.9991, auc 0.9989
epoch 10501, loss 0.0015, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9990, auc 0.9988
epoch 10601, loss 0.0045, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9990, auc 0.9989
epoch 10701, loss 0.0063, train acc 99.89%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9989
epoch 10801, loss 0.0042, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9991, auc 0.9989
epoch 10901, loss 0.0013, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 11001, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11101, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9991, auc 0.9990
epoch 11201, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 11301, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 11401, loss 0.0039, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9991, auc 0.9990
epoch 11501, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9991, auc 0.9990
epoch 11601, loss 0.0035, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9992, auc 0.9991
epoch 11701, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11801, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 11901, loss 0.0009, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9990
epoch 12001, loss 0.0037, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 12101, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 12201, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9990
epoch 12301, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9993, auc 0.9991
epoch 12401, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 12501, loss 0.0011, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9992, auc 0.9991
epoch 12601, loss 0.0034, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 12701, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 12801, loss 0.0025, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 12901, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 13001, loss 0.0053, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 13101, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 13201, loss 0.0007, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 13301, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9991
epoch 13401, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 13501, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9994, auc 0.9992
epoch 13601, loss 0.0045, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9993, auc 0.9991
epoch 13701, loss 0.0006, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9993, auc 0.9991
epoch 13801, loss 0.0036, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 13901, loss 0.0044, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 14001, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 14101, loss 0.0016, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9991
epoch 14201, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 14301, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 14401, loss 0.0003, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 14501, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9991
epoch 14601, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9992
epoch 14701, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 14801, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9994, auc 0.9992
epoch 14901, loss 0.0007, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 15001, loss 0.0003, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 15101, loss 0.0026, train acc 99.92%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9992
epoch 15201, loss 0.0005, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 15301, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 15401, loss 0.0023, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 15501, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9995, auc 0.9993
epoch 15601, loss 0.0039, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 15701, loss 0.0019, train acc 99.92%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9992
epoch 15801, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 15901, loss 0.0003, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 16001, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 16101, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 16201, loss 0.0016, train acc 99.92%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9992
epoch 16301, loss 0.0014, train acc 99.92%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9992
epoch 16401, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 16601, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9995, auc 0.9993
epoch 16701, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 16801, loss 0.0024, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9995, auc 0.9993
epoch 16901, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 17001, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 17101, loss 0.0029, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 17201, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 17301, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 17401, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 17501, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9996, auc 0.9994
epoch 17601, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9996, auc 0.9994
epoch 17701, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 17801, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9996, auc 0.9994
epoch 17901, loss 0.0043, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 18001, loss 0.0019, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 18101, loss 0.0019, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 18201, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9996, auc 0.9994
epoch 18301, loss 0.0028, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 18401, loss 0.0032, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 18501, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 18601, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 18701, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 18801, loss 0.0064, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 18901, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 19001, loss 0.0027, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 19101, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 19201, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 19301, loss 0.0035, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 19401, loss 0.0035, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 19501, loss 0.0034, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 19601, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 19701, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 19801, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 19901, loss 0.0023, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_3
./test_vehicle0/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 1.0

the Fscore is 1.0

the precision is 1.0

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_3
----------------------



epoch 1, loss 0.6932, train acc 49.81%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4586, train acc 82.90%, f1 0.8259, precision 0.8442, recall 0.8085, auc 0.8290
epoch 201, loss 0.2469, train acc 94.46%, f1 0.9448, precision 0.9459, recall 0.9437, auc 0.9447
epoch 301, loss 0.1494, train acc 97.51%, f1 0.9752, precision 0.9749, recall 0.9755, auc 0.9751
epoch 401, loss 0.0983, train acc 98.21%, f1 0.9822, precision 0.9822, recall 0.9822, auc 0.9821
epoch 501, loss 0.0772, train acc 98.55%, f1 0.9855, precision 0.9852, recall 0.9858, auc 0.9855
epoch 601, loss 0.0380, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9881, auc 0.9880
epoch 701, loss 0.0339, train acc 98.91%, f1 0.9891, precision 0.9887, recall 0.9895, auc 0.9891
epoch 801, loss 0.0287, train acc 99.05%, f1 0.9906, precision 0.9902, recall 0.9909, auc 0.9905
epoch 901, loss 0.0368, train acc 99.14%, f1 0.9914, precision 0.9910, recall 0.9918, auc 0.9914
epoch 1001, loss 0.0300, train acc 99.19%, f1 0.9919, precision 0.9917, recall 0.9921, auc 0.9919
epoch 1101, loss 0.0344, train acc 99.27%, f1 0.9927, precision 0.9922, recall 0.9933, auc 0.9927
epoch 1201, loss 0.0155, train acc 99.33%, f1 0.9934, precision 0.9930, recall 0.9937, auc 0.9933
epoch 1301, loss 0.0305, train acc 99.37%, f1 0.9937, precision 0.9931, recall 0.9943, auc 0.9937
epoch 1401, loss 0.0261, train acc 99.43%, f1 0.9943, precision 0.9937, recall 0.9950, auc 0.9943
epoch 1501, loss 0.0216, train acc 99.45%, f1 0.9945, precision 0.9942, recall 0.9949, auc 0.9945
epoch 1601, loss 0.0203, train acc 99.48%, f1 0.9948, precision 0.9944, recall 0.9953, auc 0.9948
epoch 1701, loss 0.0301, train acc 99.51%, f1 0.9951, precision 0.9948, recall 0.9954, auc 0.9951
epoch 1801, loss 0.0123, train acc 99.53%, f1 0.9953, precision 0.9947, recall 0.9959, auc 0.9953
epoch 1901, loss 0.0094, train acc 99.56%, f1 0.9956, precision 0.9952, recall 0.9959, auc 0.9956
epoch 2001, loss 0.0178, train acc 99.57%, f1 0.9957, precision 0.9954, recall 0.9960, auc 0.9957
epoch 2101, loss 0.0122, train acc 99.58%, f1 0.9958, precision 0.9956, recall 0.9960, auc 0.9958
epoch 2201, loss 0.0044, train acc 99.60%, f1 0.9960, precision 0.9958, recall 0.9962, auc 0.9960
epoch 2301, loss 0.0155, train acc 99.61%, f1 0.9962, precision 0.9961, recall 0.9963, auc 0.9961
epoch 2401, loss 0.0178, train acc 99.62%, f1 0.9963, precision 0.9960, recall 0.9965, auc 0.9962
epoch 2501, loss 0.0207, train acc 99.63%, f1 0.9963, precision 0.9961, recall 0.9965, auc 0.9963
epoch 2601, loss 0.0079, train acc 99.66%, f1 0.9966, precision 0.9964, recall 0.9967, auc 0.9966
epoch 2701, loss 0.0091, train acc 99.66%, f1 0.9966, precision 0.9964, recall 0.9968, auc 0.9966
epoch 2801, loss 0.0103, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9969, auc 0.9968
epoch 2901, loss 0.0130, train acc 99.68%, f1 0.9968, precision 0.9966, recall 0.9970, auc 0.9968
epoch 3001, loss 0.0163, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 3101, loss 0.0060, train acc 99.69%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9969
epoch 3201, loss 0.0138, train acc 99.70%, f1 0.9970, precision 0.9968, recall 0.9972, auc 0.9970
epoch 3301, loss 0.0034, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9971, auc 0.9969
epoch 3401, loss 0.0018, train acc 99.70%, f1 0.9970, precision 0.9968, recall 0.9972, auc 0.9970
epoch 3501, loss 0.0155, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 3601, loss 0.0049, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9972, auc 0.9971
epoch 3701, loss 0.0089, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 3801, loss 0.0134, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 3901, loss 0.0052, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 4001, loss 0.0123, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4101, loss 0.0082, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 4201, loss 0.0134, train acc 99.73%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9973
epoch 4301, loss 0.0040, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9976, auc 0.9974
epoch 4401, loss 0.0061, train acc 99.74%, f1 0.9974, precision 0.9975, recall 0.9974, auc 0.9974
epoch 4501, loss 0.0064, train acc 99.75%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9975
epoch 4601, loss 0.0027, train acc 99.76%, f1 0.9976, precision 0.9974, recall 0.9978, auc 0.9976
epoch 4701, loss 0.0045, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9976
epoch 4801, loss 0.0066, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9977, auc 0.9976
epoch 4901, loss 0.0049, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 5001, loss 0.0118, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 5101, loss 0.0044, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9976
epoch 5201, loss 0.0033, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 5301, loss 0.0114, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9980, auc 0.9978
epoch 5401, loss 0.0090, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9979, auc 0.9978
epoch 5501, loss 0.0056, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 5601, loss 0.0064, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 5701, loss 0.0069, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9980, auc 0.9979
epoch 5801, loss 0.0085, train acc 99.79%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9979
epoch 5901, loss 0.0057, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9982, auc 0.9980
epoch 6001, loss 0.0076, train acc 99.80%, f1 0.9981, precision 0.9979, recall 0.9982, auc 0.9980
epoch 6101, loss 0.0069, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 6201, loss 0.0051, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6301, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 6401, loss 0.0012, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 6501, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 6601, loss 0.0009, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6701, loss 0.0016, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 6801, loss 0.0027, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 6901, loss 0.0040, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 7001, loss 0.0024, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 7101, loss 0.0068, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 7201, loss 0.0076, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 7301, loss 0.0074, train acc 99.84%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9984
epoch 7401, loss 0.0113, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 7501, loss 0.0015, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 7601, loss 0.0023, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 7701, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7801, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7901, loss 0.0045, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 8001, loss 0.0043, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 8101, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 8301, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 8401, loss 0.0036, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8501, loss 0.0037, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 8601, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 8701, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 8801, loss 0.0012, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8901, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 9001, loss 0.0069, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 9101, loss 0.0017, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 9201, loss 0.0012, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9301, loss 0.0026, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9401, loss 0.0049, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 9501, loss 0.0014, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9601, loss 0.0070, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9987, auc 0.9989
epoch 9701, loss 0.0023, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 9801, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9988, auc 0.9989
epoch 9901, loss 0.0027, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 10001, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 10101, loss 0.0021, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 10201, loss 0.0036, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 10301, loss 0.0024, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 10401, loss 0.0006, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 10501, loss 0.0026, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 10601, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 10701, loss 0.0024, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9988, auc 0.9990
epoch 10801, loss 0.0056, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10901, loss 0.0045, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 11001, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 11101, loss 0.0026, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 11201, loss 0.0013, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 11301, loss 0.0016, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 11401, loss 0.0020, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 11501, loss 0.0010, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 11601, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 11701, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 11801, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 11901, loss 0.0009, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 12001, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 12101, loss 0.0005, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 12201, loss 0.0059, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 12301, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12401, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 12501, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 12601, loss 0.0025, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 12701, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 12801, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 12901, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 13001, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 13101, loss 0.0008, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 13201, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 13301, loss 0.0028, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 13401, loss 0.0007, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 13501, loss 0.0027, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 13601, loss 0.0007, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 13701, loss 0.0012, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 13801, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 13901, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 14001, loss 0.0046, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 14101, loss 0.0012, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 14201, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 14301, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 14401, loss 0.0011, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 14501, loss 0.0006, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9992
epoch 14601, loss 0.0007, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 14701, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 14801, loss 0.0003, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 14901, loss 0.0016, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_3
./test_vehicle0/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9922480620155039

the Fscore is 0.975609756097561

the precision is 0.9523809523809523

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_3
----------------------



epoch 1, loss 0.6935, train acc 49.77%, f1 0.6646, precision 0.4977, recall 1.0000, auc 0.5000
epoch 101, loss 0.4009, train acc 83.28%, f1 0.8311, precision 0.8355, recall 0.8267, auc 0.8327
epoch 201, loss 0.2503, train acc 94.30%, f1 0.9427, precision 0.9432, recall 0.9422, auc 0.9430
epoch 301, loss 0.1362, train acc 97.37%, f1 0.9736, precision 0.9735, recall 0.9737, auc 0.9737
epoch 401, loss 0.0743, train acc 98.19%, f1 0.9818, precision 0.9816, recall 0.9820, auc 0.9819
epoch 501, loss 0.0995, train acc 98.56%, f1 0.9856, precision 0.9849, recall 0.9862, auc 0.9856
epoch 601, loss 0.0464, train acc 98.74%, f1 0.9873, precision 0.9869, recall 0.9878, auc 0.9874
epoch 701, loss 0.0287, train acc 98.88%, f1 0.9888, precision 0.9886, recall 0.9889, auc 0.9888
epoch 801, loss 0.0465, train acc 99.02%, f1 0.9901, precision 0.9900, recall 0.9902, auc 0.9902
epoch 901, loss 0.0414, train acc 99.09%, f1 0.9908, precision 0.9907, recall 0.9909, auc 0.9909
epoch 1001, loss 0.0357, train acc 99.21%, f1 0.9920, precision 0.9922, recall 0.9919, auc 0.9921
epoch 1101, loss 0.0210, train acc 99.28%, f1 0.9928, precision 0.9927, recall 0.9929, auc 0.9928
epoch 1201, loss 0.0321, train acc 99.32%, f1 0.9932, precision 0.9932, recall 0.9932, auc 0.9932
epoch 1301, loss 0.0258, train acc 99.38%, f1 0.9937, precision 0.9935, recall 0.9940, auc 0.9938
epoch 1401, loss 0.0305, train acc 99.43%, f1 0.9942, precision 0.9940, recall 0.9945, auc 0.9943
epoch 1501, loss 0.0124, train acc 99.45%, f1 0.9945, precision 0.9945, recall 0.9945, auc 0.9945
epoch 1601, loss 0.0186, train acc 99.49%, f1 0.9948, precision 0.9946, recall 0.9951, auc 0.9949
epoch 1701, loss 0.0146, train acc 99.51%, f1 0.9951, precision 0.9949, recall 0.9953, auc 0.9951
epoch 1801, loss 0.0211, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9953, auc 0.9953
epoch 1901, loss 0.0167, train acc 99.55%, f1 0.9955, precision 0.9951, recall 0.9958, auc 0.9955
epoch 2001, loss 0.0109, train acc 99.57%, f1 0.9957, precision 0.9956, recall 0.9959, auc 0.9957
epoch 2101, loss 0.0121, train acc 99.59%, f1 0.9959, precision 0.9958, recall 0.9959, auc 0.9959
epoch 2201, loss 0.0135, train acc 99.60%, f1 0.9960, precision 0.9958, recall 0.9962, auc 0.9960
epoch 2301, loss 0.0093, train acc 99.62%, f1 0.9961, precision 0.9962, recall 0.9961, auc 0.9962
epoch 2401, loss 0.0098, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 2501, loss 0.0095, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9962, auc 0.9963
epoch 2601, loss 0.0188, train acc 99.66%, f1 0.9965, precision 0.9965, recall 0.9966, auc 0.9966
epoch 2701, loss 0.0118, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9966, auc 0.9965
epoch 2801, loss 0.0133, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9967, auc 0.9966
epoch 2901, loss 0.0080, train acc 99.67%, f1 0.9966, precision 0.9965, recall 0.9968, auc 0.9967
epoch 3001, loss 0.0100, train acc 99.68%, f1 0.9968, precision 0.9967, recall 0.9969, auc 0.9968
epoch 3101, loss 0.0066, train acc 99.69%, f1 0.9968, precision 0.9969, recall 0.9967, auc 0.9969
epoch 3201, loss 0.0062, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 3301, loss 0.0101, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9969, auc 0.9970
epoch 3401, loss 0.0067, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 3501, loss 0.0052, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3601, loss 0.0056, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9970, auc 0.9971
epoch 3701, loss 0.0137, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 3801, loss 0.0115, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 3901, loss 0.0062, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 4001, loss 0.0045, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 4101, loss 0.0045, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9971, auc 0.9972
epoch 4201, loss 0.0104, train acc 99.74%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9974
epoch 4301, loss 0.0096, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 4401, loss 0.0029, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 4501, loss 0.0017, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9974, auc 0.9974
epoch 4601, loss 0.0084, train acc 99.74%, f1 0.9973, precision 0.9974, recall 0.9973, auc 0.9974
epoch 4701, loss 0.0075, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 4801, loss 0.0100, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 4901, loss 0.0093, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 5001, loss 0.0059, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9976, auc 0.9976
epoch 5101, loss 0.0073, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9976, auc 0.9976
epoch 5201, loss 0.0056, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9976
epoch 5301, loss 0.0105, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 5401, loss 0.0097, train acc 99.78%, f1 0.9977, precision 0.9978, recall 0.9977, auc 0.9978
epoch 5501, loss 0.0066, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9976, auc 0.9977
epoch 5601, loss 0.0126, train acc 99.78%, f1 0.9977, precision 0.9978, recall 0.9977, auc 0.9978
epoch 5701, loss 0.0035, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9980, auc 0.9978
epoch 5801, loss 0.0054, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9979, auc 0.9978
epoch 5901, loss 0.0067, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 6001, loss 0.0019, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6101, loss 0.0057, train acc 99.81%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9981
epoch 6201, loss 0.0033, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6301, loss 0.0051, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 6401, loss 0.0101, train acc 99.81%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9981
epoch 6501, loss 0.0059, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 6601, loss 0.0032, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6701, loss 0.0024, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 6801, loss 0.0075, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6901, loss 0.0026, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9982
epoch 7001, loss 0.0049, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 7101, loss 0.0054, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 7201, loss 0.0074, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 7301, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 7401, loss 0.0043, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 7501, loss 0.0128, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 7601, loss 0.0024, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 7701, loss 0.0008, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 7801, loss 0.0034, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7901, loss 0.0062, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 8001, loss 0.0017, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 8101, loss 0.0057, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0040, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9986, auc 0.9985
epoch 8301, loss 0.0032, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 8401, loss 0.0017, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 8501, loss 0.0008, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 8601, loss 0.0014, train acc 99.87%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9987
epoch 8701, loss 0.0070, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 8801, loss 0.0044, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 8901, loss 0.0016, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 9001, loss 0.0049, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9989, auc 0.9987
epoch 9101, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 9201, loss 0.0024, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 9301, loss 0.0045, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9990, auc 0.9987
epoch 9401, loss 0.0053, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9990, auc 0.9987
epoch 9501, loss 0.0043, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 9601, loss 0.0051, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 9701, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 9801, loss 0.0007, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 9901, loss 0.0056, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_3
./test_vehicle0/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9961240310077519

the Fscore is 0.9876543209876543

the precision is 0.975609756097561

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_3
----------------------



epoch 1, loss 0.6934, train acc 50.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3724, train acc 83.03%, f1 0.8256, precision 0.8482, recall 0.8042, auc 0.8303
epoch 201, loss 0.2438, train acc 94.20%, f1 0.9418, precision 0.9437, recall 0.9399, auc 0.9419
epoch 301, loss 0.1366, train acc 97.39%, f1 0.9739, precision 0.9738, recall 0.9739, auc 0.9739
epoch 401, loss 0.1064, train acc 98.27%, f1 0.9827, precision 0.9819, recall 0.9836, auc 0.9827
epoch 501, loss 0.0748, train acc 98.59%, f1 0.9859, precision 0.9851, recall 0.9866, auc 0.9859
epoch 601, loss 0.0670, train acc 98.72%, f1 0.9872, precision 0.9868, recall 0.9875, auc 0.9872
epoch 701, loss 0.0366, train acc 98.87%, f1 0.9887, precision 0.9884, recall 0.9890, auc 0.9887
epoch 801, loss 0.0485, train acc 99.03%, f1 0.9902, precision 0.9901, recall 0.9904, auc 0.9903
epoch 901, loss 0.0179, train acc 99.15%, f1 0.9915, precision 0.9911, recall 0.9918, auc 0.9915
epoch 1001, loss 0.0225, train acc 99.18%, f1 0.9918, precision 0.9915, recall 0.9921, auc 0.9918
epoch 1101, loss 0.0215, train acc 99.26%, f1 0.9926, precision 0.9923, recall 0.9928, auc 0.9926
epoch 1201, loss 0.0176, train acc 99.32%, f1 0.9932, precision 0.9931, recall 0.9932, auc 0.9932
epoch 1301, loss 0.0181, train acc 99.37%, f1 0.9937, precision 0.9935, recall 0.9939, auc 0.9937
epoch 1401, loss 0.0133, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1501, loss 0.0176, train acc 99.44%, f1 0.9944, precision 0.9945, recall 0.9944, auc 0.9944
epoch 1601, loss 0.0239, train acc 99.48%, f1 0.9948, precision 0.9946, recall 0.9949, auc 0.9948
epoch 1701, loss 0.0166, train acc 99.50%, f1 0.9950, precision 0.9950, recall 0.9950, auc 0.9950
epoch 1801, loss 0.0119, train acc 99.53%, f1 0.9953, precision 0.9952, recall 0.9954, auc 0.9953
epoch 1901, loss 0.0133, train acc 99.55%, f1 0.9955, precision 0.9954, recall 0.9956, auc 0.9955
epoch 2001, loss 0.0097, train acc 99.57%, f1 0.9957, precision 0.9956, recall 0.9958, auc 0.9957
epoch 2101, loss 0.0134, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9959, auc 0.9958
epoch 2201, loss 0.0162, train acc 99.60%, f1 0.9960, precision 0.9959, recall 0.9962, auc 0.9960
epoch 2301, loss 0.0143, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9963, auc 0.9962
epoch 2401, loss 0.0091, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2501, loss 0.0110, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9963, auc 0.9964
epoch 2601, loss 0.0109, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9967, auc 0.9966
epoch 2701, loss 0.0155, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9968, auc 0.9967
epoch 2801, loss 0.0063, train acc 99.67%, f1 0.9967, precision 0.9966, recall 0.9968, auc 0.9967
epoch 2901, loss 0.0053, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9967, auc 0.9968
epoch 3001, loss 0.0032, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 3101, loss 0.0069, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9967, auc 0.9969
epoch 3201, loss 0.0156, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9971, auc 0.9970
epoch 3301, loss 0.0106, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3401, loss 0.0105, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3501, loss 0.0118, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9972, auc 0.9971
epoch 3601, loss 0.0058, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 3701, loss 0.0095, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9972, auc 0.9971
epoch 3801, loss 0.0098, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 3901, loss 0.0059, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 4001, loss 0.0067, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9972, auc 0.9973
epoch 4101, loss 0.0124, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 4201, loss 0.0050, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 4301, loss 0.0096, train acc 99.74%, f1 0.9974, precision 0.9972, recall 0.9976, auc 0.9974
epoch 4401, loss 0.0086, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 4501, loss 0.0092, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 4601, loss 0.0030, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 4701, loss 0.0051, train acc 99.76%, f1 0.9976, precision 0.9974, recall 0.9977, auc 0.9976
epoch 4801, loss 0.0151, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9977, auc 0.9976
epoch 4901, loss 0.0064, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 5001, loss 0.0015, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 5101, loss 0.0055, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 5201, loss 0.0047, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9976, auc 0.9977
epoch 5301, loss 0.0043, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 5401, loss 0.0094, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 5501, loss 0.0046, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 5601, loss 0.0034, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 5701, loss 0.0043, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 5801, loss 0.0034, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9981, auc 0.9980
epoch 5901, loss 0.0064, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 6001, loss 0.0038, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 6101, loss 0.0036, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 6201, loss 0.0021, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6301, loss 0.0069, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 6401, loss 0.0020, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 6501, loss 0.0071, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 6601, loss 0.0073, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 6701, loss 0.0096, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 6801, loss 0.0025, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 6901, loss 0.0032, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 7001, loss 0.0028, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7101, loss 0.0003, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7201, loss 0.0029, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 7301, loss 0.0029, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 7401, loss 0.0041, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7501, loss 0.0020, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 7601, loss 0.0028, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7701, loss 0.0018, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 7801, loss 0.0056, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 7901, loss 0.0066, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_3
./test_vehicle0/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9922480620155039

the Fscore is 0.975609756097561

the precision is 0.9523809523809523

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_3
----------------------



epoch 1, loss 0.6932, train acc 50.34%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4148, train acc 83.25%, f1 0.8325, precision 0.8271, recall 0.8379, auc 0.8325
epoch 201, loss 0.2386, train acc 94.50%, f1 0.9446, precision 0.9451, recall 0.9441, auc 0.9450
epoch 301, loss 0.1356, train acc 97.45%, f1 0.9744, precision 0.9749, recall 0.9738, auc 0.9745
epoch 401, loss 0.0718, train acc 98.28%, f1 0.9827, precision 0.9830, recall 0.9823, auc 0.9828
epoch 501, loss 0.0821, train acc 98.52%, f1 0.9851, precision 0.9850, recall 0.9853, auc 0.9852
epoch 601, loss 0.0517, train acc 98.75%, f1 0.9874, precision 0.9872, recall 0.9876, auc 0.9875
epoch 701, loss 0.0309, train acc 98.93%, f1 0.9892, precision 0.9890, recall 0.9894, auc 0.9893
epoch 801, loss 0.0255, train acc 99.03%, f1 0.9902, precision 0.9902, recall 0.9901, auc 0.9903
epoch 901, loss 0.0530, train acc 99.11%, f1 0.9910, precision 0.9911, recall 0.9910, auc 0.9911
epoch 1001, loss 0.0366, train acc 99.19%, f1 0.9918, precision 0.9916, recall 0.9920, auc 0.9919
epoch 1101, loss 0.0137, train acc 99.28%, f1 0.9928, precision 0.9927, recall 0.9929, auc 0.9928
epoch 1201, loss 0.0257, train acc 99.33%, f1 0.9932, precision 0.9931, recall 0.9933, auc 0.9933
epoch 1301, loss 0.0231, train acc 99.39%, f1 0.9938, precision 0.9937, recall 0.9940, auc 0.9939
epoch 1401, loss 0.0290, train acc 99.45%, f1 0.9944, precision 0.9945, recall 0.9944, auc 0.9945
epoch 1501, loss 0.0110, train acc 99.48%, f1 0.9947, precision 0.9945, recall 0.9949, auc 0.9948
epoch 1601, loss 0.0151, train acc 99.50%, f1 0.9949, precision 0.9948, recall 0.9951, auc 0.9950
epoch 1701, loss 0.0158, train acc 99.51%, f1 0.9951, precision 0.9950, recall 0.9952, auc 0.9951
epoch 1801, loss 0.0156, train acc 99.53%, f1 0.9953, precision 0.9952, recall 0.9953, auc 0.9953
epoch 1901, loss 0.0178, train acc 99.56%, f1 0.9956, precision 0.9954, recall 0.9957, auc 0.9956
epoch 2001, loss 0.0192, train acc 99.57%, f1 0.9957, precision 0.9956, recall 0.9957, auc 0.9957
epoch 2101, loss 0.0080, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9958, auc 0.9958
epoch 2201, loss 0.0119, train acc 99.61%, f1 0.9960, precision 0.9961, recall 0.9960, auc 0.9961
epoch 2301, loss 0.0166, train acc 99.61%, f1 0.9960, precision 0.9961, recall 0.9960, auc 0.9961
epoch 2401, loss 0.0128, train acc 99.63%, f1 0.9963, precision 0.9961, recall 0.9965, auc 0.9963
epoch 2501, loss 0.0119, train acc 99.64%, f1 0.9964, precision 0.9962, recall 0.9965, auc 0.9964
epoch 2601, loss 0.0118, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9965, auc 0.9965
epoch 2701, loss 0.0121, train acc 99.66%, f1 0.9965, precision 0.9965, recall 0.9966, auc 0.9966
epoch 2801, loss 0.0078, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9966, auc 0.9967
epoch 2901, loss 0.0208, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9965, auc 0.9967
epoch 3001, loss 0.0084, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9968, auc 0.9969
epoch 3101, loss 0.0124, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9968, auc 0.9969
epoch 3201, loss 0.0065, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 3301, loss 0.0064, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3401, loss 0.0113, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 3501, loss 0.0088, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9969, auc 0.9970
epoch 3601, loss 0.0066, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9972
epoch 3701, loss 0.0056, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9970, auc 0.9970
epoch 3801, loss 0.0088, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 3901, loss 0.0105, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 4001, loss 0.0070, train acc 99.72%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9972
epoch 4101, loss 0.0045, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 4201, loss 0.0091, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 4301, loss 0.0111, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9974, auc 0.9974
epoch 4401, loss 0.0046, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9974, auc 0.9975
epoch 4501, loss 0.0074, train acc 99.75%, f1 0.9975, precision 0.9972, recall 0.9977, auc 0.9975
epoch 4601, loss 0.0085, train acc 99.75%, f1 0.9975, precision 0.9973, recall 0.9976, auc 0.9975
epoch 4701, loss 0.0082, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 4801, loss 0.0107, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9977, auc 0.9976
epoch 4901, loss 0.0085, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9975, auc 0.9976
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_3
./test_vehicle0/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9719961240310078

the Fscore is 0.9397590361445783

the precision is 0.9069767441860465

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_3
----------------------



epoch 1, loss 0.6928, train acc 49.96%, f1 0.6663, precision 0.4996, recall 1.0000, auc 0.5000
epoch 101, loss 0.4042, train acc 82.92%, f1 0.8304, precision 0.8238, recall 0.8371, auc 0.8292
epoch 201, loss 0.2586, train acc 94.56%, f1 0.9456, precision 0.9445, recall 0.9467, auc 0.9456
epoch 301, loss 0.1423, train acc 97.52%, f1 0.9752, precision 0.9741, recall 0.9764, auc 0.9752
epoch 401, loss 0.1110, train acc 98.24%, f1 0.9824, precision 0.9817, recall 0.9832, auc 0.9824
epoch 501, loss 0.0649, train acc 98.54%, f1 0.9854, precision 0.9848, recall 0.9861, auc 0.9854
epoch 601, loss 0.0767, train acc 98.76%, f1 0.9876, precision 0.9869, recall 0.9882, auc 0.9876
epoch 701, loss 0.0388, train acc 98.95%, f1 0.9895, precision 0.9889, recall 0.9901, auc 0.9895
epoch 801, loss 0.0328, train acc 99.06%, f1 0.9906, precision 0.9901, recall 0.9911, auc 0.9906
epoch 901, loss 0.0389, train acc 99.17%, f1 0.9917, precision 0.9912, recall 0.9923, auc 0.9917
epoch 1001, loss 0.0350, train acc 99.22%, f1 0.9922, precision 0.9916, recall 0.9928, auc 0.9922
epoch 1101, loss 0.0246, train acc 99.29%, f1 0.9929, precision 0.9924, recall 0.9933, auc 0.9929
epoch 1201, loss 0.0335, train acc 99.34%, f1 0.9934, precision 0.9931, recall 0.9937, auc 0.9934
epoch 1301, loss 0.0243, train acc 99.39%, f1 0.9939, precision 0.9937, recall 0.9941, auc 0.9939
epoch 1401, loss 0.0334, train acc 99.43%, f1 0.9943, precision 0.9939, recall 0.9947, auc 0.9943
epoch 1501, loss 0.0229, train acc 99.47%, f1 0.9947, precision 0.9946, recall 0.9948, auc 0.9947
epoch 1601, loss 0.0116, train acc 99.50%, f1 0.9950, precision 0.9948, recall 0.9951, auc 0.9950
epoch 1701, loss 0.0128, train acc 99.51%, f1 0.9951, precision 0.9949, recall 0.9952, auc 0.9951
epoch 1801, loss 0.0203, train acc 99.52%, f1 0.9952, precision 0.9950, recall 0.9955, auc 0.9952
epoch 1901, loss 0.0122, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9954, auc 0.9955
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_3
./test_vehicle0/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9681201550387598

the Fscore is 0.9285714285714285

the precision is 0.8863636363636364

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_3
----------------------



epoch 1, loss 0.6918, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4658, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4109, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3692, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3336, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3033, train acc 76.96%, f1 0.0602, precision 0.7143, recall 0.0314, auc 0.5138
epoch 601, loss 0.2770, train acc 79.03%, f1 0.2111, precision 0.9048, recall 0.1195, auc 0.5578
epoch 701, loss 0.2539, train acc 82.72%, f1 0.4348, precision 0.9375, recall 0.2830, auc 0.6386
epoch 801, loss 0.2336, train acc 86.85%, f1 0.6276, precision 0.9375, recall 0.4717, auc 0.7310
epoch 901, loss 0.2155, train acc 89.81%, f1 0.7336, precision 0.9500, recall 0.5975, auc 0.7939
epoch 1001, loss 0.1994, train acc 92.76%, f1 0.8244, precision 0.9583, recall 0.7233, auc 0.8568
epoch 1101, loss 0.1851, train acc 94.68%, f1 0.8767, precision 0.9624, recall 0.8050, auc 0.8977
epoch 1201, loss 0.1723, train acc 96.16%, f1 0.9145, precision 0.9586, recall 0.8742, auc 0.9313
epoch 1301, loss 0.1608, train acc 97.19%, f1 0.9389, precision 0.9605, recall 0.9182, auc 0.9533
epoch 1401, loss 0.1504, train acc 97.78%, f1 0.9521, precision 0.9675, recall 0.9371, auc 0.9637
epoch 1501, loss 0.1410, train acc 97.78%, f1 0.9524, precision 0.9615, recall 0.9434, auc 0.9659
epoch 1601, loss 0.1323, train acc 97.93%, f1 0.9557, precision 0.9618, recall 0.9497, auc 0.9691
epoch 1701, loss 0.1243, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 1801, loss 0.1168, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 1901, loss 0.1098, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2001, loss 0.1034, train acc 98.82%, f1 0.9748, precision 0.9748, recall 0.9748, auc 0.9836
epoch 2101, loss 0.0975, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2201, loss 0.0918, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2301, loss 0.0868, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2401, loss 0.0822, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2501, loss 0.0779, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2601, loss 0.0738, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2701, loss 0.0700, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2801, loss 0.0663, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2901, loss 0.0630, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 3001, loss 0.0599, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 3101, loss 0.0570, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 3201, loss 0.0542, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 3301, loss 0.0514, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 3401, loss 0.0489, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3501, loss 0.0465, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3601, loss 0.0442, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3701, loss 0.0420, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3801, loss 0.0400, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3901, loss 0.0380, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4001, loss 0.0361, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4101, loss 0.0343, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4201, loss 0.0326, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4301, loss 0.0309, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0294, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0279, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0265, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4701, loss 0.0251, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0238, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0226, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0215, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0203, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0193, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0183, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0173, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0164, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0155, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0147, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0139, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0132, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0125, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0118, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0112, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0106, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0101, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0095, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0090, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0086, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0081, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0077, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0073, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0069, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0066, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0063, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0059, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0054, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0051, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0044, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0042, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8201, loss 0.0040, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0038, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0036, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0031, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_normal_20000
normal
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_3
./test_vehicle0/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.9797480620155039

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_3
----------------------



epoch 1, loss 0.6994, train acc 23.49%, f1 0.3804, precision 0.2349, recall 1.0000, auc 0.5000
epoch 101, loss 0.4682, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4125, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3708, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3353, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3051, train acc 76.96%, f1 0.0602, precision 0.7143, recall 0.0314, auc 0.5138
epoch 601, loss 0.2788, train acc 78.73%, f1 0.1910, precision 0.8947, recall 0.1069, auc 0.5515
epoch 701, loss 0.2558, train acc 81.83%, f1 0.3881, precision 0.9286, recall 0.2453, auc 0.6197
epoch 801, loss 0.2353, train acc 86.12%, f1 0.5983, precision 0.9333, recall 0.4403, auc 0.7153
epoch 901, loss 0.2172, train acc 89.51%, f1 0.7237, precision 0.9490, recall 0.5849, auc 0.7876
epoch 1001, loss 0.2010, train acc 92.61%, f1 0.8188, precision 0.9658, recall 0.7107, auc 0.8515
epoch 1101, loss 0.1865, train acc 94.68%, f1 0.8759, precision 0.9695, recall 0.7987, auc 0.8955
epoch 1201, loss 0.1736, train acc 96.16%, f1 0.9139, precision 0.9650, recall 0.8679, auc 0.9291
epoch 1301, loss 0.1620, train acc 97.19%, f1 0.9385, precision 0.9667, recall 0.9119, auc 0.9511
epoch 1401, loss 0.1515, train acc 97.64%, f1 0.9487, precision 0.9673, recall 0.9308, auc 0.9606
epoch 1501, loss 0.1418, train acc 97.78%, f1 0.9524, precision 0.9615, recall 0.9434, auc 0.9659
epoch 1601, loss 0.1330, train acc 97.93%, f1 0.9557, precision 0.9618, recall 0.9497, auc 0.9691
epoch 1701, loss 0.1249, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 1801, loss 0.1174, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 1901, loss 0.1104, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2001, loss 0.1040, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2101, loss 0.0981, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2201, loss 0.0927, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2301, loss 0.0874, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2401, loss 0.0828, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2501, loss 0.0786, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2601, loss 0.0746, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2701, loss 0.0706, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2801, loss 0.0670, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2901, loss 0.0637, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 3001, loss 0.0605, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 3101, loss 0.0576, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 3201, loss 0.0549, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 3301, loss 0.0523, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 3401, loss 0.0498, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3501, loss 0.0473, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3601, loss 0.0449, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3701, loss 0.0427, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3801, loss 0.0406, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3901, loss 0.0386, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4001, loss 0.0367, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4101, loss 0.0348, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4201, loss 0.0331, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4301, loss 0.0314, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4401, loss 0.0299, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0283, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0269, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4701, loss 0.0256, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0243, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0230, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0218, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0207, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0196, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0186, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0176, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0167, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0158, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0150, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0142, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0134, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0127, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0120, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0114, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0108, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0102, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0097, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0092, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0087, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0083, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0079, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0075, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0071, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0067, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0064, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0061, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0058, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0055, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0052, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0049, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0047, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0045, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0042, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0040, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0038, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0035, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0033, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0031, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0030, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_normal_15000
normal
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_3
./test_vehicle0/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.9797480620155039

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_3
----------------------



epoch 1, loss 0.6842, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4656, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4110, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3692, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3335, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3029, train acc 76.96%, f1 0.0602, precision 0.7143, recall 0.0314, auc 0.5138
epoch 601, loss 0.2764, train acc 79.32%, f1 0.2308, precision 0.9130, recall 0.1321, auc 0.5641
epoch 701, loss 0.2531, train acc 82.42%, f1 0.4306, precision 0.9000, recall 0.2830, auc 0.6367
epoch 801, loss 0.2326, train acc 87.30%, f1 0.6446, precision 0.9398, recall 0.4906, auc 0.7405
epoch 901, loss 0.2145, train acc 89.66%, f1 0.7287, precision 0.9495, recall 0.5912, auc 0.7908
epoch 1001, loss 0.1984, train acc 92.76%, f1 0.8244, precision 0.9583, recall 0.7233, auc 0.8568
epoch 1101, loss 0.1841, train acc 94.83%, f1 0.8814, precision 0.9559, recall 0.8176, auc 0.9030
epoch 1201, loss 0.1713, train acc 96.60%, f1 0.9246, precision 0.9658, recall 0.8868, auc 0.9386
epoch 1301, loss 0.1598, train acc 97.34%, f1 0.9419, precision 0.9669, recall 0.9182, auc 0.9543
epoch 1401, loss 0.1493, train acc 97.64%, f1 0.9490, precision 0.9613, recall 0.9371, auc 0.9628
epoch 1501, loss 0.1397, train acc 97.78%, f1 0.9524, precision 0.9615, recall 0.9434, auc 0.9659
epoch 1601, loss 0.1310, train acc 98.23%, f1 0.9623, precision 0.9623, recall 0.9623, auc 0.9753
epoch 1701, loss 0.1231, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 1801, loss 0.1157, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 1901, loss 0.1089, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2001, loss 0.1026, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2101, loss 0.0968, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2201, loss 0.0914, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2301, loss 0.0862, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2401, loss 0.0815, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2501, loss 0.0771, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2601, loss 0.0731, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2701, loss 0.0694, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2801, loss 0.0660, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2901, loss 0.0626, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 3001, loss 0.0594, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 3101, loss 0.0563, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 3201, loss 0.0534, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 3301, loss 0.0508, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3401, loss 0.0483, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3501, loss 0.0459, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3601, loss 0.0436, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3701, loss 0.0415, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3801, loss 0.0394, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3901, loss 0.0375, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4001, loss 0.0356, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4101, loss 0.0339, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4201, loss 0.0322, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0306, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0290, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0275, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0261, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4701, loss 0.0248, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0235, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0223, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0212, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0201, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0190, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0180, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0171, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0162, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0154, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0146, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0138, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0131, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0124, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0117, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0111, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0105, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0100, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0095, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0090, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0085, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0081, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0077, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0073, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0069, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0066, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0062, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0059, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0051, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0044, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0036, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0031, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_normal_10000
normal
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_3
./test_vehicle0/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.9797480620155039

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_3
----------------------



epoch 1, loss 0.6933, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4672, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4119, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3702, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3346, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3041, train acc 76.96%, f1 0.0602, precision 0.7143, recall 0.0314, auc 0.5138
epoch 601, loss 0.2777, train acc 79.03%, f1 0.2111, precision 0.9048, recall 0.1195, auc 0.5578
epoch 701, loss 0.2546, train acc 82.57%, f1 0.4272, precision 0.9362, recall 0.2767, auc 0.6355
epoch 801, loss 0.2341, train acc 86.56%, f1 0.6160, precision 0.9359, recall 0.4591, auc 0.7247
epoch 901, loss 0.2159, train acc 89.66%, f1 0.7287, precision 0.9495, recall 0.5912, auc 0.7908
epoch 1001, loss 0.1997, train acc 92.61%, f1 0.8201, precision 0.9580, recall 0.7170, auc 0.8537
epoch 1101, loss 0.1853, train acc 94.83%, f1 0.8805, precision 0.9627, recall 0.8113, auc 0.9008
epoch 1201, loss 0.1725, train acc 96.45%, f1 0.9211, precision 0.9655, recall 0.8805, auc 0.9354
epoch 1301, loss 0.1609, train acc 97.34%, f1 0.9419, precision 0.9669, recall 0.9182, auc 0.9543
epoch 1401, loss 0.1504, train acc 97.78%, f1 0.9521, precision 0.9675, recall 0.9371, auc 0.9637
epoch 1501, loss 0.1409, train acc 97.78%, f1 0.9524, precision 0.9615, recall 0.9434, auc 0.9659
epoch 1601, loss 0.1321, train acc 97.93%, f1 0.9557, precision 0.9618, recall 0.9497, auc 0.9691
epoch 1701, loss 0.1241, train acc 98.38%, f1 0.9655, precision 0.9625, recall 0.9686, auc 0.9785
epoch 1801, loss 0.1166, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 1901, loss 0.1097, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2001, loss 0.1033, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2101, loss 0.0974, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2201, loss 0.0919, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2301, loss 0.0867, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2401, loss 0.0819, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2501, loss 0.0775, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2601, loss 0.0733, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2701, loss 0.0693, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2801, loss 0.0657, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2901, loss 0.0623, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 3001, loss 0.0591, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 3101, loss 0.0562, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 3201, loss 0.0534, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 3301, loss 0.0508, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3401, loss 0.0483, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3501, loss 0.0459, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3601, loss 0.0436, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3701, loss 0.0415, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3801, loss 0.0395, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3901, loss 0.0375, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4001, loss 0.0357, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4101, loss 0.0339, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4201, loss 0.0323, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4301, loss 0.0307, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0291, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0277, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0263, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4701, loss 0.0250, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4801, loss 0.0237, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0225, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0214, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0203, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0192, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0182, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0173, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0164, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0155, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0147, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0139, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0132, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0125, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0119, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0112, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0107, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0101, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0096, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0091, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0086, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0082, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0078, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0074, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0070, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0066, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0063, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0060, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0057, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0054, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0051, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0049, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_normal_8000
normal
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_3
./test_vehicle0/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.9797480620155039

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_3
----------------------



epoch 1, loss 0.7031, train acc 23.49%, f1 0.3804, precision 0.2349, recall 1.0000, auc 0.5000
epoch 101, loss 0.4682, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4122, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3705, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3352, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3050, train acc 76.96%, f1 0.0602, precision 0.7143, recall 0.0314, auc 0.5138
epoch 601, loss 0.2789, train acc 78.73%, f1 0.1910, precision 0.8947, recall 0.1069, auc 0.5515
epoch 701, loss 0.2559, train acc 81.98%, f1 0.3960, precision 0.9302, recall 0.2516, auc 0.6229
epoch 801, loss 0.2355, train acc 86.12%, f1 0.5983, precision 0.9333, recall 0.4403, auc 0.7153
epoch 901, loss 0.2162, train acc 89.96%, f1 0.7385, precision 0.9505, recall 0.6038, auc 0.7971
epoch 1001, loss 0.1907, train acc 94.83%, f1 0.8822, precision 0.9493, recall 0.8239, auc 0.9052
epoch 1101, loss 0.1647, train acc 97.19%, f1 0.9389, precision 0.9605, recall 0.9182, auc 0.9533
epoch 1201, loss 0.1446, train acc 97.78%, f1 0.9527, precision 0.9557, recall 0.9497, auc 0.9681
epoch 1301, loss 0.1289, train acc 98.23%, f1 0.9625, precision 0.9565, recall 0.9686, auc 0.9775
epoch 1401, loss 0.1161, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 1501, loss 0.1053, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 1601, loss 0.0961, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 1701, loss 0.0881, train acc 98.67%, f1 0.9720, precision 0.9630, recall 0.9811, auc 0.9848
epoch 1801, loss 0.0811, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 1901, loss 0.0749, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2001, loss 0.0695, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2101, loss 0.0646, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2201, loss 0.0602, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2301, loss 0.0562, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2401, loss 0.0526, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2501, loss 0.0492, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2601, loss 0.0461, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2701, loss 0.0432, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2801, loss 0.0406, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2901, loss 0.0382, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 3001, loss 0.0360, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3101, loss 0.0339, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3201, loss 0.0320, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3301, loss 0.0302, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3401, loss 0.0285, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3501, loss 0.0269, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3601, loss 0.0254, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3701, loss 0.0239, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3801, loss 0.0226, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3901, loss 0.0213, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0201, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0190, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4201, loss 0.0179, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0169, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0159, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0150, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0142, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0134, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0126, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0119, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_normal_5000
normal
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_3
./test_vehicle0/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.983624031007752

the Fscore is 0.975

the precision is 0.975

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_3
----------------------



epoch 1, loss 0.6895, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4676, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4123, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3705, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3348, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3041, train acc 76.96%, f1 0.0602, precision 0.7143, recall 0.0314, auc 0.5138
epoch 601, loss 0.2775, train acc 79.03%, f1 0.2111, precision 0.9048, recall 0.1195, auc 0.5578
epoch 701, loss 0.2542, train acc 82.27%, f1 0.4231, precision 0.8980, recall 0.2767, auc 0.6335
epoch 801, loss 0.2336, train acc 87.00%, f1 0.6333, precision 0.9383, recall 0.4780, auc 0.7342
epoch 901, loss 0.2154, train acc 89.81%, f1 0.7336, precision 0.9500, recall 0.5975, auc 0.7939
epoch 1001, loss 0.1992, train acc 92.61%, f1 0.8201, precision 0.9580, recall 0.7170, auc 0.8537
epoch 1101, loss 0.1848, train acc 94.53%, f1 0.8746, precision 0.9485, recall 0.8113, auc 0.8989
epoch 1201, loss 0.1720, train acc 96.45%, f1 0.9216, precision 0.9592, recall 0.8868, auc 0.9376
epoch 1301, loss 0.1605, train acc 97.19%, f1 0.9389, precision 0.9605, recall 0.9182, auc 0.9533
epoch 1401, loss 0.1501, train acc 97.64%, f1 0.9490, precision 0.9613, recall 0.9371, auc 0.9628
epoch 1501, loss 0.1406, train acc 97.78%, f1 0.9524, precision 0.9615, recall 0.9434, auc 0.9659
epoch 1601, loss 0.1320, train acc 97.93%, f1 0.9557, precision 0.9618, recall 0.9497, auc 0.9691
epoch 1701, loss 0.1240, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 1801, loss 0.1166, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 1901, loss 0.1098, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_3.csv
./test_vehicle0/standlization_data/vehicle0_std_test_3.csv
MLP_normal_2000
normal
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_3
./test_vehicle0/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.983624031007752

the Fscore is 0.975

the precision is 0.975

the recall is 0.975

Done
./test_yeast3/standlization_data/yeast3_std_train_3.csv
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_3
----------------------



epoch 1, loss 0.6937, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3907, train acc 97.86%, f1 0.9786, precision 0.9826, recall 0.9745, auc 0.9786
epoch 201, loss 0.1495, train acc 98.49%, f1 0.9849, precision 0.9853, recall 0.9845, auc 0.9849
epoch 301, loss 0.0853, train acc 98.74%, f1 0.9874, precision 0.9874, recall 0.9875, auc 0.9874
epoch 401, loss 0.0606, train acc 98.83%, f1 0.9883, precision 0.9881, recall 0.9884, auc 0.9883
epoch 501, loss 0.0481, train acc 98.87%, f1 0.9887, precision 0.9885, recall 0.9888, auc 0.9887
epoch 601, loss 0.0430, train acc 98.88%, f1 0.9888, precision 0.9887, recall 0.9890, auc 0.9888
epoch 701, loss 0.0382, train acc 98.88%, f1 0.9888, precision 0.9887, recall 0.9890, auc 0.9888
epoch 801, loss 0.0356, train acc 98.89%, f1 0.9889, precision 0.9887, recall 0.9890, auc 0.9889
epoch 901, loss 0.0340, train acc 98.89%, f1 0.9889, precision 0.9888, recall 0.9890, auc 0.9889
epoch 1001, loss 0.0330, train acc 98.90%, f1 0.9890, precision 0.9889, recall 0.9892, auc 0.9890
epoch 1101, loss 0.0323, train acc 98.91%, f1 0.9891, precision 0.9889, recall 0.9892, auc 0.9891
epoch 1201, loss 0.0318, train acc 98.91%, f1 0.9891, precision 0.9890, recall 0.9892, auc 0.9891
epoch 1301, loss 0.0304, train acc 98.91%, f1 0.9891, precision 0.9890, recall 0.9892, auc 0.9891
epoch 1401, loss 0.0310, train acc 98.90%, f1 0.9890, precision 0.9889, recall 0.9891, auc 0.9890
epoch 1501, loss 0.0307, train acc 98.90%, f1 0.9890, precision 0.9889, recall 0.9891, auc 0.9890
epoch 1601, loss 0.0304, train acc 98.90%, f1 0.9890, precision 0.9889, recall 0.9891, auc 0.9890
epoch 1701, loss 0.0300, train acc 98.89%, f1 0.9889, precision 0.9888, recall 0.9890, auc 0.9889
epoch 1801, loss 0.0301, train acc 98.89%, f1 0.9889, precision 0.9888, recall 0.9890, auc 0.9889
epoch 1901, loss 0.0298, train acc 98.89%, f1 0.9889, precision 0.9888, recall 0.9890, auc 0.9889
epoch 2001, loss 0.0298, train acc 98.89%, f1 0.9889, precision 0.9889, recall 0.9890, auc 0.9889
epoch 2101, loss 0.0297, train acc 98.89%, f1 0.9889, precision 0.9888, recall 0.9890, auc 0.9889
epoch 2201, loss 0.0294, train acc 98.89%, f1 0.9889, precision 0.9889, recall 0.9890, auc 0.9889
epoch 2301, loss 0.0293, train acc 98.89%, f1 0.9889, precision 0.9888, recall 0.9890, auc 0.9889
epoch 2401, loss 0.0284, train acc 98.89%, f1 0.9889, precision 0.9890, recall 0.9889, auc 0.9889
epoch 2501, loss 0.0281, train acc 98.89%, f1 0.9889, precision 0.9890, recall 0.9889, auc 0.9889
epoch 2601, loss 0.0288, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9889, auc 0.9890
epoch 2701, loss 0.0285, train acc 98.90%, f1 0.9890, precision 0.9891, recall 0.9890, auc 0.9890
epoch 2801, loss 0.0278, train acc 98.91%, f1 0.9891, precision 0.9891, recall 0.9891, auc 0.9891
epoch 2901, loss 0.0282, train acc 98.91%, f1 0.9891, precision 0.9892, recall 0.9891, auc 0.9891
epoch 3001, loss 0.0277, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9892, auc 0.9892
epoch 3101, loss 0.0237, train acc 98.93%, f1 0.9893, precision 0.9893, recall 0.9893, auc 0.9893
epoch 3201, loss 0.0269, train acc 98.93%, f1 0.9893, precision 0.9893, recall 0.9894, auc 0.9893
epoch 3301, loss 0.0262, train acc 98.94%, f1 0.9894, precision 0.9895, recall 0.9894, auc 0.9894
epoch 3401, loss 0.0267, train acc 98.96%, f1 0.9896, precision 0.9896, recall 0.9897, auc 0.9896
epoch 3501, loss 0.0265, train acc 98.96%, f1 0.9896, precision 0.9896, recall 0.9897, auc 0.9896
epoch 3601, loss 0.0262, train acc 98.97%, f1 0.9897, precision 0.9897, recall 0.9897, auc 0.9897
epoch 3701, loss 0.0260, train acc 98.98%, f1 0.9898, precision 0.9897, recall 0.9900, auc 0.9898
epoch 3801, loss 0.0255, train acc 98.98%, f1 0.9898, precision 0.9897, recall 0.9899, auc 0.9898
epoch 3901, loss 0.0249, train acc 98.99%, f1 0.9899, precision 0.9899, recall 0.9900, auc 0.9899
epoch 4001, loss 0.0249, train acc 99.00%, f1 0.9900, precision 0.9900, recall 0.9901, auc 0.9900
epoch 4101, loss 0.0238, train acc 99.01%, f1 0.9901, precision 0.9899, recall 0.9902, auc 0.9901
epoch 4201, loss 0.0240, train acc 99.02%, f1 0.9902, precision 0.9902, recall 0.9903, auc 0.9902
epoch 4301, loss 0.0221, train acc 99.03%, f1 0.9903, precision 0.9902, recall 0.9904, auc 0.9903
epoch 4401, loss 0.0236, train acc 99.03%, f1 0.9903, precision 0.9902, recall 0.9904, auc 0.9903
epoch 4501, loss 0.0225, train acc 99.04%, f1 0.9904, precision 0.9904, recall 0.9905, auc 0.9904
epoch 4601, loss 0.0220, train acc 99.06%, f1 0.9906, precision 0.9905, recall 0.9906, auc 0.9906
epoch 4701, loss 0.0222, train acc 99.06%, f1 0.9906, precision 0.9906, recall 0.9906, auc 0.9906
epoch 4801, loss 0.0225, train acc 99.06%, f1 0.9906, precision 0.9906, recall 0.9906, auc 0.9906
epoch 4901, loss 0.0217, train acc 99.07%, f1 0.9907, precision 0.9907, recall 0.9907, auc 0.9907
epoch 5001, loss 0.0221, train acc 99.08%, f1 0.9908, precision 0.9908, recall 0.9908, auc 0.9908
epoch 5101, loss 0.0215, train acc 99.09%, f1 0.9909, precision 0.9911, recall 0.9908, auc 0.9909
epoch 5201, loss 0.0211, train acc 99.10%, f1 0.9910, precision 0.9911, recall 0.9909, auc 0.9910
epoch 5301, loss 0.0213, train acc 99.11%, f1 0.9911, precision 0.9911, recall 0.9910, auc 0.9911
epoch 5401, loss 0.0209, train acc 99.12%, f1 0.9912, precision 0.9913, recall 0.9911, auc 0.9912
epoch 5501, loss 0.0205, train acc 99.13%, f1 0.9913, precision 0.9914, recall 0.9912, auc 0.9913
epoch 5601, loss 0.0196, train acc 99.14%, f1 0.9914, precision 0.9915, recall 0.9913, auc 0.9914
epoch 5701, loss 0.0193, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9916, auc 0.9916
epoch 5801, loss 0.0194, train acc 99.17%, f1 0.9917, precision 0.9917, recall 0.9917, auc 0.9917
epoch 5901, loss 0.0192, train acc 99.19%, f1 0.9919, precision 0.9919, recall 0.9919, auc 0.9919
epoch 6001, loss 0.0185, train acc 99.20%, f1 0.9920, precision 0.9920, recall 0.9920, auc 0.9920
epoch 6101, loss 0.0179, train acc 99.23%, f1 0.9923, precision 0.9923, recall 0.9922, auc 0.9923
epoch 6201, loss 0.0179, train acc 99.26%, f1 0.9926, precision 0.9926, recall 0.9925, auc 0.9926
epoch 6301, loss 0.0174, train acc 99.28%, f1 0.9928, precision 0.9928, recall 0.9928, auc 0.9928
epoch 6401, loss 0.0164, train acc 99.31%, f1 0.9931, precision 0.9930, recall 0.9931, auc 0.9931
epoch 6501, loss 0.0164, train acc 99.32%, f1 0.9932, precision 0.9931, recall 0.9932, auc 0.9932
epoch 6601, loss 0.0160, train acc 99.35%, f1 0.9935, precision 0.9933, recall 0.9936, auc 0.9935
epoch 6701, loss 0.0149, train acc 99.37%, f1 0.9937, precision 0.9935, recall 0.9938, auc 0.9937
epoch 6801, loss 0.0148, train acc 99.39%, f1 0.9939, precision 0.9938, recall 0.9940, auc 0.9939
epoch 6901, loss 0.0143, train acc 99.41%, f1 0.9941, precision 0.9939, recall 0.9942, auc 0.9941
epoch 7001, loss 0.0133, train acc 99.43%, f1 0.9943, precision 0.9941, recall 0.9944, auc 0.9943
epoch 7101, loss 0.0129, train acc 99.45%, f1 0.9945, precision 0.9944, recall 0.9946, auc 0.9945
epoch 7201, loss 0.0118, train acc 99.48%, f1 0.9948, precision 0.9946, recall 0.9950, auc 0.9948
epoch 7301, loss 0.0124, train acc 99.51%, f1 0.9951, precision 0.9950, recall 0.9952, auc 0.9951
epoch 7401, loss 0.0120, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9954, auc 0.9953
epoch 7501, loss 0.0116, train acc 99.55%, f1 0.9955, precision 0.9954, recall 0.9955, auc 0.9955
epoch 7601, loss 0.0103, train acc 99.55%, f1 0.9955, precision 0.9956, recall 0.9955, auc 0.9955
epoch 7701, loss 0.0100, train acc 99.58%, f1 0.9958, precision 0.9956, recall 0.9959, auc 0.9958
epoch 7801, loss 0.0104, train acc 99.60%, f1 0.9960, precision 0.9958, recall 0.9962, auc 0.9960
epoch 7901, loss 0.0101, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9964, auc 0.9962
epoch 8001, loss 0.0096, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9964, auc 0.9963
epoch 8101, loss 0.0095, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9966, auc 0.9965
epoch 8201, loss 0.0092, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9967, auc 0.9966
epoch 8301, loss 0.0088, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9969, auc 0.9968
epoch 8401, loss 0.0085, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 8501, loss 0.0083, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 8601, loss 0.0078, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 8701, loss 0.0076, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9973
epoch 8801, loss 0.0073, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9973
epoch 8901, loss 0.0072, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9976, auc 0.9975
epoch 9001, loss 0.0067, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9977, auc 0.9976
epoch 9101, loss 0.0065, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 9201, loss 0.0063, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 9301, loss 0.0063, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9979, auc 0.9978
epoch 9401, loss 0.0057, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 9501, loss 0.0058, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 9601, loss 0.0057, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 9701, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 9801, loss 0.0048, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 9901, loss 0.0051, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 10001, loss 0.0048, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 10101, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 10201, loss 0.0045, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 10301, loss 0.0042, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 10401, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 10501, loss 0.0037, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 10601, loss 0.0037, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 10701, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10801, loss 0.0035, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 10901, loss 0.0033, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 11001, loss 0.0032, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9995, auc 0.9993
epoch 11101, loss 0.0030, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 11201, loss 0.0029, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 11301, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9996, auc 0.9994
epoch 11401, loss 0.0026, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 11501, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9997, auc 0.9995
epoch 11601, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9998, auc 0.9996
epoch 11701, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 11801, loss 0.0021, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9999, auc 0.9997
epoch 11901, loss 0.0020, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9999, auc 0.9997
epoch 12001, loss 0.0019, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 12101, loss 0.0018, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 12201, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9997, recall 1.0000, auc 0.9998
epoch 12301, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9997, recall 1.0000, auc 0.9998
epoch 12401, loss 0.0016, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 12501, loss 0.0015, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 12601, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 12701, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 12801, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 12901, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 13001, loss 0.0011, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0011, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0009, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_yeast3/standlization_data/yeast3_std_train_3.csv
./test_yeast3/standlization_data/yeast3_std_test_3.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_3
./test_yeast3/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.8888888888888888

the Fscore is 0.8750000000000001

the precision is 1.0

the recall is 0.7777777777777778

Done
train_mlp_4_2.sh: line 433: 22110 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=yeast3 dataset_index=3 record_index=1 device_id=4 train_method=MLP_concat_Mirror_15000
