nohup: ignoring input
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_3
----------------------



epoch 1, loss 0.6932, train acc 49.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4900
Validation loss decreased (inf --> 0.693232).  Saving model ...
Validation loss decreased (0.693232 --> 0.693161).  Saving model ...
Validation loss decreased (0.693161 --> 0.693090).  Saving model ...
Validation loss decreased (0.693090 --> 0.693008).  Saving model ...
Validation loss decreased (0.693008 --> 0.692925).  Saving model ...
Validation loss decreased (0.692925 --> 0.692840).  Saving model ...
Validation loss decreased (0.692840 --> 0.692748).  Saving model ...
Validation loss decreased (0.692748 --> 0.692643).  Saving model ...
Validation loss decreased (0.692643 --> 0.692522).  Saving model ...
Validation loss decreased (0.692522 --> 0.692382).  Saving model ...
Validation loss decreased (0.692382 --> 0.692231).  Saving model ...
Validation loss decreased (0.692231 --> 0.692063).  Saving model ...
Validation loss decreased (0.692063 --> 0.691878).  Saving model ...
Validation loss decreased (0.691878 --> 0.691673).  Saving model ...
Validation loss decreased (0.691673 --> 0.691444).  Saving model ...
Validation loss decreased (0.691444 --> 0.691191).  Saving model ...
Validation loss decreased (0.691191 --> 0.690916).  Saving model ...
Validation loss decreased (0.690916 --> 0.690629).  Saving model ...
Validation loss decreased (0.690629 --> 0.690315).  Saving model ...
Validation loss decreased (0.690315 --> 0.689980).  Saving model ...
Validation loss decreased (0.689980 --> 0.689629).  Saving model ...
Validation loss decreased (0.689629 --> 0.689259).  Saving model ...
Validation loss decreased (0.689259 --> 0.688871).  Saving model ...
Validation loss decreased (0.688871 --> 0.688470).  Saving model ...
Validation loss decreased (0.688470 --> 0.688060).  Saving model ...
Validation loss decreased (0.688060 --> 0.687608).  Saving model ...
Validation loss decreased (0.687608 --> 0.687133).  Saving model ...
Validation loss decreased (0.687133 --> 0.686639).  Saving model ...
Validation loss decreased (0.686639 --> 0.686115).  Saving model ...
Validation loss decreased (0.686115 --> 0.685590).  Saving model ...
Validation loss decreased (0.685590 --> 0.685052).  Saving model ...
Validation loss decreased (0.685052 --> 0.684486).  Saving model ...
Validation loss decreased (0.684486 --> 0.683893).  Saving model ...
Validation loss decreased (0.683893 --> 0.683263).  Saving model ...
Validation loss decreased (0.683263 --> 0.682571).  Saving model ...
Validation loss decreased (0.682571 --> 0.681863).  Saving model ...
Validation loss decreased (0.681863 --> 0.681136).  Saving model ...
Validation loss decreased (0.681136 --> 0.680377).  Saving model ...
Validation loss decreased (0.680377 --> 0.679584).  Saving model ...
Validation loss decreased (0.679584 --> 0.678766).  Saving model ...
Validation loss decreased (0.678766 --> 0.677922).  Saving model ...
Validation loss decreased (0.677922 --> 0.677103).  Saving model ...
Validation loss decreased (0.677103 --> 0.676231).  Saving model ...
Validation loss decreased (0.676231 --> 0.675344).  Saving model ...
Validation loss decreased (0.675344 --> 0.674437).  Saving model ...
Validation loss decreased (0.674437 --> 0.673475).  Saving model ...
Validation loss decreased (0.673475 --> 0.672520).  Saving model ...
Validation loss decreased (0.672520 --> 0.671518).  Saving model ...
Validation loss decreased (0.671518 --> 0.670501).  Saving model ...
Validation loss decreased (0.670501 --> 0.669464).  Saving model ...
Validation loss decreased (0.669464 --> 0.668416).  Saving model ...
Validation loss decreased (0.668416 --> 0.667334).  Saving model ...
Validation loss decreased (0.667334 --> 0.666296).  Saving model ...
Validation loss decreased (0.666296 --> 0.665254).  Saving model ...
Validation loss decreased (0.665254 --> 0.664123).  Saving model ...
Validation loss decreased (0.664123 --> 0.662974).  Saving model ...
Validation loss decreased (0.662974 --> 0.661813).  Saving model ...
Validation loss decreased (0.661813 --> 0.660617).  Saving model ...
Validation loss decreased (0.660617 --> 0.659385).  Saving model ...
Validation loss decreased (0.659385 --> 0.658165).  Saving model ...
Validation loss decreased (0.658165 --> 0.656977).  Saving model ...
Validation loss decreased (0.656977 --> 0.655760).  Saving model ...
Validation loss decreased (0.655760 --> 0.654518).  Saving model ...
Validation loss decreased (0.654518 --> 0.653226).  Saving model ...
Validation loss decreased (0.653226 --> 0.651926).  Saving model ...
Validation loss decreased (0.651926 --> 0.650641).  Saving model ...
Validation loss decreased (0.650641 --> 0.649324).  Saving model ...
Validation loss decreased (0.649324 --> 0.647985).  Saving model ...
Validation loss decreased (0.647985 --> 0.646586).  Saving model ...
Validation loss decreased (0.646586 --> 0.645229).  Saving model ...
Validation loss decreased (0.645229 --> 0.643876).  Saving model ...
Validation loss decreased (0.643876 --> 0.642576).  Saving model ...
Validation loss decreased (0.642576 --> 0.641335).  Saving model ...
Validation loss decreased (0.641335 --> 0.640091).  Saving model ...
Validation loss decreased (0.640091 --> 0.638909).  Saving model ...
Validation loss decreased (0.638909 --> 0.637790).  Saving model ...
Validation loss decreased (0.637790 --> 0.636691).  Saving model ...
Validation loss decreased (0.636691 --> 0.635607).  Saving model ...
Validation loss decreased (0.635607 --> 0.634533).  Saving model ...
Validation loss decreased (0.634533 --> 0.633438).  Saving model ...
Validation loss decreased (0.633438 --> 0.632303).  Saving model ...
Validation loss decreased (0.632303 --> 0.631041).  Saving model ...
Validation loss decreased (0.631041 --> 0.629868).  Saving model ...
Validation loss decreased (0.629868 --> 0.628666).  Saving model ...
Validation loss decreased (0.628666 --> 0.627528).  Saving model ...
Validation loss decreased (0.627528 --> 0.626382).  Saving model ...
Validation loss decreased (0.626382 --> 0.625346).  Saving model ...
Validation loss decreased (0.625346 --> 0.624319).  Saving model ...
Validation loss decreased (0.624319 --> 0.623272).  Saving model ...
Validation loss decreased (0.623272 --> 0.622300).  Saving model ...
Validation loss decreased (0.622300 --> 0.621366).  Saving model ...
Validation loss decreased (0.621366 --> 0.620504).  Saving model ...
Validation loss decreased (0.620504 --> 0.619739).  Saving model ...
Validation loss decreased (0.619739 --> 0.618943).  Saving model ...
Validation loss decreased (0.618943 --> 0.618142).  Saving model ...
Validation loss decreased (0.618142 --> 0.617314).  Saving model ...
Validation loss decreased (0.617314 --> 0.616549).  Saving model ...
Validation loss decreased (0.616549 --> 0.615709).  Saving model ...
Validation loss decreased (0.615709 --> 0.614772).  Saving model ...
Validation loss decreased (0.614772 --> 0.613846).  Saving model ...
epoch 101, loss 0.5725, train acc 66.75%, f1 0.6683, precision 0.6667, recall 0.6700, auc 0.6675
Validation loss decreased (0.613846 --> 0.612924).  Saving model ...
Validation loss decreased (0.612924 --> 0.611903).  Saving model ...
Validation loss decreased (0.611903 --> 0.610847).  Saving model ...
Validation loss decreased (0.610847 --> 0.609770).  Saving model ...
Validation loss decreased (0.609770 --> 0.608783).  Saving model ...
Validation loss decreased (0.608783 --> 0.607599).  Saving model ...
Validation loss decreased (0.607599 --> 0.606421).  Saving model ...
Validation loss decreased (0.606421 --> 0.605211).  Saving model ...
Validation loss decreased (0.605211 --> 0.604112).  Saving model ...
Validation loss decreased (0.604112 --> 0.603105).  Saving model ...
Validation loss decreased (0.603105 --> 0.602214).  Saving model ...
Validation loss decreased (0.602214 --> 0.601368).  Saving model ...
Validation loss decreased (0.601368 --> 0.600505).  Saving model ...
Validation loss decreased (0.600505 --> 0.599554).  Saving model ...
Validation loss decreased (0.599554 --> 0.598618).  Saving model ...
Validation loss decreased (0.598618 --> 0.597579).  Saving model ...
Validation loss decreased (0.597579 --> 0.596391).  Saving model ...
Validation loss decreased (0.596391 --> 0.595382).  Saving model ...
Validation loss decreased (0.595382 --> 0.594567).  Saving model ...
Validation loss decreased (0.594567 --> 0.593711).  Saving model ...
Validation loss decreased (0.593711 --> 0.592933).  Saving model ...
Validation loss decreased (0.592933 --> 0.592285).  Saving model ...
Validation loss decreased (0.592285 --> 0.591718).  Saving model ...
Validation loss decreased (0.591718 --> 0.591153).  Saving model ...
Validation loss decreased (0.591153 --> 0.590660).  Saving model ...
Validation loss decreased (0.590660 --> 0.590242).  Saving model ...
Validation loss decreased (0.590242 --> 0.589824).  Saving model ...
Validation loss decreased (0.589824 --> 0.589456).  Saving model ...
Validation loss decreased (0.589456 --> 0.588867).  Saving model ...
Validation loss decreased (0.588867 --> 0.588449).  Saving model ...
Validation loss decreased (0.588449 --> 0.588098).  Saving model ...
Validation loss decreased (0.588098 --> 0.587644).  Saving model ...
Validation loss decreased (0.587644 --> 0.586917).  Saving model ...
Validation loss decreased (0.586917 --> 0.586069).  Saving model ...
Validation loss decreased (0.586069 --> 0.585288).  Saving model ...
Validation loss decreased (0.585288 --> 0.584594).  Saving model ...
Validation loss decreased (0.584594 --> 0.583795).  Saving model ...
Validation loss decreased (0.583795 --> 0.582944).  Saving model ...
Validation loss decreased (0.582944 --> 0.582030).  Saving model ...
Validation loss decreased (0.582030 --> 0.580951).  Saving model ...
Validation loss decreased (0.580951 --> 0.579898).  Saving model ...
Validation loss decreased (0.579898 --> 0.578893).  Saving model ...
Validation loss decreased (0.578893 --> 0.577836).  Saving model ...
Validation loss decreased (0.577836 --> 0.576966).  Saving model ...
Validation loss decreased (0.576966 --> 0.576233).  Saving model ...
Validation loss decreased (0.576233 --> 0.575545).  Saving model ...
Validation loss decreased (0.575545 --> 0.574853).  Saving model ...
Validation loss decreased (0.574853 --> 0.574192).  Saving model ...
Validation loss decreased (0.574192 --> 0.573600).  Saving model ...
Validation loss decreased (0.573600 --> 0.572740).  Saving model ...
Validation loss decreased (0.572740 --> 0.571886).  Saving model ...
Validation loss decreased (0.571886 --> 0.571154).  Saving model ...
Validation loss decreased (0.571154 --> 0.570354).  Saving model ...
Validation loss decreased (0.570354 --> 0.569472).  Saving model ...
Validation loss decreased (0.569472 --> 0.568771).  Saving model ...
Validation loss decreased (0.568771 --> 0.567887).  Saving model ...
Validation loss decreased (0.567887 --> 0.566949).  Saving model ...
Validation loss decreased (0.566949 --> 0.566014).  Saving model ...
Validation loss decreased (0.566014 --> 0.565134).  Saving model ...
Validation loss decreased (0.565134 --> 0.564385).  Saving model ...
Validation loss decreased (0.564385 --> 0.563499).  Saving model ...
Validation loss decreased (0.563499 --> 0.562307).  Saving model ...
Validation loss decreased (0.562307 --> 0.561107).  Saving model ...
Validation loss decreased (0.561107 --> 0.560058).  Saving model ...
Validation loss decreased (0.560058 --> 0.559183).  Saving model ...
Validation loss decreased (0.559183 --> 0.558154).  Saving model ...
Validation loss decreased (0.558154 --> 0.557078).  Saving model ...
Validation loss decreased (0.557078 --> 0.556083).  Saving model ...
Validation loss decreased (0.556083 --> 0.555269).  Saving model ...
Validation loss decreased (0.555269 --> 0.554259).  Saving model ...
Validation loss decreased (0.554259 --> 0.553196).  Saving model ...
Validation loss decreased (0.553196 --> 0.552123).  Saving model ...
Validation loss decreased (0.552123 --> 0.551033).  Saving model ...
Validation loss decreased (0.551033 --> 0.550096).  Saving model ...
Validation loss decreased (0.550096 --> 0.548812).  Saving model ...
Validation loss decreased (0.548812 --> 0.547567).  Saving model ...
Validation loss decreased (0.547567 --> 0.546458).  Saving model ...
Validation loss decreased (0.546458 --> 0.545187).  Saving model ...
Validation loss decreased (0.545187 --> 0.544007).  Saving model ...
Validation loss decreased (0.544007 --> 0.543135).  Saving model ...
Validation loss decreased (0.543135 --> 0.542237).  Saving model ...
Validation loss decreased (0.542237 --> 0.541376).  Saving model ...
Validation loss decreased (0.541376 --> 0.540567).  Saving model ...
Validation loss decreased (0.540567 --> 0.539767).  Saving model ...
Validation loss decreased (0.539767 --> 0.539042).  Saving model ...
Validation loss decreased (0.539042 --> 0.538194).  Saving model ...
Validation loss decreased (0.538194 --> 0.537369).  Saving model ...
Validation loss decreased (0.537369 --> 0.536522).  Saving model ...
Validation loss decreased (0.536522 --> 0.535519).  Saving model ...
Validation loss decreased (0.535519 --> 0.534503).  Saving model ...
Validation loss decreased (0.534503 --> 0.533542).  Saving model ...
Validation loss decreased (0.533542 --> 0.532578).  Saving model ...
Validation loss decreased (0.532578 --> 0.531568).  Saving model ...
Validation loss decreased (0.531568 --> 0.530532).  Saving model ...
Validation loss decreased (0.530532 --> 0.529726).  Saving model ...
Validation loss decreased (0.529726 --> 0.529113).  Saving model ...
Validation loss decreased (0.529113 --> 0.528603).  Saving model ...
Validation loss decreased (0.528603 --> 0.527999).  Saving model ...
Validation loss decreased (0.527999 --> 0.527402).  Saving model ...
Validation loss decreased (0.527402 --> 0.526869).  Saving model ...
epoch 201, loss 0.4833, train acc 72.00%, f1 0.7200, precision 0.7200, recall 0.7200, auc 0.7200
Validation loss decreased (0.526869 --> 0.526324).  Saving model ...
Validation loss decreased (0.526324 --> 0.525803).  Saving model ...
Validation loss decreased (0.525803 --> 0.525291).  Saving model ...
Validation loss decreased (0.525291 --> 0.524632).  Saving model ...
Validation loss decreased (0.524632 --> 0.524065).  Saving model ...
Validation loss decreased (0.524065 --> 0.523489).  Saving model ...
Validation loss decreased (0.523489 --> 0.522980).  Saving model ...
Validation loss decreased (0.522980 --> 0.522196).  Saving model ...
Validation loss decreased (0.522196 --> 0.521640).  Saving model ...
Validation loss decreased (0.521640 --> 0.520945).  Saving model ...
Validation loss decreased (0.520945 --> 0.520249).  Saving model ...
Validation loss decreased (0.520249 --> 0.519567).  Saving model ...
Validation loss decreased (0.519567 --> 0.518882).  Saving model ...
Validation loss decreased (0.518882 --> 0.518258).  Saving model ...
Validation loss decreased (0.518258 --> 0.517746).  Saving model ...
Validation loss decreased (0.517746 --> 0.517103).  Saving model ...
Validation loss decreased (0.517103 --> 0.516541).  Saving model ...
Validation loss decreased (0.516541 --> 0.515884).  Saving model ...
Validation loss decreased (0.515884 --> 0.515285).  Saving model ...
Validation loss decreased (0.515285 --> 0.514742).  Saving model ...
Validation loss decreased (0.514742 --> 0.514161).  Saving model ...
Validation loss decreased (0.514161 --> 0.513644).  Saving model ...
Validation loss decreased (0.513644 --> 0.513058).  Saving model ...
Validation loss decreased (0.513058 --> 0.512444).  Saving model ...
Validation loss decreased (0.512444 --> 0.511727).  Saving model ...
Validation loss decreased (0.511727 --> 0.510965).  Saving model ...
Validation loss decreased (0.510965 --> 0.510249).  Saving model ...
Validation loss decreased (0.510249 --> 0.509425).  Saving model ...
Validation loss decreased (0.509425 --> 0.508795).  Saving model ...
Validation loss decreased (0.508795 --> 0.508121).  Saving model ...
Validation loss decreased (0.508121 --> 0.507394).  Saving model ...
Validation loss decreased (0.507394 --> 0.506796).  Saving model ...
Validation loss decreased (0.506796 --> 0.505971).  Saving model ...
Validation loss decreased (0.505971 --> 0.505110).  Saving model ...
Validation loss decreased (0.505110 --> 0.504450).  Saving model ...
Validation loss decreased (0.504450 --> 0.503783).  Saving model ...
Validation loss decreased (0.503783 --> 0.503097).  Saving model ...
Validation loss decreased (0.503097 --> 0.502481).  Saving model ...
Validation loss decreased (0.502481 --> 0.502015).  Saving model ...
Validation loss decreased (0.502015 --> 0.501455).  Saving model ...
Validation loss decreased (0.501455 --> 0.500805).  Saving model ...
Validation loss decreased (0.500805 --> 0.500055).  Saving model ...
Validation loss decreased (0.500055 --> 0.499325).  Saving model ...
Validation loss decreased (0.499325 --> 0.498231).  Saving model ...
Validation loss decreased (0.498231 --> 0.497323).  Saving model ...
Validation loss decreased (0.497323 --> 0.496527).  Saving model ...
Validation loss decreased (0.496527 --> 0.495628).  Saving model ...
Validation loss decreased (0.495628 --> 0.494593).  Saving model ...
Validation loss decreased (0.494593 --> 0.493465).  Saving model ...
Validation loss decreased (0.493465 --> 0.492296).  Saving model ...
Validation loss decreased (0.492296 --> 0.491241).  Saving model ...
Validation loss decreased (0.491241 --> 0.490234).  Saving model ...
Validation loss decreased (0.490234 --> 0.489376).  Saving model ...
Validation loss decreased (0.489376 --> 0.488819).  Saving model ...
Validation loss decreased (0.488819 --> 0.488296).  Saving model ...
Validation loss decreased (0.488296 --> 0.488045).  Saving model ...
Validation loss decreased (0.488045 --> 0.487646).  Saving model ...
Validation loss decreased (0.487646 --> 0.487330).  Saving model ...
Validation loss decreased (0.487330 --> 0.486839).  Saving model ...
Validation loss decreased (0.486839 --> 0.486210).  Saving model ...
Validation loss decreased (0.486210 --> 0.485379).  Saving model ...
Validation loss decreased (0.485379 --> 0.484801).  Saving model ...
Validation loss decreased (0.484801 --> 0.484225).  Saving model ...
Validation loss decreased (0.484225 --> 0.483647).  Saving model ...
Validation loss decreased (0.483647 --> 0.483062).  Saving model ...
Validation loss decreased (0.483062 --> 0.482246).  Saving model ...
Validation loss decreased (0.482246 --> 0.481500).  Saving model ...
Validation loss decreased (0.481500 --> 0.480830).  Saving model ...
Validation loss decreased (0.480830 --> 0.480083).  Saving model ...
Validation loss decreased (0.480083 --> 0.479407).  Saving model ...
Validation loss decreased (0.479407 --> 0.478968).  Saving model ...
Validation loss decreased (0.478968 --> 0.478733).  Saving model ...
Validation loss decreased (0.478733 --> 0.478165).  Saving model ...
Validation loss decreased (0.478165 --> 0.477683).  Saving model ...
Validation loss decreased (0.477683 --> 0.477086).  Saving model ...
Validation loss decreased (0.477086 --> 0.476325).  Saving model ...
Validation loss decreased (0.476325 --> 0.475715).  Saving model ...
Validation loss decreased (0.475715 --> 0.475166).  Saving model ...
Validation loss decreased (0.475166 --> 0.474528).  Saving model ...
Validation loss decreased (0.474528 --> 0.473815).  Saving model ...
Validation loss decreased (0.473815 --> 0.473173).  Saving model ...
Validation loss decreased (0.473173 --> 0.472498).  Saving model ...
Validation loss decreased (0.472498 --> 0.471746).  Saving model ...
Validation loss decreased (0.471746 --> 0.471094).  Saving model ...
Validation loss decreased (0.471094 --> 0.470576).  Saving model ...
Validation loss decreased (0.470576 --> 0.470243).  Saving model ...
Validation loss decreased (0.470243 --> 0.469895).  Saving model ...
Validation loss decreased (0.469895 --> 0.469485).  Saving model ...
Validation loss decreased (0.469485 --> 0.469116).  Saving model ...
Validation loss decreased (0.469116 --> 0.468664).  Saving model ...
Validation loss decreased (0.468664 --> 0.468269).  Saving model ...
Validation loss decreased (0.468269 --> 0.467684).  Saving model ...
Validation loss decreased (0.467684 --> 0.466843).  Saving model ...
Validation loss decreased (0.466843 --> 0.466002).  Saving model ...
Validation loss decreased (0.466002 --> 0.465316).  Saving model ...
Validation loss decreased (0.465316 --> 0.464692).  Saving model ...
Validation loss decreased (0.464692 --> 0.464110).  Saving model ...
Validation loss decreased (0.464110 --> 0.463603).  Saving model ...
Validation loss decreased (0.463603 --> 0.462920).  Saving model ...
Validation loss decreased (0.462920 --> 0.462262).  Saving model ...
epoch 301, loss 0.4256, train acc 76.00%, f1 0.7600, precision 0.7600, recall 0.7600, auc 0.7600
Validation loss decreased (0.462262 --> 0.461754).  Saving model ...
Validation loss decreased (0.461754 --> 0.461215).  Saving model ...
Validation loss decreased (0.461215 --> 0.460713).  Saving model ...
Validation loss decreased (0.460713 --> 0.460230).  Saving model ...
Validation loss decreased (0.460230 --> 0.459733).  Saving model ...
Validation loss decreased (0.459733 --> 0.459223).  Saving model ...
Validation loss decreased (0.459223 --> 0.458863).  Saving model ...
Validation loss decreased (0.458863 --> 0.458528).  Saving model ...
Validation loss decreased (0.458528 --> 0.458225).  Saving model ...
Validation loss decreased (0.458225 --> 0.457686).  Saving model ...
Validation loss decreased (0.457686 --> 0.457174).  Saving model ...
Validation loss decreased (0.457174 --> 0.456769).  Saving model ...
Validation loss decreased (0.456769 --> 0.456339).  Saving model ...
Validation loss decreased (0.456339 --> 0.455755).  Saving model ...
Validation loss decreased (0.455755 --> 0.455076).  Saving model ...
Validation loss decreased (0.455076 --> 0.454558).  Saving model ...
Validation loss decreased (0.454558 --> 0.454030).  Saving model ...
Validation loss decreased (0.454030 --> 0.453548).  Saving model ...
Validation loss decreased (0.453548 --> 0.453129).  Saving model ...
Validation loss decreased (0.453129 --> 0.452760).  Saving model ...
Validation loss decreased (0.452760 --> 0.452486).  Saving model ...
Validation loss decreased (0.452486 --> 0.452125).  Saving model ...
EarlyStopping counter: 1 out of 20
Validation loss decreased (0.452125 --> 0.452000).  Saving model ...
Validation loss decreased (0.452000 --> 0.451862).  Saving model ...
Validation loss decreased (0.451862 --> 0.451670).  Saving model ...
Validation loss decreased (0.451670 --> 0.451445).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
Validation loss decreased (0.451445 --> 0.451203).  Saving model ...
Validation loss decreased (0.451203 --> 0.450844).  Saving model ...
Validation loss decreased (0.450844 --> 0.450467).  Saving model ...
Validation loss decreased (0.450467 --> 0.450311).  Saving model ...
Validation loss decreased (0.450311 --> 0.450004).  Saving model ...
Validation loss decreased (0.450004 --> 0.449848).  Saving model ...
Validation loss decreased (0.449848 --> 0.449755).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
Validation loss decreased (0.449755 --> 0.449583).  Saving model .../home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

Validation loss decreased (0.449583 --> 0.448974).  Saving model ...
Validation loss decreased (0.448974 --> 0.448424).  Saving model ...
Validation loss decreased (0.448424 --> 0.448009).  Saving model ...
Validation loss decreased (0.448009 --> 0.447435).  Saving model ...
Validation loss decreased (0.447435 --> 0.446647).  Saving model ...
Validation loss decreased (0.446647 --> 0.445786).  Saving model ...
Validation loss decreased (0.445786 --> 0.444901).  Saving model ...
Validation loss decreased (0.444901 --> 0.444013).  Saving model ...
Validation loss decreased (0.444013 --> 0.443224).  Saving model ...
Validation loss decreased (0.443224 --> 0.442545).  Saving model ...
Validation loss decreased (0.442545 --> 0.442110).  Saving model ...
Validation loss decreased (0.442110 --> 0.441655).  Saving model ...
Validation loss decreased (0.441655 --> 0.441409).  Saving model ...
Validation loss decreased (0.441409 --> 0.441036).  Saving model ...
Validation loss decreased (0.441036 --> 0.440468).  Saving model ...
Validation loss decreased (0.440468 --> 0.439923).  Saving model ...
Validation loss decreased (0.439923 --> 0.439364).  Saving model ...
Validation loss decreased (0.439364 --> 0.438745).  Saving model ...
Validation loss decreased (0.438745 --> 0.438379).  Saving model ...
Validation loss decreased (0.438379 --> 0.437964).  Saving model ...
Validation loss decreased (0.437964 --> 0.437451).  Saving model ...
Validation loss decreased (0.437451 --> 0.436905).  Saving model ...
Validation loss decreased (0.436905 --> 0.436771).  Saving model ...
Validation loss decreased (0.436771 --> 0.436601).  Saving model ...
Validation loss decreased (0.436601 --> 0.436267).  Saving model ...
Validation loss decreased (0.436267 --> 0.435830).  Saving model ...
Validation loss decreased (0.435830 --> 0.435424).  Saving model ...
Validation loss decreased (0.435424 --> 0.434910).  Saving model ...
Validation loss decreased (0.434910 --> 0.434247).  Saving model ...
Validation loss decreased (0.434247 --> 0.433780).  Saving model ...
Validation loss decreased (0.433780 --> 0.433299).  Saving model ...
Validation loss decreased (0.433299 --> 0.432775).  Saving model ...
Validation loss decreased (0.432775 --> 0.432225).  Saving model ...
Validation loss decreased (0.432225 --> 0.431620).  Saving model ...
Validation loss decreased (0.431620 --> 0.431297).  Saving model ...
epoch 401, loss 0.3959, train acc 76.50%, f1 0.7650, precision 0.7650, recall 0.7650, auc 0.7650
Validation loss decreased (0.431297 --> 0.431110).  Saving model ...
Validation loss decreased (0.431110 --> 0.430923).  Saving model ...
Validation loss decreased (0.430923 --> 0.430763).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 423, loss 0.3432, train acc 75.50%, f1 0.7550, precision 0.7550, recall 0.7550, auc 0.7550



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_Mirror_True
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_3
./test_pima/result_MLP_concat_Mirror_True_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.605

the Fscore is 0.5775401069518716

the precision is 0.40601503759398494

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_3
----------------------



epoch 1, loss 0.6932, train acc 53.11%, f1 0.6764, precision 0.5164, recall 0.9804, auc 0.5311
epoch 101, loss 0.5291, train acc 78.10%, f1 0.7807, precision 0.7820, recall 0.7793, auc 0.7810
epoch 201, loss 0.4347, train acc 81.11%, f1 0.8111, precision 0.8111, recall 0.8112, auc 0.8111
epoch 301, loss 0.4038, train acc 82.52%, f1 0.8252, precision 0.8253, recall 0.8252, auc 0.8252
epoch 401, loss 0.3175, train acc 82.92%, f1 0.8292, precision 0.8291, recall 0.8293, auc 0.8292
epoch 501, loss 0.4907, train acc 83.01%, f1 0.8301, precision 0.8299, recall 0.8304, auc 0.8301
epoch 601, loss 0.3417, train acc 83.11%, f1 0.8312, precision 0.8308, recall 0.8316, auc 0.8311
epoch 701, loss 0.4141, train acc 83.13%, f1 0.8314, precision 0.8312, recall 0.8315, auc 0.8313
epoch 801, loss 0.4235, train acc 83.08%, f1 0.8308, precision 0.8307, recall 0.8308, auc 0.8308
epoch 901, loss 0.4914, train acc 83.12%, f1 0.8312, precision 0.8313, recall 0.8312, auc 0.8312
epoch 1001, loss 0.2988, train acc 83.11%, f1 0.8312, precision 0.8310, recall 0.8314, auc 0.8311
epoch 1101, loss 0.3049, train acc 83.15%, f1 0.8316, precision 0.8314, recall 0.8317, auc 0.8315
epoch 1201, loss 0.3851, train acc 83.11%, f1 0.8311, precision 0.8310, recall 0.8312, auc 0.8311
epoch 1301, loss 0.4851, train acc 83.14%, f1 0.8315, precision 0.8311, recall 0.8319, auc 0.8314
epoch 1401, loss 0.3652, train acc 83.16%, f1 0.8316, precision 0.8315, recall 0.8318, auc 0.8316
epoch 1501, loss 0.4516, train acc 83.18%, f1 0.8319, precision 0.8316, recall 0.8321, auc 0.8318
epoch 1601, loss 0.3738, train acc 83.18%, f1 0.8318, precision 0.8314, recall 0.8323, auc 0.8318
epoch 1701, loss 0.3103, train acc 83.16%, f1 0.8317, precision 0.8312, recall 0.8322, auc 0.8316
epoch 1801, loss 0.4364, train acc 83.17%, f1 0.8318, precision 0.8314, recall 0.8322, auc 0.8317
epoch 1901, loss 0.4610, train acc 83.20%, f1 0.8321, precision 0.8316, recall 0.8327, auc 0.8320
epoch 2001, loss 0.4010, train acc 83.15%, f1 0.8317, precision 0.8309, recall 0.8324, auc 0.8315
epoch 2101, loss 0.3058, train acc 83.19%, f1 0.8320, precision 0.8315, recall 0.8325, auc 0.8319
epoch 2201, loss 0.3416, train acc 83.24%, f1 0.8325, precision 0.8319, recall 0.8331, auc 0.8324
epoch 2301, loss 0.3287, train acc 83.32%, f1 0.8333, precision 0.8328, recall 0.8338, auc 0.8332
epoch 2401, loss 0.4517, train acc 83.36%, f1 0.8337, precision 0.8333, recall 0.8341, auc 0.8336
epoch 2501, loss 0.4246, train acc 83.44%, f1 0.8344, precision 0.8340, recall 0.8348, auc 0.8344
epoch 2601, loss 0.3229, train acc 83.51%, f1 0.8352, precision 0.8348, recall 0.8355, auc 0.8351
epoch 2701, loss 0.3419, train acc 83.65%, f1 0.8365, precision 0.8368, recall 0.8361, auc 0.8365
epoch 2801, loss 0.2623, train acc 83.67%, f1 0.8366, precision 0.8368, recall 0.8364, auc 0.8367
epoch 2901, loss 0.3407, train acc 83.89%, f1 0.8389, precision 0.8388, recall 0.8390, auc 0.8389
epoch 3001, loss 0.4210, train acc 84.02%, f1 0.8402, precision 0.8403, recall 0.8401, auc 0.8402
epoch 3101, loss 0.3559, train acc 84.23%, f1 0.8424, precision 0.8415, recall 0.8434, auc 0.8423
epoch 3201, loss 0.2885, train acc 84.24%, f1 0.8425, precision 0.8421, recall 0.8430, auc 0.8424
epoch 3301, loss 0.3491, train acc 84.39%, f1 0.8439, precision 0.8441, recall 0.8437, auc 0.8439
epoch 3401, loss 0.3110, train acc 84.58%, f1 0.8457, precision 0.8458, recall 0.8457, auc 0.8458
epoch 3501, loss 0.3576, train acc 84.75%, f1 0.8474, precision 0.8478, recall 0.8470, auc 0.8475
epoch 3601, loss 0.3149, train acc 84.89%, f1 0.8488, precision 0.8491, recall 0.8486, auc 0.8489
epoch 3701, loss 0.3002, train acc 84.99%, f1 0.8498, precision 0.8500, recall 0.8497, auc 0.8499
epoch 3801, loss 0.3769, train acc 85.06%, f1 0.8508, precision 0.8499, recall 0.8517, auc 0.8506
epoch 3901, loss 0.3271, train acc 85.11%, f1 0.8512, precision 0.8506, recall 0.8518, auc 0.8511
epoch 4001, loss 0.3281, train acc 85.19%, f1 0.8520, precision 0.8517, recall 0.8523, auc 0.8519
epoch 4101, loss 0.3204, train acc 85.31%, f1 0.8533, precision 0.8523, recall 0.8543, auc 0.8531
epoch 4201, loss 0.2973, train acc 85.43%, f1 0.8543, precision 0.8543, recall 0.8543, auc 0.8543
epoch 4301, loss 0.5107, train acc 85.51%, f1 0.8552, precision 0.8548, recall 0.8555, auc 0.8551
epoch 4401, loss 0.4072, train acc 85.59%, f1 0.8561, precision 0.8547, recall 0.8575, auc 0.8559
epoch 4501, loss 0.3398, train acc 85.68%, f1 0.8570, precision 0.8562, recall 0.8577, auc 0.8568
epoch 4601, loss 0.2594, train acc 85.75%, f1 0.8576, precision 0.8571, recall 0.8581, auc 0.8575
epoch 4701, loss 0.2379, train acc 85.82%, f1 0.8583, precision 0.8576, recall 0.8591, auc 0.8582
epoch 4801, loss 0.3108, train acc 85.86%, f1 0.8587, precision 0.8581, recall 0.8594, auc 0.8586
epoch 4901, loss 0.3086, train acc 85.94%, f1 0.8596, precision 0.8583, recall 0.8609, auc 0.8594
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_3
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.615

the Fscore is 0.5837837837837838

the precision is 0.4122137404580153

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_3
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0004, precision 0.5517, recall 0.0002, auc 0.5000
epoch 101, loss 0.5652, train acc 77.57%, f1 0.7779, precision 0.7704, recall 0.7856, auc 0.7757
epoch 201, loss 0.4443, train acc 80.94%, f1 0.8092, precision 0.8097, recall 0.8087, auc 0.8094
epoch 301, loss 0.3678, train acc 82.50%, f1 0.8246, precision 0.8262, recall 0.8230, auc 0.8250
epoch 401, loss 0.4226, train acc 82.92%, f1 0.8289, precision 0.8306, recall 0.8272, auc 0.8292
epoch 501, loss 0.3434, train acc 83.07%, f1 0.8302, precision 0.8326, recall 0.8277, auc 0.8307
epoch 601, loss 0.4003, train acc 83.04%, f1 0.8299, precision 0.8320, recall 0.8280, auc 0.8304
epoch 701, loss 0.4675, train acc 83.06%, f1 0.8301, precision 0.8323, recall 0.8279, auc 0.8306
epoch 801, loss 0.3893, train acc 83.05%, f1 0.8301, precision 0.8319, recall 0.8284, auc 0.8305
epoch 901, loss 0.2899, train acc 83.07%, f1 0.8304, precision 0.8320, recall 0.8288, auc 0.8307
epoch 1001, loss 0.4183, train acc 83.06%, f1 0.8304, precision 0.8317, recall 0.8290, auc 0.8306
epoch 1101, loss 0.5193, train acc 83.10%, f1 0.8308, precision 0.8321, recall 0.8294, auc 0.8310
epoch 1201, loss 0.4590, train acc 83.11%, f1 0.8309, precision 0.8318, recall 0.8300, auc 0.8311
epoch 1301, loss 0.4128, train acc 83.11%, f1 0.8309, precision 0.8318, recall 0.8299, auc 0.8311
epoch 1401, loss 0.2625, train acc 83.03%, f1 0.8303, precision 0.8305, recall 0.8301, auc 0.8303
epoch 1501, loss 0.3997, train acc 83.11%, f1 0.8309, precision 0.8321, recall 0.8297, auc 0.8311
epoch 1601, loss 0.4209, train acc 83.15%, f1 0.8313, precision 0.8323, recall 0.8303, auc 0.8315
epoch 1701, loss 0.3021, train acc 83.14%, f1 0.8313, precision 0.8319, recall 0.8307, auc 0.8314
epoch 1801, loss 0.4563, train acc 83.17%, f1 0.8315, precision 0.8322, recall 0.8309, auc 0.8317
epoch 1901, loss 0.3887, train acc 83.15%, f1 0.8314, precision 0.8320, recall 0.8308, auc 0.8315
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/standlization_data/pima_std_test_3.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_3
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.5607407407407408

the Fscore is 0.5492227979274612

the precision is 0.381294964028777

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_3.csv
./test_pima/model_MLP_concat_notMirror_True/record_1/MLP_concat_notMirror_True_3
----------------------



Traceback (most recent call last):
  File "./classifier_MLP/train_MLP.py", line 312, in <module>
    transformed_valid_data, transformed_valid_label = transform_data_to_train_form(transform_method, mirror_type, valid_pos_data, valid_neg_data)
  File "./classifier_MLP/train_MLP.py", line 192, in transform_data_to_train_form
    trans_pos_data, trans_pos_label, trans_neg_data, trans_neg_label = handleData_extend_not_mirror(positive_repeat_data, negetive_tile_data)
  File "./classifier_MLP/train_MLP.py", line 154, in handleData_extend_not_mirror
    transfrom_positive_data = transfrom_positive_data[positive_index[0]]
UnboundLocalError: local variable 'transfrom_positive_data' referenced before assignment
