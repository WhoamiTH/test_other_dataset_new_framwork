nohup: ignoring input
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
../1_year_data/glass0_train_5.csv
epoch 1, loss 11.5540, train acc 51.04%, f1 0.0000, precision 0.2552, recall 0.5000
epoch 101, loss 11.0863, train acc 57.08%, f1 0.5928, precision 0.5731, recall 0.5719
epoch 201, loss 8.6066, train acc 67.36%, f1 0.6669, precision 0.6736, recall 0.6735
epoch 301, loss 9.6403, train acc 66.68%, f1 0.6742, precision 0.6668, recall 0.6666
epoch 401, loss 9.2133, train acc 67.32%, f1 0.6785, precision 0.6742, recall 0.6738
epoch 501, loss 10.1296, train acc 63.40%, f1 0.6509, precision 0.6366, recall 0.6349
epoch 601, loss 9.0813, train acc 68.80%, f1 0.6907, precision 0.6880, recall 0.6880
epoch 701, loss 9.1952, train acc 66.08%, f1 0.6731, precision 0.6612, recall 0.6605
epoch 801, loss 9.0692, train acc 68.40%, f1 0.6781, precision 0.6839, recall 0.6839
epoch 901, loss 10.5553, train acc 62.84%, f1 0.6563, precision 0.6292, recall 0.6268
epoch 1001, loss 11.7595, train acc 59.04%, f1 0.6224, precision 0.5895, recall 0.5881
epoch 1101, loss 9.2621, train acc 68.32%, f1 0.6937, precision 0.6845, recall 0.6834
epoch 1201, loss 10.6067, train acc 61.88%, f1 0.6408, precision 0.6198, recall 0.6183
epoch 1301, loss 9.5713, train acc 65.20%, f1 0.6526, precision 0.6520, recall 0.6520
epoch 1401, loss 10.1862, train acc 60.60%, f1 0.6396, precision 0.6116, recall 0.6072
epoch 1501, loss 10.3599, train acc 63.60%, f1 0.6524, precision 0.6375, recall 0.6362
epoch 1601, loss 9.2651, train acc 68.00%, f1 0.6795, precision 0.6804, recall 0.6804
epoch 1701, loss 9.8922, train acc 65.16%, f1 0.6556, precision 0.6517, recall 0.6516
epoch 1801, loss 9.8885, train acc 66.00%, f1 0.6785, precision 0.6607, recall 0.6592
epoch 1901, loss 8.8202, train acc 68.64%, f1 0.6795, precision 0.6865, recall 0.6863
epoch 2001, loss 8.9526, train acc 68.00%, f1 0.6779, precision 0.6800, recall 0.6800
epoch 2101, loss 10.7287, train acc 60.64%, f1 0.6210, precision 0.6073, recall 0.6066
epoch 2201, loss 9.4430, train acc 65.84%, f1 0.6685, precision 0.6601, recall 0.6591
epoch 2301, loss 9.4208, train acc 66.52%, f1 0.6648, precision 0.6656, recall 0.6657
epoch 2401, loss 10.4563, train acc 63.44%, f1 0.6506, precision 0.6359, recall 0.6346
epoch 2501, loss 10.0995, train acc 64.36%, f1 0.6482, precision 0.6435, recall 0.6435
epoch 2601, loss 10.1214, train acc 63.32%, f1 0.6491, precision 0.6346, recall 0.6334
epoch 2701, loss 9.3405, train acc 67.52%, f1 0.6661, precision 0.6760, recall 0.6753
epoch 2801, loss 8.7767, train acc 69.12%, f1 0.6934, precision 0.6912, recall 0.6912
epoch 2901, loss 10.2132, train acc 63.84%, f1 0.6510, precision 0.6406, recall 0.6395
epoch 3001, loss 10.6774, train acc 60.80%, f1 0.6239, precision 0.6113, recall 0.6099
epoch 3101, loss 8.7646, train acc 66.44%, f1 0.6747, precision 0.6652, recall 0.6645
epoch 3201, loss 9.4470, train acc 65.08%, f1 0.6589, precision 0.6523, recall 0.6516
epoch 3301, loss 9.6532, train acc 62.04%, f1 0.6218, precision 0.6209, recall 0.6208
epoch 3401, loss 9.3291, train acc 66.72%, f1 0.6568, precision 0.6679, recall 0.6673
epoch 3501, loss 10.6009, train acc 62.12%, f1 0.6491, precision 0.6232, recall 0.6205
epoch 3601, loss 10.0550, train acc 64.08%, f1 0.6511, precision 0.6411, recall 0.6407
epoch 3701, loss 10.4163, train acc 62.68%, f1 0.6443, precision 0.6304, recall 0.6285
epoch 3801, loss 9.5838, train acc 66.12%, f1 0.6581, precision 0.6614, recall 0.6613
epoch 3901, loss 10.0523, train acc 64.92%, f1 0.6610, precision 0.6494, recall 0.6489
epoch 4001, loss 11.4476, train acc 59.32%, f1 0.6192, precision 0.5955, recall 0.5936
epoch 4101, loss 9.4262, train acc 68.36%, f1 0.6914, precision 0.6835, recall 0.6834
epoch 4201, loss 10.6558, train acc 63.36%, f1 0.6585, precision 0.6337, recall 0.6320
epoch 4301, loss 8.7536, train acc 68.08%, f1 0.6888, precision 0.6843, recall 0.6830
epoch 4401, loss 10.2581, train acc 63.72%, f1 0.6607, precision 0.6382, recall 0.6362
epoch 4501, loss 10.2359, train acc 63.32%, f1 0.6419, precision 0.6334, recall 0.6331
epoch 4601, loss 9.1430, train acc 68.28%, f1 0.6804, precision 0.6831, recall 0.6830
epoch 4701, loss 11.1831, train acc 59.88%, f1 0.6090, precision 0.5999, recall 0.5994
epoch 4801, loss 9.2205, train acc 66.56%, f1 0.6664, precision 0.6667, recall 0.6667
epoch 4901, loss 10.7230, train acc 64.08%, f1 0.6573, precision 0.6400, recall 0.6400
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
../1_year_data/glass0_train_5.csv
../1_year_data/glass0_test_origin_5.csv
../1_year_result/model_1/change_loss_model_5/my_model.pkl
(172, 9)
<class 'pandas.core.frame.DataFrame'>
train
(42, 9)
<class 'pandas.core.frame.DataFrame'>
accuracy is 0.5952380952380952
Precision is 0.5952380952380952
Recall is 0.6071428571428572
F1 is 0.5142857142857143
