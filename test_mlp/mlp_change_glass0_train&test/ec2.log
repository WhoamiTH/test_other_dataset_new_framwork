nohup: ignoring input
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
../1_year_data/glass0_train_2.csv
epoch 1, loss 0.6908, train acc 58.00%, f1 0.7342, precision 0.2900, recall 0.5000
epoch 101, loss 0.4418, train acc 76.00%, f1 0.7391, precision 0.7585, recall 0.7585
epoch 201, loss 0.2335, train acc 92.00%, f1 0.9375, precision 0.9412, recall 0.9000
epoch 301, loss 0.1117, train acc 96.00%, f1 0.9655, precision 0.9589, recall 0.9589
epoch 401, loss 0.0515, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 501, loss 0.1758, train acc 94.00%, f1 0.9474, precision 0.9417, recall 0.9367
epoch 601, loss 0.0306, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 701, loss 0.0528, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 801, loss 0.0208, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 901, loss 0.0251, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 1001, loss 0.0169, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 1101, loss 0.0092, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 1201, loss 0.0253, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 1301, loss 0.0558, train acc 98.00%, f1 0.9744, precision 0.9750, recall 0.9839
epoch 1401, loss 0.0451, train acc 98.00%, f1 0.9818, precision 0.9821, recall 0.9783
epoch 1501, loss 0.0120, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 1601, loss 0.0102, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 1701, loss 0.0113, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 1801, loss 0.0192, train acc 98.00%, f1 0.9804, precision 0.9800, recall 0.9808
epoch 1901, loss 0.0142, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2001, loss 0.0040, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2101, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2201, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2301, loss 0.0038, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2401, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2501, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2601, loss 0.0055, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2701, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 2901, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3001, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3101, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3201, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3301, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3401, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3501, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3601, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3701, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3801, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 3901, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4001, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4101, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4201, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4401, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4501, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4601, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4701, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4801, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
epoch 4901, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000
1.0 0.997497022151947
0.0 0.0019038824830204248
0.0 4.701592587474934e-11
1.0 0.9999182224273682
0.0 8.60250499307158e-12
1.0 0.9965344667434692
1.0 0.9999990463256836
0.0 1.3872821114091494e-08
0.0 9.515332931187004e-05
0.0 2.1430503322531405e-22
0.0 2.734982311697158e-14
1.0 0.9945725798606873
1.0 0.9999692440032959
0.0 1.471812096853888e-21
0.0 0.008716735057532787
1.0 0.9990399479866028
1.0 0.9990363121032715
1.0 0.9982636570930481
1.0 1.0
1.0 0.9994844198226929
1.0 0.9984627962112427
1.0 0.9980151653289795
1.0 0.999777615070343
1.0 0.9982884526252747
0.0 8.764128516485353e-20
1.0 0.9999902248382568
1.0 0.9995648264884949
0.0 0.0025302048306912184
0.0 0.0003338645619805902
0.0 3.982952205897061e-14
1.0 0.9999992847442627
0.0 0.004521801136434078
1.0 0.9996651411056519
0.0 1.8175499219555455e-28
0.0 0.006213475950062275
1.0 0.9997397065162659
1.0 1.0
0.0 0.0003431141667533666
0.0 0.0013544685207307339
0.0 8.529910387336403e-20
1.0 1.0
0.0 8.370626164833084e-05
1.0 0.9995695948600769
0.0 9.586115339520343e-21
1.0 0.9999988079071045
1.0 1.0
0.0 2.4567837536930837e-23
1.0 0.9999998807907104
0.0 6.459297680061482e-11
1.0 0.99932861328125
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=DeprecationWarning)
../1_year_data/glass0_train_2.csv
../1_year_data/glass0_test_origin_2.csv
../1_year_result/model_1/change_loss_model_2/my_model.pkl
(171, 9)
<class 'pandas.core.frame.DataFrame'>
train
(43, 9)
<class 'pandas.core.frame.DataFrame'>
1.0 0.7308911681175232
tensor([0.7309], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.7310585975646973
tensor([0.7311], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.7309972047805786
tensor([0.7310], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.7310585975646973
tensor([0.7311], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.731057345867157
tensor([0.7311], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.7310585975646973
tensor([0.7311], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.5003713965415955
tensor([0.5004], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.6310670971870422
tensor([0.6311], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.7310585975646973
tensor([0.7311], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.727037250995636
tensor([0.7270], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.5000066757202148
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.6557427048683167
tensor([0.6557], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5672533512115479
tensor([0.5673], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.7306320071220398
tensor([0.7306], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.7218313217163086
tensor([0.7218], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.619706928730011
tensor([0.6197], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5000007152557373
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.7310065031051636
tensor([0.7310], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5000002980232239
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
1.0 0.5000048875808716
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5014815926551819
tensor([0.5015], device='cuda:0', grad_fn=<SelectBackward>)
0.0 0.5
tensor([0.5000], device='cuda:0', grad_fn=<SelectBackward>)
1.0 1
1.0 1
1.0 1
1.0 1
1.0 1
1.0 1
1.0 1
1.0 1
1.0 1
1.0 1
1.0 1
1.0 1
1.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
0.0 1
1.0 1
0.0 1
0.0 1
accuracy is 0.32558139534883723
Precision is 0.32558139534883723
Recall is 1.0
F1 is 0.49122807017543857
