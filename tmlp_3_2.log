nohup: ignoring input
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_2
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
Validation loss decreased (inf --> 0.692950).  Saving model ...
Validation loss decreased (0.692950 --> 0.692770).  Saving model ...
Validation loss decreased (0.692770 --> 0.692603).  Saving model ...
Validation loss decreased (0.692603 --> 0.692423).  Saving model ...
Validation loss decreased (0.692423 --> 0.692220).  Saving model ...
Validation loss decreased (0.692220 --> 0.691992).  Saving model ...
Validation loss decreased (0.691992 --> 0.691745).  Saving model ...
Validation loss decreased (0.691745 --> 0.691477).  Saving model ...
Validation loss decreased (0.691477 --> 0.691181).  Saving model ...
Validation loss decreased (0.691181 --> 0.690854).  Saving model ...
Validation loss decreased (0.690854 --> 0.690495).  Saving model ...
Validation loss decreased (0.690495 --> 0.690103).  Saving model ...
Validation loss decreased (0.690103 --> 0.689670).  Saving model ...
Validation loss decreased (0.689670 --> 0.689210).  Saving model ...
Validation loss decreased (0.689210 --> 0.688705).  Saving model ...
Validation loss decreased (0.688705 --> 0.688164).  Saving model ...
Validation loss decreased (0.688164 --> 0.687588).  Saving model ...
Validation loss decreased (0.687588 --> 0.686978).  Saving model ...
Validation loss decreased (0.686978 --> 0.686310).  Saving model ...
Validation loss decreased (0.686310 --> 0.685593).  Saving model ...
Validation loss decreased (0.685593 --> 0.684829).  Saving model ...
Validation loss decreased (0.684829 --> 0.684020).  Saving model ...
Validation loss decreased (0.684020 --> 0.683175).  Saving model ...
Validation loss decreased (0.683175 --> 0.682279).  Saving model ...
Validation loss decreased (0.682279 --> 0.681336).  Saving model ...
Validation loss decreased (0.681336 --> 0.680332).  Saving model ...
Validation loss decreased (0.680332 --> 0.679271).  Saving model ...
Validation loss decreased (0.679271 --> 0.678163).  Saving model ...
Validation loss decreased (0.678163 --> 0.677005).  Saving model ...
Validation loss decreased (0.677005 --> 0.675805).  Saving model ...
Validation loss decreased (0.675805 --> 0.674564).  Saving model ...
Validation loss decreased (0.674564 --> 0.673260).  Saving model ...
Validation loss decreased (0.673260 --> 0.671904).  Saving model ...
Validation loss decreased (0.671904 --> 0.670496).  Saving model ...
Validation loss decreased (0.670496 --> 0.669053).  Saving model ...
Validation loss decreased (0.669053 --> 0.667567).  Saving model ...
Validation loss decreased (0.667567 --> 0.665995).  Saving model ...
Validation loss decreased (0.665995 --> 0.664355).  Saving model ...
Validation loss decreased (0.664355 --> 0.662665).  Saving model ...
Validation loss decreased (0.662665 --> 0.660925).  Saving model ...
Validation loss decreased (0.660925 --> 0.659096).  Saving model ...
Validation loss decreased (0.659096 --> 0.657200).  Saving model ...
Validation loss decreased (0.657200 --> 0.655241).  Saving model ...
Validation loss decreased (0.655241 --> 0.653275).  Saving model ...
Validation loss decreased (0.653275 --> 0.651233).  Saving model ...
Validation loss decreased (0.651233 --> 0.649167).  Saving model ...
Validation loss decreased (0.649167 --> 0.647019).  Saving model ...
Validation loss decreased (0.647019 --> 0.644810).  Saving model ...
Validation loss decreased (0.644810 --> 0.642569).  Saving model ...
Validation loss decreased (0.642569 --> 0.640285).  Saving model ...
Validation loss decreased (0.640285 --> 0.637934).  Saving model ...
Validation loss decreased (0.637934 --> 0.635570).  Saving model ...
Validation loss decreased (0.635570 --> 0.633144).  Saving model ...
Validation loss decreased (0.633144 --> 0.630656).  Saving model ...
Validation loss decreased (0.630656 --> 0.628119).  Saving model ...
Validation loss decreased (0.628119 --> 0.625626).  Saving model ...
Validation loss decreased (0.625626 --> 0.623138).  Saving model ...
Validation loss decreased (0.623138 --> 0.620657).  Saving model ...
Validation loss decreased (0.620657 --> 0.618101).  Saving model ...
Validation loss decreased (0.618101 --> 0.615502).  Saving model ...
Validation loss decreased (0.615502 --> 0.612889).  Saving model ...
Validation loss decreased (0.612889 --> 0.610191).  Saving model ...
Validation loss decreased (0.610191 --> 0.607453).  Saving model ...
Validation loss decreased (0.607453 --> 0.604705).  Saving model ...
Validation loss decreased (0.604705 --> 0.602057).  Saving model ...
Validation loss decreased (0.602057 --> 0.599353).  Saving model ...
Validation loss decreased (0.599353 --> 0.596734).  Saving model ...
Validation loss decreased (0.596734 --> 0.594085).  Saving model ...
Validation loss decreased (0.594085 --> 0.591402).  Saving model ...
Validation loss decreased (0.591402 --> 0.588685).  Saving model ...
Validation loss decreased (0.588685 --> 0.585993).  Saving model ...
Validation loss decreased (0.585993 --> 0.583321).  Saving model ...
Validation loss decreased (0.583321 --> 0.580665).  Saving model ...
Validation loss decreased (0.580665 --> 0.578065).  Saving model ...
Validation loss decreased (0.578065 --> 0.575488).  Saving model ...
Validation loss decreased (0.575488 --> 0.572847).  Saving model ...
Validation loss decreased (0.572847 --> 0.570159).  Saving model ...
Validation loss decreased (0.570159 --> 0.567486).  Saving model ...
Validation loss decreased (0.567486 --> 0.564775).  Saving model ...
Validation loss decreased (0.564775 --> 0.562123).  Saving model ...
Validation loss decreased (0.562123 --> 0.559461).  Saving model ...
Validation loss decreased (0.559461 --> 0.556737).  Saving model ...
Validation loss decreased (0.556737 --> 0.554062).  Saving model ...
Validation loss decreased (0.554062 --> 0.551378).  Saving model ...
Validation loss decreased (0.551378 --> 0.548897).  Saving model ...
Validation loss decreased (0.548897 --> 0.546454).  Saving model ...
Validation loss decreased (0.546454 --> 0.544010).  Saving model ...
Validation loss decreased (0.544010 --> 0.541624).  Saving model ...
Validation loss decreased (0.541624 --> 0.539230).  Saving model ...
Validation loss decreased (0.539230 --> 0.536829).  Saving model ...
Validation loss decreased (0.536829 --> 0.534416).  Saving model ...
Validation loss decreased (0.534416 --> 0.531957).  Saving model ...
Validation loss decreased (0.531957 --> 0.529471).  Saving model ...
Validation loss decreased (0.529471 --> 0.527001).  Saving model ...
Validation loss decreased (0.527001 --> 0.524521).  Saving model ...
Validation loss decreased (0.524521 --> 0.522060).  Saving model ...
Validation loss decreased (0.522060 --> 0.519676).  Saving model ...
Validation loss decreased (0.519676 --> 0.517330).  Saving model ...
Validation loss decreased (0.517330 --> 0.515030).  Saving model ...
Validation loss decreased (0.515030 --> 0.512759).  Saving model ...
epoch 101, loss 0.5728, train acc 77.50%, f1 0.7750, precision 0.7750, recall 0.7750, auc 0.7750
Validation loss decreased (0.512759 --> 0.510528).  Saving model ...
Validation loss decreased (0.510528 --> 0.508307).  Saving model ...
Validation loss decreased (0.508307 --> 0.506007).  Saving model ...
Validation loss decreased (0.506007 --> 0.503826).  Saving model ...
Validation loss decreased (0.503826 --> 0.501594).  Saving model ...
Validation loss decreased (0.501594 --> 0.499415).  Saving model ...
Validation loss decreased (0.499415 --> 0.497240).  Saving model ...
Validation loss decreased (0.497240 --> 0.495018).  Saving model ...
Validation loss decreased (0.495018 --> 0.492826).  Saving model ...
Validation loss decreased (0.492826 --> 0.490721).  Saving model ...
Validation loss decreased (0.490721 --> 0.488629).  Saving model ...
Validation loss decreased (0.488629 --> 0.486708).  Saving model ...
Validation loss decreased (0.486708 --> 0.484809).  Saving model ...
Validation loss decreased (0.484809 --> 0.482848).  Saving model ...
Validation loss decreased (0.482848 --> 0.480833).  Saving model ...
Validation loss decreased (0.480833 --> 0.478727).  Saving model ...
Validation loss decreased (0.478727 --> 0.476734).  Saving model ...
Validation loss decreased (0.476734 --> 0.474751).  Saving model ...
Validation loss decreased (0.474751 --> 0.472894).  Saving model ...
Validation loss decreased (0.472894 --> 0.470995).  Saving model ...
Validation loss decreased (0.470995 --> 0.469089).  Saving model ...
Validation loss decreased (0.469089 --> 0.467194).  Saving model ...
Validation loss decreased (0.467194 --> 0.465312).  Saving model ...
Validation loss decreased (0.465312 --> 0.463489).  Saving model ...
Validation loss decreased (0.463489 --> 0.461730).  Saving model ...
Validation loss decreased (0.461730 --> 0.460007).  Saving model ...
Validation loss decreased (0.460007 --> 0.458295).  Saving model ...
Validation loss decreased (0.458295 --> 0.456589).  Saving model ...
Validation loss decreased (0.456589 --> 0.454829).  Saving model ...
Validation loss decreased (0.454829 --> 0.453152).  Saving model ...
Validation loss decreased (0.453152 --> 0.451452).  Saving model ...
Validation loss decreased (0.451452 --> 0.449775).  Saving model ...
Validation loss decreased (0.449775 --> 0.448234).  Saving model ...
Validation loss decreased (0.448234 --> 0.446647).  Saving model ...
Validation loss decreased (0.446647 --> 0.445103).  Saving model ...
Validation loss decreased (0.445103 --> 0.443546).  Saving model ...
Validation loss decreased (0.443546 --> 0.441994).  Saving model ...
Validation loss decreased (0.441994 --> 0.440514).  Saving model ...
Validation loss decreased (0.440514 --> 0.438988).  Saving model ...
Validation loss decreased (0.438988 --> 0.437408).  Saving model ...
Validation loss decreased (0.437408 --> 0.435881).  Saving model ...
Validation loss decreased (0.435881 --> 0.434328).  Saving model ...
Validation loss decreased (0.434328 --> 0.432785).  Saving model ...
Validation loss decreased (0.432785 --> 0.431403).  Saving model ...
Validation loss decreased (0.431403 --> 0.430031).  Saving model ...
Validation loss decreased (0.430031 --> 0.428684).  Saving model ...
Validation loss decreased (0.428684 --> 0.427446).  Saving model ...
Validation loss decreased (0.427446 --> 0.426245).  Saving model ...
Validation loss decreased (0.426245 --> 0.424992).  Saving model ...
Validation loss decreased (0.424992 --> 0.423717).  Saving model ...
Validation loss decreased (0.423717 --> 0.422546).  Saving model ...
Validation loss decreased (0.422546 --> 0.421360).  Saving model ...
Validation loss decreased (0.421360 --> 0.420198).  Saving model ...
Validation loss decreased (0.420198 --> 0.419065).  Saving model ...
Validation loss decreased (0.419065 --> 0.417919).  Saving model ...
Validation loss decreased (0.417919 --> 0.416778).  Saving model ...
Validation loss decreased (0.416778 --> 0.415547).  Saving model ...
Validation loss decreased (0.415547 --> 0.414335).  Saving model ...
Validation loss decreased (0.414335 --> 0.413143).  Saving model ...
Validation loss decreased (0.413143 --> 0.411983).  Saving model ...
Validation loss decreased (0.411983 --> 0.410739).  Saving model ...
Validation loss decreased (0.410739 --> 0.409537).  Saving model ...
Validation loss decreased (0.409537 --> 0.408338).  Saving model ...
Validation loss decreased (0.408338 --> 0.407130).  Saving model ...
Validation loss decreased (0.407130 --> 0.405934).  Saving model ...
Validation loss decreased (0.405934 --> 0.404795).  Saving model ...
Validation loss decreased (0.404795 --> 0.403716).  Saving model ...
Validation loss decreased (0.403716 --> 0.402588).  Saving model ...
Validation loss decreased (0.402588 --> 0.401496).  Saving model ...
Validation loss decreased (0.401496 --> 0.400414).  Saving model ...
Validation loss decreased (0.400414 --> 0.399377).  Saving model ...
Validation loss decreased (0.399377 --> 0.398341).  Saving model ...
Validation loss decreased (0.398341 --> 0.397309).  Saving model ...
Validation loss decreased (0.397309 --> 0.396240).  Saving model ...
Validation loss decreased (0.396240 --> 0.395122).  Saving model ...
Validation loss decreased (0.395122 --> 0.394028).  Saving model ...
Validation loss decreased (0.394028 --> 0.393016).  Saving model ...
Validation loss decreased (0.393016 --> 0.391994).  Saving model ...
Validation loss decreased (0.391994 --> 0.390923).  Saving model ...
Validation loss decreased (0.390923 --> 0.389855).  Saving model ...
Validation loss decreased (0.389855 --> 0.388847).  Saving model ...
Validation loss decreased (0.388847 --> 0.387848).  Saving model ...
Validation loss decreased (0.387848 --> 0.386831).  Saving model ...
Validation loss decreased (0.386831 --> 0.385825).  Saving model ...
Validation loss decreased (0.385825 --> 0.384813).  Saving model ...
Validation loss decreased (0.384813 --> 0.383887).  Saving model ...
Validation loss decreased (0.383887 --> 0.383011).  Saving model ...
Validation loss decreased (0.383011 --> 0.382228).  Saving model ...
Validation loss decreased (0.382228 --> 0.381438).  Saving model ...
Validation loss decreased (0.381438 --> 0.380692).  Saving model ...
Validation loss decreased (0.380692 --> 0.379896).  Saving model ...
Validation loss decreased (0.379896 --> 0.379111).  Saving model ...
Validation loss decreased (0.379111 --> 0.378350).  Saving model ...
Validation loss decreased (0.378350 --> 0.377593).  Saving model ...
Validation loss decreased (0.377593 --> 0.376833).  Saving model ...
Validation loss decreased (0.376833 --> 0.376043).  Saving model ...
Validation loss decreased (0.376043 --> 0.375262).  Saving model ...
Validation loss decreased (0.375262 --> 0.374522).  Saving model ...
Validation loss decreased (0.374522 --> 0.373832).  Saving model ...
Validation loss decreased (0.373832 --> 0.373164).  Saving model ...
epoch 201, loss 0.3671, train acc 81.75%, f1 0.8180, precision 0.8159, recall 0.8200, auc 0.8175
Validation loss decreased (0.373164 --> 0.372484).  Saving model ...
Validation loss decreased (0.372484 --> 0.371863).  Saving model ...
Validation loss decreased (0.371863 --> 0.371210).  Saving model ...
Validation loss decreased (0.371210 --> 0.370504).  Saving model ...
Validation loss decreased (0.370504 --> 0.369840).  Saving model ...
Validation loss decreased (0.369840 --> 0.369152).  Saving model ...
Validation loss decreased (0.369152 --> 0.368433).  Saving model ...
Validation loss decreased (0.368433 --> 0.367751).  Saving model ...
Validation loss decreased (0.367751 --> 0.367018).  Saving model ...
Validation loss decreased (0.367018 --> 0.366423).  Saving model ...
Validation loss decreased (0.366423 --> 0.365787).  Saving model ...
Validation loss decreased (0.365787 --> 0.365195).  Saving model ...
Validation loss decreased (0.365195 --> 0.364612).  Saving model ...
Validation loss decreased (0.364612 --> 0.364006).  Saving model ...
Validation loss decreased (0.364006 --> 0.363376).  Saving model ...
Validation loss decreased (0.363376 --> 0.362733).  Saving model ...
Validation loss decreased (0.362733 --> 0.362093).  Saving model ...
Validation loss decreased (0.362093 --> 0.361479).  Saving model ...
Validation loss decreased (0.361479 --> 0.360867).  Saving model ...
Validation loss decreased (0.360867 --> 0.360266).  Saving model ...
Validation loss decreased (0.360266 --> 0.359651).  Saving model ...
Validation loss decreased (0.359651 --> 0.358994).  Saving model ...
Validation loss decreased (0.358994 --> 0.358343).  Saving model ...
Validation loss decreased (0.358343 --> 0.357706).  Saving model ...
Validation loss decreased (0.357706 --> 0.357077).  Saving model ...
Validation loss decreased (0.357077 --> 0.356537).  Saving model ...
Validation loss decreased (0.356537 --> 0.356043).  Saving model ...
Validation loss decreased (0.356043 --> 0.355501).  Saving model ...
Validation loss decreased (0.355501 --> 0.354948).  Saving model ...
Validation loss decreased (0.354948 --> 0.354380).  Saving model ...
Validation loss decreased (0.354380 --> 0.353824).  Saving model ...
Validation loss decreased (0.353824 --> 0.353232).  Saving model ...
Validation loss decreased (0.353232 --> 0.352621).  Saving model ...
Validation loss decreased (0.352621 --> 0.351966).  Saving model ...
Validation loss decreased (0.351966 --> 0.351316).  Saving model ...
Validation loss decreased (0.351316 --> 0.350694).  Saving model ...
Validation loss decreased (0.350694 --> 0.350110).  Saving model ...
Validation loss decreased (0.350110 --> 0.349524).  Saving model ...
Validation loss decreased (0.349524 --> 0.348889).  Saving model ...
Validation loss decreased (0.348889 --> 0.348291).  Saving model ...
Validation loss decreased (0.348291 --> 0.347697).  Saving model ...
Validation loss decreased (0.347697 --> 0.347140).  Saving model ...
Validation loss decreased (0.347140 --> 0.346608).  Saving model ...
Validation loss decreased (0.346608 --> 0.346026).  Saving model ...
Validation loss decreased (0.346026 --> 0.345448).  Saving model ...
Validation loss decreased (0.345448 --> 0.344878).  Saving model ...
Validation loss decreased (0.344878 --> 0.344326).  Saving model ...
Validation loss decreased (0.344326 --> 0.343742).  Saving model ...
Validation loss decreased (0.343742 --> 0.343207).  Saving model ...
Validation loss decreased (0.343207 --> 0.342705).  Saving model ...
Validation loss decreased (0.342705 --> 0.342264).  Saving model ...
Validation loss decreased (0.342264 --> 0.341875).  Saving model ...
Validation loss decreased (0.341875 --> 0.341539).  Saving model ...
Validation loss decreased (0.341539 --> 0.341206).  Saving model ...
Validation loss decreased (0.341206 --> 0.340867).  Saving model ...
Validation loss decreased (0.340867 --> 0.340509).  Saving model ...
Validation loss decreased (0.340509 --> 0.340106).  Saving model ...
Validation loss decreased (0.340106 --> 0.339749).  Saving model ...
Validation loss decreased (0.339749 --> 0.339390).  Saving model ...
Validation loss decreased (0.339390 --> 0.339058).  Saving model ...
Validation loss decreased (0.339058 --> 0.338650).  Saving model ...
Validation loss decreased (0.338650 --> 0.338224).  Saving model ...
Validation loss decreased (0.338224 --> 0.337752).  Saving model ...
Validation loss decreased (0.337752 --> 0.337337).  Saving model ...
Validation loss decreased (0.337337 --> 0.336948).  Saving model ...
Validation loss decreased (0.336948 --> 0.336597).  Saving model ...
Validation loss decreased (0.336597 --> 0.336259).  Saving model ...
Validation loss decreased (0.336259 --> 0.335959).  Saving model ...
Validation loss decreased (0.335959 --> 0.335595).  Saving model ...
Validation loss decreased (0.335595 --> 0.335168).  Saving model ...
Validation loss decreased (0.335168 --> 0.334739).  Saving model ...
Validation loss decreased (0.334739 --> 0.334352).  Saving model ...
Validation loss decreased (0.334352 --> 0.333987).  Saving model ...
Validation loss decreased (0.333987 --> 0.333567).  Saving model ...
Validation loss decreased (0.333567 --> 0.333180).  Saving model ...
Validation loss decreased (0.333180 --> 0.332825).  Saving model ...
Validation loss decreased (0.332825 --> 0.332527).  Saving model ...
Validation loss decreased (0.332527 --> 0.332166).  Saving model ...
Validation loss decreased (0.332166 --> 0.331749).  Saving model ...
Validation loss decreased (0.331749 --> 0.331362).  Saving model ...
Validation loss decreased (0.331362 --> 0.330949).  Saving model ...
Validation loss decreased (0.330949 --> 0.330551).  Saving model ...
Validation loss decreased (0.330551 --> 0.330185).  Saving model ...
Validation loss decreased (0.330185 --> 0.329815).  Saving model ...
Validation loss decreased (0.329815 --> 0.329533).  Saving model ...
Validation loss decreased (0.329533 --> 0.329291).  Saving model ...
Validation loss decreased (0.329291 --> 0.329035).  Saving model ...
Validation loss decreased (0.329035 --> 0.328838).  Saving model ...
Validation loss decreased (0.328838 --> 0.328600).  Saving model ...
Validation loss decreased (0.328600 --> 0.328397).  Saving model ...
Validation loss decreased (0.328397 --> 0.328212).  Saving model ...
Validation loss decreased (0.328212 --> 0.328046).  Saving model ...
Validation loss decreased (0.328046 --> 0.327888).  Saving model ...
Validation loss decreased (0.327888 --> 0.327706).  Saving model ...
Validation loss decreased (0.327706 --> 0.327579).  Saving model ...
Validation loss decreased (0.327579 --> 0.327392).  Saving model ...
Validation loss decreased (0.327392 --> 0.327228).  Saving model ...
Validation loss decreased (0.327228 --> 0.327061).  Saving model ...
Validation loss decreased (0.327061 --> 0.326882).  Saving model ...
Validation loss decreased (0.326882 --> 0.326701).  Saving model ...
epoch 301, loss 0.4251, train acc 83.00%, f1 0.8300, precision 0.8300, recall 0.8300, auc 0.8300
Validation loss decreased (0.326701 --> 0.326569).  Saving model ...
Validation loss decreased (0.326569 --> 0.326390).  Saving model ...
Validation loss decreased (0.326390 --> 0.326121).  Saving model ...
Validation loss decreased (0.326121 --> 0.325821).  Saving model ...
Validation loss decreased (0.325821 --> 0.325470).  Saving model ...
Validation loss decreased (0.325470 --> 0.325179).  Saving model ...
Validation loss decreased (0.325179 --> 0.324885).  Saving model ...
Validation loss decreased (0.324885 --> 0.324504).  Saving model ...
Validation loss decreased (0.324504 --> 0.324191).  Saving model ...
Validation loss decreased (0.324191 --> 0.323913).  Saving model ...
Validation loss decreased (0.323913 --> 0.323583).  Saving model ...
Validation loss decreased (0.323583 --> 0.323228).  Saving model ...
Validation loss decreased (0.323228 --> 0.322904).  Saving model ...
Validation loss decreased (0.322904 --> 0.322589).  Saving model ...
Validation loss decreased (0.322589 --> 0.322296).  Saving model ...
Validation loss decreased (0.322296 --> 0.322031).  Saving model ...
Validation loss decreased (0.322031 --> 0.321852).  Saving model ...
Validation loss decreased (0.321852 --> 0.321652).  Saving model ...
Validation loss decreased (0.321652 --> 0.321436).  Saving model ...
Validation loss decreased (0.321436 --> 0.321236).  Saving model ...
Validation loss decreased (0.321236 --> 0.321104).  Saving model ...
Validation loss decreased (0.321104 --> 0.320982).  Saving model ...
Validation loss decreased (0.320982 --> 0.320755).  Saving model ...
Validation loss decreased (0.320755 --> 0.320586).  Saving model ...
Validation loss decreased (0.320586 --> 0.320395).  Saving model ...
Validation loss decreased (0.320395 --> 0.320197).  Saving model ...
Validation loss decreased (0.320197 --> 0.319968).  Saving model ...
Validation loss decreased (0.319968 --> 0.319682).  Saving model ...
Validation loss decreased (0.319682 --> 0.319333).  Saving model ...
Validation loss decreased (0.319333 --> 0.319117).  Saving model ...
Validation loss decreased (0.319117 --> 0.318971).  Saving model ...
Validation loss decreased (0.318971 --> 0.318816).  Saving model ...
Validation loss decreased (0.318816 --> 0.318665).  Saving model ...
Validation loss decreased (0.318665 --> 0.318465).  Saving model ...
Validation loss decreased (0.318465 --> 0.318196).  Saving model ...
Validation loss decreased (0.318196 --> 0.317865).  Saving model ...
Validation loss decreased (0.317865 --> 0.317454).  Saving model ...
Validation loss decreased (0.317454 --> 0.317062).  Saving model ...
Validation loss decreased (0.317062 --> 0.316710).  Saving model ...
Validation loss decreased (0.316710 --> 0.316393).  Saving model ...
Validation loss decreased (0.316393 --> 0.316119).  Saving model ...
Validation loss decreased (0.316119 --> 0.315896).  Saving model ...
Validation loss decreased (0.315896 --> 0.315565).  Saving model ...
Validation loss decreased (0.315565 --> 0.315396).  Saving model ...
Validation loss decreased (0.315396 --> 0.315232).  Saving model ...
Validation loss decreased (0.315232 --> 0.315005).  Saving model ...
Validation loss decreased (0.315005 --> 0.314757).  Saving model ...
Validation loss decreased (0.314757 --> 0.314463).  Saving model ...
Validation loss decreased (0.314463 --> 0.314160).  Saving model ...
Validation loss decreased (0.314160 --> 0.313875).  Saving model .../home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Validation loss decreased (0.313875 --> 0.313575).  Saving model ...
Validation loss decreased (0.313575 --> 0.313263).  Saving model ...
Validation loss decreased (0.313263 --> 0.312921).  Saving model ...
Validation loss decreased (0.312921 --> 0.312571).  Saving model ...
Validation loss decreased (0.312571 --> 0.312087).  Saving model ...
Validation loss decreased (0.312087 --> 0.311656).  Saving model ...
Validation loss decreased (0.311656 --> 0.311233).  Saving model ...
Validation loss decreased (0.311233 --> 0.310872).  Saving model ...
Validation loss decreased (0.310872 --> 0.310505).  Saving model ...
Validation loss decreased (0.310505 --> 0.310084).  Saving model ...
Validation loss decreased (0.310084 --> 0.309655).  Saving model ...
Validation loss decreased (0.309655 --> 0.309206).  Saving model ...
Validation loss decreased (0.309206 --> 0.308860).  Saving model ...
Validation loss decreased (0.308860 --> 0.308429).  Saving model ...
Validation loss decreased (0.308429 --> 0.308007).  Saving model ...
Validation loss decreased (0.308007 --> 0.307592).  Saving model ...
Validation loss decreased (0.307592 --> 0.307168).  Saving model ...
Validation loss decreased (0.307168 --> 0.306798).  Saving model ...
Validation loss decreased (0.306798 --> 0.306365).  Saving model ...
Validation loss decreased (0.306365 --> 0.305902).  Saving model ...
Validation loss decreased (0.305902 --> 0.305443).  Saving model ...
Validation loss decreased (0.305443 --> 0.305064).  Saving model ...
Validation loss decreased (0.305064 --> 0.304691).  Saving model ...
Validation loss decreased (0.304691 --> 0.304259).  Saving model ...
Validation loss decreased (0.304259 --> 0.303893).  Saving model ...
Validation loss decreased (0.303893 --> 0.303510).  Saving model ...
Validation loss decreased (0.303510 --> 0.303161).  Saving model ...
Validation loss decreased (0.303161 --> 0.302735).  Saving model ...
Validation loss decreased (0.302735 --> 0.302385).  Saving model ...
Validation loss decreased (0.302385 --> 0.302058).  Saving model ...
Validation loss decreased (0.302058 --> 0.301742).  Saving model ...
Validation loss decreased (0.301742 --> 0.301455).  Saving model ...
Validation loss decreased (0.301455 --> 0.301214).  Saving model ...
Validation loss decreased (0.301214 --> 0.301025).  Saving model ...
Validation loss decreased (0.301025 --> 0.300765).  Saving model ...
Validation loss decreased (0.300765 --> 0.300564).  Saving model ...
Validation loss decreased (0.300564 --> 0.300437).  Saving model ...
Validation loss decreased (0.300437 --> 0.300265).  Saving model ...
Validation loss decreased (0.300265 --> 0.300078).  Saving model ...
Validation loss decreased (0.300078 --> 0.299890).  Saving model ...
Validation loss decreased (0.299890 --> 0.299745).  Saving model ...
Validation loss decreased (0.299745 --> 0.299531).  Saving model ...
Validation loss decreased (0.299531 --> 0.299328).  Saving model ...
Validation loss decreased (0.299328 --> 0.299058).  Saving model ...
Validation loss decreased (0.299058 --> 0.298802).  Saving model ...
Validation loss decreased (0.298802 --> 0.298600).  Saving model ...
Validation loss decreased (0.298600 --> 0.298421).  Saving model ...
Validation loss decreased (0.298421 --> 0.298228).  Saving model ...
Validation loss decreased (0.298228 --> 0.298022).  Saving model ...
Validation loss decreased (0.298022 --> 0.297857).  Saving model ...
epoch 401, loss 0.4441, train acc 86.25%, f1 0.8628, precision 0.8607, recall 0.8650, auc 0.8625
Validation loss decreased (0.297857 --> 0.297744).  Saving model ...
Validation loss decreased (0.297744 --> 0.297662).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
Validation loss decreased (0.297662 --> 0.297621).  Saving model ...
Validation loss decreased (0.297621 --> 0.297428).  Saving model ...
Validation loss decreased (0.297428 --> 0.297314).  Saving model ...
Validation loss decreased (0.297314 --> 0.297174).  Saving model ...
Validation loss decreased (0.297174 --> 0.297060).  Saving model ...
Validation loss decreased (0.297060 --> 0.296872).  Saving model ...
Validation loss decreased (0.296872 --> 0.296747).  Saving model ...
Validation loss decreased (0.296747 --> 0.296645).  Saving model ...
Validation loss decreased (0.296645 --> 0.296520).  Saving model ...
Validation loss decreased (0.296520 --> 0.296473).  Saving model ...
Validation loss decreased (0.296473 --> 0.296455).  Saving model ...
Validation loss decreased (0.296455 --> 0.296411).  Saving model ...
Validation loss decreased (0.296411 --> 0.296355).  Saving model ...
Validation loss decreased (0.296355 --> 0.296308).  Saving model ...
Validation loss decreased (0.296308 --> 0.296187).  Saving model ...
Validation loss decreased (0.296187 --> 0.296051).  Saving model ...
Validation loss decreased (0.296051 --> 0.295945).  Saving model ...
Validation loss decreased (0.295945 --> 0.295877).  Saving model ...
EarlyStopping counter: 1 out of 20
Validation loss decreased (0.295877 --> 0.295769).  Saving model ...
Validation loss decreased (0.295769 --> 0.295649).  Saving model ...
Validation loss decreased (0.295649 --> 0.295556).  Saving model ...
Validation loss decreased (0.295556 --> 0.295448).  Saving model ...
Validation loss decreased (0.295448 --> 0.295304).  Saving model ...
Validation loss decreased (0.295304 --> 0.295162).  Saving model ...
Validation loss decreased (0.295162 --> 0.295043).  Saving model ...
Validation loss decreased (0.295043 --> 0.294851).  Saving model ...
Validation loss decreased (0.294851 --> 0.294640).  Saving model ...
Validation loss decreased (0.294640 --> 0.294399).  Saving model ...
Validation loss decreased (0.294399 --> 0.294133).  Saving model ...
Validation loss decreased (0.294133 --> 0.293887).  Saving model ...
Validation loss decreased (0.293887 --> 0.293561).  Saving model ...
Validation loss decreased (0.293561 --> 0.293262).  Saving model ...
Validation loss decreased (0.293262 --> 0.293062).  Saving model ...
Validation loss decreased (0.293062 --> 0.292898).  Saving model ...
Validation loss decreased (0.292898 --> 0.292812).  Saving model ...
Validation loss decreased (0.292812 --> 0.292709).  Saving model ...
Validation loss decreased (0.292709 --> 0.292600).  Saving model ...
Validation loss decreased (0.292600 --> 0.292507).  Saving model ...
Validation loss decreased (0.292507 --> 0.292343).  Saving model ...
Validation loss decreased (0.292343 --> 0.292259).  Saving model ...
Validation loss decreased (0.292259 --> 0.292173).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 466, loss 0.4137, train acc 87.50%, f1 0.8750, precision 0.8750, recall 0.8750, auc 0.8750



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_Mirror_True
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_2
./test_pima/result_MLP_concat_Mirror_True_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.5807407407407408

the Fscore is 0.5608465608465608

the precision is 0.3925925925925926

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_2
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5338, train acc 78.68%, f1 0.7869, precision 0.7865, recall 0.7872, auc 0.7868
epoch 201, loss 0.4066, train acc 81.62%, f1 0.8163, precision 0.8162, recall 0.8164, auc 0.8162
epoch 301, loss 0.4554, train acc 83.02%, f1 0.8303, precision 0.8302, recall 0.8303, auc 0.8302
epoch 401, loss 0.2937, train acc 83.58%, f1 0.8358, precision 0.8358, recall 0.8358, auc 0.8358
epoch 501, loss 0.3550, train acc 83.85%, f1 0.8385, precision 0.8385, recall 0.8384, auc 0.8385
epoch 601, loss 0.4630, train acc 83.92%, f1 0.8392, precision 0.8392, recall 0.8391, auc 0.8392
epoch 701, loss 0.3810, train acc 84.05%, f1 0.8405, precision 0.8406, recall 0.8405, auc 0.8405
epoch 801, loss 0.2880, train acc 84.11%, f1 0.8410, precision 0.8411, recall 0.8410, auc 0.8411
epoch 901, loss 0.4296, train acc 84.11%, f1 0.8411, precision 0.8412, recall 0.8410, auc 0.8411
epoch 1001, loss 0.3002, train acc 84.09%, f1 0.8409, precision 0.8410, recall 0.8409, auc 0.8409
epoch 1101, loss 0.3976, train acc 84.06%, f1 0.8406, precision 0.8407, recall 0.8405, auc 0.8406
epoch 1201, loss 0.3063, train acc 84.09%, f1 0.8409, precision 0.8410, recall 0.8408, auc 0.8409
epoch 1301, loss 0.3171, train acc 84.14%, f1 0.8413, precision 0.8415, recall 0.8412, auc 0.8414
epoch 1401, loss 0.3044, train acc 84.10%, f1 0.8410, precision 0.8411, recall 0.8408, auc 0.8410
epoch 1501, loss 0.3649, train acc 84.09%, f1 0.8409, precision 0.8410, recall 0.8409, auc 0.8409
epoch 1601, loss 0.4388, train acc 84.10%, f1 0.8410, precision 0.8410, recall 0.8410, auc 0.8410
epoch 1701, loss 0.3705, train acc 84.01%, f1 0.8401, precision 0.8401, recall 0.8401, auc 0.8401
epoch 1801, loss 0.3884, train acc 84.06%, f1 0.8406, precision 0.8406, recall 0.8406, auc 0.8406
epoch 1901, loss 0.4424, train acc 84.13%, f1 0.8413, precision 0.8413, recall 0.8414, auc 0.8413
epoch 2001, loss 0.2807, train acc 84.18%, f1 0.8418, precision 0.8417, recall 0.8418, auc 0.8418
epoch 2101, loss 0.2873, train acc 84.11%, f1 0.8411, precision 0.8412, recall 0.8410, auc 0.8411
epoch 2201, loss 0.4771, train acc 84.17%, f1 0.8417, precision 0.8416, recall 0.8419, auc 0.8417
epoch 2301, loss 0.2747, train acc 84.13%, f1 0.8413, precision 0.8413, recall 0.8412, auc 0.8413
epoch 2401, loss 0.3249, train acc 84.27%, f1 0.8427, precision 0.8426, recall 0.8429, auc 0.8427
epoch 2501, loss 0.3856, train acc 84.33%, f1 0.8434, precision 0.8427, recall 0.8440, auc 0.8433
epoch 2601, loss 0.3031, train acc 84.40%, f1 0.8441, precision 0.8439, recall 0.8443, auc 0.8440
epoch 2701, loss 0.3308, train acc 84.49%, f1 0.8449, precision 0.8449, recall 0.8449, auc 0.8449
epoch 2801, loss 0.4365, train acc 84.55%, f1 0.8455, precision 0.8454, recall 0.8456, auc 0.8455
epoch 2901, loss 0.3398, train acc 84.68%, f1 0.8469, precision 0.8467, recall 0.8471, auc 0.8468
epoch 3001, loss 0.4038, train acc 84.79%, f1 0.8480, precision 0.8477, recall 0.8482, auc 0.8479
epoch 3101, loss 0.3045, train acc 84.91%, f1 0.8492, precision 0.8485, recall 0.8500, auc 0.8491
epoch 3201, loss 0.3692, train acc 85.08%, f1 0.8509, precision 0.8504, recall 0.8514, auc 0.8508
epoch 3301, loss 0.3117, train acc 85.18%, f1 0.8519, precision 0.8513, recall 0.8525, auc 0.8518
epoch 3401, loss 0.4157, train acc 85.30%, f1 0.8531, precision 0.8522, recall 0.8541, auc 0.8530
epoch 3501, loss 0.3049, train acc 85.45%, f1 0.8546, precision 0.8539, recall 0.8553, auc 0.8545
epoch 3601, loss 0.3298, train acc 85.49%, f1 0.8549, precision 0.8547, recall 0.8552, auc 0.8549
epoch 3701, loss 0.3276, train acc 85.67%, f1 0.8568, precision 0.8559, recall 0.8578, auc 0.8567
epoch 3801, loss 0.3488, train acc 85.72%, f1 0.8573, precision 0.8564, recall 0.8583, auc 0.8572
epoch 3901, loss 0.3785, train acc 85.83%, f1 0.8584, precision 0.8577, recall 0.8590, auc 0.8583
epoch 4001, loss 0.3240, train acc 85.95%, f1 0.8596, precision 0.8590, recall 0.8601, auc 0.8595
epoch 4101, loss 0.2833, train acc 86.12%, f1 0.8613, precision 0.8605, recall 0.8622, auc 0.8612
epoch 4201, loss 0.3361, train acc 86.18%, f1 0.8619, precision 0.8614, recall 0.8624, auc 0.8618
epoch 4301, loss 0.3041, train acc 86.29%, f1 0.8629, precision 0.8628, recall 0.8629, auc 0.8629
epoch 4401, loss 0.2390, train acc 86.32%, f1 0.8632, precision 0.8632, recall 0.8633, auc 0.8632
epoch 4501, loss 0.4242, train acc 86.46%, f1 0.8647, precision 0.8641, recall 0.8654, auc 0.8646
epoch 4601, loss 0.3364, train acc 86.56%, f1 0.8657, precision 0.8653, recall 0.8661, auc 0.8656
epoch 4701, loss 0.3483, train acc 86.62%, f1 0.8663, precision 0.8655, recall 0.8671, auc 0.8662
epoch 4801, loss 0.2047, train acc 86.71%, f1 0.8673, precision 0.8660, recall 0.8687, auc 0.8671
epoch 4901, loss 0.2637, train acc 86.74%, f1 0.8676, precision 0.8665, recall 0.8686, auc 0.8674
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_2
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6657407407407407

the Fscore is 0.616279069767442

the precision is 0.4491525423728814

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_2
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5875, train acc 78.98%, f1 0.7898, precision 0.7898, recall 0.7898, auc 0.7898
epoch 201, loss 0.3929, train acc 81.87%, f1 0.8187, precision 0.8186, recall 0.8189, auc 0.8187
epoch 301, loss 0.3166, train acc 83.07%, f1 0.8307, precision 0.8307, recall 0.8307, auc 0.8307
epoch 401, loss 0.4025, train acc 83.62%, f1 0.8362, precision 0.8362, recall 0.8362, auc 0.8362
epoch 501, loss 0.3237, train acc 83.93%, f1 0.8393, precision 0.8393, recall 0.8393, auc 0.8393
epoch 601, loss 0.5266, train acc 84.04%, f1 0.8405, precision 0.8404, recall 0.8405, auc 0.8404
epoch 701, loss 0.3778, train acc 84.15%, f1 0.8415, precision 0.8414, recall 0.8416, auc 0.8415
epoch 801, loss 0.4122, train acc 84.12%, f1 0.8413, precision 0.8411, recall 0.8414, auc 0.8412
epoch 901, loss 0.4714, train acc 84.05%, f1 0.8406, precision 0.8403, recall 0.8408, auc 0.8405
epoch 1001, loss 0.4114, train acc 84.08%, f1 0.8409, precision 0.8407, recall 0.8411, auc 0.8408
epoch 1101, loss 0.3072, train acc 84.05%, f1 0.8405, precision 0.8402, recall 0.8408, auc 0.8405
epoch 1201, loss 0.6177, train acc 84.10%, f1 0.8411, precision 0.8408, recall 0.8413, auc 0.8410
epoch 1301, loss 0.4252, train acc 84.13%, f1 0.8413, precision 0.8410, recall 0.8416, auc 0.8413
epoch 1401, loss 0.4618, train acc 84.13%, f1 0.8414, precision 0.8409, recall 0.8419, auc 0.8413
epoch 1501, loss 0.3333, train acc 84.11%, f1 0.8413, precision 0.8405, recall 0.8420, auc 0.8411
epoch 1601, loss 0.3561, train acc 84.11%, f1 0.8412, precision 0.8405, recall 0.8420, auc 0.8411
epoch 1701, loss 0.3888, train acc 84.11%, f1 0.8412, precision 0.8404, recall 0.8421, auc 0.8411
epoch 1801, loss 0.3569, train acc 84.12%, f1 0.8414, precision 0.8406, recall 0.8422, auc 0.8412
epoch 1901, loss 0.4459, train acc 84.15%, f1 0.8417, precision 0.8407, recall 0.8428, auc 0.8415
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_2
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.63

the Fscore is 0.5934065934065934

the precision is 0.421875

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_notMirror_True/record_1/MLP_concat_notMirror_True_2
----------------------



Traceback (most recent call last):
  File "./classifier_MLP/train_MLP.py", line 312, in <module>
    transformed_valid_data, transformed_valid_label = transform_data_to_train_form(transform_method, mirror_type, valid_pos_data, valid_neg_data)
  File "./classifier_MLP/train_MLP.py", line 192, in transform_data_to_train_form
    trans_pos_data, trans_pos_label, trans_neg_data, trans_neg_label = handleData_extend_not_mirror(positive_repeat_data, negetive_tile_data)
  File "./classifier_MLP/train_MLP.py", line 154, in handleData_extend_not_mirror
    transfrom_positive_data = transfrom_positive_data[positive_index[0]]
UnboundLocalError: local variable 'transfrom_positive_data' referenced before assignment
