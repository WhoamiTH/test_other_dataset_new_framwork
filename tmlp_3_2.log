nohup: ignoring input
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_2
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5356, train acc 79.11%, f1 0.7903, precision 0.7934, recall 0.7873, auc 0.7911
epoch 201, loss 0.4198, train acc 81.82%, f1 0.8183, precision 0.8180, recall 0.8186, auc 0.8182
epoch 301, loss 0.3513, train acc 83.11%, f1 0.8311, precision 0.8309, recall 0.8314, auc 0.8311
epoch 401, loss 0.3053, train acc 83.70%, f1 0.8371, precision 0.8366, recall 0.8375, auc 0.8370
epoch 501, loss 0.4496, train acc 83.97%, f1 0.8398, precision 0.8392, recall 0.8403, auc 0.8397
epoch 601, loss 0.3013, train acc 84.02%, f1 0.8403, precision 0.8399, recall 0.8406, auc 0.8402
epoch 701, loss 0.3268, train acc 84.04%, f1 0.8404, precision 0.8400, recall 0.8409, auc 0.8404
epoch 801, loss 0.3871, train acc 84.09%, f1 0.8409, precision 0.8408, recall 0.8411, auc 0.8409
epoch 901, loss 0.3619, train acc 84.09%, f1 0.8410, precision 0.8409, recall 0.8410, auc 0.8409
epoch 1001, loss 0.3578, train acc 84.07%, f1 0.8407, precision 0.8405, recall 0.8409, auc 0.8407
epoch 1101, loss 0.3604, train acc 84.11%, f1 0.8411, precision 0.8410, recall 0.8413, auc 0.8411
epoch 1201, loss 0.3658, train acc 84.01%, f1 0.8401, precision 0.8399, recall 0.8402, auc 0.8401
epoch 1301, loss 0.3214, train acc 84.11%, f1 0.8411, precision 0.8409, recall 0.8412, auc 0.8411
epoch 1401, loss 0.3375, train acc 84.10%, f1 0.8410, precision 0.8409, recall 0.8411, auc 0.8410
epoch 1501, loss 0.3262, train acc 84.13%, f1 0.8413, precision 0.8413, recall 0.8413, auc 0.8413
epoch 1601, loss 0.2738, train acc 84.14%, f1 0.8414, precision 0.8413, recall 0.8416, auc 0.8414
epoch 1701, loss 0.2273, train acc 84.10%, f1 0.8410, precision 0.8408, recall 0.8412, auc 0.8410
epoch 1801, loss 0.3625, train acc 84.14%, f1 0.8414, precision 0.8414, recall 0.8415, auc 0.8414
epoch 1901, loss 0.3671, train acc 84.14%, f1 0.8414, precision 0.8413, recall 0.8415, auc 0.8414
epoch 2001, loss 0.4744, train acc 84.07%, f1 0.8407, precision 0.8406, recall 0.8409, auc 0.8407
epoch 2101, loss 0.3350, train acc 84.19%, f1 0.8419, precision 0.8418, recall 0.8420, auc 0.8419
epoch 2201, loss 0.3423, train acc 84.21%, f1 0.8420, precision 0.8423, recall 0.8418, auc 0.8421
epoch 2301, loss 0.3641, train acc 84.21%, f1 0.8421, precision 0.8420, recall 0.8422, auc 0.8421
epoch 2401, loss 0.4413, train acc 84.27%, f1 0.8427, precision 0.8428, recall 0.8427, auc 0.8427
epoch 2501, loss 0.3272, train acc 84.32%, f1 0.8432, precision 0.8431, recall 0.8433, auc 0.8432
epoch 2601, loss 0.3626, train acc 84.35%, f1 0.8435, precision 0.8436, recall 0.8434, auc 0.8435
epoch 2701, loss 0.3607, train acc 84.53%, f1 0.8453, precision 0.8453, recall 0.8452, auc 0.8453
epoch 2801, loss 0.2710, train acc 84.56%, f1 0.8456, precision 0.8456, recall 0.8456, auc 0.8456
epoch 2901, loss 0.3007, train acc 84.70%, f1 0.8470, precision 0.8470, recall 0.8469, auc 0.8470
epoch 3001, loss 0.2420, train acc 84.88%, f1 0.8487, precision 0.8491, recall 0.8483, auc 0.8488
epoch 3101, loss 0.3187, train acc 84.94%, f1 0.8494, precision 0.8495, recall 0.8493, auc 0.8494
epoch 3201, loss 0.3031, train acc 85.05%, f1 0.8504, precision 0.8508, recall 0.8500, auc 0.8505
epoch 3301, loss 0.3406, train acc 85.16%, f1 0.8515, precision 0.8520, recall 0.8510, auc 0.8516
epoch 3401, loss 0.4147, train acc 85.31%, f1 0.8531, precision 0.8532, recall 0.8529, auc 0.8531
epoch 3501, loss 0.3390, train acc 85.50%, f1 0.8550, precision 0.8549, recall 0.8552, auc 0.8550
epoch 3601, loss 0.4113, train acc 85.62%, f1 0.8561, precision 0.8562, recall 0.8561, auc 0.8562
epoch 3701, loss 0.4110, train acc 85.74%, f1 0.8573, precision 0.8580, recall 0.8567, auc 0.8574
epoch 3801, loss 0.3340, train acc 85.84%, f1 0.8583, precision 0.8590, recall 0.8577, auc 0.8584
epoch 3901, loss 0.3498, train acc 85.96%, f1 0.8594, precision 0.8606, recall 0.8583, auc 0.8596
epoch 4001, loss 0.4472, train acc 86.09%, f1 0.8607, precision 0.8619, recall 0.8594, auc 0.8609
epoch 4101, loss 0.2910, train acc 86.25%, f1 0.8625, precision 0.8627, recall 0.8623, auc 0.8625
epoch 4201, loss 0.3971, train acc 86.33%, f1 0.8632, precision 0.8637, recall 0.8628, auc 0.8633
epoch 4301, loss 0.3029, train acc 86.39%, f1 0.8637, precision 0.8646, recall 0.8629, auc 0.8639
epoch 4401, loss 0.3535, train acc 86.47%, f1 0.8646, precision 0.8653, recall 0.8639, auc 0.8647
epoch 4501, loss 0.3371, train acc 86.57%, f1 0.8656, precision 0.8657, recall 0.8655, auc 0.8657
epoch 4601, loss 0.3033, train acc 86.65%, f1 0.8665, precision 0.8667, recall 0.8663, auc 0.8665
epoch 4701, loss 0.3664, train acc 86.76%, f1 0.8675, precision 0.8682, recall 0.8668, auc 0.8676
epoch 4801, loss 0.2371, train acc 86.74%, f1 0.8674, precision 0.8676, recall 0.8672, auc 0.8674
epoch 4901, loss 0.3255, train acc 86.79%, f1 0.8679, precision 0.8683, recall 0.8674, auc 0.8679
epoch 5001, loss 0.2866, train acc 86.85%, f1 0.8684, precision 0.8689, recall 0.8679, auc 0.8685
epoch 5101, loss 0.2753, train acc 86.87%, f1 0.8687, precision 0.8687, recall 0.8687, auc 0.8687
epoch 5201, loss 0.3117, train acc 86.94%, f1 0.8693, precision 0.8702, recall 0.8684, auc 0.8694
epoch 5301, loss 0.2545, train acc 87.03%, f1 0.8703, precision 0.8702, recall 0.8705, auc 0.8703
epoch 5401, loss 0.2773, train acc 87.06%, f1 0.8705, precision 0.8712, recall 0.8698, auc 0.8706
epoch 5501, loss 0.2438, train acc 87.05%, f1 0.8705, precision 0.8706, recall 0.8704, auc 0.8705
epoch 5601, loss 0.3494, train acc 87.13%, f1 0.8713, precision 0.8713, recall 0.8713, auc 0.8713
epoch 5701, loss 0.2969, train acc 87.14%, f1 0.8714, precision 0.8712, recall 0.8715, auc 0.8714
epoch 5801, loss 0.2765, train acc 87.16%, f1 0.8717, precision 0.8713, recall 0.8721, auc 0.8716
epoch 5901, loss 0.2261, train acc 87.16%, f1 0.8716, precision 0.8715, recall 0.8716, auc 0.8716
epoch 6001, loss 0.2775, train acc 87.21%, f1 0.8721, precision 0.8722, recall 0.8720, auc 0.8721
epoch 6101, loss 0.2993, train acc 87.21%, f1 0.8721, precision 0.8723, recall 0.8718, auc 0.8721
epoch 6201, loss 0.3266, train acc 87.30%, f1 0.8731, precision 0.8727, recall 0.8735, auc 0.8730
epoch 6301, loss 0.2590, train acc 87.25%, f1 0.8726, precision 0.8718, recall 0.8733, auc 0.8725
epoch 6401, loss 0.2416, train acc 87.29%, f1 0.8728, precision 0.8729, recall 0.8728, auc 0.8729
epoch 6501, loss 0.2051, train acc 87.28%, f1 0.8728, precision 0.8729, recall 0.8728, auc 0.8728
epoch 6601, loss 0.2809, train acc 87.41%, f1 0.8741, precision 0.8739, recall 0.8743, auc 0.8741
epoch 6701, loss 0.2539, train acc 87.38%, f1 0.8738, precision 0.8738, recall 0.8738, auc 0.8738
epoch 6801, loss 0.2628, train acc 87.43%, f1 0.8743, precision 0.8740, recall 0.8746, auc 0.8743
epoch 6901, loss 0.3502, train acc 87.44%, f1 0.8746, precision 0.8734, recall 0.8758, auc 0.8744
epoch 7001, loss 0.2415, train acc 87.42%, f1 0.8743, precision 0.8736, recall 0.8749, auc 0.8742
epoch 7101, loss 0.1839, train acc 87.51%, f1 0.8751, precision 0.8751, recall 0.8751, auc 0.8751
epoch 7201, loss 0.3453, train acc 87.50%, f1 0.8750, precision 0.8750, recall 0.8749, auc 0.8750
epoch 7301, loss 0.3003, train acc 87.51%, f1 0.8752, precision 0.8741, recall 0.8763, auc 0.8751
epoch 7401, loss 0.2772, train acc 87.60%, f1 0.8760, precision 0.8755, recall 0.8766, auc 0.8760
epoch 7501, loss 0.1976, train acc 87.57%, f1 0.8756, precision 0.8759, recall 0.8753, auc 0.8757
epoch 7601, loss 0.2326, train acc 87.62%, f1 0.8761, precision 0.8769, recall 0.8752, auc 0.8762
epoch 7701, loss 0.3034, train acc 87.64%, f1 0.8765, precision 0.8755, recall 0.8775, auc 0.8764
epoch 7801, loss 0.2345, train acc 87.69%, f1 0.8770, precision 0.8766, recall 0.8773, auc 0.8769
epoch 7901, loss 0.3031, train acc 87.74%, f1 0.8773, precision 0.8778, recall 0.8769, auc 0.8774
epoch 8001, loss 0.2877, train acc 87.74%, f1 0.8775, precision 0.8770, recall 0.8780, auc 0.8774
epoch 8101, loss 0.2690, train acc 87.77%, f1 0.8776, precision 0.8778, recall 0.8775, auc 0.8777
epoch 8201, loss 0.2598, train acc 87.80%, f1 0.8781, precision 0.8773, recall 0.8788, auc 0.8780
epoch 8301, loss 0.2509, train acc 87.84%, f1 0.8784, precision 0.8785, recall 0.8784, auc 0.8784
epoch 8401, loss 0.2414, train acc 87.88%, f1 0.8788, precision 0.8788, recall 0.8788, auc 0.8788
epoch 8501, loss 0.2358, train acc 87.83%, f1 0.8785, precision 0.8769, recall 0.8800, auc 0.8783
epoch 8601, loss 0.2954, train acc 87.91%, f1 0.8791, precision 0.8795, recall 0.8786, auc 0.8791
epoch 8701, loss 0.3267, train acc 87.92%, f1 0.8794, precision 0.8783, recall 0.8805, auc 0.8792
epoch 8801, loss 0.3860, train acc 87.96%, f1 0.8797, precision 0.8792, recall 0.8802, auc 0.8796
epoch 8901, loss 0.3443, train acc 88.02%, f1 0.8804, precision 0.8792, recall 0.8816, auc 0.8802
epoch 9001, loss 0.2360, train acc 88.05%, f1 0.8805, precision 0.8801, recall 0.8810, auc 0.8805
epoch 9101, loss 0.3724, train acc 88.07%, f1 0.8806, precision 0.8811, recall 0.8802, auc 0.8807
epoch 9201, loss 0.2710, train acc 88.15%, f1 0.8814, precision 0.8815, recall 0.8814, auc 0.8815
epoch 9301, loss 0.3278, train acc 88.15%, f1 0.8817, precision 0.8806, recall 0.8827, auc 0.8815
epoch 9401, loss 0.2928, train acc 88.19%, f1 0.8820, precision 0.8810, recall 0.8831, auc 0.8819
epoch 9501, loss 0.3018, train acc 88.24%, f1 0.8824, precision 0.8825, recall 0.8822, auc 0.8824
epoch 9601, loss 0.2733, train acc 88.24%, f1 0.8823, precision 0.8829, recall 0.8816, auc 0.8824
epoch 9701, loss 0.2548, train acc 88.32%, f1 0.8833, precision 0.8820, recall 0.8847, auc 0.8832
epoch 9801, loss 0.2561, train acc 88.40%, f1 0.8839, precision 0.8846, recall 0.8832, auc 0.8840
epoch 9901, loss 0.3366, train acc 88.40%, f1 0.8840, precision 0.8841, recall 0.8839, auc 0.8840
epoch 10001, loss 0.2596, train acc 88.40%, f1 0.8840, precision 0.8838, recall 0.8843, auc 0.8840
epoch 10101, loss 0.1727, train acc 88.45%, f1 0.8849, precision 0.8823, recall 0.8875, auc 0.8845
epoch 10201, loss 0.2200, train acc 88.50%, f1 0.8850, precision 0.8852, recall 0.8847, auc 0.8850
epoch 10301, loss 0.2639, train acc 88.52%, f1 0.8854, precision 0.8839, recall 0.8869, auc 0.8852
epoch 10401, loss 0.2624, train acc 88.58%, f1 0.8859, precision 0.8850, recall 0.8868, auc 0.8858
epoch 10501, loss 0.2717, train acc 88.61%, f1 0.8862, precision 0.8851, recall 0.8873, auc 0.8861
epoch 10601, loss 0.2169, train acc 88.64%, f1 0.8865, precision 0.8857, recall 0.8873, auc 0.8864
epoch 10701, loss 0.2454, train acc 88.73%, f1 0.8872, precision 0.8878, recall 0.8866, auc 0.8873
epoch 10801, loss 0.3615, train acc 88.73%, f1 0.8873, precision 0.8871, recall 0.8875, auc 0.8873
epoch 10901, loss 0.2893, train acc 88.78%, f1 0.8878, precision 0.8875, recall 0.8881, auc 0.8878
epoch 11001, loss 0.2146, train acc 88.81%, f1 0.8883, precision 0.8870, recall 0.8896, auc 0.8881
epoch 11101, loss 0.2899, train acc 88.88%, f1 0.8888, precision 0.8884, recall 0.8892, auc 0.8888
epoch 11201, loss 0.1939, train acc 88.89%, f1 0.8890, precision 0.8882, recall 0.8898, auc 0.8889
epoch 11301, loss 0.3074, train acc 88.90%, f1 0.8891, precision 0.8881, recall 0.8901, auc 0.8890
epoch 11401, loss 0.2842, train acc 89.00%, f1 0.8901, precision 0.8893, recall 0.8908, auc 0.8900
epoch 11501, loss 0.2788, train acc 89.08%, f1 0.8908, precision 0.8911, recall 0.8904, auc 0.8908
epoch 11601, loss 0.2642, train acc 89.10%, f1 0.8910, precision 0.8915, recall 0.8905, auc 0.8910
epoch 11701, loss 0.2221, train acc 89.07%, f1 0.8907, precision 0.8912, recall 0.8902, auc 0.8907
epoch 11801, loss 0.2918, train acc 89.18%, f1 0.8919, precision 0.8911, recall 0.8927, auc 0.8918
epoch 11901, loss 0.3138, train acc 89.23%, f1 0.8924, precision 0.8916, recall 0.8933, auc 0.8923
epoch 12001, loss 0.2287, train acc 89.23%, f1 0.8924, precision 0.8919, recall 0.8929, auc 0.8923
epoch 12101, loss 0.2292, train acc 89.29%, f1 0.8930, precision 0.8926, recall 0.8933, auc 0.8929
epoch 12201, loss 0.2405, train acc 89.40%, f1 0.8940, precision 0.8937, recall 0.8942, auc 0.8940
epoch 12301, loss 0.2393, train acc 89.46%, f1 0.8946, precision 0.8948, recall 0.8944, auc 0.8946
epoch 12401, loss 0.2521, train acc 89.49%, f1 0.8949, precision 0.8949, recall 0.8949, auc 0.8949
epoch 12501, loss 0.1527, train acc 89.53%, f1 0.8954, precision 0.8944, recall 0.8963, auc 0.8953
epoch 12601, loss 0.2090, train acc 89.58%, f1 0.8959, precision 0.8949, recall 0.8970, auc 0.8958
epoch 12701, loss 0.2337, train acc 89.58%, f1 0.8958, precision 0.8963, recall 0.8952, auc 0.8958
epoch 12801, loss 0.2831, train acc 89.66%, f1 0.8966, precision 0.8963, recall 0.8970, auc 0.8966
epoch 12901, loss 0.2745, train acc 89.67%, f1 0.8966, precision 0.8969, recall 0.8964, auc 0.8967
epoch 13001, loss 0.2286, train acc 89.72%, f1 0.8971, precision 0.8982, recall 0.8961, auc 0.8972
epoch 13101, loss 0.3691, train acc 89.80%, f1 0.8979, precision 0.8992, recall 0.8966, auc 0.8980
epoch 13201, loss 0.1914, train acc 89.85%, f1 0.8983, precision 0.8997, recall 0.8969, auc 0.8985
epoch 13301, loss 0.2867, train acc 89.89%, f1 0.8989, precision 0.8992, recall 0.8985, auc 0.8989
epoch 13401, loss 0.2360, train acc 89.81%, f1 0.8980, precision 0.8986, recall 0.8974, auc 0.8981
epoch 13501, loss 0.1564, train acc 89.90%, f1 0.8990, precision 0.8988, recall 0.8992, auc 0.8990
epoch 13601, loss 0.1985, train acc 90.04%, f1 0.9004, precision 0.9005, recall 0.9002, auc 0.9004
epoch 13701, loss 0.2753, train acc 90.10%, f1 0.9010, precision 0.9005, recall 0.9016, auc 0.9010
epoch 13801, loss 0.2258, train acc 90.15%, f1 0.9016, precision 0.9009, recall 0.9022, auc 0.9015
epoch 13901, loss 0.3333, train acc 90.19%, f1 0.9021, precision 0.9005, recall 0.9037, auc 0.9019
epoch 14001, loss 0.2137, train acc 90.28%, f1 0.9028, precision 0.9031, recall 0.9025, auc 0.9028
epoch 14101, loss 0.2850, train acc 90.28%, f1 0.9030, precision 0.9016, recall 0.9044, auc 0.9028
epoch 14201, loss 0.2109, train acc 90.34%, f1 0.9033, precision 0.9036, recall 0.9031, auc 0.9034
epoch 14301, loss 0.2384, train acc 90.44%, f1 0.9044, precision 0.9039, recall 0.9050, auc 0.9044
epoch 14401, loss 0.1840, train acc 90.51%, f1 0.9051, precision 0.9047, recall 0.9056, auc 0.9051
epoch 14501, loss 0.2097, train acc 90.50%, f1 0.9051, precision 0.9036, recall 0.9066, auc 0.9050
epoch 14601, loss 0.2911, train acc 90.55%, f1 0.9054, precision 0.9058, recall 0.9050, auc 0.9055
epoch 14701, loss 0.2361, train acc 90.58%, f1 0.9058, precision 0.9055, recall 0.9061, auc 0.9058
epoch 14801, loss 0.2675, train acc 90.63%, f1 0.9062, precision 0.9066, recall 0.9059, auc 0.9063
epoch 14901, loss 0.2665, train acc 90.70%, f1 0.9071, precision 0.9065, recall 0.9076, auc 0.9070
epoch 15001, loss 0.2172, train acc 90.77%, f1 0.9076, precision 0.9079, recall 0.9074, auc 0.9077
epoch 15101, loss 0.4413, train acc 90.81%, f1 0.9081, precision 0.9076, recall 0.9086, auc 0.9081
epoch 15201, loss 0.3095, train acc 90.76%, f1 0.9077, precision 0.9061, recall 0.9094, auc 0.9076
epoch 15301, loss 0.3555, train acc 90.87%, f1 0.9087, precision 0.9088, recall 0.9085, auc 0.9087
epoch 15401, loss 0.1464, train acc 90.92%, f1 0.9093, precision 0.9084, recall 0.9102, auc 0.9092
epoch 15501, loss 0.1945, train acc 90.92%, f1 0.9092, precision 0.9090, recall 0.9093, auc 0.9092
epoch 15601, loss 0.2113, train acc 91.02%, f1 0.9103, precision 0.9093, recall 0.9114, auc 0.9102
epoch 15701, loss 0.1493, train acc 91.08%, f1 0.9109, precision 0.9105, recall 0.9112, auc 0.9108
epoch 15801, loss 0.2340, train acc 91.11%, f1 0.9115, precision 0.9076, recall 0.9154, auc 0.9111
epoch 15901, loss 0.1859, train acc 91.19%, f1 0.9118, precision 0.9129, recall 0.9107, auc 0.9119
epoch 16001, loss 0.1430, train acc 91.10%, f1 0.9110, precision 0.9113, recall 0.9107, auc 0.9110
epoch 16101, loss 0.1722, train acc 91.23%, f1 0.9125, precision 0.9108, recall 0.9143, auc 0.9123
epoch 16201, loss 0.2877, train acc 91.33%, f1 0.9134, precision 0.9121, recall 0.9146, auc 0.9133
epoch 16301, loss 0.1554, train acc 91.29%, f1 0.9128, precision 0.9142, recall 0.9114, auc 0.9129
epoch 16401, loss 0.1958, train acc 91.41%, f1 0.9142, precision 0.9131, recall 0.9152, auc 0.9141
epoch 16501, loss 0.2302, train acc 91.41%, f1 0.9141, precision 0.9140, recall 0.9143, auc 0.9141/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.1790, train acc 91.46%, f1 0.9148, precision 0.9126, recall 0.9170, auc 0.9146
epoch 16701, loss 0.1928, train acc 91.50%, f1 0.9151, precision 0.9135, recall 0.9168, auc 0.9150
epoch 16801, loss 0.2177, train acc 91.62%, f1 0.9161, precision 0.9172, recall 0.9150, auc 0.9162
epoch 16901, loss 0.1630, train acc 91.59%, f1 0.9158, precision 0.9174, recall 0.9142, auc 0.9159
epoch 17001, loss 0.1805, train acc 91.65%, f1 0.9166, precision 0.9151, recall 0.9181, auc 0.9165
epoch 17101, loss 0.1818, train acc 91.63%, f1 0.9162, precision 0.9165, recall 0.9160, auc 0.9163
epoch 17201, loss 0.2017, train acc 91.75%, f1 0.9177, precision 0.9163, recall 0.9191, auc 0.9175
epoch 17301, loss 0.2505, train acc 91.81%, f1 0.9182, precision 0.9166, recall 0.9199, auc 0.9181
epoch 17401, loss 0.1999, train acc 91.83%, f1 0.9181, precision 0.9202, recall 0.9161, auc 0.9183
epoch 17501, loss 0.2095, train acc 91.86%, f1 0.9186, precision 0.9185, recall 0.9187, auc 0.9186
epoch 17601, loss 0.2214, train acc 91.93%, f1 0.9195, precision 0.9170, recall 0.9220, auc 0.9193
epoch 17701, loss 0.3070, train acc 91.98%, f1 0.9198, precision 0.9199, recall 0.9196, auc 0.9198
epoch 17801, loss 0.1261, train acc 91.94%, f1 0.9191, precision 0.9216, recall 0.9167, auc 0.9194
epoch 17901, loss 0.2078, train acc 92.02%, f1 0.9203, precision 0.9192, recall 0.9213, auc 0.9202
epoch 18001, loss 0.1373, train acc 92.11%, f1 0.9213, precision 0.9194, recall 0.9232, auc 0.9211
epoch 18101, loss 0.1575, train acc 92.10%, f1 0.9213, precision 0.9183, recall 0.9243, auc 0.9210
epoch 18201, loss 0.2445, train acc 92.12%, f1 0.9212, precision 0.9209, recall 0.9216, auc 0.9212
epoch 18301, loss 0.2043, train acc 92.23%, f1 0.9222, precision 0.9230, recall 0.9214, auc 0.9223
epoch 18401, loss 0.1859, train acc 92.12%, f1 0.9213, precision 0.9196, recall 0.9230, auc 0.9212
epoch 18501, loss 0.1685, train acc 92.18%, f1 0.9220, precision 0.9194, recall 0.9247, auc 0.9218
epoch 18601, loss 0.1778, train acc 92.24%, f1 0.9225, precision 0.9212, recall 0.9238, auc 0.9224
epoch 18701, loss 0.1359, train acc 92.35%, f1 0.9236, precision 0.9220, recall 0.9251, auc 0.9235
epoch 18801, loss 0.2478, train acc 92.35%, f1 0.9234, precision 0.9246, recall 0.9223, auc 0.9235
epoch 18901, loss 0.1793, train acc 92.36%, f1 0.9237, precision 0.9229, recall 0.9245, auc 0.9236
epoch 19001, loss 0.2172, train acc 92.39%, f1 0.9239, precision 0.9230, recall 0.9249, auc 0.9239
epoch 19101, loss 0.1770, train acc 92.49%, f1 0.9247, precision 0.9264, recall 0.9231, auc 0.9249
epoch 19201, loss 0.2177, train acc 92.48%, f1 0.9250, precision 0.9226, recall 0.9275, auc 0.9248
epoch 19301, loss 0.2057, train acc 92.47%, f1 0.9247, precision 0.9245, recall 0.9250, auc 0.9247
epoch 19401, loss 0.1561, train acc 92.55%, f1 0.9255, precision 0.9252, recall 0.9258, auc 0.9255
epoch 19501, loss 0.1076, train acc 92.57%, f1 0.9260, precision 0.9224, recall 0.9295, auc 0.9257
epoch 19601, loss 0.1440, train acc 92.56%, f1 0.9257, precision 0.9242, recall 0.9271, auc 0.9256
epoch 19701, loss 0.1930, train acc 92.68%, f1 0.9266, precision 0.9287, recall 0.9246, auc 0.9268
epoch 19801, loss 0.1664, train acc 92.69%, f1 0.9270, precision 0.9260, recall 0.9279, auc 0.9269
epoch 19901, loss 0.2019, train acc 92.69%, f1 0.9269, precision 0.9261, recall 0.9278, auc 0.9269
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_2
./test_pima/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.7314814814814814

the Fscore is 0.6666666666666666

the precision is 0.5098039215686274

the recall is 0.9629629629629629

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_2
----------------------



epoch 1, loss 0.6932, train acc 49.98%, f1 0.0003, precision 0.2154, recall 0.0002, auc 0.4998
epoch 101, loss 0.5625, train acc 79.11%, f1 0.7917, precision 0.7896, recall 0.7939, auc 0.7911
epoch 201, loss 0.4084, train acc 81.85%, f1 0.8185, precision 0.8185, recall 0.8185, auc 0.8185
epoch 301, loss 0.3583, train acc 82.93%, f1 0.8292, precision 0.8296, recall 0.8289, auc 0.8293
epoch 401, loss 0.3188, train acc 83.52%, f1 0.8351, precision 0.8356, recall 0.8346, auc 0.8352
epoch 501, loss 0.3178, train acc 83.84%, f1 0.8383, precision 0.8388, recall 0.8379, auc 0.8384
epoch 601, loss 0.2795, train acc 83.92%, f1 0.8391, precision 0.8394, recall 0.8388, auc 0.8392
epoch 701, loss 0.4163, train acc 84.06%, f1 0.8405, precision 0.8409, recall 0.8401, auc 0.8406
epoch 801, loss 0.3132, train acc 84.12%, f1 0.8411, precision 0.8413, recall 0.8410, auc 0.8412
epoch 901, loss 0.4040, train acc 84.10%, f1 0.8410, precision 0.8409, recall 0.8410, auc 0.8410
epoch 1001, loss 0.3601, train acc 84.15%, f1 0.8414, precision 0.8416, recall 0.8413, auc 0.8415
epoch 1101, loss 0.2599, train acc 84.13%, f1 0.8413, precision 0.8415, recall 0.8411, auc 0.8413
epoch 1201, loss 0.4233, train acc 84.10%, f1 0.8410, precision 0.8411, recall 0.8410, auc 0.8410
epoch 1301, loss 0.3364, train acc 84.10%, f1 0.8409, precision 0.8411, recall 0.8408, auc 0.8410
epoch 1401, loss 0.3752, train acc 84.05%, f1 0.8404, precision 0.8410, recall 0.8399, auc 0.8405
epoch 1501, loss 0.3242, train acc 84.05%, f1 0.8404, precision 0.8407, recall 0.8401, auc 0.8405
epoch 1601, loss 0.3636, train acc 84.08%, f1 0.8407, precision 0.8411, recall 0.8404, auc 0.8408
epoch 1701, loss 0.4171, train acc 84.11%, f1 0.8411, precision 0.8414, recall 0.8407, auc 0.8411
epoch 1801, loss 0.3364, train acc 84.11%, f1 0.8410, precision 0.8414, recall 0.8406, auc 0.8411
epoch 1901, loss 0.3665, train acc 84.09%, f1 0.8408, precision 0.8412, recall 0.8405, auc 0.8409
epoch 2001, loss 0.3661, train acc 84.17%, f1 0.8416, precision 0.8421, recall 0.8412, auc 0.8417
epoch 2101, loss 0.3203, train acc 84.25%, f1 0.8425, precision 0.8429, recall 0.8420, auc 0.8425
epoch 2201, loss 0.2590, train acc 84.31%, f1 0.8430, precision 0.8434, recall 0.8427, auc 0.8431
epoch 2301, loss 0.2982, train acc 84.25%, f1 0.8424, precision 0.8426, recall 0.8423, auc 0.8425
epoch 2401, loss 0.4112, train acc 84.28%, f1 0.8427, precision 0.8431, recall 0.8424, auc 0.8428
epoch 2501, loss 0.2919, train acc 84.34%, f1 0.8433, precision 0.8437, recall 0.8429, auc 0.8434
epoch 2601, loss 0.2790, train acc 84.38%, f1 0.8437, precision 0.8439, recall 0.8436, auc 0.8438
epoch 2701, loss 0.5094, train acc 84.43%, f1 0.8442, precision 0.8446, recall 0.8438, auc 0.8443
epoch 2801, loss 0.2205, train acc 84.59%, f1 0.8459, precision 0.8459, recall 0.8458, auc 0.8459
epoch 2901, loss 0.3434, train acc 84.70%, f1 0.8470, precision 0.8473, recall 0.8467, auc 0.8470
epoch 3001, loss 0.2439, train acc 84.84%, f1 0.8484, precision 0.8485, recall 0.8483, auc 0.8484
epoch 3101, loss 0.4472, train acc 84.92%, f1 0.8491, precision 0.8493, recall 0.8489, auc 0.8492
epoch 3201, loss 0.4027, train acc 84.95%, f1 0.8494, precision 0.8497, recall 0.8492, auc 0.8495
epoch 3301, loss 0.3697, train acc 85.16%, f1 0.8516, precision 0.8516, recall 0.8517, auc 0.8516
epoch 3401, loss 0.3342, train acc 85.31%, f1 0.8531, precision 0.8531, recall 0.8532, auc 0.8531
epoch 3501, loss 0.3066, train acc 85.44%, f1 0.8545, precision 0.8541, recall 0.8548, auc 0.8544
epoch 3601, loss 0.3012, train acc 85.60%, f1 0.8559, precision 0.8563, recall 0.8556, auc 0.8560
epoch 3701, loss 0.3458, train acc 85.70%, f1 0.8569, precision 0.8575, recall 0.8563, auc 0.8570
epoch 3801, loss 0.3022, train acc 85.79%, f1 0.8579, precision 0.8579, recall 0.8579, auc 0.8579
epoch 3901, loss 0.2139, train acc 85.93%, f1 0.8593, precision 0.8590, recall 0.8596, auc 0.8593
epoch 4001, loss 0.3805, train acc 86.03%, f1 0.8603, precision 0.8601, recall 0.8606, auc 0.8603
epoch 4101, loss 0.3018, train acc 86.13%, f1 0.8614, precision 0.8611, recall 0.8617, auc 0.8613
epoch 4201, loss 0.3294, train acc 86.24%, f1 0.8624, precision 0.8623, recall 0.8625, auc 0.8624
epoch 4301, loss 0.3793, train acc 86.33%, f1 0.8633, precision 0.8635, recall 0.8631, auc 0.8633
epoch 4401, loss 0.2472, train acc 86.37%, f1 0.8638, precision 0.8632, recall 0.8644, auc 0.8637
epoch 4501, loss 0.3049, train acc 86.42%, f1 0.8641, precision 0.8646, recall 0.8637, auc 0.8642
epoch 4601, loss 0.2462, train acc 86.60%, f1 0.8659, precision 0.8668, recall 0.8650, auc 0.8660
epoch 4701, loss 0.2225, train acc 86.68%, f1 0.8668, precision 0.8670, recall 0.8666, auc 0.8668
epoch 4801, loss 0.3449, train acc 86.77%, f1 0.8676, precision 0.8682, recall 0.8670, auc 0.8677
epoch 4901, loss 0.3527, train acc 86.79%, f1 0.8677, precision 0.8687, recall 0.8668, auc 0.8679
epoch 5001, loss 0.3741, train acc 86.77%, f1 0.8676, precision 0.8678, recall 0.8674, auc 0.8677
epoch 5101, loss 0.1579, train acc 86.84%, f1 0.8684, precision 0.8684, recall 0.8685, auc 0.8684
epoch 5201, loss 0.2742, train acc 86.87%, f1 0.8687, precision 0.8689, recall 0.8686, auc 0.8687
epoch 5301, loss 0.2916, train acc 87.00%, f1 0.8700, precision 0.8699, recall 0.8701, auc 0.8700
epoch 5401, loss 0.2881, train acc 86.98%, f1 0.8698, precision 0.8699, recall 0.8697, auc 0.8698
epoch 5501, loss 0.2958, train acc 87.02%, f1 0.8702, precision 0.8702, recall 0.8702, auc 0.8702
epoch 5601, loss 0.1761, train acc 87.04%, f1 0.8704, precision 0.8705, recall 0.8702, auc 0.8704
epoch 5701, loss 0.2767, train acc 87.04%, f1 0.8704, precision 0.8704, recall 0.8705, auc 0.8704
epoch 5801, loss 0.2129, train acc 87.06%, f1 0.8706, precision 0.8708, recall 0.8704, auc 0.8706
epoch 5901, loss 0.4588, train acc 87.12%, f1 0.8712, precision 0.8716, recall 0.8708, auc 0.8712
epoch 6001, loss 0.3646, train acc 87.09%, f1 0.8709, precision 0.8709, recall 0.8709, auc 0.8709
epoch 6101, loss 0.2498, train acc 87.17%, f1 0.8717, precision 0.8718, recall 0.8716, auc 0.8717
epoch 6201, loss 0.3382, train acc 87.12%, f1 0.8710, precision 0.8720, recall 0.8700, auc 0.8712
epoch 6301, loss 0.2404, train acc 87.22%, f1 0.8722, precision 0.8721, recall 0.8724, auc 0.8722
epoch 6401, loss 0.3462, train acc 87.19%, f1 0.8718, precision 0.8723, recall 0.8713, auc 0.8719
epoch 6501, loss 0.2699, train acc 87.29%, f1 0.8727, precision 0.8736, recall 0.8719, auc 0.8729
epoch 6601, loss 0.2387, train acc 87.32%, f1 0.8732, precision 0.8733, recall 0.8730, auc 0.8732
epoch 6701, loss 0.1640, train acc 87.39%, f1 0.8740, precision 0.8735, recall 0.8745, auc 0.8739
epoch 6801, loss 0.2405, train acc 87.40%, f1 0.8739, precision 0.8741, recall 0.8738, auc 0.8740
epoch 6901, loss 0.2343, train acc 87.38%, f1 0.8739, precision 0.8732, recall 0.8746, auc 0.8738
epoch 7001, loss 0.3779, train acc 87.39%, f1 0.8739, precision 0.8738, recall 0.8740, auc 0.8739
epoch 7101, loss 0.3201, train acc 87.44%, f1 0.8745, precision 0.8737, recall 0.8754, auc 0.8744
epoch 7201, loss 0.2900, train acc 87.41%, f1 0.8740, precision 0.8746, recall 0.8733, auc 0.8741
epoch 7301, loss 0.3979, train acc 87.46%, f1 0.8746, precision 0.8744, recall 0.8748, auc 0.8746
epoch 7401, loss 0.3277, train acc 87.45%, f1 0.8745, precision 0.8741, recall 0.8750, auc 0.8745
epoch 7501, loss 0.2845, train acc 87.50%, f1 0.8750, precision 0.8749, recall 0.8751, auc 0.8750
epoch 7601, loss 0.2788, train acc 87.55%, f1 0.8755, precision 0.8756, recall 0.8754, auc 0.8755
epoch 7701, loss 0.2963, train acc 87.62%, f1 0.8762, precision 0.8762, recall 0.8762, auc 0.8762
epoch 7801, loss 0.2145, train acc 87.59%, f1 0.8761, precision 0.8751, recall 0.8770, auc 0.8759
epoch 7901, loss 0.3572, train acc 87.62%, f1 0.8762, precision 0.8768, recall 0.8756, auc 0.8762
epoch 8001, loss 0.2842, train acc 87.61%, f1 0.8761, precision 0.8758, recall 0.8763, auc 0.8761
epoch 8101, loss 0.2887, train acc 87.65%, f1 0.8765, precision 0.8768, recall 0.8762, auc 0.8765
epoch 8201, loss 0.2320, train acc 87.69%, f1 0.8769, precision 0.8770, recall 0.8768, auc 0.8769/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.3199, train acc 87.68%, f1 0.8767, precision 0.8774, recall 0.8760, auc 0.8768
epoch 8401, loss 0.3258, train acc 87.74%, f1 0.8775, precision 0.8771, recall 0.8778, auc 0.8774
epoch 8501, loss 0.2085, train acc 87.72%, f1 0.8772, precision 0.8774, recall 0.8771, auc 0.8772
epoch 8601, loss 0.3336, train acc 87.78%, f1 0.8779, precision 0.8771, recall 0.8787, auc 0.8778
epoch 8701, loss 0.3463, train acc 87.77%, f1 0.8776, precision 0.8778, recall 0.8775, auc 0.8777
epoch 8801, loss 0.2580, train acc 87.83%, f1 0.8784, precision 0.8778, recall 0.8789, auc 0.8783
epoch 8901, loss 0.3516, train acc 87.89%, f1 0.8790, precision 0.8783, recall 0.8798, auc 0.8789
epoch 9001, loss 0.1997, train acc 87.90%, f1 0.8790, precision 0.8785, recall 0.8795, auc 0.8790
epoch 9101, loss 0.3012, train acc 87.91%, f1 0.8791, precision 0.8789, recall 0.8792, auc 0.8791
epoch 9201, loss 0.1883, train acc 88.03%, f1 0.8802, precision 0.8803, recall 0.8802, auc 0.8803
epoch 9301, loss 0.3497, train acc 88.00%, f1 0.8802, precision 0.8788, recall 0.8817, auc 0.8800
epoch 9401, loss 0.1929, train acc 88.07%, f1 0.8809, precision 0.8801, recall 0.8816, auc 0.8807
epoch 9501, loss 0.2887, train acc 88.08%, f1 0.8808, precision 0.8811, recall 0.8805, auc 0.8808
epoch 9601, loss 0.2799, train acc 88.15%, f1 0.8815, precision 0.8816, recall 0.8814, auc 0.8815
epoch 9701, loss 0.2362, train acc 88.16%, f1 0.8817, precision 0.8807, recall 0.8828, auc 0.8816
epoch 9801, loss 0.2552, train acc 88.15%, f1 0.8816, precision 0.8809, recall 0.8822, auc 0.8815
epoch 9901, loss 0.2423, train acc 88.19%, f1 0.8819, precision 0.8815, recall 0.8823, auc 0.8819
epoch 10001, loss 0.2901, train acc 88.30%, f1 0.8831, precision 0.8827, recall 0.8835, auc 0.8830
epoch 10101, loss 0.2658, train acc 88.28%, f1 0.8828, precision 0.8827, recall 0.8830, auc 0.8828
epoch 10201, loss 0.2260, train acc 88.34%, f1 0.8835, precision 0.8826, recall 0.8843, auc 0.8834
epoch 10301, loss 0.1863, train acc 88.38%, f1 0.8837, precision 0.8840, recall 0.8834, auc 0.8838
epoch 10401, loss 0.2467, train acc 88.38%, f1 0.8838, precision 0.8837, recall 0.8839, auc 0.8838
epoch 10501, loss 0.2678, train acc 88.46%, f1 0.8845, precision 0.8846, recall 0.8845, auc 0.8846
epoch 10601, loss 0.1812, train acc 88.52%, f1 0.8853, precision 0.8843, recall 0.8863, auc 0.8852
epoch 10701, loss 0.2482, train acc 88.57%, f1 0.8857, precision 0.8857, recall 0.8858, auc 0.8857
epoch 10801, loss 0.3123, train acc 88.59%, f1 0.8859, precision 0.8855, recall 0.8863, auc 0.8859
epoch 10901, loss 0.3280, train acc 88.65%, f1 0.8865, precision 0.8865, recall 0.8866, auc 0.8865
epoch 11001, loss 0.3178, train acc 88.66%, f1 0.8866, precision 0.8866, recall 0.8866, auc 0.8866
epoch 11101, loss 0.3231, train acc 88.66%, f1 0.8866, precision 0.8864, recall 0.8868, auc 0.8866
epoch 11201, loss 0.3085, train acc 88.76%, f1 0.8878, precision 0.8867, recall 0.8889, auc 0.8876
epoch 11301, loss 0.2109, train acc 88.79%, f1 0.8881, precision 0.8870, recall 0.8891, auc 0.8879
epoch 11401, loss 0.1967, train acc 88.88%, f1 0.8888, precision 0.8890, recall 0.8885, auc 0.8888
epoch 11501, loss 0.2639, train acc 88.94%, f1 0.8894, precision 0.8894, recall 0.8893, auc 0.8894
epoch 11601, loss 0.2920, train acc 88.93%, f1 0.8894, precision 0.8892, recall 0.8895, auc 0.8893
epoch 11701, loss 0.2919, train acc 88.93%, f1 0.8893, precision 0.8894, recall 0.8891, auc 0.8893
epoch 11801, loss 0.3007, train acc 89.07%, f1 0.8908, precision 0.8903, recall 0.8913, auc 0.8907
epoch 11901, loss 0.3503, train acc 89.11%, f1 0.8911, precision 0.8914, recall 0.8907, auc 0.8911
epoch 12001, loss 0.2138, train acc 89.19%, f1 0.8919, precision 0.8922, recall 0.8915, auc 0.8919
epoch 12101, loss 0.2843, train acc 89.21%, f1 0.8921, precision 0.8928, recall 0.8913, auc 0.8921
epoch 12201, loss 0.2502, train acc 89.19%, f1 0.8920, precision 0.8911, recall 0.8929, auc 0.8919
epoch 12301, loss 0.2633, train acc 89.28%, f1 0.8929, precision 0.8918, recall 0.8941, auc 0.8928
epoch 12401, loss 0.2330, train acc 89.40%, f1 0.8942, precision 0.8922, recall 0.8962, auc 0.8940
epoch 12501, loss 0.3002, train acc 89.43%, f1 0.8943, precision 0.8944, recall 0.8943, auc 0.8943
epoch 12601, loss 0.2458, train acc 89.46%, f1 0.8947, precision 0.8938, recall 0.8956, auc 0.8946
epoch 12701, loss 0.3233, train acc 89.51%, f1 0.8951, precision 0.8950, recall 0.8952, auc 0.8951
epoch 12801, loss 0.2257, train acc 89.58%, f1 0.8958, precision 0.8956, recall 0.8961, auc 0.8958
epoch 12901, loss 0.2971, train acc 89.64%, f1 0.8965, precision 0.8953, recall 0.8977, auc 0.8964
epoch 13001, loss 0.2215, train acc 89.61%, f1 0.8963, precision 0.8952, recall 0.8973, auc 0.8961
epoch 13101, loss 0.3085, train acc 89.75%, f1 0.8974, precision 0.8986, recall 0.8962, auc 0.8975
epoch 13201, loss 0.1792, train acc 89.77%, f1 0.8977, precision 0.8978, recall 0.8976, auc 0.8977
epoch 13301, loss 0.3506, train acc 89.86%, f1 0.8986, precision 0.8982, recall 0.8991, auc 0.8986
epoch 13401, loss 0.3307, train acc 89.84%, f1 0.8985, precision 0.8977, recall 0.8993, auc 0.8984
epoch 13501, loss 0.2778, train acc 89.79%, f1 0.8979, precision 0.8976, recall 0.8982, auc 0.8979
epoch 13601, loss 0.2476, train acc 89.96%, f1 0.8997, precision 0.8983, recall 0.9012, auc 0.8996
epoch 13701, loss 0.2863, train acc 89.91%, f1 0.8991, precision 0.8986, recall 0.8997, auc 0.8991
epoch 13801, loss 0.2747, train acc 90.06%, f1 0.9007, precision 0.8998, recall 0.9016, auc 0.9006
epoch 13901, loss 0.2878, train acc 90.08%, f1 0.9008, precision 0.9001, recall 0.9015, auc 0.9008
epoch 14001, loss 0.2491, train acc 90.05%, f1 0.9006, precision 0.8999, recall 0.9013, auc 0.9005
epoch 14101, loss 0.2876, train acc 90.20%, f1 0.9020, precision 0.9013, recall 0.9028, auc 0.9020
epoch 14201, loss 0.2770, train acc 90.24%, f1 0.9024, precision 0.9030, recall 0.9017, auc 0.9024
epoch 14301, loss 0.2618, train acc 90.26%, f1 0.9025, precision 0.9032, recall 0.9019, auc 0.9026
epoch 14401, loss 0.2718, train acc 90.25%, f1 0.9027, precision 0.9010, recall 0.9045, auc 0.9025
epoch 14501, loss 0.2600, train acc 90.31%, f1 0.9032, precision 0.9028, recall 0.9036, auc 0.9031
epoch 14601, loss 0.1939, train acc 90.34%, f1 0.9037, precision 0.9015, recall 0.9058, auc 0.9034
epoch 14701, loss 0.2358, train acc 90.34%, f1 0.9034, precision 0.9027, recall 0.9042, auc 0.9034
epoch 14801, loss 0.1830, train acc 90.37%, f1 0.9037, precision 0.9032, recall 0.9042, auc 0.9037
epoch 14901, loss 0.1770, train acc 90.44%, f1 0.9045, precision 0.9043, recall 0.9046, auc 0.9044
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_2
./test_pima/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6622222222222223

the Fscore is 0.6107784431137724

the precision is 0.45132743362831856

the recall is 0.9444444444444444

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_2
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5437, train acc 78.81%, f1 0.7855, precision 0.7952, recall 0.7761, auc 0.7881
epoch 201, loss 0.3854, train acc 81.85%, f1 0.8187, precision 0.8180, recall 0.8194, auc 0.8185
epoch 301, loss 0.3960, train acc 83.05%, f1 0.8310, precision 0.8285, recall 0.8335, auc 0.8305
epoch 401, loss 0.2082, train acc 83.70%, f1 0.8374, precision 0.8354, recall 0.8394, auc 0.8370
epoch 501, loss 0.5023, train acc 83.88%, f1 0.8392, precision 0.8370, recall 0.8414, auc 0.8388
epoch 601, loss 0.5222, train acc 83.95%, f1 0.8400, precision 0.8377, recall 0.8423, auc 0.8395
epoch 701, loss 0.3622, train acc 84.00%, f1 0.8404, precision 0.8387, recall 0.8421, auc 0.8400
epoch 801, loss 0.3692, train acc 84.07%, f1 0.8410, precision 0.8393, recall 0.8426, auc 0.8407
epoch 901, loss 0.4857, train acc 84.07%, f1 0.8410, precision 0.8396, recall 0.8424, auc 0.8407
epoch 1001, loss 0.4294, train acc 84.03%, f1 0.8407, precision 0.8384, recall 0.8431, auc 0.8403
epoch 1101, loss 0.3960, train acc 84.04%, f1 0.8407, precision 0.8392, recall 0.8422, auc 0.8404
epoch 1201, loss 0.2964, train acc 84.04%, f1 0.8406, precision 0.8393, recall 0.8419, auc 0.8404
epoch 1301, loss 0.3012, train acc 84.01%, f1 0.8404, precision 0.8389, recall 0.8420, auc 0.8401
epoch 1401, loss 0.3244, train acc 84.09%, f1 0.8411, precision 0.8401, recall 0.8422, auc 0.8409
epoch 1501, loss 0.2790, train acc 84.05%, f1 0.8406, precision 0.8402, recall 0.8409, auc 0.8405
epoch 1601, loss 0.3246, train acc 84.08%, f1 0.8408, precision 0.8406, recall 0.8409, auc 0.8408
epoch 1701, loss 0.3395, train acc 84.11%, f1 0.8412, precision 0.8404, recall 0.8420, auc 0.8411
epoch 1801, loss 0.3593, train acc 84.16%, f1 0.8418, precision 0.8410, recall 0.8426, auc 0.8416
epoch 1901, loss 0.3671, train acc 84.13%, f1 0.8413, precision 0.8412, recall 0.8415, auc 0.8413
epoch 2001, loss 0.2834, train acc 84.21%, f1 0.8422, precision 0.8416, recall 0.8428, auc 0.8421
epoch 2101, loss 0.3721, train acc 84.20%, f1 0.8421, precision 0.8415, recall 0.8426, auc 0.8420
epoch 2201, loss 0.3955, train acc 84.18%, f1 0.8419, precision 0.8411, recall 0.8427, auc 0.8418
epoch 2301, loss 0.4028, train acc 84.23%, f1 0.8424, precision 0.8419, recall 0.8430, auc 0.8423
epoch 2401, loss 0.3365, train acc 84.27%, f1 0.8427, precision 0.8424, recall 0.8431, auc 0.8427
epoch 2501, loss 0.2581, train acc 84.36%, f1 0.8437, precision 0.8434, recall 0.8439, auc 0.8436
epoch 2601, loss 0.4036, train acc 84.39%, f1 0.8440, precision 0.8434, recall 0.8446, auc 0.8439
epoch 2701, loss 0.2183, train acc 84.50%, f1 0.8450, precision 0.8449, recall 0.8452, auc 0.8450
epoch 2801, loss 0.3282, train acc 84.58%, f1 0.8458, precision 0.8456, recall 0.8461, auc 0.8458
epoch 2901, loss 0.4107, train acc 84.65%, f1 0.8466, precision 0.8462, recall 0.8470, auc 0.8465
epoch 3001, loss 0.5023, train acc 84.76%, f1 0.8476, precision 0.8479, recall 0.8472, auc 0.8476
epoch 3101, loss 0.4402, train acc 84.89%, f1 0.8490, precision 0.8487, recall 0.8492, auc 0.8489
epoch 3201, loss 0.3275, train acc 85.00%, f1 0.8501, precision 0.8496, recall 0.8505, auc 0.8500
epoch 3301, loss 0.3087, train acc 85.16%, f1 0.8517, precision 0.8511, recall 0.8524, auc 0.8516
epoch 3401, loss 0.4918, train acc 85.28%, f1 0.8528, precision 0.8528, recall 0.8529, auc 0.8528
epoch 3501, loss 0.3044, train acc 85.43%, f1 0.8542, precision 0.8547, recall 0.8536, auc 0.8543
epoch 3601, loss 0.3760, train acc 85.52%, f1 0.8551, precision 0.8556, recall 0.8546, auc 0.8552
epoch 3701, loss 0.3599, train acc 85.70%, f1 0.8570, precision 0.8569, recall 0.8571, auc 0.8570
epoch 3801, loss 0.3493, train acc 85.79%, f1 0.8579, precision 0.8582, recall 0.8576, auc 0.8579
epoch 3901, loss 0.2926, train acc 85.95%, f1 0.8595, precision 0.8595, recall 0.8595, auc 0.8595
epoch 4001, loss 0.3182, train acc 86.03%, f1 0.8603, precision 0.8604, recall 0.8601, auc 0.8603
epoch 4101, loss 0.2653, train acc 86.18%, f1 0.8617, precision 0.8620, recall 0.8613, auc 0.8618
epoch 4201, loss 0.2967, train acc 86.24%, f1 0.8623, precision 0.8632, recall 0.8614, auc 0.8624
epoch 4301, loss 0.2734, train acc 86.32%, f1 0.8630, precision 0.8641, recall 0.8619, auc 0.8632
epoch 4401, loss 0.3859, train acc 86.41%, f1 0.8640, precision 0.8644, recall 0.8637, auc 0.8641
epoch 4501, loss 0.2994, train acc 86.48%, f1 0.8647, precision 0.8656, recall 0.8638, auc 0.8648
epoch 4601, loss 0.2413, train acc 86.58%, f1 0.8657, precision 0.8661, recall 0.8654, auc 0.8658
epoch 4701, loss 0.2523, train acc 86.65%, f1 0.8665, precision 0.8669, recall 0.8661, auc 0.8665
epoch 4801, loss 0.3239, train acc 86.72%, f1 0.8671, precision 0.8676, recall 0.8667, auc 0.8672
epoch 4901, loss 0.3410, train acc 86.77%, f1 0.8678, precision 0.8673, recall 0.8682, auc 0.8677
epoch 5001, loss 0.3576, train acc 86.84%, f1 0.8684, precision 0.8687, recall 0.8681, auc 0.8684
epoch 5101, loss 0.2842, train acc 86.87%, f1 0.8685, precision 0.8696, recall 0.8675, auc 0.8687
epoch 5201, loss 0.2073, train acc 86.88%, f1 0.8688, precision 0.8687, recall 0.8688, auc 0.8688
epoch 5301, loss 0.3615, train acc 86.95%, f1 0.8695, precision 0.8694, recall 0.8697, auc 0.8695
epoch 5401, loss 0.3720, train acc 87.00%, f1 0.8700, precision 0.8700, recall 0.8700, auc 0.8700
epoch 5501, loss 0.3151, train acc 87.08%, f1 0.8707, precision 0.8712, recall 0.8701, auc 0.8708
epoch 5601, loss 0.2636, train acc 87.12%, f1 0.8712, precision 0.8712, recall 0.8713, auc 0.8712
epoch 5701, loss 0.3420, train acc 87.13%, f1 0.8712, precision 0.8723, recall 0.8700, auc 0.8713
epoch 5801, loss 0.2670, train acc 87.16%, f1 0.8715, precision 0.8719, recall 0.8711, auc 0.8716
epoch 5901, loss 0.2911, train acc 87.19%, f1 0.8721, precision 0.8706, recall 0.8737, auc 0.8719
epoch 6001, loss 0.2548, train acc 87.24%, f1 0.8723, precision 0.8725, recall 0.8722, auc 0.8724
epoch 6101, loss 0.2082, train acc 87.26%, f1 0.8725, precision 0.8729, recall 0.8721, auc 0.8726
epoch 6201, loss 0.4191, train acc 87.25%, f1 0.8725, precision 0.8721, recall 0.8730, auc 0.8725
epoch 6301, loss 0.2378, train acc 87.32%, f1 0.8733, precision 0.8726, recall 0.8739, auc 0.8732
epoch 6401, loss 0.2637, train acc 87.39%, f1 0.8740, precision 0.8737, recall 0.8743, auc 0.8739
epoch 6501, loss 0.2584, train acc 87.35%, f1 0.8735, precision 0.8733, recall 0.8737, auc 0.8735
epoch 6601, loss 0.2757, train acc 87.40%, f1 0.8740, precision 0.8740, recall 0.8739, auc 0.8740
epoch 6701, loss 0.3446, train acc 87.45%, f1 0.8746, precision 0.8744, recall 0.8748, auc 0.8745
epoch 6801, loss 0.2655, train acc 87.46%, f1 0.8747, precision 0.8740, recall 0.8754, auc 0.8746
epoch 6901, loss 0.2493, train acc 87.49%, f1 0.8749, precision 0.8749, recall 0.8748, auc 0.8749
epoch 7001, loss 0.2848, train acc 87.53%, f1 0.8753, precision 0.8752, recall 0.8754, auc 0.8753
epoch 7101, loss 0.2281, train acc 87.60%, f1 0.8761, precision 0.8754, recall 0.8767, auc 0.8760
epoch 7201, loss 0.4097, train acc 87.64%, f1 0.8764, precision 0.8760, recall 0.8768, auc 0.8764
epoch 7301, loss 0.2074, train acc 87.71%, f1 0.8771, precision 0.8772, recall 0.8769, auc 0.8771
epoch 7401, loss 0.3178, train acc 87.67%, f1 0.8768, precision 0.8760, recall 0.8777, auc 0.8767
epoch 7501, loss 0.2709, train acc 87.72%, f1 0.8773, precision 0.8766, recall 0.8781, auc 0.8772
epoch 7601, loss 0.2405, train acc 87.74%, f1 0.8774, precision 0.8773, recall 0.8776, auc 0.8774
epoch 7701, loss 0.3234, train acc 87.74%, f1 0.8776, precision 0.8766, recall 0.8786, auc 0.8774
epoch 7801, loss 0.2223, train acc 87.78%, f1 0.8778, precision 0.8778, recall 0.8777, auc 0.8778
epoch 7901, loss 0.3146, train acc 87.79%, f1 0.8780, precision 0.8772, recall 0.8788, auc 0.8779
epoch 8001, loss 0.2713, train acc 87.81%, f1 0.8782, precision 0.8777, recall 0.8787, auc 0.8781
epoch 8101, loss 0.3034, train acc 87.84%, f1 0.8783, precision 0.8787, recall 0.8780, auc 0.8784
epoch 8201, loss 0.3549, train acc 87.86%, f1 0.8785, precision 0.8789, recall 0.8781, auc 0.8786/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2065, train acc 87.87%, f1 0.8788, precision 0.8784, recall 0.8791, auc 0.8787
epoch 8401, loss 0.3580, train acc 87.97%, f1 0.8800, precision 0.8783, recall 0.8817, auc 0.8797
epoch 8501, loss 0.2783, train acc 88.02%, f1 0.8802, precision 0.8801, recall 0.8804, auc 0.8802
epoch 8601, loss 0.2837, train acc 88.02%, f1 0.8803, precision 0.8796, recall 0.8810, auc 0.8802
epoch 8701, loss 0.2124, train acc 88.08%, f1 0.8809, precision 0.8805, recall 0.8812, auc 0.8808
epoch 8801, loss 0.3486, train acc 88.08%, f1 0.8808, precision 0.8805, recall 0.8811, auc 0.8808
epoch 8901, loss 0.3717, train acc 88.10%, f1 0.8811, precision 0.8804, recall 0.8818, auc 0.8810
epoch 9001, loss 0.2190, train acc 88.14%, f1 0.8814, precision 0.8811, recall 0.8817, auc 0.8814
epoch 9101, loss 0.2585, train acc 88.12%, f1 0.8811, precision 0.8815, recall 0.8807, auc 0.8812
epoch 9201, loss 0.3575, train acc 88.20%, f1 0.8821, precision 0.8819, recall 0.8822, auc 0.8820
epoch 9301, loss 0.2798, train acc 88.23%, f1 0.8823, precision 0.8821, recall 0.8825, auc 0.8823
epoch 9401, loss 0.2793, train acc 88.28%, f1 0.8829, precision 0.8820, recall 0.8838, auc 0.8828
epoch 9501, loss 0.2686, train acc 88.35%, f1 0.8835, precision 0.8829, recall 0.8842, auc 0.8835
epoch 9601, loss 0.3341, train acc 88.30%, f1 0.8830, precision 0.8834, recall 0.8826, auc 0.8830
epoch 9701, loss 0.3256, train acc 88.38%, f1 0.8839, precision 0.8834, recall 0.8843, auc 0.8838
epoch 9801, loss 0.2461, train acc 88.35%, f1 0.8834, precision 0.8837, recall 0.8831, auc 0.8835
epoch 9901, loss 0.2493, train acc 88.43%, f1 0.8843, precision 0.8841, recall 0.8844, auc 0.8843
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_2
./test_pima/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6814814814814815

the Fscore is 0.6265060240963856

the precision is 0.4642857142857143

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_2
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5618, train acc 78.94%, f1 0.7918, precision 0.7828, recall 0.8009, auc 0.7894
epoch 201, loss 0.4456, train acc 81.76%, f1 0.8173, precision 0.8189, recall 0.8157, auc 0.8176
epoch 301, loss 0.3686, train acc 83.14%, f1 0.8311, precision 0.8325, recall 0.8296, auc 0.8314
epoch 401, loss 0.4365, train acc 83.67%, f1 0.8363, precision 0.8385, recall 0.8341, auc 0.8367
epoch 501, loss 0.3309, train acc 83.87%, f1 0.8383, precision 0.8405, recall 0.8362, auc 0.8387
epoch 601, loss 0.3390, train acc 84.04%, f1 0.8401, precision 0.8421, recall 0.8380, auc 0.8404
epoch 701, loss 0.4981, train acc 84.10%, f1 0.8407, precision 0.8425, recall 0.8389, auc 0.8410
epoch 801, loss 0.3057, train acc 84.11%, f1 0.8409, precision 0.8422, recall 0.8396, auc 0.8411
epoch 901, loss 0.4151, train acc 84.18%, f1 0.8415, precision 0.8430, recall 0.8401, auc 0.8418
epoch 1001, loss 0.2605, train acc 84.13%, f1 0.8411, precision 0.8422, recall 0.8400, auc 0.8413
epoch 1101, loss 0.3038, train acc 84.08%, f1 0.8406, precision 0.8417, recall 0.8396, auc 0.8408
epoch 1201, loss 0.4482, train acc 84.02%, f1 0.8400, precision 0.8411, recall 0.8390, auc 0.8402
epoch 1301, loss 0.2482, train acc 84.00%, f1 0.8399, precision 0.8404, recall 0.8395, auc 0.8400
epoch 1401, loss 0.4648, train acc 84.13%, f1 0.8412, precision 0.8416, recall 0.8409, auc 0.8413
epoch 1501, loss 0.2858, train acc 84.11%, f1 0.8410, precision 0.8416, recall 0.8405, auc 0.8411
epoch 1601, loss 0.3084, train acc 84.13%, f1 0.8411, precision 0.8420, recall 0.8402, auc 0.8413
epoch 1701, loss 0.2774, train acc 84.11%, f1 0.8411, precision 0.8415, recall 0.8406, auc 0.8411
epoch 1801, loss 0.4090, train acc 84.10%, f1 0.8409, precision 0.8414, recall 0.8404, auc 0.8410
epoch 1901, loss 0.4407, train acc 84.13%, f1 0.8413, precision 0.8411, recall 0.8414, auc 0.8413
epoch 2001, loss 0.3343, train acc 84.11%, f1 0.8409, precision 0.8421, recall 0.8397, auc 0.8411
epoch 2101, loss 0.3042, train acc 84.15%, f1 0.8414, precision 0.8419, recall 0.8409, auc 0.8415
epoch 2201, loss 0.2845, train acc 84.16%, f1 0.8415, precision 0.8419, recall 0.8411, auc 0.8416
epoch 2301, loss 0.3601, train acc 84.18%, f1 0.8418, precision 0.8417, recall 0.8418, auc 0.8418
epoch 2401, loss 0.2789, train acc 84.24%, f1 0.8423, precision 0.8428, recall 0.8418, auc 0.8424
epoch 2501, loss 0.2937, train acc 84.30%, f1 0.8429, precision 0.8432, recall 0.8427, auc 0.8430
epoch 2601, loss 0.3110, train acc 84.41%, f1 0.8441, precision 0.8442, recall 0.8440, auc 0.8441
epoch 2701, loss 0.4152, train acc 84.48%, f1 0.8447, precision 0.8452, recall 0.8441, auc 0.8448
epoch 2801, loss 0.4444, train acc 84.50%, f1 0.8448, precision 0.8456, recall 0.8441, auc 0.8450
epoch 2901, loss 0.3262, train acc 84.63%, f1 0.8463, precision 0.8463, recall 0.8464, auc 0.8463
epoch 3001, loss 0.3523, train acc 84.76%, f1 0.8476, precision 0.8475, recall 0.8476, auc 0.8476
epoch 3101, loss 0.3009, train acc 84.92%, f1 0.8491, precision 0.8493, recall 0.8489, auc 0.8492
epoch 3201, loss 0.2542, train acc 85.08%, f1 0.8507, precision 0.8511, recall 0.8503, auc 0.8508
epoch 3301, loss 0.3476, train acc 85.19%, f1 0.8519, precision 0.8518, recall 0.8519, auc 0.8519
epoch 3401, loss 0.3002, train acc 85.24%, f1 0.8524, precision 0.8525, recall 0.8522, auc 0.8524
epoch 3501, loss 0.3139, train acc 85.33%, f1 0.8533, precision 0.8533, recall 0.8532, auc 0.8533
epoch 3601, loss 0.4062, train acc 85.47%, f1 0.8546, precision 0.8548, recall 0.8545, auc 0.8547
epoch 3701, loss 0.2603, train acc 85.63%, f1 0.8564, precision 0.8559, recall 0.8569, auc 0.8563
epoch 3801, loss 0.2929, train acc 85.66%, f1 0.8567, precision 0.8563, recall 0.8571, auc 0.8566
epoch 3901, loss 0.4015, train acc 85.84%, f1 0.8584, precision 0.8582, recall 0.8587, auc 0.8584
epoch 4001, loss 0.3021, train acc 85.88%, f1 0.8589, precision 0.8582, recall 0.8596, auc 0.8588
epoch 4101, loss 0.2977, train acc 86.01%, f1 0.8600, precision 0.8605, recall 0.8596, auc 0.8601
epoch 4201, loss 0.3448, train acc 86.10%, f1 0.8609, precision 0.8615, recall 0.8603, auc 0.8610
epoch 4301, loss 0.3055, train acc 86.21%, f1 0.8622, precision 0.8615, recall 0.8629, auc 0.8621
epoch 4401, loss 0.2685, train acc 86.28%, f1 0.8630, precision 0.8622, recall 0.8637, auc 0.8628
epoch 4501, loss 0.4199, train acc 86.38%, f1 0.8637, precision 0.8641, recall 0.8633, auc 0.8638
epoch 4601, loss 0.2419, train acc 86.41%, f1 0.8640, precision 0.8646, recall 0.8634, auc 0.8641
epoch 4701, loss 0.4535, train acc 86.51%, f1 0.8652, precision 0.8645, recall 0.8660, auc 0.8651
epoch 4801, loss 0.2696, train acc 86.57%, f1 0.8656, precision 0.8659, recall 0.8653, auc 0.8657
epoch 4901, loss 0.3376, train acc 86.67%, f1 0.8669, precision 0.8659, recall 0.8678, auc 0.8667
epoch 5001, loss 0.2645, train acc 86.71%, f1 0.8672, precision 0.8667, recall 0.8677, auc 0.8671
epoch 5101, loss 0.3393, train acc 86.74%, f1 0.8675, precision 0.8668, recall 0.8683, auc 0.8674
epoch 5201, loss 0.4660, train acc 86.78%, f1 0.8679, precision 0.8672, recall 0.8686, auc 0.8678
epoch 5301, loss 0.3802, train acc 86.85%, f1 0.8687, precision 0.8674, recall 0.8699, auc 0.8685
epoch 5401, loss 0.2880, train acc 86.91%, f1 0.8693, precision 0.8684, recall 0.8701, auc 0.8691
epoch 5501, loss 0.3865, train acc 86.89%, f1 0.8689, precision 0.8689, recall 0.8690, auc 0.8689
epoch 5601, loss 0.4117, train acc 86.98%, f1 0.8697, precision 0.8703, recall 0.8690, auc 0.8698
epoch 5701, loss 0.2978, train acc 87.01%, f1 0.8701, precision 0.8703, recall 0.8699, auc 0.8701
epoch 5801, loss 0.2843, train acc 87.04%, f1 0.8705, precision 0.8702, recall 0.8708, auc 0.8704
epoch 5901, loss 0.2377, train acc 87.10%, f1 0.8711, precision 0.8706, recall 0.8715, auc 0.8710
epoch 6001, loss 0.2547, train acc 87.18%, f1 0.8718, precision 0.8722, recall 0.8713, auc 0.8718
epoch 6101, loss 0.3010, train acc 87.22%, f1 0.8722, precision 0.8723, recall 0.8722, auc 0.8722
epoch 6201, loss 0.1807, train acc 87.15%, f1 0.8714, precision 0.8719, recall 0.8709, auc 0.8715
epoch 6301, loss 0.2757, train acc 87.21%, f1 0.8722, precision 0.8720, recall 0.8723, auc 0.8721
epoch 6401, loss 0.2848, train acc 87.19%, f1 0.8720, precision 0.8719, recall 0.8720, auc 0.8719
epoch 6501, loss 0.3029, train acc 87.26%, f1 0.8725, precision 0.8730, recall 0.8720, auc 0.8726
epoch 6601, loss 0.2557, train acc 87.33%, f1 0.8734, precision 0.8727, recall 0.8741, auc 0.8733
epoch 6701, loss 0.2533, train acc 87.31%, f1 0.8731, precision 0.8727, recall 0.8735, auc 0.8731
epoch 6801, loss 0.3470, train acc 87.40%, f1 0.8739, precision 0.8743, recall 0.8735, auc 0.8740
epoch 6901, loss 0.3408, train acc 87.45%, f1 0.8745, precision 0.8743, recall 0.8748, auc 0.8745
epoch 7001, loss 0.1850, train acc 87.47%, f1 0.8748, precision 0.8737, recall 0.8760, auc 0.8747
epoch 7101, loss 0.3379, train acc 87.50%, f1 0.8750, precision 0.8746, recall 0.8754, auc 0.8750
epoch 7201, loss 0.2641, train acc 87.51%, f1 0.8750, precision 0.8754, recall 0.8746, auc 0.8751
epoch 7301, loss 0.3254, train acc 87.56%, f1 0.8758, precision 0.8750, recall 0.8765, auc 0.8756
epoch 7401, loss 0.3361, train acc 87.53%, f1 0.8754, precision 0.8744, recall 0.8764, auc 0.8753
epoch 7501, loss 0.3996, train acc 87.58%, f1 0.8757, precision 0.8759, recall 0.8756, auc 0.8758
epoch 7601, loss 0.2862, train acc 87.62%, f1 0.8763, precision 0.8753, recall 0.8773, auc 0.8762
epoch 7701, loss 0.2918, train acc 87.66%, f1 0.8764, precision 0.8781, recall 0.8746, auc 0.8766
epoch 7801, loss 0.2170, train acc 87.64%, f1 0.8765, precision 0.8763, recall 0.8766, auc 0.8764
epoch 7901, loss 0.2079, train acc 87.66%, f1 0.8768, precision 0.8754, recall 0.8781, auc 0.8766
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_2
./test_pima/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6657407407407407

the Fscore is 0.616279069767442

the precision is 0.4491525423728814

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_2
----------------------



epoch 1, loss 0.6937, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5704, train acc 78.76%, f1 0.7882, precision 0.7860, recall 0.7904, auc 0.7876
epoch 201, loss 0.4146, train acc 81.80%, f1 0.8180, precision 0.8183, recall 0.8176, auc 0.8180
epoch 301, loss 0.3008, train acc 83.01%, f1 0.8300, precision 0.8304, recall 0.8296, auc 0.8301
epoch 401, loss 0.4883, train acc 83.65%, f1 0.8363, precision 0.8374, recall 0.8353, auc 0.8365
epoch 501, loss 0.2981, train acc 83.83%, f1 0.8381, precision 0.8388, recall 0.8375, auc 0.8383
epoch 601, loss 0.4628, train acc 83.97%, f1 0.8395, precision 0.8403, recall 0.8387, auc 0.8397
epoch 701, loss 0.3630, train acc 84.02%, f1 0.8401, precision 0.8405, recall 0.8397, auc 0.8402
epoch 801, loss 0.3374, train acc 84.09%, f1 0.8408, precision 0.8412, recall 0.8404, auc 0.8409
epoch 901, loss 0.3573, train acc 84.07%, f1 0.8406, precision 0.8409, recall 0.8403, auc 0.8407
epoch 1001, loss 0.3247, train acc 84.08%, f1 0.8408, precision 0.8408, recall 0.8408, auc 0.8408
epoch 1101, loss 0.3416, train acc 84.10%, f1 0.8409, precision 0.8412, recall 0.8407, auc 0.8410
epoch 1201, loss 0.2636, train acc 84.06%, f1 0.8406, precision 0.8407, recall 0.8404, auc 0.8406
epoch 1301, loss 0.4107, train acc 84.07%, f1 0.8407, precision 0.8410, recall 0.8403, auc 0.8407
epoch 1401, loss 0.4746, train acc 84.14%, f1 0.8414, precision 0.8415, recall 0.8413, auc 0.8414
epoch 1501, loss 0.2961, train acc 84.11%, f1 0.8411, precision 0.8411, recall 0.8411, auc 0.8411
epoch 1601, loss 0.3440, train acc 84.18%, f1 0.8417, precision 0.8420, recall 0.8414, auc 0.8418
epoch 1701, loss 0.4435, train acc 84.15%, f1 0.8415, precision 0.8416, recall 0.8414, auc 0.8415
epoch 1801, loss 0.3804, train acc 84.15%, f1 0.8415, precision 0.8416, recall 0.8414, auc 0.8415
epoch 1901, loss 0.4042, train acc 84.12%, f1 0.8412, precision 0.8413, recall 0.8411, auc 0.8412
epoch 2001, loss 0.3238, train acc 84.14%, f1 0.8413, precision 0.8415, recall 0.8412, auc 0.8414
epoch 2101, loss 0.4229, train acc 84.19%, f1 0.8418, precision 0.8422, recall 0.8414, auc 0.8419
epoch 2201, loss 0.2873, train acc 84.22%, f1 0.8422, precision 0.8421, recall 0.8423, auc 0.8422
epoch 2301, loss 0.4274, train acc 84.23%, f1 0.8423, precision 0.8426, recall 0.8420, auc 0.8423
epoch 2401, loss 0.2938, train acc 84.20%, f1 0.8420, precision 0.8422, recall 0.8417, auc 0.8420
epoch 2501, loss 0.2858, train acc 84.34%, f1 0.8433, precision 0.8437, recall 0.8430, auc 0.8434
epoch 2601, loss 0.3446, train acc 84.38%, f1 0.8438, precision 0.8439, recall 0.8436, auc 0.8438
epoch 2701, loss 0.2792, train acc 84.40%, f1 0.8439, precision 0.8445, recall 0.8434, auc 0.8440
epoch 2801, loss 0.3204, train acc 84.47%, f1 0.8446, precision 0.8451, recall 0.8441, auc 0.8447
epoch 2901, loss 0.3648, train acc 84.58%, f1 0.8457, precision 0.8460, recall 0.8454, auc 0.8458
epoch 3001, loss 0.2960, train acc 84.70%, f1 0.8470, precision 0.8474, recall 0.8466, auc 0.8470
epoch 3101, loss 0.3333, train acc 84.90%, f1 0.8489, precision 0.8495, recall 0.8484, auc 0.8490
epoch 3201, loss 0.2768, train acc 85.04%, f1 0.8504, precision 0.8508, recall 0.8499, auc 0.8504
epoch 3301, loss 0.3629, train acc 85.11%, f1 0.8511, precision 0.8514, recall 0.8508, auc 0.8511
epoch 3401, loss 0.2778, train acc 85.23%, f1 0.8522, precision 0.8528, recall 0.8516, auc 0.8523
epoch 3501, loss 0.3343, train acc 85.36%, f1 0.8534, precision 0.8541, recall 0.8527, auc 0.8536
epoch 3601, loss 0.2395, train acc 85.51%, f1 0.8551, precision 0.8554, recall 0.8547, auc 0.8551
epoch 3701, loss 0.2777, train acc 85.65%, f1 0.8564, precision 0.8572, recall 0.8556, auc 0.8565
epoch 3801, loss 0.3453, train acc 85.77%, f1 0.8576, precision 0.8584, recall 0.8568, auc 0.8577
epoch 3901, loss 0.2326, train acc 85.90%, f1 0.8590, precision 0.8593, recall 0.8586, auc 0.8590
epoch 4001, loss 0.4231, train acc 85.99%, f1 0.8599, precision 0.8600, recall 0.8598, auc 0.8599
epoch 4101, loss 0.3562, train acc 86.13%, f1 0.8612, precision 0.8616, recall 0.8608, auc 0.8613
epoch 4201, loss 0.2348, train acc 86.19%, f1 0.8618, precision 0.8626, recall 0.8610, auc 0.8619
epoch 4301, loss 0.2255, train acc 86.27%, f1 0.8626, precision 0.8631, recall 0.8620, auc 0.8627
epoch 4401, loss 0.2248, train acc 86.47%, f1 0.8646, precision 0.8648, recall 0.8645, auc 0.8647
epoch 4501, loss 0.3041, train acc 86.54%, f1 0.8653, precision 0.8656, recall 0.8650, auc 0.8654
epoch 4601, loss 0.3200, train acc 86.56%, f1 0.8655, precision 0.8659, recall 0.8652, auc 0.8656
epoch 4701, loss 0.3111, train acc 86.68%, f1 0.8668, precision 0.8667, recall 0.8669, auc 0.8668
epoch 4801, loss 0.3210, train acc 86.66%, f1 0.8665, precision 0.8671, recall 0.8659, auc 0.8666
epoch 4901, loss 0.3433, train acc 86.76%, f1 0.8676, precision 0.8676, recall 0.8676, auc 0.8676
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_2
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6457407407407407

the Fscore is 0.6022727272727272

the precision is 0.4344262295081967

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_2
----------------------



epoch 1, loss 0.6935, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5622, train acc 78.54%, f1 0.7848, precision 0.7871, recall 0.7824, auc 0.7854
epoch 201, loss 0.4075, train acc 81.67%, f1 0.8169, precision 0.8162, recall 0.8176, auc 0.8167
epoch 301, loss 0.3652, train acc 83.04%, f1 0.8305, precision 0.8298, recall 0.8313, auc 0.8304
epoch 401, loss 0.4931, train acc 83.64%, f1 0.8365, precision 0.8358, recall 0.8373, auc 0.8364
epoch 501, loss 0.3847, train acc 83.84%, f1 0.8385, precision 0.8380, recall 0.8391, auc 0.8384
epoch 601, loss 0.3503, train acc 84.05%, f1 0.8406, precision 0.8401, recall 0.8412, auc 0.8405
epoch 701, loss 0.5073, train acc 84.12%, f1 0.8413, precision 0.8405, recall 0.8421, auc 0.8412
epoch 801, loss 0.4104, train acc 84.10%, f1 0.8411, precision 0.8406, recall 0.8416, auc 0.8410
epoch 901, loss 0.4684, train acc 84.09%, f1 0.8409, precision 0.8406, recall 0.8412, auc 0.8409
epoch 1001, loss 0.2690, train acc 84.08%, f1 0.8409, precision 0.8402, recall 0.8416, auc 0.8408
epoch 1101, loss 0.4225, train acc 84.10%, f1 0.8411, precision 0.8408, recall 0.8413, auc 0.8410
epoch 1201, loss 0.3210, train acc 84.06%, f1 0.8407, precision 0.8398, recall 0.8416, auc 0.8406
epoch 1301, loss 0.4551, train acc 84.07%, f1 0.8408, precision 0.8404, recall 0.8413, auc 0.8407
epoch 1401, loss 0.3365, train acc 84.09%, f1 0.8410, precision 0.8403, recall 0.8417, auc 0.8409
epoch 1501, loss 0.3848, train acc 84.11%, f1 0.8412, precision 0.8405, recall 0.8420, auc 0.8411
epoch 1601, loss 0.2878, train acc 84.15%, f1 0.8416, precision 0.8409, recall 0.8423, auc 0.8415
epoch 1701, loss 0.3998, train acc 84.12%, f1 0.8414, precision 0.8405, recall 0.8423, auc 0.8412
epoch 1801, loss 0.3876, train acc 84.14%, f1 0.8415, precision 0.8409, recall 0.8420, auc 0.8414
epoch 1901, loss 0.4227, train acc 84.16%, f1 0.8417, precision 0.8411, recall 0.8422, auc 0.8416
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_2
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.5900000000000001

the Fscore is 0.5684210526315789

the precision is 0.39705882352941174

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_2
----------------------



epoch 1, loss 0.6932, train acc 49.93%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5196, train acc 78.60%, f1 0.7867, precision 0.7850, recall 0.7883, auc 0.7860
epoch 201, loss 0.4658, train acc 81.78%, f1 0.8171, precision 0.8212, recall 0.8130, auc 0.8178
epoch 301, loss 0.3683, train acc 83.02%, f1 0.8297, precision 0.8331, recall 0.8263, auc 0.8302
epoch 401, loss 0.4299, train acc 83.61%, f1 0.8364, precision 0.8359, recall 0.8369, auc 0.8361
epoch 501, loss 0.3708, train acc 83.83%, f1 0.8380, precision 0.8405, recall 0.8356, auc 0.8383
epoch 601, loss 0.4710, train acc 83.96%, f1 0.8398, precision 0.8400, recall 0.8395, auc 0.8396
epoch 701, loss 0.3357, train acc 84.04%, f1 0.8400, precision 0.8429, recall 0.8372, auc 0.8404
epoch 801, loss 0.2862, train acc 84.03%, f1 0.8401, precision 0.8418, recall 0.8385, auc 0.8403
epoch 901, loss 0.3155, train acc 84.10%, f1 0.8411, precision 0.8417, recall 0.8405, auc 0.8410
epoch 1001, loss 0.5171, train acc 84.08%, f1 0.8412, precision 0.8399, recall 0.8425, auc 0.8408
epoch 1101, loss 0.3083, train acc 84.07%, f1 0.8409, precision 0.8411, recall 0.8406, auc 0.8407
epoch 1201, loss 0.4162, train acc 84.08%, f1 0.8410, precision 0.8407, recall 0.8413, auc 0.8408
epoch 1301, loss 0.5947, train acc 84.12%, f1 0.8411, precision 0.8425, recall 0.8397, auc 0.8412
epoch 1401, loss 0.3305, train acc 84.12%, f1 0.8416, precision 0.8406, recall 0.8427, auc 0.8412
epoch 1501, loss 0.3606, train acc 84.14%, f1 0.8408, precision 0.8450, recall 0.8367, auc 0.8414
epoch 1601, loss 0.3060, train acc 84.13%, f1 0.8411, precision 0.8434, recall 0.8388, auc 0.8413
epoch 1701, loss 0.2688, train acc 84.20%, f1 0.8421, precision 0.8424, recall 0.8418, auc 0.8420
epoch 1801, loss 0.3250, train acc 84.13%, f1 0.8412, precision 0.8429, recall 0.8394, auc 0.8413
epoch 1901, loss 0.2941, train acc 84.08%, f1 0.8408, precision 0.8420, recall 0.8396, auc 0.8408
epoch 2001, loss 0.2780, train acc 84.10%, f1 0.8413, precision 0.8409, recall 0.8417, auc 0.8410
epoch 2101, loss 0.3806, train acc 84.04%, f1 0.8407, precision 0.8402, recall 0.8412, auc 0.8404
epoch 2201, loss 0.4037, train acc 84.15%, f1 0.8409, precision 0.8453, recall 0.8365, auc 0.8415
epoch 2301, loss 0.2378, train acc 84.15%, f1 0.8420, precision 0.8409, recall 0.8431, auc 0.8415
epoch 2401, loss 0.2769, train acc 84.15%, f1 0.8421, precision 0.8403, recall 0.8439, auc 0.8415
epoch 2501, loss 0.3439, train acc 84.17%, f1 0.8417, precision 0.8428, recall 0.8406, auc 0.8417
epoch 2601, loss 0.3682, train acc 84.04%, f1 0.8410, precision 0.8392, recall 0.8428, auc 0.8404
epoch 2701, loss 0.3719, train acc 84.15%, f1 0.8418, precision 0.8413, recall 0.8422, auc 0.8415
epoch 2801, loss 0.3386, train acc 84.21%, f1 0.8426, precision 0.8412, recall 0.8441, auc 0.8421
epoch 2901, loss 0.4321, train acc 84.23%, f1 0.8426, precision 0.8422, recall 0.8430, auc 0.8423
epoch 3001, loss 0.3805, train acc 84.28%, f1 0.8430, precision 0.8429, recall 0.8430, auc 0.8428
epoch 3101, loss 0.3992, train acc 84.28%, f1 0.8435, precision 0.8408, recall 0.8463, auc 0.8428
epoch 3201, loss 0.3022, train acc 84.36%, f1 0.8437, precision 0.8441, recall 0.8433, auc 0.8436
epoch 3301, loss 0.3043, train acc 84.41%, f1 0.8445, precision 0.8438, recall 0.8452, auc 0.8441
epoch 3401, loss 0.3074, train acc 84.45%, f1 0.8444, precision 0.8463, recall 0.8424, auc 0.8445
epoch 3501, loss 0.3172, train acc 84.50%, f1 0.8450, precision 0.8461, recall 0.8440, auc 0.8450
epoch 3601, loss 0.4706, train acc 84.55%, f1 0.8458, precision 0.8450, recall 0.8467, auc 0.8455
epoch 3701, loss 0.2105, train acc 84.68%, f1 0.8469, precision 0.8472, recall 0.8466, auc 0.8468
epoch 3801, loss 0.3351, train acc 84.75%, f1 0.8471, precision 0.8507, recall 0.8435, auc 0.8475
epoch 3901, loss 0.2740, train acc 84.88%, f1 0.8488, precision 0.8501, recall 0.8475, auc 0.8488
epoch 4001, loss 0.3091, train acc 85.04%, f1 0.8504, precision 0.8516, recall 0.8491, auc 0.8504
epoch 4101, loss 0.4170, train acc 85.11%, f1 0.8510, precision 0.8526, recall 0.8494, auc 0.8511
epoch 4201, loss 0.4135, train acc 85.25%, f1 0.8526, precision 0.8536, recall 0.8515, auc 0.8525
epoch 4301, loss 0.3659, train acc 85.35%, f1 0.8537, precision 0.8535, recall 0.8539, auc 0.8535
epoch 4401, loss 0.3431, train acc 85.48%, f1 0.8553, precision 0.8532, recall 0.8575, auc 0.8548
epoch 4501, loss 0.2916, train acc 85.55%, f1 0.8558, precision 0.8554, recall 0.8561, auc 0.8555
epoch 4601, loss 0.2436, train acc 85.69%, f1 0.8571, precision 0.8572, recall 0.8569, auc 0.8569
epoch 4701, loss 0.3607, train acc 85.79%, f1 0.8579, precision 0.8590, recall 0.8568, auc 0.8579
epoch 4801, loss 0.3496, train acc 85.89%, f1 0.8586, precision 0.8616, recall 0.8557, auc 0.8589
epoch 4901, loss 0.3848, train acc 86.02%, f1 0.8605, precision 0.8600, recall 0.8610, auc 0.8602
epoch 5001, loss 0.1927, train acc 86.14%, f1 0.8617, precision 0.8612, recall 0.8621, auc 0.8614
epoch 5101, loss 0.3197, train acc 86.22%, f1 0.8632, precision 0.8578, recall 0.8687, auc 0.8622
epoch 5201, loss 0.2136, train acc 86.37%, f1 0.8641, precision 0.8625, recall 0.8658, auc 0.8637
epoch 5301, loss 0.2815, train acc 86.42%, f1 0.8646, precision 0.8627, recall 0.8666, auc 0.8642
epoch 5401, loss 0.3193, train acc 86.47%, f1 0.8647, precision 0.8658, recall 0.8636, auc 0.8647
epoch 5501, loss 0.2642, train acc 86.56%, f1 0.8663, precision 0.8625, recall 0.8702, auc 0.8656
epoch 5601, loss 0.3754, train acc 86.59%, f1 0.8666, precision 0.8630, recall 0.8703, auc 0.8659
epoch 5701, loss 0.3670, train acc 86.67%, f1 0.8669, precision 0.8669, recall 0.8668, auc 0.8667
epoch 5801, loss 0.2427, train acc 86.72%, f1 0.8676, precision 0.8663, recall 0.8688, auc 0.8672
epoch 5901, loss 0.2456, train acc 86.77%, f1 0.8680, precision 0.8669, recall 0.8690, auc 0.8677
epoch 6001, loss 0.2790, train acc 86.83%, f1 0.8686, precision 0.8679, recall 0.8692, auc 0.8683
epoch 6101, loss 0.3493, train acc 86.91%, f1 0.8695, precision 0.8677, recall 0.8714, auc 0.8691
epoch 6201, loss 0.2516, train acc 86.98%, f1 0.8703, precision 0.8681, recall 0.8725, auc 0.8698
epoch 6301, loss 0.2404, train acc 86.98%, f1 0.8698, precision 0.8710, recall 0.8686, auc 0.8698
epoch 6401, loss 0.2935, train acc 87.03%, f1 0.8708, precision 0.8686, recall 0.8731, auc 0.8703
epoch 6501, loss 0.4181, train acc 87.10%, f1 0.8714, precision 0.8693, recall 0.8736, auc 0.8710
epoch 6601, loss 0.2664, train acc 87.07%, f1 0.8713, precision 0.8689, recall 0.8737, auc 0.8707
epoch 6701, loss 0.2786, train acc 87.10%, f1 0.8712, precision 0.8712, recall 0.8712, auc 0.8710
epoch 6801, loss 0.2162, train acc 87.15%, f1 0.8715, precision 0.8727, recall 0.8703, auc 0.8715
epoch 6901, loss 0.2171, train acc 87.20%, f1 0.8725, precision 0.8707, recall 0.8743, auc 0.8720
epoch 7001, loss 0.3045, train acc 87.24%, f1 0.8724, precision 0.8731, recall 0.8718, auc 0.8724
epoch 7101, loss 0.1868, train acc 87.28%, f1 0.8728, precision 0.8741, recall 0.8715, auc 0.8728
epoch 7201, loss 0.4397, train acc 87.32%, f1 0.8732, precision 0.8743, recall 0.8722, auc 0.8732
epoch 7301, loss 0.2430, train acc 87.32%, f1 0.8737, precision 0.8715, recall 0.8759, auc 0.8732
epoch 7401, loss 0.2338, train acc 87.44%, f1 0.8743, precision 0.8761, recall 0.8725, auc 0.8744
epoch 7501, loss 0.3232, train acc 87.47%, f1 0.8751, precision 0.8737, recall 0.8764, auc 0.8747
epoch 7601, loss 0.2619, train acc 87.46%, f1 0.8748, precision 0.8745, recall 0.8750, auc 0.8746
epoch 7701, loss 0.3031, train acc 87.52%, f1 0.8756, precision 0.8743, recall 0.8768, auc 0.8752
epoch 7801, loss 0.2882, train acc 87.62%, f1 0.8762, precision 0.8775, recall 0.8749, auc 0.8763
epoch 7901, loss 0.2071, train acc 87.58%, f1 0.8758, precision 0.8770, recall 0.8746, auc 0.8758
epoch 8001, loss 0.2584, train acc 87.63%, f1 0.8766, precision 0.8761, recall 0.8771, auc 0.8763
epoch 8101, loss 0.3990, train acc 87.77%, f1 0.8776, precision 0.8798, recall 0.8754, auc 0.8777
epoch 8201, loss 0.2414, train acc 87.68%, f1 0.8772, precision 0.8756, recall 0.8789, auc 0.8768
epoch 8301, loss 0.2636, train acc 87.77%, f1 0.8780, precision 0.8774, recall 0.8785, auc 0.8777
epoch 8401, loss 0.2149, train acc 87.80%, f1 0.8779, precision 0.8795, recall 0.8764, auc 0.8780
epoch 8501, loss 0.3176, train acc 87.85%, f1 0.8790, precision 0.8765, recall 0.8816, auc 0.8785
epoch 8601, loss 0.2390, train acc 87.96%, f1 0.8794, precision 0.8820, recall 0.8768, auc 0.8796
epoch 8701, loss 0.2234, train acc 87.93%, f1 0.8795, precision 0.8790, recall 0.8800, auc 0.8793
epoch 8801, loss 0.3547, train acc 87.96%, f1 0.8799, precision 0.8793, recall 0.8805, auc 0.8796
epoch 8901, loss 0.2801, train acc 88.00%, f1 0.8799, precision 0.8818, recall 0.8780, auc 0.8800
epoch 9001, loss 0.2303, train acc 87.99%, f1 0.8801, precision 0.8799, recall 0.8802, auc 0.8799
epoch 9101, loss 0.3892, train acc 88.12%, f1 0.8811, precision 0.8828, recall 0.8795, auc 0.8812
epoch 9201, loss 0.2617, train acc 88.12%, f1 0.8814, precision 0.8807, recall 0.8822, auc 0.8812
epoch 9301, loss 0.2327, train acc 88.18%, f1 0.8820, precision 0.8818, recall 0.8822, auc 0.8818
epoch 9401, loss 0.2209, train acc 88.15%, f1 0.8818, precision 0.8809, recall 0.8826, auc 0.8815
epoch 9501, loss 0.1778, train acc 88.24%, f1 0.8830, precision 0.8797, recall 0.8864, auc 0.8824
epoch 9601, loss 0.3300, train acc 88.29%, f1 0.8828, precision 0.8847, recall 0.8810, auc 0.8829
epoch 9701, loss 0.3158, train acc 88.31%, f1 0.8834, precision 0.8820, recall 0.8848, auc 0.8831
epoch 9801, loss 0.2045, train acc 88.33%, f1 0.8836, precision 0.8824, recall 0.8848, auc 0.8833
epoch 9901, loss 0.3196, train acc 88.39%, f1 0.8842, precision 0.8828, recall 0.8857, auc 0.8839
epoch 10001, loss 0.2182, train acc 88.40%, f1 0.8840, precision 0.8848, recall 0.8833, auc 0.8840
epoch 10101, loss 0.3012, train acc 88.47%, f1 0.8845, precision 0.8873, recall 0.8818, auc 0.8847
epoch 10201, loss 0.2270, train acc 88.49%, f1 0.8853, precision 0.8837, recall 0.8869, auc 0.8849
epoch 10301, loss 0.2794, train acc 88.53%, f1 0.8854, precision 0.8855, recall 0.8853, auc 0.8853
epoch 10401, loss 0.2821, train acc 88.55%, f1 0.8859, precision 0.8841, recall 0.8877, auc 0.8855
epoch 10501, loss 0.3865, train acc 88.57%, f1 0.8860, precision 0.8846, recall 0.8874, auc 0.8857
epoch 10601, loss 0.3059, train acc 88.64%, f1 0.8868, precision 0.8851, recall 0.8884, auc 0.8864
epoch 10701, loss 0.2527, train acc 88.67%, f1 0.8868, precision 0.8866, recall 0.8871, auc 0.8867
epoch 10801, loss 0.3129, train acc 88.59%, f1 0.8861, precision 0.8859, recall 0.8862, auc 0.8859
epoch 10901, loss 0.2217, train acc 88.71%, f1 0.8871, precision 0.8879, recall 0.8864, auc 0.8871
epoch 11001, loss 0.3574, train acc 88.70%, f1 0.8872, precision 0.8869, recall 0.8876, auc 0.8870
epoch 11101, loss 0.3685, train acc 88.75%, f1 0.8877, precision 0.8873, recall 0.8880, auc 0.8875
epoch 11201, loss 0.2595, train acc 88.76%, f1 0.8879, precision 0.8867, recall 0.8891, auc 0.8876
epoch 11301, loss 0.2679, train acc 88.84%, f1 0.8884, precision 0.8896, recall 0.8872, auc 0.8884
epoch 11401, loss 0.2440, train acc 88.83%, f1 0.8887, precision 0.8868, recall 0.8905, auc 0.8883
epoch 11501, loss 0.2649, train acc 88.83%, f1 0.8890, precision 0.8851, recall 0.8929, auc 0.8883
epoch 11601, loss 0.2141, train acc 88.81%, f1 0.8882, precision 0.8883, recall 0.8881, auc 0.8881
epoch 11701, loss 0.2221, train acc 88.91%, f1 0.8892, precision 0.8895, recall 0.8889, auc 0.8891
epoch 11801, loss 0.3045, train acc 88.92%, f1 0.8892, precision 0.8903, recall 0.8880, auc 0.8892
epoch 11901, loss 0.2618, train acc 88.98%, f1 0.8906, precision 0.8854, recall 0.8958, auc 0.8898
epoch 12001, loss 0.3443, train acc 89.00%, f1 0.8900, precision 0.8909, recall 0.8892, auc 0.8900
epoch 12101, loss 0.2591, train acc 89.02%, f1 0.8903, precision 0.8907, recall 0.8899, auc 0.8902
epoch 12201, loss 0.2737, train acc 89.06%, f1 0.8904, precision 0.8928, recall 0.8880, auc 0.8906
epoch 12301, loss 0.3950, train acc 89.10%, f1 0.8911, precision 0.8913, recall 0.8910, auc 0.8910
epoch 12401, loss 0.2594, train acc 89.06%, f1 0.8906, precision 0.8921, recall 0.8890, auc 0.8906
epoch 12501, loss 0.3698, train acc 89.19%, f1 0.8919, precision 0.8932, recall 0.8906, auc 0.8919
epoch 12601, loss 0.2437, train acc 89.17%, f1 0.8916, precision 0.8930, recall 0.8902, auc 0.8917
epoch 12701, loss 0.2749, train acc 89.18%, f1 0.8919, precision 0.8925, recall 0.8913, auc 0.8918
epoch 12801, loss 0.2339, train acc 89.25%, f1 0.8925, precision 0.8939, recall 0.8911, auc 0.8925
epoch 12901, loss 0.2482, train acc 89.26%, f1 0.8926, precision 0.8940, recall 0.8911, auc 0.8926
epoch 13001, loss 0.1797, train acc 89.24%, f1 0.8925, precision 0.8925, recall 0.8925, auc 0.8924
epoch 13101, loss 0.2720, train acc 89.30%, f1 0.8930, precision 0.8938, recall 0.8923, auc 0.8930
epoch 13201, loss 0.2709, train acc 89.36%, f1 0.8939, precision 0.8926, recall 0.8951, auc 0.8936
epoch 13301, loss 0.2975, train acc 89.37%, f1 0.8939, precision 0.8932, recall 0.8946, auc 0.8937
epoch 13401, loss 0.3135, train acc 89.41%, f1 0.8941, precision 0.8951, recall 0.8932, auc 0.8941
epoch 13501, loss 0.2541, train acc 89.39%, f1 0.8940, precision 0.8943, recall 0.8936, auc 0.8939
epoch 13601, loss 0.2047, train acc 89.47%, f1 0.8949, precision 0.8939, recall 0.8960, auc 0.8947
epoch 13701, loss 0.2878, train acc 89.51%, f1 0.8954, precision 0.8940, recall 0.8969, auc 0.8951
epoch 13801, loss 0.2969, train acc 89.47%, f1 0.8949, precision 0.8948, recall 0.8950, auc 0.8947
epoch 13901, loss 0.1570, train acc 89.51%, f1 0.8954, precision 0.8939, recall 0.8970, auc 0.8951
epoch 14001, loss 0.2561, train acc 89.56%, f1 0.8955, precision 0.8976, recall 0.8934, auc 0.8956
epoch 14101, loss 0.2349, train acc 89.61%, f1 0.8961, precision 0.8969, recall 0.8954, auc 0.8961
epoch 14201, loss 0.2558, train acc 89.64%, f1 0.8963, precision 0.8986, recall 0.8940, auc 0.8964
epoch 14301, loss 0.1553, train acc 89.70%, f1 0.8969, precision 0.8991, recall 0.8946, auc 0.8970
epoch 14401, loss 0.2037, train acc 89.69%, f1 0.8969, precision 0.8985, recall 0.8953, auc 0.8969
epoch 14501, loss 0.2005, train acc 89.72%, f1 0.8972, precision 0.8988, recall 0.8956, auc 0.8972
epoch 14601, loss 0.3018, train acc 89.80%, f1 0.8979, precision 0.8999, recall 0.8960, auc 0.8980
epoch 14701, loss 0.2331, train acc 89.80%, f1 0.8980, precision 0.8993, recall 0.8967, auc 0.8980
epoch 14801, loss 0.2645, train acc 89.84%, f1 0.8983, precision 0.9002, recall 0.8964, auc 0.8984
epoch 14901, loss 0.2199, train acc 89.92%, f1 0.8990, precision 0.9015, recall 0.8966, auc 0.8992
epoch 15001, loss 0.2052, train acc 89.94%, f1 0.8991, precision 0.9032, recall 0.8950, auc 0.8994
epoch 15101, loss 0.2091, train acc 89.98%, f1 0.8997, precision 0.9018, recall 0.8977, auc 0.8998
epoch 15201, loss 0.4510, train acc 89.90%, f1 0.8989, precision 0.9011, recall 0.8968, auc 0.8990
epoch 15301, loss 0.2414, train acc 90.04%, f1 0.9007, precision 0.8995, recall 0.9019, auc 0.9004
epoch 15401, loss 0.2420, train acc 90.06%, f1 0.9006, precision 0.9021, recall 0.8990, auc 0.9006
epoch 15501, loss 0.2334, train acc 90.16%, f1 0.9016, precision 0.9024, recall 0.9008, auc 0.9016
epoch 15601, loss 0.2300, train acc 90.19%, f1 0.9014, precision 0.9069, recall 0.8960, auc 0.9019
epoch 15701, loss 0.2046, train acc 90.21%, f1 0.9017, precision 0.9067, recall 0.8967, auc 0.9021
epoch 15801, loss 0.2288, train acc 90.30%, f1 0.9030, precision 0.9043, recall 0.9016, auc 0.9030
epoch 15901, loss 0.2388, train acc 90.35%, f1 0.9038, precision 0.9022, recall 0.9054, auc 0.9035
epoch 16001, loss 0.2112, train acc 90.33%, f1 0.9035, precision 0.9031, recall 0.9038, auc 0.9033
epoch 16101, loss 0.1909, train acc 90.39%, f1 0.9037, precision 0.9064, recall 0.9010, auc 0.9039
epoch 16201, loss 0.3428, train acc 90.40%, f1 0.9040, precision 0.9055, recall 0.9024, auc 0.9040
epoch 16301, loss 0.1757, train acc 90.48%, f1 0.9047, precision 0.9066, recall 0.9028, auc 0.9048
epoch 16401, loss 0.2276, train acc 90.46%, f1 0.9046, precision 0.9061, recall 0.9032, auc 0.9047
epoch 16501, loss 0.2060, train acc 90.59%, f1 0.9058, precision 0.9077, recall 0.9039, auc 0.9059/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.2142, train acc 90.60%, f1 0.9060, precision 0.9068, recall 0.9053, auc 0.9060
epoch 16701, loss 0.2661, train acc 90.62%, f1 0.9060, precision 0.9090, recall 0.9029, auc 0.9062
epoch 16801, loss 0.2879, train acc 90.66%, f1 0.9067, precision 0.9067, recall 0.9068, auc 0.9066
epoch 16901, loss 0.2736, train acc 90.68%, f1 0.9067, precision 0.9085, recall 0.9050, auc 0.9068
epoch 17001, loss 0.2422, train acc 90.71%, f1 0.9071, precision 0.9087, recall 0.9055, auc 0.9072
epoch 17101, loss 0.1652, train acc 90.76%, f1 0.9075, precision 0.9098, recall 0.9053, auc 0.9076
epoch 17201, loss 0.2940, train acc 90.79%, f1 0.9077, precision 0.9104, recall 0.9051, auc 0.9079
epoch 17301, loss 0.2601, train acc 90.84%, f1 0.9086, precision 0.9079, recall 0.9093, auc 0.9084
epoch 17401, loss 0.2165, train acc 90.91%, f1 0.9092, precision 0.9099, recall 0.9084, auc 0.9091
epoch 17501, loss 0.2325, train acc 90.94%, f1 0.9091, precision 0.9130, recall 0.9052, auc 0.9094
epoch 17601, loss 0.1854, train acc 90.95%, f1 0.9097, precision 0.9095, recall 0.9099, auc 0.9095
epoch 17701, loss 0.1827, train acc 90.98%, f1 0.9097, precision 0.9113, recall 0.9082, auc 0.9098
epoch 17801, loss 0.1886, train acc 90.99%, f1 0.9098, precision 0.9120, recall 0.9077, auc 0.9099
epoch 17901, loss 0.2593, train acc 91.09%, f1 0.9108, precision 0.9127, recall 0.9089, auc 0.9109
epoch 18001, loss 0.1947, train acc 91.06%, f1 0.9104, precision 0.9134, recall 0.9074, auc 0.9106
epoch 18101, loss 0.2176, train acc 91.12%, f1 0.9113, precision 0.9113, recall 0.9113, auc 0.9112
epoch 18201, loss 0.2736, train acc 91.18%, f1 0.9117, precision 0.9141, recall 0.9093, auc 0.9118
epoch 18301, loss 0.1730, train acc 91.19%, f1 0.9116, precision 0.9154, recall 0.9079, auc 0.9119
epoch 18401, loss 0.1547, train acc 91.24%, f1 0.9126, precision 0.9119, recall 0.9132, auc 0.9124
epoch 18501, loss 0.1645, train acc 91.31%, f1 0.9128, precision 0.9167, recall 0.9089, auc 0.9131
epoch 18601, loss 0.1377, train acc 91.28%, f1 0.9129, precision 0.9128, recall 0.9131, auc 0.9128
epoch 18701, loss 0.1910, train acc 91.32%, f1 0.9132, precision 0.9139, recall 0.9126, auc 0.9132
epoch 18801, loss 0.1901, train acc 91.41%, f1 0.9140, precision 0.9158, recall 0.9122, auc 0.9141
epoch 18901, loss 0.1896, train acc 91.40%, f1 0.9138, precision 0.9166, recall 0.9111, auc 0.9140
epoch 19001, loss 0.1569, train acc 91.46%, f1 0.9146, precision 0.9166, recall 0.9125, auc 0.9146
epoch 19101, loss 0.2040, train acc 91.46%, f1 0.9143, precision 0.9185, recall 0.9103, auc 0.9146
epoch 19201, loss 0.1422, train acc 91.50%, f1 0.9149, precision 0.9171, recall 0.9126, auc 0.9150
epoch 19301, loss 0.2246, train acc 91.51%, f1 0.9151, precision 0.9155, recall 0.9148, auc 0.9151
epoch 19401, loss 0.2230, train acc 91.56%, f1 0.9158, precision 0.9149, recall 0.9168, auc 0.9156
epoch 19501, loss 0.2350, train acc 91.60%, f1 0.9160, precision 0.9169, recall 0.9152, auc 0.9160
epoch 19601, loss 0.2403, train acc 91.67%, f1 0.9168, precision 0.9163, recall 0.9174, auc 0.9167
epoch 19701, loss 0.2108, train acc 91.66%, f1 0.9167, precision 0.9176, recall 0.9157, auc 0.9166
epoch 19801, loss 0.2188, train acc 91.70%, f1 0.9170, precision 0.9184, recall 0.9156, auc 0.9170
epoch 19901, loss 0.1850, train acc 91.75%, f1 0.9175, precision 0.9183, recall 0.9166, auc 0.9175
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_2
./test_pima/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6772222222222222

the Fscore is 0.6219512195121951

the precision is 0.4636363636363636

the recall is 0.9444444444444444

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_2
----------------------



epoch 1, loss 0.6933, train acc 50.12%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5240, train acc 79.21%, f1 0.7946, precision 0.7833, recall 0.8062, auc 0.7921
epoch 201, loss 0.4376, train acc 81.88%, f1 0.8188, precision 0.8168, recall 0.8207, auc 0.8188
epoch 301, loss 0.4172, train acc 83.00%, f1 0.8288, precision 0.8327, recall 0.8248, auc 0.8300
epoch 401, loss 0.3603, train acc 83.57%, f1 0.8347, precision 0.8374, recall 0.8320, auc 0.8356
epoch 501, loss 0.3337, train acc 83.87%, f1 0.8376, precision 0.8410, recall 0.8343, auc 0.8387
epoch 601, loss 0.3573, train acc 83.94%, f1 0.8382, precision 0.8424, recall 0.8340, auc 0.8394
epoch 701, loss 0.3674, train acc 83.99%, f1 0.8392, precision 0.8408, recall 0.8376, auc 0.8399
epoch 801, loss 0.3172, train acc 84.02%, f1 0.8394, precision 0.8414, recall 0.8374, auc 0.8402
epoch 901, loss 0.2671, train acc 84.05%, f1 0.8398, precision 0.8415, recall 0.8381, auc 0.8405
epoch 1001, loss 0.3614, train acc 84.14%, f1 0.8402, precision 0.8444, recall 0.8360, auc 0.8414
epoch 1101, loss 0.3035, train acc 84.08%, f1 0.8400, precision 0.8421, recall 0.8378, auc 0.8408
epoch 1201, loss 0.5740, train acc 84.12%, f1 0.8403, precision 0.8428, recall 0.8378, auc 0.8411
epoch 1301, loss 0.4720, train acc 84.11%, f1 0.8404, precision 0.8419, recall 0.8388, auc 0.8411
epoch 1401, loss 0.4558, train acc 84.03%, f1 0.8398, precision 0.8402, recall 0.8395, auc 0.8403
epoch 1501, loss 0.4644, train acc 84.10%, f1 0.8401, precision 0.8426, recall 0.8375, auc 0.8409
epoch 1601, loss 0.2833, train acc 84.13%, f1 0.8399, precision 0.8454, recall 0.8344, auc 0.8413
epoch 1701, loss 0.2655, train acc 84.12%, f1 0.8404, precision 0.8423, recall 0.8386, auc 0.8412
epoch 1801, loss 0.3439, train acc 84.09%, f1 0.8402, precision 0.8420, recall 0.8383, auc 0.8409
epoch 1901, loss 0.3568, train acc 84.15%, f1 0.8414, precision 0.8402, recall 0.8425, auc 0.8415
epoch 2001, loss 0.3998, train acc 84.06%, f1 0.8401, precision 0.8408, recall 0.8393, auc 0.8406
epoch 2101, loss 0.3436, train acc 84.10%, f1 0.8401, precision 0.8427, recall 0.8375, auc 0.8409
epoch 2201, loss 0.3188, train acc 84.07%, f1 0.8401, precision 0.8410, recall 0.8393, auc 0.8407
epoch 2301, loss 0.2649, train acc 84.04%, f1 0.8397, precision 0.8409, recall 0.8386, auc 0.8403
epoch 2401, loss 0.4017, train acc 84.11%, f1 0.8409, precision 0.8396, recall 0.8422, auc 0.8411
epoch 2501, loss 0.3558, train acc 84.16%, f1 0.8409, precision 0.8426, recall 0.8392, auc 0.8416
epoch 2601, loss 0.2411, train acc 84.15%, f1 0.8399, precision 0.8462, recall 0.8336, auc 0.8414
epoch 2701, loss 0.3049, train acc 84.17%, f1 0.8403, precision 0.8453, recall 0.8354, auc 0.8416
epoch 2801, loss 0.3369, train acc 84.15%, f1 0.8407, precision 0.8429, recall 0.8384, auc 0.8415
epoch 2901, loss 0.3346, train acc 84.17%, f1 0.8411, precision 0.8418, recall 0.8405, auc 0.8417
epoch 3001, loss 0.3508, train acc 84.26%, f1 0.8419, precision 0.8437, recall 0.8402, auc 0.8426
epoch 3101, loss 0.3601, train acc 84.26%, f1 0.8427, precision 0.8400, recall 0.8455, auc 0.8426
epoch 3201, loss 0.2332, train acc 84.30%, f1 0.8427, precision 0.8423, recall 0.8430, auc 0.8430
epoch 3301, loss 0.4143, train acc 84.31%, f1 0.8413, precision 0.8487, recall 0.8341, auc 0.8430
epoch 3401, loss 0.5031, train acc 84.41%, f1 0.8427, precision 0.8479, recall 0.8377, auc 0.8441
epoch 3501, loss 0.4265, train acc 84.46%, f1 0.8445, precision 0.8431, recall 0.8458, auc 0.8446
epoch 3601, loss 0.2796, train acc 84.55%, f1 0.8452, precision 0.8444, recall 0.8461, auc 0.8455
epoch 3701, loss 0.2800, train acc 84.60%, f1 0.8455, precision 0.8460, recall 0.8450, auc 0.8460
epoch 3801, loss 0.3370, train acc 84.69%, f1 0.8471, precision 0.8434, recall 0.8509, auc 0.8469
epoch 3901, loss 0.3770, train acc 84.83%, f1 0.8479, precision 0.8482, recall 0.8475, auc 0.8483
epoch 4001, loss 0.3472, train acc 84.95%, f1 0.8489, precision 0.8500, recall 0.8478, auc 0.8495
epoch 4101, loss 0.2981, train acc 85.11%, f1 0.8504, precision 0.8524, recall 0.8485, auc 0.8511
epoch 4201, loss 0.3334, train acc 85.16%, f1 0.8517, precision 0.8491, recall 0.8544, auc 0.8516
epoch 4301, loss 0.4032, train acc 85.25%, f1 0.8520, precision 0.8529, recall 0.8511, auc 0.8525
epoch 4401, loss 0.4272, train acc 85.36%, f1 0.8532, precision 0.8534, recall 0.8530, auc 0.8536
epoch 4501, loss 0.3953, train acc 85.51%, f1 0.8540, precision 0.8581, recall 0.8499, auc 0.8551
epoch 4601, loss 0.3070, train acc 85.62%, f1 0.8557, precision 0.8566, recall 0.8548, auc 0.8562
epoch 4701, loss 0.2784, train acc 85.70%, f1 0.8561, precision 0.8594, recall 0.8528, auc 0.8570
epoch 4801, loss 0.2833, train acc 85.79%, f1 0.8576, precision 0.8574, recall 0.8577, auc 0.8579
epoch 4901, loss 0.3322, train acc 85.93%, f1 0.8584, precision 0.8617, recall 0.8551, auc 0.8593
epoch 5001, loss 0.2979, train acc 86.08%, f1 0.8601, precision 0.8624, recall 0.8578, auc 0.8608
epoch 5101, loss 0.2235, train acc 86.12%, f1 0.8607, precision 0.8616, recall 0.8597, auc 0.8612
epoch 5201, loss 0.2876, train acc 86.26%, f1 0.8618, precision 0.8645, recall 0.8592, auc 0.8626
epoch 5301, loss 0.3185, train acc 86.36%, f1 0.8631, precision 0.8639, recall 0.8624, auc 0.8636
epoch 5401, loss 0.2863, train acc 86.45%, f1 0.8641, precision 0.8646, recall 0.8635, auc 0.8645
epoch 5501, loss 0.2423, train acc 86.51%, f1 0.8646, precision 0.8656, recall 0.8635, auc 0.8651
epoch 5601, loss 0.3575, train acc 86.60%, f1 0.8654, precision 0.8669, recall 0.8640, auc 0.8660
epoch 5701, loss 0.2936, train acc 86.66%, f1 0.8663, precision 0.8662, recall 0.8664, auc 0.8666
epoch 5801, loss 0.2393, train acc 86.76%, f1 0.8668, precision 0.8696, recall 0.8641, auc 0.8676
epoch 5901, loss 0.3383, train acc 86.81%, f1 0.8672, precision 0.8708, recall 0.8636, auc 0.8681
epoch 6001, loss 0.3187, train acc 86.88%, f1 0.8685, precision 0.8682, recall 0.8688, auc 0.8688
epoch 6101, loss 0.3219, train acc 86.89%, f1 0.8680, precision 0.8716, recall 0.8645, auc 0.8689
epoch 6201, loss 0.1626, train acc 86.96%, f1 0.8696, precision 0.8675, recall 0.8718, auc 0.8697
epoch 6301, loss 0.3065, train acc 87.03%, f1 0.8696, precision 0.8720, recall 0.8672, auc 0.8702
epoch 6401, loss 0.3728, train acc 87.07%, f1 0.8706, precision 0.8692, recall 0.8720, auc 0.8707
epoch 6501, loss 0.2901, train acc 87.11%, f1 0.8709, precision 0.8702, recall 0.8716, auc 0.8711
epoch 6601, loss 0.3066, train acc 87.15%, f1 0.8713, precision 0.8705, recall 0.8721, auc 0.8715
epoch 6701, loss 0.2918, train acc 87.20%, f1 0.8715, precision 0.8729, recall 0.8701, auc 0.8720
epoch 6801, loss 0.3690, train acc 87.21%, f1 0.8711, precision 0.8759, recall 0.8663, auc 0.8721
epoch 6901, loss 0.3418, train acc 87.26%, f1 0.8728, precision 0.8695, recall 0.8761, auc 0.8726
epoch 7001, loss 0.3977, train acc 87.32%, f1 0.8728, precision 0.8736, recall 0.8719, auc 0.8732
epoch 7101, loss 0.2869, train acc 87.36%, f1 0.8729, precision 0.8750, recall 0.8709, auc 0.8736
epoch 7201, loss 0.3280, train acc 87.33%, f1 0.8728, precision 0.8742, recall 0.8714, auc 0.8733
epoch 7301, loss 0.2782, train acc 87.36%, f1 0.8737, precision 0.8711, recall 0.8764, auc 0.8737
epoch 7401, loss 0.2793, train acc 87.43%, f1 0.8737, precision 0.8751, recall 0.8724, auc 0.8742
epoch 7501, loss 0.2827, train acc 87.52%, f1 0.8743, precision 0.8788, recall 0.8698, auc 0.8752
epoch 7601, loss 0.2434, train acc 87.53%, f1 0.8749, precision 0.8754, recall 0.8745, auc 0.8753
epoch 7701, loss 0.2995, train acc 87.60%, f1 0.8759, precision 0.8743, recall 0.8775, auc 0.8760
epoch 7801, loss 0.3001, train acc 87.70%, f1 0.8761, precision 0.8804, recall 0.8719, auc 0.8770
epoch 7901, loss 0.2850, train acc 87.65%, f1 0.8760, precision 0.8775, recall 0.8745, auc 0.8765
epoch 8001, loss 0.2299, train acc 87.70%, f1 0.8768, precision 0.8758, recall 0.8778, auc 0.8770
epoch 8101, loss 0.2243, train acc 87.69%, f1 0.8766, precision 0.8763, recall 0.8770, auc 0.8769
epoch 8201, loss 0.2591, train acc 87.76%, f1 0.8770, precision 0.8788, recall 0.8752, auc 0.8776/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2855, train acc 87.82%, f1 0.8781, precision 0.8769, recall 0.8793, auc 0.8782
epoch 8401, loss 0.4060, train acc 87.85%, f1 0.8780, precision 0.8790, recall 0.8771, auc 0.8785
epoch 8501, loss 0.2513, train acc 87.89%, f1 0.8786, precision 0.8784, recall 0.8789, auc 0.8789
epoch 8601, loss 0.3247, train acc 87.93%, f1 0.8785, precision 0.8819, recall 0.8751, auc 0.8793
epoch 8701, loss 0.3042, train acc 87.98%, f1 0.8792, precision 0.8818, recall 0.8765, auc 0.8798
epoch 8801, loss 0.2903, train acc 88.02%, f1 0.8796, precision 0.8820, recall 0.8773, auc 0.8802
epoch 8901, loss 0.2785, train acc 88.01%, f1 0.8796, precision 0.8811, recall 0.8782, auc 0.8801
epoch 9001, loss 0.2651, train acc 88.04%, f1 0.8803, precision 0.8784, recall 0.8823, auc 0.8804
epoch 9101, loss 0.3683, train acc 88.10%, f1 0.8804, precision 0.8824, recall 0.8784, auc 0.8810
epoch 9201, loss 0.1526, train acc 88.07%, f1 0.8803, precision 0.8808, recall 0.8799, auc 0.8807
epoch 9301, loss 0.2902, train acc 88.13%, f1 0.8810, precision 0.8811, recall 0.8809, auc 0.8813
epoch 9401, loss 0.2062, train acc 88.22%, f1 0.8817, precision 0.8831, recall 0.8803, auc 0.8822
epoch 9501, loss 0.2677, train acc 88.21%, f1 0.8820, precision 0.8804, recall 0.8836, auc 0.8821
epoch 9601, loss 0.2771, train acc 88.24%, f1 0.8824, precision 0.8803, recall 0.8845, auc 0.8824
epoch 9701, loss 0.3779, train acc 88.35%, f1 0.8829, precision 0.8852, recall 0.8806, auc 0.8835
epoch 9801, loss 0.2141, train acc 88.36%, f1 0.8835, precision 0.8819, recall 0.8852, auc 0.8836
epoch 9901, loss 0.2262, train acc 88.36%, f1 0.8828, precision 0.8869, recall 0.8788, auc 0.8836
epoch 10001, loss 0.3180, train acc 88.42%, f1 0.8837, precision 0.8856, recall 0.8817, auc 0.8842
epoch 10101, loss 0.2645, train acc 88.38%, f1 0.8834, precision 0.8840, recall 0.8829, auc 0.8838
epoch 10201, loss 0.2422, train acc 88.45%, f1 0.8843, precision 0.8842, recall 0.8843, auc 0.8845
epoch 10301, loss 0.2404, train acc 88.44%, f1 0.8841, precision 0.8838, recall 0.8844, auc 0.8844
epoch 10401, loss 0.3299, train acc 88.54%, f1 0.8853, precision 0.8838, recall 0.8867, auc 0.8854
epoch 10501, loss 0.2885, train acc 88.60%, f1 0.8854, precision 0.8880, recall 0.8828, auc 0.8860
epoch 10601, loss 0.2821, train acc 88.62%, f1 0.8861, precision 0.8847, recall 0.8875, auc 0.8862
epoch 10701, loss 0.2490, train acc 88.62%, f1 0.8860, precision 0.8860, recall 0.8859, auc 0.8862
epoch 10801, loss 0.1805, train acc 88.71%, f1 0.8866, precision 0.8882, recall 0.8851, auc 0.8871
epoch 10901, loss 0.4455, train acc 88.75%, f1 0.8874, precision 0.8863, recall 0.8885, auc 0.8875
epoch 11001, loss 0.2493, train acc 88.79%, f1 0.8874, precision 0.8893, recall 0.8854, auc 0.8879
epoch 11101, loss 0.2006, train acc 88.83%, f1 0.8877, precision 0.8903, recall 0.8850, auc 0.8883
epoch 11201, loss 0.1778, train acc 88.89%, f1 0.8884, precision 0.8902, recall 0.8867, auc 0.8889
epoch 11301, loss 0.1757, train acc 88.92%, f1 0.8885, precision 0.8912, recall 0.8859, auc 0.8892
epoch 11401, loss 0.1592, train acc 88.95%, f1 0.8891, precision 0.8905, recall 0.8876, auc 0.8895
epoch 11501, loss 0.2424, train acc 89.03%, f1 0.8901, precision 0.8894, recall 0.8908, auc 0.8903
epoch 11601, loss 0.2338, train acc 89.04%, f1 0.8896, precision 0.8932, recall 0.8861, auc 0.8904
epoch 11701, loss 0.2482, train acc 89.06%, f1 0.8904, precision 0.8904, recall 0.8903, auc 0.8906
epoch 11801, loss 0.2038, train acc 89.15%, f1 0.8908, precision 0.8947, recall 0.8869, auc 0.8915
epoch 11901, loss 0.2207, train acc 89.14%, f1 0.8914, precision 0.8895, recall 0.8932, auc 0.8914
epoch 12001, loss 0.2364, train acc 89.20%, f1 0.8915, precision 0.8933, recall 0.8897, auc 0.8920
epoch 12101, loss 0.2349, train acc 89.30%, f1 0.8925, precision 0.8944, recall 0.8907, auc 0.8930
epoch 12201, loss 0.2764, train acc 89.29%, f1 0.8923, precision 0.8948, recall 0.8897, auc 0.8929
epoch 12301, loss 0.2549, train acc 89.35%, f1 0.8924, precision 0.8995, recall 0.8855, auc 0.8935
epoch 12401, loss 0.1931, train acc 89.38%, f1 0.8935, precision 0.8939, recall 0.8930, auc 0.8938
epoch 12501, loss 0.2068, train acc 89.39%, f1 0.8936, precision 0.8939, recall 0.8932, auc 0.8939
epoch 12601, loss 0.3551, train acc 89.52%, f1 0.8950, precision 0.8945, recall 0.8956, auc 0.8952
epoch 12701, loss 0.2326, train acc 89.48%, f1 0.8942, precision 0.8968, recall 0.8916, auc 0.8947
epoch 12801, loss 0.2818, train acc 89.57%, f1 0.8952, precision 0.8978, recall 0.8926, auc 0.8957
epoch 12901, loss 0.2681, train acc 89.56%, f1 0.8951, precision 0.8973, recall 0.8929, auc 0.8956
epoch 13001, loss 0.2825, train acc 89.63%, f1 0.8961, precision 0.8959, recall 0.8963, auc 0.8963
epoch 13101, loss 0.2541, train acc 89.70%, f1 0.8966, precision 0.8981, recall 0.8951, auc 0.8970
epoch 13201, loss 0.2706, train acc 89.76%, f1 0.8972, precision 0.8986, recall 0.8957, auc 0.8976
epoch 13301, loss 0.1815, train acc 89.78%, f1 0.8975, precision 0.8984, recall 0.8965, auc 0.8978
epoch 13401, loss 0.1759, train acc 89.87%, f1 0.8985, precision 0.8977, recall 0.8994, auc 0.8987
epoch 13501, loss 0.2421, train acc 89.83%, f1 0.8979, precision 0.8986, recall 0.8973, auc 0.8983
epoch 13601, loss 0.3006, train acc 89.89%, f1 0.8987, precision 0.8982, recall 0.8991, auc 0.8989
epoch 13701, loss 0.2652, train acc 90.02%, f1 0.8998, precision 0.9013, recall 0.8984, auc 0.9002
epoch 13801, loss 0.1752, train acc 90.02%, f1 0.8999, precision 0.9004, recall 0.8994, auc 0.9002
epoch 13901, loss 0.2338, train acc 90.04%, f1 0.9003, precision 0.8992, recall 0.9014, auc 0.9004
epoch 14001, loss 0.2675, train acc 90.08%, f1 0.9006, precision 0.9003, recall 0.9010, auc 0.9008
epoch 14101, loss 0.1954, train acc 90.14%, f1 0.9013, precision 0.9001, recall 0.9025, auc 0.9014
epoch 14201, loss 0.2305, train acc 90.22%, f1 0.9018, precision 0.9029, recall 0.9007, auc 0.9022
epoch 14301, loss 0.2139, train acc 90.22%, f1 0.9019, precision 0.9026, recall 0.9012, auc 0.9022
epoch 14401, loss 0.1900, train acc 90.34%, f1 0.9030, precision 0.9038, recall 0.9022, auc 0.9033
epoch 14501, loss 0.2439, train acc 90.34%, f1 0.9033, precision 0.9020, recall 0.9046, auc 0.9034
epoch 14601, loss 0.1665, train acc 90.39%, f1 0.9037, precision 0.9037, recall 0.9037, auc 0.9039
epoch 14701, loss 0.2155, train acc 90.38%, f1 0.9033, precision 0.9055, recall 0.9012, auc 0.9038
epoch 14801, loss 0.1516, train acc 90.49%, f1 0.9046, precision 0.9048, recall 0.9045, auc 0.9049
epoch 14901, loss 0.2589, train acc 90.56%, f1 0.9051, precision 0.9081, recall 0.9021, auc 0.9056
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_2
./test_pima/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6764814814814815

the Fscore is 0.6227544910179641

the precision is 0.46017699115044247

the recall is 0.9629629629629629

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_2
----------------------



epoch 1, loss 0.6932, train acc 50.13%, f1 0.0001, precision 0.5000, recall 0.0001, auc 0.5000
epoch 101, loss 0.5733, train acc 78.93%, f1 0.7895, precision 0.7866, recall 0.7924, auc 0.7893
epoch 201, loss 0.4147, train acc 81.62%, f1 0.8147, precision 0.8192, recall 0.8101, auc 0.8162
epoch 301, loss 0.5547, train acc 82.91%, f1 0.8285, precision 0.8294, recall 0.8276, auc 0.8291
epoch 401, loss 0.3360, train acc 83.48%, f1 0.8343, precision 0.8347, recall 0.8338, auc 0.8348
epoch 501, loss 0.3460, train acc 83.82%, f1 0.8378, precision 0.8379, recall 0.8377, auc 0.8382
epoch 601, loss 0.2985, train acc 83.94%, f1 0.8391, precision 0.8383, recall 0.8400, auc 0.8394
epoch 701, loss 0.4739, train acc 84.06%, f1 0.8402, precision 0.8402, recall 0.8402, auc 0.8406
epoch 801, loss 0.3783, train acc 84.12%, f1 0.8411, precision 0.8392, recall 0.8431, auc 0.8412
epoch 901, loss 0.3237, train acc 84.08%, f1 0.8404, precision 0.8403, recall 0.8405, auc 0.8408
epoch 1001, loss 0.3214, train acc 84.05%, f1 0.8394, precision 0.8430, recall 0.8358, auc 0.8405
epoch 1101, loss 0.4178, train acc 84.12%, f1 0.8407, precision 0.8407, recall 0.8407, auc 0.8412
epoch 1201, loss 0.5234, train acc 84.17%, f1 0.8410, precision 0.8421, recall 0.8400, auc 0.8417
epoch 1301, loss 0.4530, train acc 84.16%, f1 0.8410, precision 0.8423, recall 0.8397, auc 0.8416
epoch 1401, loss 0.4104, train acc 84.15%, f1 0.8409, precision 0.8420, recall 0.8397, auc 0.8415
epoch 1501, loss 0.2385, train acc 84.20%, f1 0.8423, precision 0.8383, recall 0.8464, auc 0.8420
epoch 1601, loss 0.3661, train acc 84.12%, f1 0.8404, precision 0.8426, recall 0.8382, auc 0.8412
epoch 1701, loss 0.4402, train acc 84.07%, f1 0.8401, precision 0.8414, recall 0.8388, auc 0.8407
epoch 1801, loss 0.2610, train acc 84.12%, f1 0.8403, precision 0.8431, recall 0.8375, auc 0.8412
epoch 1901, loss 0.3215, train acc 84.09%, f1 0.8406, precision 0.8401, recall 0.8411, auc 0.8409
epoch 2001, loss 0.4667, train acc 84.07%, f1 0.8403, precision 0.8405, recall 0.8401, auc 0.8407
epoch 2101, loss 0.3064, train acc 84.13%, f1 0.8409, precision 0.8407, recall 0.8410, auc 0.8413
epoch 2201, loss 0.4833, train acc 84.14%, f1 0.8407, precision 0.8419, recall 0.8396, auc 0.8414
epoch 2301, loss 0.2925, train acc 84.17%, f1 0.8413, precision 0.8412, recall 0.8414, auc 0.8417
epoch 2401, loss 0.4244, train acc 84.16%, f1 0.8417, precision 0.8389, recall 0.8445, auc 0.8416
epoch 2501, loss 0.4244, train acc 84.18%, f1 0.8418, precision 0.8391, recall 0.8446, auc 0.8418
epoch 2601, loss 0.4618, train acc 84.16%, f1 0.8413, precision 0.8408, recall 0.8417, auc 0.8416
epoch 2701, loss 0.4315, train acc 84.15%, f1 0.8411, precision 0.8409, recall 0.8412, auc 0.8415
epoch 2801, loss 0.3049, train acc 84.20%, f1 0.8414, precision 0.8424, recall 0.8404, auc 0.8420
epoch 2901, loss 0.4213, train acc 84.16%, f1 0.8412, precision 0.8410, recall 0.8415, auc 0.8416
epoch 3001, loss 0.3259, train acc 84.25%, f1 0.8420, precision 0.8425, recall 0.8415, auc 0.8425
epoch 3101, loss 0.2990, train acc 84.25%, f1 0.8423, precision 0.8412, recall 0.8434, auc 0.8425
epoch 3201, loss 0.3382, train acc 84.23%, f1 0.8423, precision 0.8398, recall 0.8449, auc 0.8423
epoch 3301, loss 0.3704, train acc 84.35%, f1 0.8435, precision 0.8409, recall 0.8462, auc 0.8435
epoch 3401, loss 0.3345, train acc 84.41%, f1 0.8442, precision 0.8417, recall 0.8467, auc 0.8441
epoch 3501, loss 0.2577, train acc 84.48%, f1 0.8442, precision 0.8449, recall 0.8435, auc 0.8448
epoch 3601, loss 0.5242, train acc 84.57%, f1 0.8455, precision 0.8445, recall 0.8464, auc 0.8457
epoch 3701, loss 0.3374, train acc 84.72%, f1 0.8472, precision 0.8452, recall 0.8491, auc 0.8472
epoch 3801, loss 0.3798, train acc 84.86%, f1 0.8487, precision 0.8460, recall 0.8514, auc 0.8486
epoch 3901, loss 0.3820, train acc 84.93%, f1 0.8493, precision 0.8468, recall 0.8519, auc 0.8493
epoch 4001, loss 0.3071, train acc 85.07%, f1 0.8503, precision 0.8502, recall 0.8503, auc 0.8507
epoch 4101, loss 0.3237, train acc 85.20%, f1 0.8515, precision 0.8520, recall 0.8509, auc 0.8520
epoch 4201, loss 0.3796, train acc 85.31%, f1 0.8529, precision 0.8518, recall 0.8541, auc 0.8531
epoch 4301, loss 0.2645, train acc 85.48%, f1 0.8548, precision 0.8526, recall 0.8570, auc 0.8548
epoch 4401, loss 0.3507, train acc 85.62%, f1 0.8558, precision 0.8557, recall 0.8559, auc 0.8562
epoch 4501, loss 0.3045, train acc 85.76%, f1 0.8574, precision 0.8563, recall 0.8585, auc 0.8576
epoch 4601, loss 0.2133, train acc 85.84%, f1 0.8582, precision 0.8574, recall 0.8590, auc 0.8584
epoch 4701, loss 0.3247, train acc 85.94%, f1 0.8591, precision 0.8586, recall 0.8596, auc 0.8594
epoch 4801, loss 0.2682, train acc 86.04%, f1 0.8604, precision 0.8582, recall 0.8626, auc 0.8604
epoch 4901, loss 0.2608, train acc 86.19%, f1 0.8614, precision 0.8621, recall 0.8607, auc 0.8619
epoch 5001, loss 0.2309, train acc 86.28%, f1 0.8630, precision 0.8596, recall 0.8664, auc 0.8628
epoch 5101, loss 0.2274, train acc 86.38%, f1 0.8635, precision 0.8633, recall 0.8638, auc 0.8638
epoch 5201, loss 0.2936, train acc 86.45%, f1 0.8640, precision 0.8649, recall 0.8632, auc 0.8645
epoch 5301, loss 0.3365, train acc 86.52%, f1 0.8651, precision 0.8639, recall 0.8662, auc 0.8652
epoch 5401, loss 0.4274, train acc 86.63%, f1 0.8660, precision 0.8655, recall 0.8665, auc 0.8663
epoch 5501, loss 0.3272, train acc 86.65%, f1 0.8662, precision 0.8659, recall 0.8666, auc 0.8665
epoch 5601, loss 0.3322, train acc 86.72%, f1 0.8663, precision 0.8700, recall 0.8627, auc 0.8672
epoch 5701, loss 0.2565, train acc 86.79%, f1 0.8680, precision 0.8652, recall 0.8708, auc 0.8679
epoch 5801, loss 0.2441, train acc 86.86%, f1 0.8686, precision 0.8663, recall 0.8709, auc 0.8686
epoch 5901, loss 0.1988, train acc 86.87%, f1 0.8684, precision 0.8678, recall 0.8691, auc 0.8687
epoch 6001, loss 0.3601, train acc 86.95%, f1 0.8694, precision 0.8682, recall 0.8706, auc 0.8695
epoch 6101, loss 0.3579, train acc 86.97%, f1 0.8692, precision 0.8703, recall 0.8681, auc 0.8697
epoch 6201, loss 0.2698, train acc 87.03%, f1 0.8697, precision 0.8709, recall 0.8685, auc 0.8703
epoch 6301, loss 0.3046, train acc 87.07%, f1 0.8711, precision 0.8664, recall 0.8759, auc 0.8707
epoch 6401, loss 0.4512, train acc 87.11%, f1 0.8706, precision 0.8718, recall 0.8694, auc 0.8711
epoch 6501, loss 0.2224, train acc 87.14%, f1 0.8708, precision 0.8723, recall 0.8693, auc 0.8714
epoch 6601, loss 0.2830, train acc 87.17%, f1 0.8717, precision 0.8694, recall 0.8741, auc 0.8717
epoch 6701, loss 0.2878, train acc 87.22%, f1 0.8720, precision 0.8709, recall 0.8731, auc 0.8722
epoch 6801, loss 0.3384, train acc 87.26%, f1 0.8725, precision 0.8713, recall 0.8736, auc 0.8726
epoch 6901, loss 0.2703, train acc 87.30%, f1 0.8725, precision 0.8733, recall 0.8717, auc 0.8730
epoch 7001, loss 0.2989, train acc 87.37%, f1 0.8735, precision 0.8722, recall 0.8749, auc 0.8737
epoch 7101, loss 0.2645, train acc 87.40%, f1 0.8738, precision 0.8726, recall 0.8751, auc 0.8740
epoch 7201, loss 0.3321, train acc 87.42%, f1 0.8739, precision 0.8735, recall 0.8743, auc 0.8742
epoch 7301, loss 0.3161, train acc 87.43%, f1 0.8742, precision 0.8724, recall 0.8761, auc 0.8743
epoch 7401, loss 0.2008, train acc 87.48%, f1 0.8749, precision 0.8721, recall 0.8777, auc 0.8748
epoch 7501, loss 0.2759, train acc 87.56%, f1 0.8749, precision 0.8776, recall 0.8723, auc 0.8756
epoch 7601, loss 0.2491, train acc 87.54%, f1 0.8747, precision 0.8777, recall 0.8717, auc 0.8754
epoch 7701, loss 0.3839, train acc 87.55%, f1 0.8751, precision 0.8751, recall 0.8752, auc 0.8755
epoch 7801, loss 0.3088, train acc 87.55%, f1 0.8752, precision 0.8752, recall 0.8752, auc 0.8755
epoch 7901, loss 0.4291, train acc 87.61%, f1 0.8756, precision 0.8764, recall 0.8748, auc 0.8760
epoch 8001, loss 0.2762, train acc 87.68%, f1 0.8764, precision 0.8764, recall 0.8765, auc 0.8768
epoch 8101, loss 0.3724, train acc 87.63%, f1 0.8763, precision 0.8740, recall 0.8786, auc 0.8763
epoch 8201, loss 0.2761, train acc 87.70%, f1 0.8767, precision 0.8762, recall 0.8773, auc 0.8770/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.3471, train acc 87.75%, f1 0.8769, precision 0.8794, recall 0.8743, auc 0.8775
epoch 8401, loss 0.2775, train acc 87.77%, f1 0.8772, precision 0.8787, recall 0.8757, auc 0.8777
epoch 8501, loss 0.2561, train acc 87.85%, f1 0.8780, precision 0.8793, recall 0.8767, auc 0.8785
epoch 8601, loss 0.3463, train acc 87.85%, f1 0.8780, precision 0.8788, recall 0.8772, auc 0.8785
epoch 8701, loss 0.3002, train acc 87.80%, f1 0.8776, precision 0.8782, recall 0.8770, auc 0.8780
epoch 8801, loss 0.2133, train acc 87.89%, f1 0.8790, precision 0.8757, recall 0.8824, auc 0.8789
epoch 8901, loss 0.3298, train acc 87.90%, f1 0.8791, precision 0.8766, recall 0.8815, auc 0.8790
epoch 9001, loss 0.2869, train acc 87.95%, f1 0.8788, precision 0.8820, recall 0.8756, auc 0.8795
epoch 9101, loss 0.3853, train acc 87.96%, f1 0.8795, precision 0.8781, recall 0.8809, auc 0.8796
epoch 9201, loss 0.2041, train acc 88.02%, f1 0.8799, precision 0.8796, recall 0.8803, auc 0.8802
epoch 9301, loss 0.2718, train acc 88.04%, f1 0.8801, precision 0.8803, recall 0.8799, auc 0.8804
epoch 9401, loss 0.3586, train acc 88.04%, f1 0.8803, precision 0.8793, recall 0.8813, auc 0.8804
epoch 9501, loss 0.2631, train acc 88.07%, f1 0.8803, precision 0.8809, recall 0.8797, auc 0.8807
epoch 9601, loss 0.2502, train acc 88.16%, f1 0.8812, precision 0.8824, recall 0.8799, auc 0.8816
epoch 9701, loss 0.1693, train acc 88.17%, f1 0.8812, precision 0.8829, recall 0.8794, auc 0.8817
epoch 9801, loss 0.3404, train acc 88.23%, f1 0.8816, precision 0.8844, recall 0.8789, auc 0.8823
epoch 9901, loss 0.1970, train acc 88.21%, f1 0.8815, precision 0.8835, recall 0.8795, auc 0.8821
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_2
./test_pima/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6607407407407406

the Fscore is 0.6127167630057804

the precision is 0.44537815126050423

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_2
----------------------



epoch 1, loss 0.6932, train acc 50.22%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5236, train acc 79.00%, f1 0.7930, precision 0.7784, recall 0.8082, auc 0.7901
epoch 201, loss 0.3883, train acc 81.72%, f1 0.8168, precision 0.8151, recall 0.8184, auc 0.8172
epoch 301, loss 0.4549, train acc 83.02%, f1 0.8300, precision 0.8273, recall 0.8327, auc 0.8302
epoch 401, loss 0.3941, train acc 83.55%, f1 0.8350, precision 0.8340, recall 0.8359, auc 0.8355
epoch 501, loss 0.4298, train acc 83.95%, f1 0.8394, precision 0.8361, recall 0.8427, auc 0.8395
epoch 601, loss 0.3948, train acc 83.99%, f1 0.8393, precision 0.8388, recall 0.8398, auc 0.8399
epoch 701, loss 0.3695, train acc 84.05%, f1 0.8396, precision 0.8408, recall 0.8383, auc 0.8405
epoch 801, loss 0.4036, train acc 84.11%, f1 0.8406, precision 0.8396, recall 0.8415, auc 0.8411
epoch 901, loss 0.3086, train acc 84.05%, f1 0.8392, precision 0.8423, recall 0.8360, auc 0.8404
epoch 1001, loss 0.3878, train acc 84.13%, f1 0.8408, precision 0.8397, recall 0.8419, auc 0.8413
epoch 1101, loss 0.4022, train acc 84.12%, f1 0.8412, precision 0.8377, recall 0.8447, auc 0.8413
epoch 1201, loss 0.3132, train acc 84.11%, f1 0.8404, precision 0.8400, recall 0.8409, auc 0.8411
epoch 1301, loss 0.3559, train acc 84.12%, f1 0.8404, precision 0.8410, recall 0.8398, auc 0.8412
epoch 1401, loss 0.2730, train acc 84.10%, f1 0.8402, precision 0.8408, recall 0.8396, auc 0.8410
epoch 1501, loss 0.3742, train acc 84.18%, f1 0.8414, precision 0.8396, recall 0.8432, auc 0.8418
epoch 1601, loss 0.3694, train acc 84.16%, f1 0.8413, precision 0.8391, recall 0.8435, auc 0.8416
epoch 1701, loss 0.4009, train acc 84.09%, f1 0.8403, precision 0.8399, recall 0.8406, auc 0.8409
epoch 1801, loss 0.4968, train acc 84.01%, f1 0.8392, precision 0.8404, recall 0.8381, auc 0.8401
epoch 1901, loss 0.3301, train acc 84.01%, f1 0.8394, precision 0.8395, recall 0.8394, auc 0.8401
epoch 2001, loss 0.3333, train acc 84.04%, f1 0.8394, precision 0.8411, recall 0.8377, auc 0.8404
epoch 2101, loss 0.4013, train acc 84.11%, f1 0.8407, precision 0.8392, recall 0.8422, auc 0.8411
epoch 2201, loss 0.4418, train acc 84.14%, f1 0.8405, precision 0.8415, recall 0.8396, auc 0.8414
epoch 2301, loss 0.2489, train acc 84.14%, f1 0.8408, precision 0.8405, recall 0.8410, auc 0.8414
epoch 2401, loss 0.3760, train acc 84.12%, f1 0.8406, precision 0.8401, recall 0.8412, auc 0.8412
epoch 2501, loss 0.3487, train acc 84.10%, f1 0.8409, precision 0.8376, recall 0.8442, auc 0.8410
epoch 2601, loss 0.4558, train acc 84.12%, f1 0.8407, precision 0.8400, recall 0.8413, auc 0.8412
epoch 2701, loss 0.2459, train acc 84.15%, f1 0.8410, precision 0.8402, recall 0.8418, auc 0.8415
epoch 2801, loss 0.3446, train acc 84.18%, f1 0.8416, precision 0.8388, recall 0.8444, auc 0.8418
epoch 2901, loss 0.2871, train acc 84.17%, f1 0.8411, precision 0.8405, recall 0.8416, auc 0.8417
epoch 3001, loss 0.2325, train acc 84.19%, f1 0.8415, precision 0.8400, recall 0.8430, auc 0.8419
epoch 3101, loss 0.3731, train acc 84.19%, f1 0.8410, precision 0.8422, recall 0.8398, auc 0.8419
epoch 3201, loss 0.3764, train acc 84.29%, f1 0.8426, precision 0.8406, recall 0.8446, auc 0.8429
epoch 3301, loss 0.3364, train acc 84.37%, f1 0.8430, precision 0.8431, recall 0.8430, auc 0.8437
epoch 3401, loss 0.2351, train acc 84.46%, f1 0.8434, precision 0.8462, recall 0.8406, auc 0.8446
epoch 3501, loss 0.3646, train acc 84.50%, f1 0.8446, precision 0.8433, recall 0.8459, auc 0.8451
epoch 3601, loss 0.3937, train acc 84.67%, f1 0.8468, precision 0.8427, recall 0.8510, auc 0.8467
epoch 3701, loss 0.2711, train acc 84.67%, f1 0.8467, precision 0.8432, recall 0.8503, auc 0.8467
epoch 3801, loss 0.3493, train acc 84.82%, f1 0.8481, precision 0.8447, recall 0.8517, auc 0.8482
epoch 3901, loss 0.4050, train acc 84.92%, f1 0.8483, precision 0.8496, recall 0.8469, auc 0.8492
epoch 4001, loss 0.3729, train acc 85.03%, f1 0.8502, precision 0.8468, recall 0.8537, auc 0.8503
epoch 4101, loss 0.2872, train acc 85.17%, f1 0.8511, precision 0.8507, recall 0.8515, auc 0.8517
epoch 4201, loss 0.3877, train acc 85.23%, f1 0.8516, precision 0.8521, recall 0.8511, auc 0.8523
epoch 4301, loss 0.2720, train acc 85.35%, f1 0.8538, precision 0.8487, recall 0.8589, auc 0.8536
epoch 4401, loss 0.3679, train acc 85.52%, f1 0.8545, precision 0.8547, recall 0.8544, auc 0.8552
epoch 4501, loss 0.3680, train acc 85.64%, f1 0.8552, precision 0.8586, recall 0.8518, auc 0.8564
epoch 4601, loss 0.3515, train acc 85.68%, f1 0.8565, precision 0.8542, recall 0.8589, auc 0.8568
epoch 4701, loss 0.3533, train acc 85.79%, f1 0.8567, precision 0.8601, recall 0.8533, auc 0.8578
epoch 4801, loss 0.3324, train acc 85.90%, f1 0.8581, precision 0.8597, recall 0.8565, auc 0.8590
epoch 4901, loss 0.2892, train acc 85.98%, f1 0.8599, precision 0.8555, recall 0.8644, auc 0.8599
epoch 5001, loss 0.2307, train acc 86.08%, f1 0.8607, precision 0.8577, recall 0.8637, auc 0.8608
epoch 5101, loss 0.2508, train acc 86.16%, f1 0.8614, precision 0.8589, recall 0.8638, auc 0.8616
epoch 5201, loss 0.3225, train acc 86.28%, f1 0.8624, precision 0.8614, recall 0.8634, auc 0.8628
epoch 5301, loss 0.4455, train acc 86.36%, f1 0.8631, precision 0.8622, recall 0.8640, auc 0.8636
epoch 5401, loss 0.3201, train acc 86.44%, f1 0.8640, precision 0.8627, recall 0.8652, auc 0.8644
epoch 5501, loss 0.3287, train acc 86.54%, f1 0.8650, precision 0.8637, recall 0.8664, auc 0.8654
epoch 5601, loss 0.3757, train acc 86.58%, f1 0.8655, precision 0.8640, recall 0.8670, auc 0.8658
epoch 5701, loss 0.2760, train acc 86.68%, f1 0.8662, precision 0.8663, recall 0.8661, auc 0.8668
epoch 5801, loss 0.3370, train acc 86.78%, f1 0.8674, precision 0.8661, recall 0.8687, auc 0.8678
epoch 5901, loss 0.3360, train acc 86.79%, f1 0.8677, precision 0.8651, recall 0.8703, auc 0.8679
epoch 6001, loss 0.2039, train acc 86.88%, f1 0.8687, precision 0.8661, recall 0.8713, auc 0.8689
epoch 6101, loss 0.2779, train acc 86.93%, f1 0.8688, precision 0.8679, recall 0.8697, auc 0.8693
epoch 6201, loss 0.3372, train acc 86.95%, f1 0.8691, precision 0.8681, recall 0.8702, auc 0.8695
epoch 6301, loss 0.3524, train acc 87.02%, f1 0.8700, precision 0.8672, recall 0.8729, auc 0.8702
epoch 6401, loss 0.2572, train acc 87.08%, f1 0.8701, precision 0.8707, recall 0.8695, auc 0.8708
epoch 6501, loss 0.1449, train acc 87.04%, f1 0.8699, precision 0.8693, recall 0.8704, auc 0.8704
epoch 6601, loss 0.4168, train acc 87.09%, f1 0.8706, precision 0.8692, recall 0.8720, auc 0.8709
epoch 6701, loss 0.3658, train acc 87.11%, f1 0.8702, precision 0.8722, recall 0.8683, auc 0.8711
epoch 6801, loss 0.3370, train acc 87.14%, f1 0.8709, precision 0.8705, recall 0.8713, auc 0.8714
epoch 6901, loss 0.2411, train acc 87.29%, f1 0.8723, precision 0.8724, recall 0.8722, auc 0.8729
epoch 7001, loss 0.3493, train acc 87.30%, f1 0.8725, precision 0.8722, recall 0.8729, auc 0.8730
epoch 7101, loss 0.3136, train acc 87.23%, f1 0.8715, precision 0.8732, recall 0.8698, auc 0.8723
epoch 7201, loss 0.3394, train acc 87.30%, f1 0.8725, precision 0.8723, recall 0.8727, auc 0.8730
epoch 7301, loss 0.3149, train acc 87.33%, f1 0.8730, precision 0.8715, recall 0.8745, auc 0.8733
epoch 7401, loss 0.3047, train acc 87.36%, f1 0.8731, precision 0.8723, recall 0.8740, auc 0.8736
epoch 7501, loss 0.3260, train acc 87.44%, f1 0.8741, precision 0.8724, recall 0.8758, auc 0.8744
epoch 7601, loss 0.2205, train acc 87.42%, f1 0.8742, precision 0.8709, recall 0.8775, auc 0.8743
epoch 7701, loss 0.3190, train acc 87.55%, f1 0.8748, precision 0.8754, recall 0.8742, auc 0.8755
epoch 7801, loss 0.1976, train acc 87.53%, f1 0.8744, precision 0.8765, recall 0.8723, auc 0.8752
epoch 7901, loss 0.3538, train acc 87.59%, f1 0.8751, precision 0.8772, recall 0.8729, auc 0.8759
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_2
./test_pima/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6422222222222222

the Fscore is 0.5964912280701755

the precision is 0.4358974358974359

the recall is 0.9444444444444444

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_2
----------------------



epoch 1, loss 0.6932, train acc 50.06%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5854, train acc 78.29%, f1 0.7684, precision 0.8224, recall 0.7211, auc 0.7828
epoch 201, loss 0.4118, train acc 81.68%, f1 0.8157, precision 0.8197, recall 0.8118, auc 0.8168
epoch 301, loss 0.3811, train acc 82.93%, f1 0.8294, precision 0.8278, recall 0.8311, auc 0.8293
epoch 401, loss 0.3297, train acc 83.54%, f1 0.8360, precision 0.8321, recall 0.8399, auc 0.8354
epoch 501, loss 0.3673, train acc 83.80%, f1 0.8389, precision 0.8337, recall 0.8441, auc 0.8380
epoch 601, loss 0.2747, train acc 83.87%, f1 0.8396, precision 0.8341, recall 0.8451, auc 0.8387
epoch 701, loss 0.3944, train acc 84.00%, f1 0.8410, precision 0.8351, recall 0.8469, auc 0.8401
epoch 801, loss 0.3169, train acc 84.03%, f1 0.8411, precision 0.8362, recall 0.8461, auc 0.8403
epoch 901, loss 0.3218, train acc 84.02%, f1 0.8410, precision 0.8356, recall 0.8465, auc 0.8402
epoch 1001, loss 0.5246, train acc 84.06%, f1 0.8414, precision 0.8366, recall 0.8461, auc 0.8406
epoch 1101, loss 0.4328, train acc 84.07%, f1 0.8411, precision 0.8381, recall 0.8441, auc 0.8407
epoch 1201, loss 0.4252, train acc 84.06%, f1 0.8412, precision 0.8373, recall 0.8451, auc 0.8406
epoch 1301, loss 0.3217, train acc 84.08%, f1 0.8409, precision 0.8392, recall 0.8427, auc 0.8408
epoch 1401, loss 0.3954, train acc 84.03%, f1 0.8410, precision 0.8364, recall 0.8455, auc 0.8403
epoch 1501, loss 0.4094, train acc 83.99%, f1 0.8409, precision 0.8351, recall 0.8467, auc 0.8399
epoch 1601, loss 0.2731, train acc 84.02%, f1 0.8406, precision 0.8374, recall 0.8439, auc 0.8402
epoch 1701, loss 0.3099, train acc 84.05%, f1 0.8403, precision 0.8403, recall 0.8403, auc 0.8405
epoch 1801, loss 0.4695, train acc 84.08%, f1 0.8410, precision 0.8390, recall 0.8430, auc 0.8408
epoch 1901, loss 0.2910, train acc 84.08%, f1 0.8411, precision 0.8390, recall 0.8432, auc 0.8408
epoch 2001, loss 0.3914, train acc 84.04%, f1 0.8408, precision 0.8378, recall 0.8439, auc 0.8404
epoch 2101, loss 0.3620, train acc 84.12%, f1 0.8417, precision 0.8380, recall 0.8454, auc 0.8412
epoch 2201, loss 0.3679, train acc 84.07%, f1 0.8408, precision 0.8395, recall 0.8421, auc 0.8407
epoch 2301, loss 0.3678, train acc 84.08%, f1 0.8407, precision 0.8404, recall 0.8410, auc 0.8408
epoch 2401, loss 0.3358, train acc 84.11%, f1 0.8421, precision 0.8359, recall 0.8484, auc 0.8411
epoch 2501, loss 0.3507, train acc 84.13%, f1 0.8415, precision 0.8396, recall 0.8434, auc 0.8413
epoch 2601, loss 0.3611, train acc 84.13%, f1 0.8412, precision 0.8408, recall 0.8416, auc 0.8413
epoch 2701, loss 0.3870, train acc 84.12%, f1 0.8417, precision 0.8384, recall 0.8450, auc 0.8412
epoch 2801, loss 0.3508, train acc 84.15%, f1 0.8420, precision 0.8384, recall 0.8455, auc 0.8415
epoch 2901, loss 0.2509, train acc 84.21%, f1 0.8425, precision 0.8394, recall 0.8456, auc 0.8421
epoch 3001, loss 0.3646, train acc 84.22%, f1 0.8425, precision 0.8398, recall 0.8453, auc 0.8422
epoch 3101, loss 0.3820, train acc 84.33%, f1 0.8429, precision 0.8437, recall 0.8422, auc 0.8433
epoch 3201, loss 0.5595, train acc 84.30%, f1 0.8430, precision 0.8419, recall 0.8441, auc 0.8430
epoch 3301, loss 0.3076, train acc 84.37%, f1 0.8436, precision 0.8431, recall 0.8442, auc 0.8437
epoch 3401, loss 0.3877, train acc 84.43%, f1 0.8439, precision 0.8453, recall 0.8425, auc 0.8443
epoch 3501, loss 0.4079, train acc 84.45%, f1 0.8442, precision 0.8451, recall 0.8432, auc 0.8445
epoch 3601, loss 0.3756, train acc 84.61%, f1 0.8464, precision 0.8440, recall 0.8489, auc 0.8461
epoch 3701, loss 0.3977, train acc 84.68%, f1 0.8469, precision 0.8455, recall 0.8483, auc 0.8468
epoch 3801, loss 0.3906, train acc 84.78%, f1 0.8476, precision 0.8473, recall 0.8480, auc 0.8478
epoch 3901, loss 0.2348, train acc 84.90%, f1 0.8487, precision 0.8491, recall 0.8484, auc 0.8490
epoch 4001, loss 0.2576, train acc 84.97%, f1 0.8491, precision 0.8513, recall 0.8470, auc 0.8497
epoch 4101, loss 0.2867, train acc 85.14%, f1 0.8506, precision 0.8545, recall 0.8467, auc 0.8514
epoch 4201, loss 0.2547, train acc 85.21%, f1 0.8519, precision 0.8518, recall 0.8520, auc 0.8521
epoch 4301, loss 0.3977, train acc 85.33%, f1 0.8530, precision 0.8540, recall 0.8519, auc 0.8533
epoch 4401, loss 0.3602, train acc 85.48%, f1 0.8543, precision 0.8562, recall 0.8523, auc 0.8548
epoch 4501, loss 0.3471, train acc 85.57%, f1 0.8554, precision 0.8561, recall 0.8546, auc 0.8557
epoch 4601, loss 0.2831, train acc 85.65%, f1 0.8567, precision 0.8545, recall 0.8589, auc 0.8565
epoch 4701, loss 0.3966, train acc 85.77%, f1 0.8575, precision 0.8579, recall 0.8570, auc 0.8577
epoch 4801, loss 0.3366, train acc 85.87%, f1 0.8585, precision 0.8587, recall 0.8583, auc 0.8587
epoch 4901, loss 0.3484, train acc 85.97%, f1 0.8593, precision 0.8603, recall 0.8584, auc 0.8597
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_2
./test_pima/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.655

the Fscore is 0.6101694915254238

the precision is 0.43902439024390244

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_2
----------------------



epoch 1, loss 0.6930, train acc 49.82%, f1 0.6651, precision 0.4982, recall 1.0000, auc 0.5000
epoch 101, loss 0.5944, train acc 78.14%, f1 0.7916, precision 0.7539, recall 0.8333, auc 0.7816
epoch 201, loss 0.4542, train acc 81.53%, f1 0.8159, precision 0.8106, recall 0.8212, auc 0.8153
epoch 301, loss 0.3758, train acc 82.89%, f1 0.8275, precision 0.8312, recall 0.8239, auc 0.8289
epoch 401, loss 0.4495, train acc 83.65%, f1 0.8350, precision 0.8395, recall 0.8306, auc 0.8365
epoch 501, loss 0.2756, train acc 83.91%, f1 0.8376, precision 0.8426, recall 0.8326, auc 0.8391
epoch 601, loss 0.3671, train acc 83.96%, f1 0.8383, precision 0.8423, recall 0.8343, auc 0.8396
epoch 701, loss 0.3231, train acc 83.97%, f1 0.8383, precision 0.8426, recall 0.8341, auc 0.8397
epoch 801, loss 0.4075, train acc 83.98%, f1 0.8383, precision 0.8434, recall 0.8332, auc 0.8398
epoch 901, loss 0.3863, train acc 83.98%, f1 0.8378, precision 0.8450, recall 0.8308, auc 0.8397
epoch 1001, loss 0.5096, train acc 84.05%, f1 0.8392, precision 0.8432, recall 0.8351, auc 0.8405
epoch 1101, loss 0.3381, train acc 84.08%, f1 0.8394, precision 0.8440, recall 0.8348, auc 0.8408
epoch 1201, loss 0.4183, train acc 84.13%, f1 0.8400, precision 0.8435, recall 0.8366, auc 0.8412
epoch 1301, loss 0.3296, train acc 84.07%, f1 0.8400, precision 0.8406, recall 0.8395, auc 0.8407
epoch 1401, loss 0.2572, train acc 84.11%, f1 0.8397, precision 0.8440, recall 0.8354, auc 0.8410
epoch 1501, loss 0.4117, train acc 84.07%, f1 0.8393, precision 0.8437, recall 0.8350, auc 0.8407
epoch 1601, loss 0.3232, train acc 84.12%, f1 0.8400, precision 0.8436, recall 0.8364, auc 0.8412
epoch 1701, loss 0.4146, train acc 84.10%, f1 0.8401, precision 0.8415, recall 0.8388, auc 0.8410
epoch 1801, loss 0.3833, train acc 84.12%, f1 0.8407, precision 0.8404, recall 0.8410, auc 0.8412
epoch 1901, loss 0.3646, train acc 84.04%, f1 0.8389, precision 0.8439, recall 0.8339, auc 0.8404
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_2
./test_pima/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.575

the Fscore is 0.5595854922279793

the precision is 0.38848920863309355

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_2
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6089, train acc 78.43%, f1 0.7811, precision 0.7929, recall 0.7696, auc 0.7843
epoch 201, loss 0.4302, train acc 80.70%, f1 0.8066, precision 0.8080, recall 0.8053, auc 0.8070
epoch 301, loss 0.4874, train acc 82.18%, f1 0.8221, precision 0.8210, recall 0.8231, auc 0.8218
epoch 401, loss 0.3670, train acc 82.96%, f1 0.8297, precision 0.8291, recall 0.8302, auc 0.8296
epoch 501, loss 0.3894, train acc 83.53%, f1 0.8355, precision 0.8343, recall 0.8367, auc 0.8353
epoch 601, loss 0.4128, train acc 83.78%, f1 0.8381, precision 0.8369, recall 0.8393, auc 0.8378
epoch 701, loss 0.3621, train acc 83.91%, f1 0.8393, precision 0.8386, recall 0.8400, auc 0.8391
epoch 801, loss 0.2857, train acc 84.02%, f1 0.8403, precision 0.8398, recall 0.8408, auc 0.8402
epoch 901, loss 0.3917, train acc 84.06%, f1 0.8407, precision 0.8399, recall 0.8415, auc 0.8406
epoch 1001, loss 0.3469, train acc 84.07%, f1 0.8408, precision 0.8405, recall 0.8411, auc 0.8407
epoch 1101, loss 0.3493, train acc 84.14%, f1 0.8414, precision 0.8413, recall 0.8416, auc 0.8414
epoch 1201, loss 0.3117, train acc 84.15%, f1 0.8415, precision 0.8415, recall 0.8415, auc 0.8415
epoch 1301, loss 0.5934, train acc 84.11%, f1 0.8411, precision 0.8408, recall 0.8415, auc 0.8411
epoch 1401, loss 0.3985, train acc 84.12%, f1 0.8412, precision 0.8410, recall 0.8414, auc 0.8412
epoch 1501, loss 0.3653, train acc 84.15%, f1 0.8415, precision 0.8413, recall 0.8418, auc 0.8415
epoch 1601, loss 0.3991, train acc 84.11%, f1 0.8410, precision 0.8411, recall 0.8410, auc 0.8411
epoch 1701, loss 0.2940, train acc 84.03%, f1 0.8403, precision 0.8404, recall 0.8402, auc 0.8403
epoch 1801, loss 0.4164, train acc 84.13%, f1 0.8414, precision 0.8413, recall 0.8414, auc 0.8413
epoch 1901, loss 0.3695, train acc 84.12%, f1 0.8413, precision 0.8411, recall 0.8415, auc 0.8412
epoch 2001, loss 0.4426, train acc 84.13%, f1 0.8412, precision 0.8413, recall 0.8412, auc 0.8413
epoch 2101, loss 0.2838, train acc 84.20%, f1 0.8420, precision 0.8421, recall 0.8419, auc 0.8420
epoch 2201, loss 0.5861, train acc 84.20%, f1 0.8420, precision 0.8420, recall 0.8420, auc 0.8420
epoch 2301, loss 0.4944, train acc 84.12%, f1 0.8412, precision 0.8413, recall 0.8412, auc 0.8412
epoch 2401, loss 0.2530, train acc 84.12%, f1 0.8412, precision 0.8412, recall 0.8412, auc 0.8412
epoch 2501, loss 0.3229, train acc 84.12%, f1 0.8412, precision 0.8411, recall 0.8412, auc 0.8412
epoch 2601, loss 0.4056, train acc 84.14%, f1 0.8414, precision 0.8414, recall 0.8414, auc 0.8414
epoch 2701, loss 0.2939, train acc 84.13%, f1 0.8413, precision 0.8412, recall 0.8413, auc 0.8413
epoch 2801, loss 0.3614, train acc 84.15%, f1 0.8415, precision 0.8414, recall 0.8415, auc 0.8415
epoch 2901, loss 0.3068, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 3001, loss 0.3685, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 3101, loss 0.3841, train acc 84.10%, f1 0.8410, precision 0.8411, recall 0.8410, auc 0.8410
epoch 3201, loss 0.3610, train acc 84.17%, f1 0.8417, precision 0.8417, recall 0.8416, auc 0.8417
epoch 3301, loss 0.3363, train acc 84.15%, f1 0.8415, precision 0.8416, recall 0.8415, auc 0.8415
epoch 3401, loss 0.2937, train acc 84.15%, f1 0.8415, precision 0.8415, recall 0.8415, auc 0.8415
epoch 3501, loss 0.2964, train acc 84.09%, f1 0.8409, precision 0.8410, recall 0.8409, auc 0.8409
epoch 3601, loss 0.4068, train acc 84.20%, f1 0.8420, precision 0.8420, recall 0.8420, auc 0.8420
epoch 3701, loss 0.3314, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8422, auc 0.8423
epoch 3801, loss 0.2945, train acc 84.17%, f1 0.8417, precision 0.8418, recall 0.8417, auc 0.8417
epoch 3901, loss 0.4500, train acc 84.15%, f1 0.8415, precision 0.8416, recall 0.8415, auc 0.8415
epoch 4001, loss 0.3497, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8423, auc 0.8423
epoch 4101, loss 0.3979, train acc 84.21%, f1 0.8421, precision 0.8422, recall 0.8420, auc 0.8421
epoch 4201, loss 0.4377, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8423, auc 0.8423
epoch 4301, loss 0.3795, train acc 84.21%, f1 0.8421, precision 0.8421, recall 0.8421, auc 0.8421
epoch 4401, loss 0.3839, train acc 84.22%, f1 0.8422, precision 0.8423, recall 0.8422, auc 0.8422
epoch 4501, loss 0.3489, train acc 84.28%, f1 0.8428, precision 0.8429, recall 0.8427, auc 0.8428
epoch 4601, loss 0.3194, train acc 84.23%, f1 0.8423, precision 0.8423, recall 0.8423, auc 0.8423
epoch 4701, loss 0.2920, train acc 84.26%, f1 0.8426, precision 0.8428, recall 0.8425, auc 0.8426
epoch 4801, loss 0.5325, train acc 84.33%, f1 0.8433, precision 0.8433, recall 0.8433, auc 0.8433
epoch 4901, loss 0.3721, train acc 84.32%, f1 0.8432, precision 0.8431, recall 0.8432, auc 0.8432
epoch 5001, loss 0.3397, train acc 84.35%, f1 0.8435, precision 0.8435, recall 0.8435, auc 0.8435
epoch 5101, loss 0.2903, train acc 84.34%, f1 0.8434, precision 0.8435, recall 0.8433, auc 0.8434
epoch 5201, loss 0.3322, train acc 84.36%, f1 0.8436, precision 0.8436, recall 0.8436, auc 0.8436
epoch 5301, loss 0.3641, train acc 84.37%, f1 0.8437, precision 0.8438, recall 0.8437, auc 0.8437
epoch 5401, loss 0.4171, train acc 84.44%, f1 0.8444, precision 0.8444, recall 0.8444, auc 0.8444
epoch 5501, loss 0.4016, train acc 84.42%, f1 0.8441, precision 0.8442, recall 0.8441, auc 0.8442
epoch 5601, loss 0.3122, train acc 84.43%, f1 0.8443, precision 0.8443, recall 0.8443, auc 0.8443
epoch 5701, loss 0.3291, train acc 84.41%, f1 0.8440, precision 0.8441, recall 0.8440, auc 0.8441
epoch 5801, loss 0.3606, train acc 84.44%, f1 0.8444, precision 0.8445, recall 0.8442, auc 0.8444
epoch 5901, loss 0.4438, train acc 84.41%, f1 0.8441, precision 0.8441, recall 0.8441, auc 0.8441
epoch 6001, loss 0.4430, train acc 84.46%, f1 0.8446, precision 0.8447, recall 0.8445, auc 0.8446
epoch 6101, loss 0.3804, train acc 84.45%, f1 0.8445, precision 0.8446, recall 0.8445, auc 0.8445
epoch 6201, loss 0.3586, train acc 84.46%, f1 0.8446, precision 0.8447, recall 0.8445, auc 0.8446
epoch 6301, loss 0.3770, train acc 84.49%, f1 0.8449, precision 0.8450, recall 0.8449, auc 0.8449
epoch 6401, loss 0.4017, train acc 84.55%, f1 0.8455, precision 0.8455, recall 0.8455, auc 0.8455
epoch 6501, loss 0.4171, train acc 84.58%, f1 0.8458, precision 0.8458, recall 0.8457, auc 0.8458
epoch 6601, loss 0.3887, train acc 84.56%, f1 0.8456, precision 0.8457, recall 0.8456, auc 0.8456
epoch 6701, loss 0.4314, train acc 84.58%, f1 0.8458, precision 0.8457, recall 0.8458, auc 0.8458
epoch 6801, loss 0.3013, train acc 84.57%, f1 0.8457, precision 0.8457, recall 0.8456, auc 0.8457
epoch 6901, loss 0.3090, train acc 84.58%, f1 0.8458, precision 0.8458, recall 0.8458, auc 0.8458
epoch 7001, loss 0.4009, train acc 84.63%, f1 0.8463, precision 0.8464, recall 0.8463, auc 0.8463
epoch 7101, loss 0.3391, train acc 84.63%, f1 0.8463, precision 0.8463, recall 0.8463, auc 0.8463
epoch 7201, loss 0.2503, train acc 84.64%, f1 0.8464, precision 0.8465, recall 0.8463, auc 0.8464
epoch 7301, loss 0.2576, train acc 84.71%, f1 0.8470, precision 0.8471, recall 0.8470, auc 0.8471
epoch 7401, loss 0.3244, train acc 84.68%, f1 0.8468, precision 0.8468, recall 0.8468, auc 0.8468
epoch 7501, loss 0.3602, train acc 84.72%, f1 0.8472, precision 0.8472, recall 0.8472, auc 0.8472
epoch 7601, loss 0.4340, train acc 84.73%, f1 0.8473, precision 0.8473, recall 0.8473, auc 0.8473
epoch 7701, loss 0.3878, train acc 84.79%, f1 0.8479, precision 0.8479, recall 0.8479, auc 0.8479
epoch 7801, loss 0.3525, train acc 84.76%, f1 0.8476, precision 0.8476, recall 0.8476, auc 0.8476
epoch 7901, loss 0.3883, train acc 84.77%, f1 0.8477, precision 0.8477, recall 0.8477, auc 0.8477
epoch 8001, loss 0.3452, train acc 84.75%, f1 0.8475, precision 0.8475, recall 0.8475, auc 0.8475
epoch 8101, loss 0.3785, train acc 84.80%, f1 0.8480, precision 0.8480, recall 0.8480, auc 0.8480
epoch 8201, loss 0.3071, train acc 84.83%, f1 0.8483, precision 0.8483, recall 0.8483, auc 0.8483
epoch 8301, loss 0.2368, train acc 84.85%, f1 0.8485, precision 0.8484, recall 0.8486, auc 0.8485
epoch 8401, loss 0.3655, train acc 84.87%, f1 0.8487, precision 0.8486, recall 0.8487, auc 0.8487
epoch 8501, loss 0.4622, train acc 84.87%, f1 0.8487, precision 0.8487, recall 0.8488, auc 0.8487
epoch 8601, loss 0.3529, train acc 84.86%, f1 0.8486, precision 0.8486, recall 0.8486, auc 0.8486
epoch 8701, loss 0.2604, train acc 84.87%, f1 0.8487, precision 0.8487, recall 0.8487, auc 0.8487
epoch 8801, loss 0.3019, train acc 84.86%, f1 0.8486, precision 0.8486, recall 0.8486, auc 0.8486
epoch 8901, loss 0.2388, train acc 84.92%, f1 0.8492, precision 0.8492, recall 0.8492, auc 0.8492
epoch 9001, loss 0.3136, train acc 84.95%, f1 0.8495, precision 0.8495, recall 0.8496, auc 0.8495
epoch 9101, loss 0.3686, train acc 84.93%, f1 0.8493, precision 0.8494, recall 0.8493, auc 0.8493
epoch 9201, loss 0.3199, train acc 84.96%, f1 0.8496, precision 0.8496, recall 0.8496, auc 0.8496
epoch 9301, loss 0.2855, train acc 84.97%, f1 0.8497, precision 0.8497, recall 0.8497, auc 0.8497
epoch 9401, loss 0.2624, train acc 85.06%, f1 0.8506, precision 0.8506, recall 0.8506, auc 0.8506
epoch 9501, loss 0.3522, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8505
epoch 9601, loss 0.3522, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 9701, loss 0.3617, train acc 85.07%, f1 0.8507, precision 0.8508, recall 0.8507, auc 0.8507
epoch 9801, loss 0.3349, train acc 85.12%, f1 0.8512, precision 0.8512, recall 0.8512, auc 0.8512
epoch 9901, loss 0.3169, train acc 85.11%, f1 0.8511, precision 0.8511, recall 0.8512, auc 0.8511
epoch 10001, loss 0.2862, train acc 85.15%, f1 0.8515, precision 0.8515, recall 0.8515, auc 0.8515
epoch 10101, loss 0.3874, train acc 85.15%, f1 0.8515, precision 0.8515, recall 0.8515, auc 0.8515
epoch 10201, loss 0.3292, train acc 85.22%, f1 0.8522, precision 0.8522, recall 0.8522, auc 0.8522
epoch 10301, loss 0.3554, train acc 85.26%, f1 0.8526, precision 0.8526, recall 0.8525, auc 0.8526
epoch 10401, loss 0.3522, train acc 85.14%, f1 0.8514, precision 0.8514, recall 0.8514, auc 0.8514
epoch 10501, loss 0.4078, train acc 85.30%, f1 0.8530, precision 0.8530, recall 0.8530, auc 0.8530
epoch 10601, loss 0.4155, train acc 85.23%, f1 0.8523, precision 0.8523, recall 0.8523, auc 0.8523
epoch 10701, loss 0.4673, train acc 85.30%, f1 0.8530, precision 0.8530, recall 0.8530, auc 0.8530
epoch 10801, loss 0.3449, train acc 85.28%, f1 0.8528, precision 0.8528, recall 0.8528, auc 0.8528
epoch 10901, loss 0.2837, train acc 85.37%, f1 0.8537, precision 0.8537, recall 0.8537, auc 0.8537
epoch 11001, loss 0.3469, train acc 85.35%, f1 0.8535, precision 0.8535, recall 0.8535, auc 0.8535
epoch 11101, loss 0.2614, train acc 85.33%, f1 0.8533, precision 0.8533, recall 0.8534, auc 0.8533
epoch 11201, loss 0.2123, train acc 85.31%, f1 0.8531, precision 0.8531, recall 0.8532, auc 0.8531
epoch 11301, loss 0.3048, train acc 85.30%, f1 0.8530, precision 0.8530, recall 0.8530, auc 0.8530
epoch 11401, loss 0.4415, train acc 85.39%, f1 0.8539, precision 0.8538, recall 0.8539, auc 0.8539
epoch 11501, loss 0.3189, train acc 85.42%, f1 0.8542, precision 0.8542, recall 0.8542, auc 0.8542
epoch 11601, loss 0.3897, train acc 85.39%, f1 0.8539, precision 0.8538, recall 0.8539, auc 0.8539
epoch 11701, loss 0.4484, train acc 85.38%, f1 0.8538, precision 0.8538, recall 0.8539, auc 0.8538
epoch 11801, loss 0.2948, train acc 85.45%, f1 0.8545, precision 0.8544, recall 0.8546, auc 0.8545
epoch 11901, loss 0.1954, train acc 85.46%, f1 0.8546, precision 0.8547, recall 0.8546, auc 0.8546
epoch 12001, loss 0.4544, train acc 85.45%, f1 0.8545, precision 0.8545, recall 0.8545, auc 0.8545
epoch 12101, loss 0.2596, train acc 85.49%, f1 0.8549, precision 0.8548, recall 0.8550, auc 0.8549
epoch 12201, loss 0.4167, train acc 85.49%, f1 0.8549, precision 0.8549, recall 0.8549, auc 0.8549
epoch 12301, loss 0.3640, train acc 85.49%, f1 0.8549, precision 0.8549, recall 0.8550, auc 0.8549
epoch 12401, loss 0.2924, train acc 85.54%, f1 0.8554, precision 0.8554, recall 0.8554, auc 0.8554
epoch 12501, loss 0.2556, train acc 85.54%, f1 0.8554, precision 0.8553, recall 0.8555, auc 0.8554
epoch 12601, loss 0.3628, train acc 85.59%, f1 0.8559, precision 0.8559, recall 0.8559, auc 0.8559
epoch 12701, loss 0.2272, train acc 85.54%, f1 0.8554, precision 0.8554, recall 0.8554, auc 0.8554
epoch 12801, loss 0.3292, train acc 85.53%, f1 0.8553, precision 0.8553, recall 0.8553, auc 0.8553
epoch 12901, loss 0.2771, train acc 85.57%, f1 0.8557, precision 0.8557, recall 0.8556, auc 0.8557
epoch 13001, loss 0.2857, train acc 85.55%, f1 0.8555, precision 0.8556, recall 0.8555, auc 0.8555
epoch 13101, loss 0.2932, train acc 85.53%, f1 0.8553, precision 0.8553, recall 0.8554, auc 0.8553
epoch 13201, loss 0.3482, train acc 85.65%, f1 0.8565, precision 0.8564, recall 0.8566, auc 0.8565
epoch 13301, loss 0.3432, train acc 85.59%, f1 0.8559, precision 0.8559, recall 0.8559, auc 0.8559
epoch 13401, loss 0.4838, train acc 85.66%, f1 0.8566, precision 0.8565, recall 0.8566, auc 0.8566
epoch 13501, loss 0.3632, train acc 85.58%, f1 0.8558, precision 0.8558, recall 0.8558, auc 0.8558
epoch 13601, loss 0.3274, train acc 85.61%, f1 0.8561, precision 0.8561, recall 0.8561, auc 0.8561
epoch 13701, loss 0.2376, train acc 85.66%, f1 0.8566, precision 0.8566, recall 0.8566, auc 0.8566
epoch 13801, loss 0.2822, train acc 85.70%, f1 0.8570, precision 0.8570, recall 0.8570, auc 0.8570
epoch 13901, loss 0.3362, train acc 85.61%, f1 0.8561, precision 0.8561, recall 0.8561, auc 0.8561
epoch 14001, loss 0.3179, train acc 85.68%, f1 0.8568, precision 0.8568, recall 0.8568, auc 0.8568
epoch 14101, loss 0.3190, train acc 85.62%, f1 0.8562, precision 0.8562, recall 0.8562, auc 0.8562
epoch 14201, loss 0.3681, train acc 85.71%, f1 0.8571, precision 0.8571, recall 0.8571, auc 0.8571
epoch 14301, loss 0.3765, train acc 85.69%, f1 0.8569, precision 0.8568, recall 0.8570, auc 0.8569
epoch 14401, loss 0.2033, train acc 85.69%, f1 0.8568, precision 0.8569, recall 0.8568, auc 0.8569
epoch 14501, loss 0.3311, train acc 85.68%, f1 0.8568, precision 0.8568, recall 0.8568, auc 0.8568
epoch 14601, loss 0.3350, train acc 85.63%, f1 0.8563, precision 0.8563, recall 0.8563, auc 0.8563
epoch 14701, loss 0.2755, train acc 85.72%, f1 0.8572, precision 0.8572, recall 0.8573, auc 0.8572
epoch 14801, loss 0.3462, train acc 85.74%, f1 0.8574, precision 0.8574, recall 0.8574, auc 0.8574
epoch 14901, loss 0.4001, train acc 85.69%, f1 0.8569, precision 0.8569, recall 0.8569, auc 0.8569
epoch 15001, loss 0.2602, train acc 85.66%, f1 0.8566, precision 0.8565, recall 0.8567, auc 0.8566
epoch 15101, loss 0.3671, train acc 85.71%, f1 0.8571, precision 0.8572, recall 0.8571, auc 0.8571
epoch 15201, loss 0.4665, train acc 85.67%, f1 0.8567, precision 0.8567, recall 0.8568, auc 0.8567
epoch 15301, loss 0.3469, train acc 85.69%, f1 0.8569, precision 0.8570, recall 0.8569, auc 0.8569
epoch 15401, loss 0.4074, train acc 85.73%, f1 0.8573, precision 0.8574, recall 0.8573, auc 0.8573
epoch 15501, loss 0.2074, train acc 85.74%, f1 0.8574, precision 0.8574, recall 0.8575, auc 0.8574
epoch 15601, loss 0.2533, train acc 85.72%, f1 0.8572, precision 0.8571, recall 0.8573, auc 0.8572
epoch 15701, loss 0.3663, train acc 85.74%, f1 0.8575, precision 0.8573, recall 0.8576, auc 0.8574
epoch 15801, loss 0.2490, train acc 85.73%, f1 0.8573, precision 0.8573, recall 0.8573, auc 0.8573
epoch 15901, loss 0.3105, train acc 85.74%, f1 0.8574, precision 0.8574, recall 0.8574, auc 0.8574
epoch 16001, loss 0.3267, train acc 85.76%, f1 0.8576, precision 0.8576, recall 0.8577, auc 0.8576
epoch 16101, loss 0.3021, train acc 85.76%, f1 0.8576, precision 0.8576, recall 0.8576, auc 0.8576
epoch 16201, loss 0.3196, train acc 85.76%, f1 0.8576, precision 0.8577, recall 0.8576, auc 0.8576
epoch 16301, loss 0.4170, train acc 85.82%, f1 0.8582, precision 0.8583, recall 0.8581, auc 0.8582
epoch 16401, loss 0.3362, train acc 85.76%, f1 0.8576, precision 0.8576, recall 0.8575, auc 0.8576
epoch 16501, loss 0.2976, train acc 85.80%, f1 0.8580, precision 0.8580, recall 0.8579, auc 0.8580/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.3571, train acc 85.76%, f1 0.8576, precision 0.8576, recall 0.8576, auc 0.8576
epoch 16701, loss 0.4483, train acc 85.80%, f1 0.8580, precision 0.8580, recall 0.8580, auc 0.8580
epoch 16801, loss 0.3815, train acc 85.80%, f1 0.8580, precision 0.8580, recall 0.8581, auc 0.8580
epoch 16901, loss 0.3240, train acc 85.80%, f1 0.8580, precision 0.8580, recall 0.8580, auc 0.8580
epoch 17001, loss 0.2971, train acc 85.80%, f1 0.8580, precision 0.8580, recall 0.8580, auc 0.8580
epoch 17101, loss 0.3118, train acc 85.78%, f1 0.8578, precision 0.8578, recall 0.8579, auc 0.8578
epoch 17201, loss 0.3541, train acc 85.81%, f1 0.8581, precision 0.8582, recall 0.8581, auc 0.8581
epoch 17301, loss 0.4406, train acc 85.74%, f1 0.8574, precision 0.8574, recall 0.8574, auc 0.8574
epoch 17401, loss 0.3143, train acc 85.82%, f1 0.8582, precision 0.8582, recall 0.8582, auc 0.8582
epoch 17501, loss 0.2383, train acc 85.83%, f1 0.8583, precision 0.8583, recall 0.8583, auc 0.8583
epoch 17601, loss 0.2773, train acc 85.87%, f1 0.8587, precision 0.8587, recall 0.8587, auc 0.8587
epoch 17701, loss 0.3935, train acc 85.86%, f1 0.8586, precision 0.8587, recall 0.8586, auc 0.8586
epoch 17801, loss 0.2857, train acc 85.87%, f1 0.8586, precision 0.8587, recall 0.8586, auc 0.8587
epoch 17901, loss 0.3421, train acc 85.90%, f1 0.8590, precision 0.8590, recall 0.8589, auc 0.8590
epoch 18001, loss 0.3416, train acc 85.88%, f1 0.8588, precision 0.8589, recall 0.8588, auc 0.8588
epoch 18101, loss 0.4862, train acc 85.92%, f1 0.8592, precision 0.8591, recall 0.8592, auc 0.8592
epoch 18201, loss 0.2575, train acc 85.85%, f1 0.8585, precision 0.8585, recall 0.8584, auc 0.8585
epoch 18301, loss 0.2335, train acc 85.81%, f1 0.8581, precision 0.8580, recall 0.8582, auc 0.8581
epoch 18401, loss 0.4507, train acc 85.91%, f1 0.8591, precision 0.8590, recall 0.8591, auc 0.8591
epoch 18501, loss 0.3579, train acc 85.88%, f1 0.8588, precision 0.8589, recall 0.8588, auc 0.8588
epoch 18601, loss 0.3812, train acc 85.84%, f1 0.8585, precision 0.8584, recall 0.8586, auc 0.8584
epoch 18701, loss 0.3079, train acc 85.90%, f1 0.8590, precision 0.8590, recall 0.8590, auc 0.8590
epoch 18801, loss 0.3028, train acc 85.90%, f1 0.8590, precision 0.8589, recall 0.8591, auc 0.8590
epoch 18901, loss 0.3255, train acc 85.95%, f1 0.8595, precision 0.8594, recall 0.8597, auc 0.8595
epoch 19001, loss 0.3446, train acc 85.89%, f1 0.8589, precision 0.8589, recall 0.8588, auc 0.8589
epoch 19101, loss 0.2114, train acc 85.92%, f1 0.8592, precision 0.8592, recall 0.8591, auc 0.8592
epoch 19201, loss 0.3050, train acc 85.95%, f1 0.8596, precision 0.8595, recall 0.8596, auc 0.8595
epoch 19301, loss 0.3822, train acc 85.97%, f1 0.8597, precision 0.8597, recall 0.8597, auc 0.8597
epoch 19401, loss 0.3711, train acc 85.96%, f1 0.8596, precision 0.8596, recall 0.8597, auc 0.8596
epoch 19501, loss 0.4539, train acc 85.93%, f1 0.8593, precision 0.8594, recall 0.8592, auc 0.8593
epoch 19601, loss 0.2098, train acc 85.91%, f1 0.8591, precision 0.8592, recall 0.8590, auc 0.8591
epoch 19701, loss 0.3802, train acc 85.94%, f1 0.8594, precision 0.8595, recall 0.8592, auc 0.8594
epoch 19801, loss 0.2360, train acc 85.91%, f1 0.8591, precision 0.8592, recall 0.8591, auc 0.8591
epoch 19901, loss 0.2955, train acc 85.89%, f1 0.8589, precision 0.8588, recall 0.8589, auc 0.8589
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_2
./test_pima/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6257407407407407

the Fscore is 0.5888888888888889

the precision is 0.42063492063492064

the recall is 0.9814814814814815

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_2
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5913, train acc 78.41%, f1 0.7842, precision 0.7841, recall 0.7842, auc 0.7841
epoch 201, loss 0.5102, train acc 80.45%, f1 0.8045, precision 0.8044, recall 0.8045, auc 0.8045
epoch 301, loss 0.4662, train acc 82.07%, f1 0.8207, precision 0.8207, recall 0.8207, auc 0.8207
epoch 401, loss 0.4487, train acc 82.95%, f1 0.8295, precision 0.8295, recall 0.8295, auc 0.8295
epoch 501, loss 0.4146, train acc 83.45%, f1 0.8345, precision 0.8345, recall 0.8345, auc 0.8345
epoch 601, loss 0.4895, train acc 83.73%, f1 0.8373, precision 0.8373, recall 0.8373, auc 0.8373
epoch 701, loss 0.3336, train acc 83.88%, f1 0.8388, precision 0.8388, recall 0.8388, auc 0.8388
epoch 801, loss 0.3304, train acc 84.01%, f1 0.8401, precision 0.8401, recall 0.8401, auc 0.8401
epoch 901, loss 0.4519, train acc 84.09%, f1 0.8409, precision 0.8409, recall 0.8409, auc 0.8409
epoch 1001, loss 0.3894, train acc 84.12%, f1 0.8412, precision 0.8412, recall 0.8412, auc 0.8412
epoch 1101, loss 0.3502, train acc 84.19%, f1 0.8419, precision 0.8419, recall 0.8419, auc 0.8419
epoch 1201, loss 0.3838, train acc 84.19%, f1 0.8419, precision 0.8419, recall 0.8419, auc 0.8419
epoch 1301, loss 0.3906, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 1401, loss 0.4386, train acc 84.17%, f1 0.8417, precision 0.8417, recall 0.8417, auc 0.8417
epoch 1501, loss 0.3732, train acc 84.20%, f1 0.8420, precision 0.8420, recall 0.8420, auc 0.8420
epoch 1601, loss 0.5260, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 1701, loss 0.4081, train acc 84.23%, f1 0.8423, precision 0.8423, recall 0.8423, auc 0.8423
epoch 1801, loss 0.3188, train acc 84.19%, f1 0.8419, precision 0.8419, recall 0.8419, auc 0.8419
epoch 1901, loss 0.3894, train acc 84.13%, f1 0.8413, precision 0.8413, recall 0.8413, auc 0.8413
epoch 2001, loss 0.3766, train acc 84.17%, f1 0.8417, precision 0.8417, recall 0.8417, auc 0.8417
epoch 2101, loss 0.4050, train acc 84.20%, f1 0.8420, precision 0.8420, recall 0.8420, auc 0.8420
epoch 2201, loss 0.2189, train acc 84.23%, f1 0.8423, precision 0.8423, recall 0.8423, auc 0.8423
epoch 2301, loss 0.3963, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 2401, loss 0.3154, train acc 84.18%, f1 0.8418, precision 0.8418, recall 0.8418, auc 0.8418
epoch 2501, loss 0.3530, train acc 84.17%, f1 0.8417, precision 0.8417, recall 0.8417, auc 0.8417
epoch 2601, loss 0.3992, train acc 84.18%, f1 0.8418, precision 0.8418, recall 0.8418, auc 0.8418
epoch 2701, loss 0.4263, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8415, auc 0.8416
epoch 2801, loss 0.3306, train acc 84.12%, f1 0.8412, precision 0.8411, recall 0.8412, auc 0.8412
epoch 2901, loss 0.3508, train acc 84.09%, f1 0.8409, precision 0.8409, recall 0.8410, auc 0.8409
epoch 3001, loss 0.3990, train acc 84.14%, f1 0.8414, precision 0.8414, recall 0.8414, auc 0.8414
epoch 3101, loss 0.4998, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 3201, loss 0.3567, train acc 84.19%, f1 0.8419, precision 0.8419, recall 0.8420, auc 0.8419
epoch 3301, loss 0.3726, train acc 84.19%, f1 0.8419, precision 0.8418, recall 0.8420, auc 0.8419
epoch 3401, loss 0.4211, train acc 84.23%, f1 0.8423, precision 0.8423, recall 0.8422, auc 0.8423
epoch 3501, loss 0.3806, train acc 84.23%, f1 0.8423, precision 0.8425, recall 0.8421, auc 0.8423
epoch 3601, loss 0.2478, train acc 84.21%, f1 0.8421, precision 0.8421, recall 0.8421, auc 0.8421
epoch 3701, loss 0.4165, train acc 84.21%, f1 0.8421, precision 0.8420, recall 0.8423, auc 0.8421
epoch 3801, loss 0.4119, train acc 84.20%, f1 0.8420, precision 0.8421, recall 0.8419, auc 0.8420
epoch 3901, loss 0.4679, train acc 84.20%, f1 0.8421, precision 0.8419, recall 0.8423, auc 0.8420
epoch 4001, loss 0.4092, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8422, auc 0.8423
epoch 4101, loss 0.2995, train acc 84.30%, f1 0.8430, precision 0.8430, recall 0.8430, auc 0.8430
epoch 4201, loss 0.3590, train acc 84.34%, f1 0.8434, precision 0.8436, recall 0.8432, auc 0.8434
epoch 4301, loss 0.2744, train acc 84.35%, f1 0.8435, precision 0.8435, recall 0.8436, auc 0.8435
epoch 4401, loss 0.4351, train acc 84.28%, f1 0.8428, precision 0.8428, recall 0.8429, auc 0.8428
epoch 4501, loss 0.5438, train acc 84.32%, f1 0.8432, precision 0.8432, recall 0.8433, auc 0.8432
epoch 4601, loss 0.3518, train acc 84.27%, f1 0.8427, precision 0.8427, recall 0.8427, auc 0.8427
epoch 4701, loss 0.3214, train acc 84.33%, f1 0.8432, precision 0.8434, recall 0.8431, auc 0.8433
epoch 4801, loss 0.3688, train acc 84.37%, f1 0.8437, precision 0.8437, recall 0.8436, auc 0.8437
epoch 4901, loss 0.3493, train acc 84.33%, f1 0.8433, precision 0.8432, recall 0.8434, auc 0.8433
epoch 5001, loss 0.3323, train acc 84.32%, f1 0.8432, precision 0.8434, recall 0.8430, auc 0.8432
epoch 5101, loss 0.4239, train acc 84.40%, f1 0.8440, precision 0.8441, recall 0.8438, auc 0.8440
epoch 5201, loss 0.5011, train acc 84.41%, f1 0.8441, precision 0.8442, recall 0.8441, auc 0.8441
epoch 5301, loss 0.3604, train acc 84.39%, f1 0.8440, precision 0.8437, recall 0.8443, auc 0.8439
epoch 5401, loss 0.2794, train acc 84.39%, f1 0.8439, precision 0.8439, recall 0.8439, auc 0.8439
epoch 5501, loss 0.3173, train acc 84.40%, f1 0.8440, precision 0.8441, recall 0.8440, auc 0.8440
epoch 5601, loss 0.3435, train acc 84.39%, f1 0.8439, precision 0.8438, recall 0.8440, auc 0.8439
epoch 5701, loss 0.4971, train acc 84.44%, f1 0.8443, precision 0.8445, recall 0.8441, auc 0.8444
epoch 5801, loss 0.3562, train acc 84.46%, f1 0.8446, precision 0.8444, recall 0.8449, auc 0.8446
epoch 5901, loss 0.4093, train acc 84.51%, f1 0.8450, precision 0.8451, recall 0.8450, auc 0.8451
epoch 6001, loss 0.3819, train acc 84.47%, f1 0.8447, precision 0.8449, recall 0.8445, auc 0.8447
epoch 6101, loss 0.3231, train acc 84.48%, f1 0.8448, precision 0.8450, recall 0.8446, auc 0.8448
epoch 6201, loss 0.3964, train acc 84.47%, f1 0.8447, precision 0.8448, recall 0.8446, auc 0.8447
epoch 6301, loss 0.3439, train acc 84.52%, f1 0.8453, precision 0.8450, recall 0.8456, auc 0.8452
epoch 6401, loss 0.4102, train acc 84.54%, f1 0.8454, precision 0.8456, recall 0.8452, auc 0.8454
epoch 6501, loss 0.3100, train acc 84.52%, f1 0.8452, precision 0.8453, recall 0.8451, auc 0.8452
epoch 6601, loss 0.2679, train acc 84.57%, f1 0.8457, precision 0.8457, recall 0.8456, auc 0.8457
epoch 6701, loss 0.3658, train acc 84.55%, f1 0.8455, precision 0.8454, recall 0.8456, auc 0.8455
epoch 6801, loss 0.2837, train acc 84.60%, f1 0.8459, precision 0.8462, recall 0.8457, auc 0.8460
epoch 6901, loss 0.3031, train acc 84.62%, f1 0.8463, precision 0.8461, recall 0.8464, auc 0.8462
epoch 7001, loss 0.3716, train acc 84.63%, f1 0.8463, precision 0.8466, recall 0.8460, auc 0.8463
epoch 7101, loss 0.3469, train acc 84.64%, f1 0.8464, precision 0.8464, recall 0.8463, auc 0.8464
epoch 7201, loss 0.3295, train acc 84.65%, f1 0.8465, precision 0.8463, recall 0.8467, auc 0.8465
epoch 7301, loss 0.3353, train acc 84.68%, f1 0.8469, precision 0.8466, recall 0.8471, auc 0.8468
epoch 7401, loss 0.2017, train acc 84.69%, f1 0.8468, precision 0.8470, recall 0.8466, auc 0.8469
epoch 7501, loss 0.3121, train acc 84.67%, f1 0.8466, precision 0.8467, recall 0.8466, auc 0.8467
epoch 7601, loss 0.3384, train acc 84.70%, f1 0.8470, precision 0.8469, recall 0.8471, auc 0.8470
epoch 7701, loss 0.3809, train acc 84.72%, f1 0.8472, precision 0.8471, recall 0.8473, auc 0.8472
epoch 7801, loss 0.3443, train acc 84.76%, f1 0.8476, precision 0.8476, recall 0.8477, auc 0.8476
epoch 7901, loss 0.4539, train acc 84.76%, f1 0.8476, precision 0.8475, recall 0.8477, auc 0.8476
epoch 8001, loss 0.2710, train acc 84.78%, f1 0.8478, precision 0.8480, recall 0.8476, auc 0.8478
epoch 8101, loss 0.3756, train acc 84.80%, f1 0.8479, precision 0.8482, recall 0.8477, auc 0.8480
epoch 8201, loss 0.3296, train acc 84.81%, f1 0.8481, precision 0.8481, recall 0.8481, auc 0.8481/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2910, train acc 84.82%, f1 0.8483, precision 0.8479, recall 0.8487, auc 0.8482
epoch 8401, loss 0.2691, train acc 84.84%, f1 0.8484, precision 0.8485, recall 0.8483, auc 0.8484
epoch 8501, loss 0.2985, train acc 84.87%, f1 0.8487, precision 0.8487, recall 0.8486, auc 0.8487
epoch 8601, loss 0.3667, train acc 84.89%, f1 0.8489, precision 0.8490, recall 0.8489, auc 0.8489
epoch 8701, loss 0.3210, train acc 84.89%, f1 0.8489, precision 0.8487, recall 0.8491, auc 0.8489
epoch 8801, loss 0.3143, train acc 84.93%, f1 0.8493, precision 0.8495, recall 0.8491, auc 0.8493
epoch 8901, loss 0.2817, train acc 84.91%, f1 0.8491, precision 0.8489, recall 0.8493, auc 0.8491
epoch 9001, loss 0.3461, train acc 84.93%, f1 0.8492, precision 0.8494, recall 0.8491, auc 0.8493
epoch 9101, loss 0.3342, train acc 84.98%, f1 0.8498, precision 0.8497, recall 0.8499, auc 0.8498
epoch 9201, loss 0.3830, train acc 85.00%, f1 0.8500, precision 0.8500, recall 0.8501, auc 0.8500
epoch 9301, loss 0.3825, train acc 85.01%, f1 0.8501, precision 0.8501, recall 0.8501, auc 0.8501
epoch 9401, loss 0.2520, train acc 84.99%, f1 0.8499, precision 0.8499, recall 0.8498, auc 0.8499
epoch 9501, loss 0.2962, train acc 85.01%, f1 0.8501, precision 0.8502, recall 0.8500, auc 0.8501
epoch 9601, loss 0.4899, train acc 85.02%, f1 0.8502, precision 0.8503, recall 0.8502, auc 0.8502
epoch 9701, loss 0.4154, train acc 85.01%, f1 0.8501, precision 0.8501, recall 0.8501, auc 0.8501
epoch 9801, loss 0.2565, train acc 85.01%, f1 0.8501, precision 0.8502, recall 0.8500, auc 0.8501
epoch 9901, loss 0.4415, train acc 85.02%, f1 0.8502, precision 0.8502, recall 0.8501, auc 0.8502
epoch 10001, loss 0.3748, train acc 85.03%, f1 0.8503, precision 0.8502, recall 0.8503, auc 0.8503
epoch 10101, loss 0.4217, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8504, auc 0.8505
epoch 10201, loss 0.2916, train acc 85.04%, f1 0.8505, precision 0.8503, recall 0.8507, auc 0.8504
epoch 10301, loss 0.2855, train acc 85.09%, f1 0.8510, precision 0.8509, recall 0.8511, auc 0.8509
epoch 10401, loss 0.3977, train acc 85.13%, f1 0.8513, precision 0.8514, recall 0.8512, auc 0.8513
epoch 10501, loss 0.3583, train acc 85.12%, f1 0.8512, precision 0.8514, recall 0.8509, auc 0.8512
epoch 10601, loss 0.2730, train acc 85.15%, f1 0.8515, precision 0.8516, recall 0.8514, auc 0.8515
epoch 10701, loss 0.3265, train acc 85.15%, f1 0.8515, precision 0.8514, recall 0.8516, auc 0.8515
epoch 10801, loss 0.3914, train acc 85.17%, f1 0.8517, precision 0.8517, recall 0.8518, auc 0.8517
epoch 10901, loss 0.4734, train acc 85.20%, f1 0.8520, precision 0.8521, recall 0.8519, auc 0.8520
epoch 11001, loss 0.4068, train acc 85.18%, f1 0.8518, precision 0.8517, recall 0.8519, auc 0.8518
epoch 11101, loss 0.3561, train acc 85.21%, f1 0.8521, precision 0.8521, recall 0.8521, auc 0.8521
epoch 11201, loss 0.4137, train acc 85.16%, f1 0.8516, precision 0.8517, recall 0.8515, auc 0.8516
epoch 11301, loss 0.3144, train acc 85.17%, f1 0.8517, precision 0.8518, recall 0.8515, auc 0.8517
epoch 11401, loss 0.2644, train acc 85.20%, f1 0.8520, precision 0.8517, recall 0.8523, auc 0.8520
epoch 11501, loss 0.2922, train acc 85.24%, f1 0.8524, precision 0.8524, recall 0.8523, auc 0.8524
epoch 11601, loss 0.3712, train acc 85.27%, f1 0.8527, precision 0.8529, recall 0.8526, auc 0.8527
epoch 11701, loss 0.2485, train acc 85.26%, f1 0.8526, precision 0.8526, recall 0.8526, auc 0.8526
epoch 11801, loss 0.4136, train acc 85.30%, f1 0.8530, precision 0.8531, recall 0.8530, auc 0.8530
epoch 11901, loss 0.3127, train acc 85.27%, f1 0.8527, precision 0.8528, recall 0.8527, auc 0.8527
epoch 12001, loss 0.4623, train acc 85.33%, f1 0.8533, precision 0.8534, recall 0.8532, auc 0.8533
epoch 12101, loss 0.3477, train acc 85.33%, f1 0.8532, precision 0.8534, recall 0.8530, auc 0.8533
epoch 12201, loss 0.3161, train acc 85.32%, f1 0.8533, precision 0.8531, recall 0.8534, auc 0.8532
epoch 12301, loss 0.2468, train acc 85.38%, f1 0.8538, precision 0.8539, recall 0.8537, auc 0.8538
epoch 12401, loss 0.3335, train acc 85.38%, f1 0.8538, precision 0.8537, recall 0.8538, auc 0.8538
epoch 12501, loss 0.3716, train acc 85.39%, f1 0.8540, precision 0.8537, recall 0.8543, auc 0.8539
epoch 12601, loss 0.3650, train acc 85.36%, f1 0.8536, precision 0.8537, recall 0.8535, auc 0.8536
epoch 12701, loss 0.4337, train acc 85.40%, f1 0.8540, precision 0.8539, recall 0.8541, auc 0.8540
epoch 12801, loss 0.3441, train acc 85.40%, f1 0.8540, precision 0.8542, recall 0.8538, auc 0.8540
epoch 12901, loss 0.3192, train acc 85.44%, f1 0.8543, precision 0.8546, recall 0.8540, auc 0.8544
epoch 13001, loss 0.3017, train acc 85.48%, f1 0.8548, precision 0.8546, recall 0.8550, auc 0.8548
epoch 13101, loss 0.3969, train acc 85.45%, f1 0.8545, precision 0.8545, recall 0.8544, auc 0.8545
epoch 13201, loss 0.3422, train acc 85.46%, f1 0.8547, precision 0.8546, recall 0.8547, auc 0.8546
epoch 13301, loss 0.3337, train acc 85.48%, f1 0.8548, precision 0.8546, recall 0.8550, auc 0.8548
epoch 13401, loss 0.4502, train acc 85.46%, f1 0.8546, precision 0.8547, recall 0.8544, auc 0.8546
epoch 13501, loss 0.2710, train acc 85.45%, f1 0.8545, precision 0.8546, recall 0.8543, auc 0.8545
epoch 13601, loss 0.2791, train acc 85.54%, f1 0.8554, precision 0.8553, recall 0.8555, auc 0.8554
epoch 13701, loss 0.2907, train acc 85.52%, f1 0.8552, precision 0.8550, recall 0.8555, auc 0.8552
epoch 13801, loss 0.3926, train acc 85.57%, f1 0.8557, precision 0.8557, recall 0.8558, auc 0.8557
epoch 13901, loss 0.5276, train acc 85.59%, f1 0.8559, precision 0.8560, recall 0.8557, auc 0.8559
epoch 14001, loss 0.3255, train acc 85.59%, f1 0.8559, precision 0.8557, recall 0.8561, auc 0.8559
epoch 14101, loss 0.3690, train acc 85.62%, f1 0.8562, precision 0.8561, recall 0.8564, auc 0.8562
epoch 14201, loss 0.3088, train acc 85.60%, f1 0.8560, precision 0.8559, recall 0.8561, auc 0.8560
epoch 14301, loss 0.3005, train acc 85.60%, f1 0.8560, precision 0.8559, recall 0.8561, auc 0.8560
epoch 14401, loss 0.2488, train acc 85.54%, f1 0.8555, precision 0.8554, recall 0.8555, auc 0.8554
epoch 14501, loss 0.2038, train acc 85.56%, f1 0.8556, precision 0.8554, recall 0.8557, auc 0.8556
epoch 14601, loss 0.3034, train acc 85.62%, f1 0.8562, precision 0.8562, recall 0.8562, auc 0.8562
epoch 14701, loss 0.3656, train acc 85.63%, f1 0.8563, precision 0.8563, recall 0.8562, auc 0.8563
epoch 14801, loss 0.3788, train acc 85.63%, f1 0.8563, precision 0.8564, recall 0.8562, auc 0.8563
epoch 14901, loss 0.4006, train acc 85.62%, f1 0.8563, precision 0.8561, recall 0.8564, auc 0.8562
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_2
./test_pima/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.65

the Fscore is 0.6067415730337078

the precision is 0.43548387096774194

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_2
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5984, train acc 78.39%, f1 0.7837, precision 0.7843, recall 0.7831, auc 0.7839
epoch 201, loss 0.5106, train acc 80.95%, f1 0.8095, precision 0.8095, recall 0.8095, auc 0.8095
epoch 301, loss 0.5035, train acc 82.12%, f1 0.8212, precision 0.8212, recall 0.8212, auc 0.8212
epoch 401, loss 0.3309, train acc 83.05%, f1 0.8305, precision 0.8305, recall 0.8305, auc 0.8305
epoch 501, loss 0.4175, train acc 83.59%, f1 0.8360, precision 0.8359, recall 0.8360, auc 0.8359
epoch 601, loss 0.2342, train acc 83.77%, f1 0.8377, precision 0.8377, recall 0.8378, auc 0.8377
epoch 701, loss 0.4667, train acc 83.96%, f1 0.8396, precision 0.8396, recall 0.8396, auc 0.8396
epoch 801, loss 0.3654, train acc 84.00%, f1 0.8400, precision 0.8400, recall 0.8400, auc 0.8400
epoch 901, loss 0.3793, train acc 84.06%, f1 0.8406, precision 0.8406, recall 0.8406, auc 0.8406
epoch 1001, loss 0.3487, train acc 84.06%, f1 0.8406, precision 0.8406, recall 0.8406, auc 0.8406
epoch 1101, loss 0.3405, train acc 84.12%, f1 0.8412, precision 0.8412, recall 0.8412, auc 0.8412
epoch 1201, loss 0.3662, train acc 84.10%, f1 0.8410, precision 0.8410, recall 0.8410, auc 0.8410
epoch 1301, loss 0.3776, train acc 84.14%, f1 0.8414, precision 0.8414, recall 0.8414, auc 0.8414
epoch 1401, loss 0.3865, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 1501, loss 0.3422, train acc 84.19%, f1 0.8419, precision 0.8418, recall 0.8419, auc 0.8419
epoch 1601, loss 0.4155, train acc 84.12%, f1 0.8412, precision 0.8412, recall 0.8413, auc 0.8412
epoch 1701, loss 0.3696, train acc 84.14%, f1 0.8414, precision 0.8414, recall 0.8414, auc 0.8414
epoch 1801, loss 0.3374, train acc 84.18%, f1 0.8418, precision 0.8418, recall 0.8418, auc 0.8418
epoch 1901, loss 0.3882, train acc 84.14%, f1 0.8414, precision 0.8414, recall 0.8414, auc 0.8414
epoch 2001, loss 0.3301, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 2101, loss 0.3392, train acc 84.17%, f1 0.8417, precision 0.8418, recall 0.8417, auc 0.8417
epoch 2201, loss 0.3444, train acc 84.19%, f1 0.8419, precision 0.8418, recall 0.8420, auc 0.8419
epoch 2301, loss 0.2819, train acc 84.20%, f1 0.8420, precision 0.8419, recall 0.8421, auc 0.8420
epoch 2401, loss 0.4469, train acc 84.17%, f1 0.8416, precision 0.8417, recall 0.8416, auc 0.8417
epoch 2501, loss 0.2931, train acc 84.10%, f1 0.8410, precision 0.8410, recall 0.8411, auc 0.8410
epoch 2601, loss 0.2478, train acc 84.10%, f1 0.8410, precision 0.8410, recall 0.8410, auc 0.8410
epoch 2701, loss 0.4109, train acc 84.11%, f1 0.8412, precision 0.8411, recall 0.8412, auc 0.8411
epoch 2801, loss 0.3787, train acc 84.09%, f1 0.8409, precision 0.8409, recall 0.8409, auc 0.8409
epoch 2901, loss 0.3328, train acc 84.11%, f1 0.8411, precision 0.8412, recall 0.8410, auc 0.8411
epoch 3001, loss 0.2444, train acc 84.12%, f1 0.8412, precision 0.8412, recall 0.8412, auc 0.8412
epoch 3101, loss 0.4786, train acc 84.11%, f1 0.8411, precision 0.8411, recall 0.8411, auc 0.8411
epoch 3201, loss 0.4195, train acc 84.18%, f1 0.8418, precision 0.8418, recall 0.8418, auc 0.8418
epoch 3301, loss 0.3430, train acc 84.18%, f1 0.8418, precision 0.8417, recall 0.8419, auc 0.8418
epoch 3401, loss 0.3658, train acc 84.15%, f1 0.8415, precision 0.8415, recall 0.8415, auc 0.8415
epoch 3501, loss 0.3989, train acc 84.15%, f1 0.8415, precision 0.8416, recall 0.8415, auc 0.8415
epoch 3601, loss 0.4328, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8417, auc 0.8416
epoch 3701, loss 0.4799, train acc 84.17%, f1 0.8417, precision 0.8416, recall 0.8418, auc 0.8417
epoch 3801, loss 0.4870, train acc 84.20%, f1 0.8420, precision 0.8420, recall 0.8419, auc 0.8420
epoch 3901, loss 0.3583, train acc 84.21%, f1 0.8421, precision 0.8420, recall 0.8421, auc 0.8421
epoch 4001, loss 0.3108, train acc 84.18%, f1 0.8418, precision 0.8417, recall 0.8418, auc 0.8418
epoch 4101, loss 0.3077, train acc 84.21%, f1 0.8421, precision 0.8422, recall 0.8420, auc 0.8421
epoch 4201, loss 0.3906, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8421, auc 0.8422
epoch 4301, loss 0.3777, train acc 84.25%, f1 0.8425, precision 0.8426, recall 0.8425, auc 0.8425
epoch 4401, loss 0.2581, train acc 84.24%, f1 0.8424, precision 0.8425, recall 0.8422, auc 0.8424
epoch 4501, loss 0.3771, train acc 84.24%, f1 0.8423, precision 0.8424, recall 0.8423, auc 0.8424
epoch 4601, loss 0.2983, train acc 84.25%, f1 0.8425, precision 0.8426, recall 0.8425, auc 0.8425
epoch 4701, loss 0.3653, train acc 84.30%, f1 0.8430, precision 0.8430, recall 0.8430, auc 0.8430
epoch 4801, loss 0.3950, train acc 84.26%, f1 0.8426, precision 0.8427, recall 0.8425, auc 0.8426
epoch 4901, loss 0.5549, train acc 84.31%, f1 0.8431, precision 0.8432, recall 0.8430, auc 0.8431
epoch 5001, loss 0.3186, train acc 84.34%, f1 0.8434, precision 0.8436, recall 0.8432, auc 0.8434
epoch 5101, loss 0.3792, train acc 84.36%, f1 0.8436, precision 0.8436, recall 0.8436, auc 0.8436
epoch 5201, loss 0.3890, train acc 84.36%, f1 0.8436, precision 0.8437, recall 0.8436, auc 0.8436
epoch 5301, loss 0.4059, train acc 84.40%, f1 0.8440, precision 0.8441, recall 0.8440, auc 0.8440
epoch 5401, loss 0.2590, train acc 84.42%, f1 0.8441, precision 0.8443, recall 0.8440, auc 0.8442
epoch 5501, loss 0.4182, train acc 84.42%, f1 0.8442, precision 0.8443, recall 0.8441, auc 0.8442
epoch 5601, loss 0.3352, train acc 84.40%, f1 0.8440, precision 0.8441, recall 0.8439, auc 0.8440
epoch 5701, loss 0.3811, train acc 84.43%, f1 0.8442, precision 0.8444, recall 0.8441, auc 0.8443
epoch 5801, loss 0.4063, train acc 84.47%, f1 0.8446, precision 0.8448, recall 0.8445, auc 0.8447
epoch 5901, loss 0.3605, train acc 84.53%, f1 0.8453, precision 0.8453, recall 0.8454, auc 0.8453
epoch 6001, loss 0.3361, train acc 84.52%, f1 0.8453, precision 0.8452, recall 0.8453, auc 0.8452
epoch 6101, loss 0.3536, train acc 84.52%, f1 0.8452, precision 0.8452, recall 0.8451, auc 0.8452
epoch 6201, loss 0.3468, train acc 84.54%, f1 0.8453, precision 0.8454, recall 0.8453, auc 0.8454
epoch 6301, loss 0.2704, train acc 84.54%, f1 0.8455, precision 0.8454, recall 0.8455, auc 0.8454
epoch 6401, loss 0.3739, train acc 84.60%, f1 0.8460, precision 0.8458, recall 0.8461, auc 0.8460
epoch 6501, loss 0.3465, train acc 84.57%, f1 0.8457, precision 0.8457, recall 0.8457, auc 0.8457
epoch 6601, loss 0.3274, train acc 84.59%, f1 0.8459, precision 0.8460, recall 0.8458, auc 0.8459
epoch 6701, loss 0.4111, train acc 84.58%, f1 0.8458, precision 0.8459, recall 0.8458, auc 0.8458
epoch 6801, loss 0.3016, train acc 84.60%, f1 0.8460, precision 0.8462, recall 0.8459, auc 0.8460
epoch 6901, loss 0.3259, train acc 84.59%, f1 0.8459, precision 0.8458, recall 0.8461, auc 0.8459
epoch 7001, loss 0.3710, train acc 84.63%, f1 0.8463, precision 0.8465, recall 0.8461, auc 0.8463
epoch 7101, loss 0.3467, train acc 84.65%, f1 0.8465, precision 0.8467, recall 0.8463, auc 0.8465
epoch 7201, loss 0.3129, train acc 84.66%, f1 0.8466, precision 0.8465, recall 0.8467, auc 0.8466
epoch 7301, loss 0.3715, train acc 84.68%, f1 0.8469, precision 0.8468, recall 0.8470, auc 0.8468
epoch 7401, loss 0.4516, train acc 84.66%, f1 0.8466, precision 0.8465, recall 0.8468, auc 0.8466
epoch 7501, loss 0.3694, train acc 84.71%, f1 0.8471, precision 0.8471, recall 0.8470, auc 0.8471
epoch 7601, loss 0.3139, train acc 84.70%, f1 0.8470, precision 0.8470, recall 0.8470, auc 0.8470
epoch 7701, loss 0.3396, train acc 84.75%, f1 0.8475, precision 0.8475, recall 0.8474, auc 0.8475
epoch 7801, loss 0.3711, train acc 84.74%, f1 0.8474, precision 0.8475, recall 0.8473, auc 0.8474
epoch 7901, loss 0.3736, train acc 84.74%, f1 0.8475, precision 0.8474, recall 0.8475, auc 0.8474
epoch 8001, loss 0.3681, train acc 84.73%, f1 0.8473, precision 0.8474, recall 0.8473, auc 0.8473
epoch 8101, loss 0.3274, train acc 84.79%, f1 0.8479, precision 0.8479, recall 0.8479, auc 0.8479
epoch 8201, loss 0.3918, train acc 84.84%, f1 0.8484, precision 0.8485, recall 0.8483, auc 0.8484/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2706, train acc 84.86%, f1 0.8486, precision 0.8484, recall 0.8489, auc 0.8486
epoch 8401, loss 0.3120, train acc 84.84%, f1 0.8485, precision 0.8482, recall 0.8487, auc 0.8484
epoch 8501, loss 0.3099, train acc 84.83%, f1 0.8483, precision 0.8482, recall 0.8484, auc 0.8483
epoch 8601, loss 0.3359, train acc 84.83%, f1 0.8483, precision 0.8483, recall 0.8482, auc 0.8483
epoch 8701, loss 0.2709, train acc 84.84%, f1 0.8484, precision 0.8481, recall 0.8487, auc 0.8484
epoch 8801, loss 0.2601, train acc 84.90%, f1 0.8490, precision 0.8490, recall 0.8491, auc 0.8490
epoch 8901, loss 0.3581, train acc 84.89%, f1 0.8489, precision 0.8487, recall 0.8491, auc 0.8489
epoch 9001, loss 0.3238, train acc 84.94%, f1 0.8494, precision 0.8492, recall 0.8496, auc 0.8494
epoch 9101, loss 0.3947, train acc 84.90%, f1 0.8490, precision 0.8491, recall 0.8489, auc 0.8490
epoch 9201, loss 0.3879, train acc 84.95%, f1 0.8494, precision 0.8496, recall 0.8492, auc 0.8495
epoch 9301, loss 0.2415, train acc 84.98%, f1 0.8499, precision 0.8497, recall 0.8501, auc 0.8498
epoch 9401, loss 0.3558, train acc 84.95%, f1 0.8495, precision 0.8495, recall 0.8495, auc 0.8495
epoch 9501, loss 0.3644, train acc 84.96%, f1 0.8496, precision 0.8495, recall 0.8496, auc 0.8496
epoch 9601, loss 0.3539, train acc 84.99%, f1 0.8499, precision 0.8499, recall 0.8499, auc 0.8499
epoch 9701, loss 0.3069, train acc 84.99%, f1 0.8499, precision 0.8498, recall 0.8499, auc 0.8499
epoch 9801, loss 0.3173, train acc 84.99%, f1 0.8499, precision 0.8499, recall 0.8500, auc 0.8499
epoch 9901, loss 0.3445, train acc 85.00%, f1 0.8500, precision 0.8500, recall 0.8500, auc 0.8500
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_2
./test_pima/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.615

the Fscore is 0.5837837837837838

the precision is 0.4122137404580153

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_2
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6323, train acc 75.64%, f1 0.7847, precision 0.7030, recall 0.8878, auc 0.7564
epoch 201, loss 0.5096, train acc 80.53%, f1 0.8112, precision 0.7873, recall 0.8365, auc 0.8053
epoch 301, loss 0.4053, train acc 82.10%, f1 0.8216, precision 0.8185, recall 0.8248, auc 0.8210
epoch 401, loss 0.4350, train acc 83.00%, f1 0.8290, precision 0.8337, recall 0.8243, auc 0.8300
epoch 501, loss 0.4594, train acc 83.46%, f1 0.8332, precision 0.8404, recall 0.8262, auc 0.8346
epoch 601, loss 0.3386, train acc 83.73%, f1 0.8354, precision 0.8450, recall 0.8260, auc 0.8373
epoch 701, loss 0.3438, train acc 83.91%, f1 0.8375, precision 0.8463, recall 0.8289, auc 0.8391
epoch 801, loss 0.3413, train acc 84.04%, f1 0.8385, precision 0.8483, recall 0.8290, auc 0.8404
epoch 901, loss 0.3321, train acc 84.02%, f1 0.8388, precision 0.8465, recall 0.8311, auc 0.8402
epoch 1001, loss 0.3589, train acc 84.09%, f1 0.8395, precision 0.8470, recall 0.8321, auc 0.8409
epoch 1101, loss 0.2972, train acc 84.05%, f1 0.8391, precision 0.8463, recall 0.8320, auc 0.8405
epoch 1201, loss 0.3246, train acc 84.10%, f1 0.8401, precision 0.8451, recall 0.8352, auc 0.8410
epoch 1301, loss 0.4727, train acc 84.14%, f1 0.8402, precision 0.8468, recall 0.8336, auc 0.8414
epoch 1401, loss 0.3016, train acc 84.17%, f1 0.8407, precision 0.8458, recall 0.8357, auc 0.8417
epoch 1501, loss 0.4405, train acc 84.15%, f1 0.8405, precision 0.8458, recall 0.8353, auc 0.8415
epoch 1601, loss 0.3340, train acc 84.19%, f1 0.8412, precision 0.8447, recall 0.8377, auc 0.8419
epoch 1701, loss 0.4405, train acc 84.17%, f1 0.8409, precision 0.8451, recall 0.8368, auc 0.8417
epoch 1801, loss 0.3864, train acc 84.16%, f1 0.8409, precision 0.8447, recall 0.8371, auc 0.8416
epoch 1901, loss 0.3513, train acc 84.13%, f1 0.8408, precision 0.8435, recall 0.8381, auc 0.8413
epoch 2001, loss 0.3306, train acc 84.12%, f1 0.8406, precision 0.8435, recall 0.8377, auc 0.8412
epoch 2101, loss 0.4000, train acc 84.18%, f1 0.8416, precision 0.8427, recall 0.8405, auc 0.8418
epoch 2201, loss 0.3532, train acc 84.14%, f1 0.8411, precision 0.8427, recall 0.8395, auc 0.8414
epoch 2301, loss 0.3292, train acc 84.14%, f1 0.8409, precision 0.8433, recall 0.8386, auc 0.8414
epoch 2401, loss 0.4862, train acc 84.13%, f1 0.8410, precision 0.8424, recall 0.8397, auc 0.8413
epoch 2501, loss 0.3080, train acc 84.15%, f1 0.8413, precision 0.8423, recall 0.8404, auc 0.8415
epoch 2601, loss 0.3656, train acc 84.17%, f1 0.8414, precision 0.8426, recall 0.8403, auc 0.8417
epoch 2701, loss 0.4334, train acc 84.10%, f1 0.8408, precision 0.8415, recall 0.8401, auc 0.8410
epoch 2801, loss 0.3152, train acc 84.14%, f1 0.8413, precision 0.8420, recall 0.8406, auc 0.8414
epoch 2901, loss 0.4806, train acc 84.15%, f1 0.8413, precision 0.8425, recall 0.8401, auc 0.8415
epoch 3001, loss 0.3949, train acc 84.14%, f1 0.8413, precision 0.8416, recall 0.8411, auc 0.8414
epoch 3101, loss 0.3835, train acc 84.15%, f1 0.8413, precision 0.8422, recall 0.8405, auc 0.8415
epoch 3201, loss 0.4957, train acc 84.18%, f1 0.8419, precision 0.8414, recall 0.8425, auc 0.8418
epoch 3301, loss 0.3182, train acc 84.17%, f1 0.8416, precision 0.8419, recall 0.8413, auc 0.8417
epoch 3401, loss 0.4219, train acc 84.14%, f1 0.8414, precision 0.8415, recall 0.8413, auc 0.8414
epoch 3501, loss 0.3722, train acc 84.12%, f1 0.8411, precision 0.8420, recall 0.8402, auc 0.8412
epoch 3601, loss 0.2928, train acc 84.18%, f1 0.8416, precision 0.8423, recall 0.8409, auc 0.8418
epoch 3701, loss 0.3910, train acc 84.15%, f1 0.8414, precision 0.8419, recall 0.8408, auc 0.8415
epoch 3801, loss 0.3414, train acc 84.18%, f1 0.8416, precision 0.8425, recall 0.8407, auc 0.8418
epoch 3901, loss 0.4742, train acc 84.23%, f1 0.8422, precision 0.8424, recall 0.8420, auc 0.8423
epoch 4001, loss 0.3804, train acc 84.22%, f1 0.8422, precision 0.8421, recall 0.8423, auc 0.8422
epoch 4101, loss 0.3472, train acc 84.16%, f1 0.8414, precision 0.8422, recall 0.8406, auc 0.8416
epoch 4201, loss 0.2960, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8422, auc 0.8422
epoch 4301, loss 0.3292, train acc 84.23%, f1 0.8423, precision 0.8425, recall 0.8421, auc 0.8423
epoch 4401, loss 0.4067, train acc 84.26%, f1 0.8424, precision 0.8435, recall 0.8412, auc 0.8426
epoch 4501, loss 0.3541, train acc 84.21%, f1 0.8421, precision 0.8418, recall 0.8425, auc 0.8421
epoch 4601, loss 0.4056, train acc 84.23%, f1 0.8422, precision 0.8428, recall 0.8416, auc 0.8423
epoch 4701, loss 0.2676, train acc 84.23%, f1 0.8422, precision 0.8429, recall 0.8415, auc 0.8423
epoch 4801, loss 0.2508, train acc 84.27%, f1 0.8426, precision 0.8432, recall 0.8420, auc 0.8427
epoch 4901, loss 0.2831, train acc 84.29%, f1 0.8428, precision 0.8435, recall 0.8420, auc 0.8429
epoch 5001, loss 0.3995, train acc 84.33%, f1 0.8432, precision 0.8434, recall 0.8430, auc 0.8433
epoch 5101, loss 0.3864, train acc 84.39%, f1 0.8438, precision 0.8444, recall 0.8431, auc 0.8439
epoch 5201, loss 0.4010, train acc 84.36%, f1 0.8436, precision 0.8435, recall 0.8437, auc 0.8436
epoch 5301, loss 0.3138, train acc 84.38%, f1 0.8437, precision 0.8444, recall 0.8430, auc 0.8438
epoch 5401, loss 0.2115, train acc 84.38%, f1 0.8438, precision 0.8440, recall 0.8436, auc 0.8438
epoch 5501, loss 0.3781, train acc 84.38%, f1 0.8436, precision 0.8447, recall 0.8425, auc 0.8438
epoch 5601, loss 0.4467, train acc 84.42%, f1 0.8441, precision 0.8445, recall 0.8437, auc 0.8442
epoch 5701, loss 0.4481, train acc 84.41%, f1 0.8441, precision 0.8440, recall 0.8443, auc 0.8441
epoch 5801, loss 0.4091, train acc 84.45%, f1 0.8444, precision 0.8447, recall 0.8441, auc 0.8445
epoch 5901, loss 0.3511, train acc 84.46%, f1 0.8445, precision 0.8449, recall 0.8442, auc 0.8446
epoch 6001, loss 0.2123, train acc 84.41%, f1 0.8441, precision 0.8442, recall 0.8440, auc 0.8441
epoch 6101, loss 0.3417, train acc 84.48%, f1 0.8448, precision 0.8449, recall 0.8447, auc 0.8448
epoch 6201, loss 0.3971, train acc 84.53%, f1 0.8453, precision 0.8451, recall 0.8455, auc 0.8453
epoch 6301, loss 0.3816, train acc 84.48%, f1 0.8448, precision 0.8447, recall 0.8449, auc 0.8448
epoch 6401, loss 0.4496, train acc 84.53%, f1 0.8453, precision 0.8453, recall 0.8452, auc 0.8453
epoch 6501, loss 0.2669, train acc 84.57%, f1 0.8457, precision 0.8455, recall 0.8459, auc 0.8457
epoch 6601, loss 0.3870, train acc 84.55%, f1 0.8455, precision 0.8455, recall 0.8455, auc 0.8455
epoch 6701, loss 0.3213, train acc 84.57%, f1 0.8457, precision 0.8458, recall 0.8457, auc 0.8457
epoch 6801, loss 0.2764, train acc 84.60%, f1 0.8460, precision 0.8462, recall 0.8458, auc 0.8460
epoch 6901, loss 0.3354, train acc 84.59%, f1 0.8460, precision 0.8457, recall 0.8463, auc 0.8459
epoch 7001, loss 0.3129, train acc 84.61%, f1 0.8460, precision 0.8467, recall 0.8452, auc 0.8461
epoch 7101, loss 0.4138, train acc 84.67%, f1 0.8468, precision 0.8466, recall 0.8469, auc 0.8467
epoch 7201, loss 0.3570, train acc 84.65%, f1 0.8464, precision 0.8468, recall 0.8460, auc 0.8465
epoch 7301, loss 0.3192, train acc 84.68%, f1 0.8468, precision 0.8470, recall 0.8465, auc 0.8468
epoch 7401, loss 0.3666, train acc 84.71%, f1 0.8471, precision 0.8473, recall 0.8468, auc 0.8471
epoch 7501, loss 0.3369, train acc 84.71%, f1 0.8471, precision 0.8472, recall 0.8470, auc 0.8471
epoch 7601, loss 0.3510, train acc 84.76%, f1 0.8476, precision 0.8474, recall 0.8477, auc 0.8476
epoch 7701, loss 0.5502, train acc 84.80%, f1 0.8480, precision 0.8483, recall 0.8477, auc 0.8480
epoch 7801, loss 0.3986, train acc 84.76%, f1 0.8476, precision 0.8476, recall 0.8477, auc 0.8476
epoch 7901, loss 0.2749, train acc 84.77%, f1 0.8478, precision 0.8476, recall 0.8479, auc 0.8477
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_2
./test_pima/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.625

the Fscore is 0.5901639344262295

the precision is 0.4186046511627907

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_2
----------------------



epoch 1, loss 0.6932, train acc 53.31%, f1 0.6067, precision 0.5241, recall 0.7202, auc 0.5331
epoch 101, loss 0.5927, train acc 77.95%, f1 0.7659, precision 0.8166, recall 0.7211, auc 0.7795
epoch 201, loss 0.4733, train acc 80.72%, f1 0.8058, precision 0.8115, recall 0.8002, auc 0.8072
epoch 301, loss 0.5633, train acc 82.02%, f1 0.8205, precision 0.8189, recall 0.8222, auc 0.8202
epoch 401, loss 0.4993, train acc 82.95%, f1 0.8301, precision 0.8273, recall 0.8330, auc 0.8295
epoch 501, loss 0.3551, train acc 83.56%, f1 0.8362, precision 0.8329, recall 0.8396, auc 0.8356
epoch 601, loss 0.4259, train acc 83.78%, f1 0.8385, precision 0.8345, recall 0.8426, auc 0.8378
epoch 701, loss 0.3205, train acc 83.88%, f1 0.8395, precision 0.8359, recall 0.8432, auc 0.8388
epoch 801, loss 0.3099, train acc 84.03%, f1 0.8409, precision 0.8373, recall 0.8446, auc 0.8403
epoch 901, loss 0.4815, train acc 84.02%, f1 0.8409, precision 0.8373, recall 0.8444, auc 0.8402
epoch 1001, loss 0.3592, train acc 84.12%, f1 0.8418, precision 0.8385, recall 0.8451, auc 0.8412
epoch 1101, loss 0.3475, train acc 84.13%, f1 0.8418, precision 0.8393, recall 0.8442, auc 0.8413
epoch 1201, loss 0.3631, train acc 84.13%, f1 0.8417, precision 0.8399, recall 0.8434, auc 0.8413
epoch 1301, loss 0.4324, train acc 84.16%, f1 0.8419, precision 0.8402, recall 0.8436, auc 0.8416
epoch 1401, loss 0.3633, train acc 84.18%, f1 0.8421, precision 0.8405, recall 0.8436, auc 0.8418
epoch 1501, loss 0.5384, train acc 84.16%, f1 0.8418, precision 0.8408, recall 0.8427, auc 0.8416
epoch 1601, loss 0.3957, train acc 84.17%, f1 0.8419, precision 0.8409, recall 0.8429, auc 0.8417
epoch 1701, loss 0.4090, train acc 84.15%, f1 0.8417, precision 0.8409, recall 0.8424, auc 0.8415
epoch 1801, loss 0.3196, train acc 84.13%, f1 0.8414, precision 0.8406, recall 0.8423, auc 0.8413
epoch 1901, loss 0.3562, train acc 84.13%, f1 0.8415, precision 0.8406, recall 0.8423, auc 0.8413
epoch 2001, loss 0.3578, train acc 84.17%, f1 0.8417, precision 0.8416, recall 0.8418, auc 0.8417
epoch 2101, loss 0.4398, train acc 84.14%, f1 0.8414, precision 0.8417, recall 0.8410, auc 0.8414
epoch 2201, loss 0.3729, train acc 84.20%, f1 0.8422, precision 0.8411, recall 0.8434, auc 0.8420
epoch 2301, loss 0.4148, train acc 84.17%, f1 0.8417, precision 0.8414, recall 0.8421, auc 0.8417
epoch 2401, loss 0.4104, train acc 84.15%, f1 0.8418, precision 0.8406, recall 0.8429, auc 0.8415
epoch 2501, loss 0.3795, train acc 84.17%, f1 0.8417, precision 0.8420, recall 0.8414, auc 0.8417
epoch 2601, loss 0.4221, train acc 84.20%, f1 0.8420, precision 0.8418, recall 0.8423, auc 0.8420
epoch 2701, loss 0.3456, train acc 84.22%, f1 0.8421, precision 0.8423, recall 0.8419, auc 0.8422
epoch 2801, loss 0.3680, train acc 84.22%, f1 0.8423, precision 0.8419, recall 0.8426, auc 0.8422
epoch 2901, loss 0.3330, train acc 84.12%, f1 0.8413, precision 0.8411, recall 0.8414, auc 0.8412
epoch 3001, loss 0.3990, train acc 84.15%, f1 0.8414, precision 0.8421, recall 0.8407, auc 0.8415
epoch 3101, loss 0.3514, train acc 84.14%, f1 0.8414, precision 0.8413, recall 0.8415, auc 0.8414
epoch 3201, loss 0.2539, train acc 84.14%, f1 0.8413, precision 0.8418, recall 0.8407, auc 0.8414
epoch 3301, loss 0.3920, train acc 84.14%, f1 0.8415, precision 0.8411, recall 0.8420, auc 0.8414
epoch 3401, loss 0.3418, train acc 84.19%, f1 0.8419, precision 0.8419, recall 0.8419, auc 0.8419
epoch 3501, loss 0.3423, train acc 84.17%, f1 0.8418, precision 0.8416, recall 0.8420, auc 0.8417
epoch 3601, loss 0.4715, train acc 84.15%, f1 0.8415, precision 0.8415, recall 0.8415, auc 0.8415
epoch 3701, loss 0.4241, train acc 84.21%, f1 0.8420, precision 0.8425, recall 0.8414, auc 0.8421
epoch 3801, loss 0.4760, train acc 84.16%, f1 0.8416, precision 0.8415, recall 0.8418, auc 0.8416
epoch 3901, loss 0.4153, train acc 84.18%, f1 0.8419, precision 0.8417, recall 0.8420, auc 0.8418
epoch 4001, loss 0.2532, train acc 84.17%, f1 0.8418, precision 0.8415, recall 0.8420, auc 0.8417
epoch 4101, loss 0.4836, train acc 84.18%, f1 0.8418, precision 0.8416, recall 0.8419, auc 0.8418
epoch 4201, loss 0.4243, train acc 84.15%, f1 0.8415, precision 0.8418, recall 0.8411, auc 0.8415
epoch 4301, loss 0.3726, train acc 84.16%, f1 0.8417, precision 0.8414, recall 0.8419, auc 0.8416
epoch 4401, loss 0.3442, train acc 84.23%, f1 0.8422, precision 0.8426, recall 0.8418, auc 0.8423
epoch 4501, loss 0.3594, train acc 84.22%, f1 0.8423, precision 0.8418, recall 0.8427, auc 0.8422
epoch 4601, loss 0.3526, train acc 84.19%, f1 0.8420, precision 0.8416, recall 0.8423, auc 0.8419
epoch 4701, loss 0.4166, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8423, auc 0.8423
epoch 4801, loss 0.2909, train acc 84.25%, f1 0.8426, precision 0.8423, recall 0.8428, auc 0.8425
epoch 4901, loss 0.2564, train acc 84.24%, f1 0.8424, precision 0.8424, recall 0.8425, auc 0.8424
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_2
./test_pima/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.5800000000000001

the Fscore is 0.5625

the precision is 0.391304347826087

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_2
----------------------



epoch 1, loss 0.6936, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5759, train acc 78.34%, f1 0.7805, precision 0.7910, recall 0.7703, auc 0.7834
epoch 201, loss 0.4526, train acc 80.63%, f1 0.8060, precision 0.8070, recall 0.8051, auc 0.8063
epoch 301, loss 0.4429, train acc 82.21%, f1 0.8221, precision 0.8221, recall 0.8221, auc 0.8221
epoch 401, loss 0.4574, train acc 83.06%, f1 0.8307, precision 0.8304, recall 0.8310, auc 0.8306
epoch 501, loss 0.3676, train acc 83.46%, f1 0.8348, precision 0.8339, recall 0.8358, auc 0.8346
epoch 601, loss 0.3363, train acc 83.66%, f1 0.8367, precision 0.8361, recall 0.8373, auc 0.8366
epoch 701, loss 0.3523, train acc 83.93%, f1 0.8394, precision 0.8388, recall 0.8399, auc 0.8393
epoch 801, loss 0.4118, train acc 84.04%, f1 0.8405, precision 0.8402, recall 0.8408, auc 0.8404
epoch 901, loss 0.3588, train acc 84.02%, f1 0.8403, precision 0.8398, recall 0.8407, auc 0.8402
epoch 1001, loss 0.3529, train acc 84.11%, f1 0.8412, precision 0.8406, recall 0.8419, auc 0.8411
epoch 1101, loss 0.4349, train acc 84.12%, f1 0.8413, precision 0.8410, recall 0.8415, auc 0.8412
epoch 1201, loss 0.4480, train acc 84.22%, f1 0.8422, precision 0.8421, recall 0.8423, auc 0.8422
epoch 1301, loss 0.4405, train acc 84.13%, f1 0.8414, precision 0.8412, recall 0.8416, auc 0.8413
epoch 1401, loss 0.3398, train acc 84.13%, f1 0.8413, precision 0.8413, recall 0.8413, auc 0.8413
epoch 1501, loss 0.3433, train acc 84.17%, f1 0.8417, precision 0.8417, recall 0.8417, auc 0.8417
epoch 1601, loss 0.3593, train acc 84.18%, f1 0.8418, precision 0.8417, recall 0.8418, auc 0.8418
epoch 1701, loss 0.2838, train acc 84.13%, f1 0.8413, precision 0.8412, recall 0.8414, auc 0.8413
epoch 1801, loss 0.3993, train acc 84.07%, f1 0.8407, precision 0.8407, recall 0.8407, auc 0.8407
epoch 1901, loss 0.3119, train acc 84.18%, f1 0.8418, precision 0.8419, recall 0.8417, auc 0.8418
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_2
./test_pima/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.605

the Fscore is 0.5775401069518716

the precision is 0.40601503759398494

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_2
----------------------



epoch 1, loss 0.6932, train acc 49.99%, f1 0.6666, precision 0.4999, recall 1.0000, auc 0.5000
epoch 101, loss 0.6132, train acc 77.62%, f1 0.7688, precision 0.7950, recall 0.7442, auc 0.7762
epoch 201, loss 0.4757, train acc 80.47%, f1 0.8042, precision 0.8059, recall 0.8026, auc 0.8047
epoch 301, loss 0.3330, train acc 82.25%, f1 0.8227, precision 0.8216, recall 0.8238, auc 0.8225
epoch 401, loss 0.4517, train acc 82.96%, f1 0.8297, precision 0.8291, recall 0.8302, auc 0.8296
epoch 501, loss 0.4331, train acc 83.51%, f1 0.8354, precision 0.8340, recall 0.8367, auc 0.8351
epoch 601, loss 0.3569, train acc 83.69%, f1 0.8368, precision 0.8371, recall 0.8366, auc 0.8369
epoch 701, loss 0.4241, train acc 83.84%, f1 0.8388, precision 0.8364, recall 0.8413, auc 0.8384
epoch 801, loss 0.4340, train acc 83.96%, f1 0.8401, precision 0.8376, recall 0.8426, auc 0.8396
epoch 901, loss 0.3079, train acc 84.11%, f1 0.8412, precision 0.8406, recall 0.8418, auc 0.8411
epoch 1001, loss 0.3262, train acc 84.08%, f1 0.8411, precision 0.8396, recall 0.8426, auc 0.8408
epoch 1101, loss 0.2810, train acc 84.09%, f1 0.8409, precision 0.8410, recall 0.8407, auc 0.8409
epoch 1201, loss 0.2159, train acc 84.16%, f1 0.8419, precision 0.8399, recall 0.8439, auc 0.8416
epoch 1301, loss 0.2936, train acc 84.18%, f1 0.8421, precision 0.8402, recall 0.8440, auc 0.8418
epoch 1401, loss 0.3522, train acc 84.19%, f1 0.8422, precision 0.8404, recall 0.8440, auc 0.8419
epoch 1501, loss 0.3932, train acc 84.15%, f1 0.8419, precision 0.8395, recall 0.8443, auc 0.8415
epoch 1601, loss 0.4200, train acc 84.17%, f1 0.8420, precision 0.8402, recall 0.8438, auc 0.8417
epoch 1701, loss 0.3538, train acc 84.17%, f1 0.8417, precision 0.8417, recall 0.8416, auc 0.8417
epoch 1801, loss 0.3027, train acc 84.14%, f1 0.8418, precision 0.8397, recall 0.8438, auc 0.8414
epoch 1901, loss 0.3406, train acc 84.17%, f1 0.8418, precision 0.8411, recall 0.8426, auc 0.8417
epoch 2001, loss 0.2585, train acc 84.15%, f1 0.8416, precision 0.8411, recall 0.8421, auc 0.8415
epoch 2101, loss 0.3418, train acc 84.15%, f1 0.8415, precision 0.8414, recall 0.8415, auc 0.8415
epoch 2201, loss 0.4683, train acc 84.14%, f1 0.8413, precision 0.8415, recall 0.8411, auc 0.8414
epoch 2301, loss 0.3354, train acc 84.14%, f1 0.8414, precision 0.8416, recall 0.8411, auc 0.8414
epoch 2401, loss 0.4323, train acc 84.12%, f1 0.8413, precision 0.8408, recall 0.8418, auc 0.8412
epoch 2501, loss 0.4194, train acc 84.17%, f1 0.8420, precision 0.8403, recall 0.8437, auc 0.8417
epoch 2601, loss 0.2478, train acc 84.16%, f1 0.8416, precision 0.8416, recall 0.8416, auc 0.8416
epoch 2701, loss 0.4764, train acc 84.19%, f1 0.8419, precision 0.8418, recall 0.8420, auc 0.8419
epoch 2801, loss 0.4032, train acc 84.21%, f1 0.8420, precision 0.8426, recall 0.8414, auc 0.8421
epoch 2901, loss 0.5770, train acc 84.16%, f1 0.8416, precision 0.8415, recall 0.8417, auc 0.8416
epoch 3001, loss 0.2953, train acc 84.13%, f1 0.8414, precision 0.8410, recall 0.8417, auc 0.8413
epoch 3101, loss 0.2591, train acc 84.18%, f1 0.8417, precision 0.8423, recall 0.8411, auc 0.8418
epoch 3201, loss 0.4941, train acc 84.18%, f1 0.8417, precision 0.8421, recall 0.8413, auc 0.8418
epoch 3301, loss 0.3164, train acc 84.15%, f1 0.8419, precision 0.8399, recall 0.8439, auc 0.8415
epoch 3401, loss 0.4278, train acc 84.18%, f1 0.8418, precision 0.8416, recall 0.8420, auc 0.8418
epoch 3501, loss 0.3751, train acc 84.19%, f1 0.8419, precision 0.8415, recall 0.8424, auc 0.8419
epoch 3601, loss 0.3802, train acc 84.23%, f1 0.8421, precision 0.8434, recall 0.8407, auc 0.8423
epoch 3701, loss 0.2889, train acc 84.21%, f1 0.8423, precision 0.8411, recall 0.8436, auc 0.8421
epoch 3801, loss 0.4550, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8421, auc 0.8422
epoch 3901, loss 0.3674, train acc 84.19%, f1 0.8419, precision 0.8418, recall 0.8419, auc 0.8419
epoch 4001, loss 0.2579, train acc 84.22%, f1 0.8419, precision 0.8433, recall 0.8405, auc 0.8422
epoch 4101, loss 0.3631, train acc 84.20%, f1 0.8421, precision 0.8415, recall 0.8426, auc 0.8420
epoch 4201, loss 0.4466, train acc 84.22%, f1 0.8424, precision 0.8415, recall 0.8433, auc 0.8422
epoch 4301, loss 0.3063, train acc 84.28%, f1 0.8430, precision 0.8419, recall 0.8441, auc 0.8428
epoch 4401, loss 0.3388, train acc 84.35%, f1 0.8435, precision 0.8436, recall 0.8434, auc 0.8435
epoch 4501, loss 0.4391, train acc 84.32%, f1 0.8431, precision 0.8434, recall 0.8428, auc 0.8432
epoch 4601, loss 0.3910, train acc 84.29%, f1 0.8429, precision 0.8425, recall 0.8434, auc 0.8429
epoch 4701, loss 0.3405, train acc 84.32%, f1 0.8431, precision 0.8438, recall 0.8423, auc 0.8432
epoch 4801, loss 0.4565, train acc 84.35%, f1 0.8437, precision 0.8426, recall 0.8449, auc 0.8435
epoch 4901, loss 0.3810, train acc 84.35%, f1 0.8434, precision 0.8437, recall 0.8432, auc 0.8435
epoch 5001, loss 0.3154, train acc 84.40%, f1 0.8436, precision 0.8453, recall 0.8419, auc 0.8440
epoch 5101, loss 0.3348, train acc 84.40%, f1 0.8444, precision 0.8422, recall 0.8466, auc 0.8440
epoch 5201, loss 0.4600, train acc 84.43%, f1 0.8445, precision 0.8432, recall 0.8459, auc 0.8443
epoch 5301, loss 0.3924, train acc 84.44%, f1 0.8446, precision 0.8433, recall 0.8459, auc 0.8444
epoch 5401, loss 0.2364, train acc 84.46%, f1 0.8445, precision 0.8447, recall 0.8444, auc 0.8446
epoch 5501, loss 0.2976, train acc 84.44%, f1 0.8446, precision 0.8433, recall 0.8460, auc 0.8444
epoch 5601, loss 0.2834, train acc 84.50%, f1 0.8450, precision 0.8451, recall 0.8449, auc 0.8450
epoch 5701, loss 0.4370, train acc 84.50%, f1 0.8447, precision 0.8462, recall 0.8431, auc 0.8450
epoch 5801, loss 0.2906, train acc 84.51%, f1 0.8448, precision 0.8464, recall 0.8432, auc 0.8451
epoch 5901, loss 0.3812, train acc 84.51%, f1 0.8449, precision 0.8458, recall 0.8440, auc 0.8451
epoch 6001, loss 0.2219, train acc 84.60%, f1 0.8461, precision 0.8454, recall 0.8467, auc 0.8460
epoch 6101, loss 0.2600, train acc 84.59%, f1 0.8457, precision 0.8465, recall 0.8450, auc 0.8459
epoch 6201, loss 0.4291, train acc 84.59%, f1 0.8461, precision 0.8449, recall 0.8472, auc 0.8459
epoch 6301, loss 0.3316, train acc 84.66%, f1 0.8465, precision 0.8469, recall 0.8461, auc 0.8466
epoch 6401, loss 0.2467, train acc 84.60%, f1 0.8461, precision 0.8456, recall 0.8465, auc 0.8460
epoch 6501, loss 0.3884, train acc 84.69%, f1 0.8475, precision 0.8440, recall 0.8511, auc 0.8469
epoch 6601, loss 0.3538, train acc 84.69%, f1 0.8470, precision 0.8463, recall 0.8476, auc 0.8469
epoch 6701, loss 0.2973, train acc 84.70%, f1 0.8472, precision 0.8464, recall 0.8479, auc 0.8470
epoch 6801, loss 0.3296, train acc 84.72%, f1 0.8470, precision 0.8479, recall 0.8461, auc 0.8472
epoch 6901, loss 0.4015, train acc 84.75%, f1 0.8477, precision 0.8466, recall 0.8488, auc 0.8475
epoch 7001, loss 0.4150, train acc 84.75%, f1 0.8470, precision 0.8497, recall 0.8444, auc 0.8475
epoch 7101, loss 0.2794, train acc 84.78%, f1 0.8471, precision 0.8506, recall 0.8436, auc 0.8478
epoch 7201, loss 0.4346, train acc 84.82%, f1 0.8480, precision 0.8492, recall 0.8468, auc 0.8482
epoch 7301, loss 0.4731, train acc 84.84%, f1 0.8485, precision 0.8479, recall 0.8490, auc 0.8484
epoch 7401, loss 0.2878, train acc 84.84%, f1 0.8485, precision 0.8480, recall 0.8490, auc 0.8484
epoch 7501, loss 0.3860, train acc 84.90%, f1 0.8493, precision 0.8474, recall 0.8513, auc 0.8490
epoch 7601, loss 0.4036, train acc 84.91%, f1 0.8493, precision 0.8481, recall 0.8506, auc 0.8491
epoch 7701, loss 0.3763, train acc 84.90%, f1 0.8490, precision 0.8490, recall 0.8491, auc 0.8490
epoch 7801, loss 0.3004, train acc 84.91%, f1 0.8493, precision 0.8480, recall 0.8506, auc 0.8491
epoch 7901, loss 0.4369, train acc 84.96%, f1 0.8496, precision 0.8495, recall 0.8498, auc 0.8496
epoch 8001, loss 0.3480, train acc 84.96%, f1 0.8496, precision 0.8493, recall 0.8500, auc 0.8496
epoch 8101, loss 0.3371, train acc 85.00%, f1 0.8501, precision 0.8494, recall 0.8508, auc 0.8500
epoch 8201, loss 0.3298, train acc 85.01%, f1 0.8501, precision 0.8498, recall 0.8504, auc 0.8501
epoch 8301, loss 0.2610, train acc 84.99%, f1 0.8495, precision 0.8517, recall 0.8473, auc 0.8499
epoch 8401, loss 0.3512, train acc 85.01%, f1 0.8502, precision 0.8495, recall 0.8509, auc 0.8501
epoch 8501, loss 0.3600, train acc 85.03%, f1 0.8507, precision 0.8485, recall 0.8530, auc 0.8503
epoch 8601, loss 0.3048, train acc 85.03%, f1 0.8503, precision 0.8505, recall 0.8501, auc 0.8503
epoch 8701, loss 0.2916, train acc 85.00%, f1 0.8501, precision 0.8498, recall 0.8504, auc 0.8500
epoch 8801, loss 0.3637, train acc 85.10%, f1 0.8507, precision 0.8525, recall 0.8488, auc 0.8510
epoch 8901, loss 0.1961, train acc 85.10%, f1 0.8513, precision 0.8497, recall 0.8528, auc 0.8510
epoch 9001, loss 0.3929, train acc 85.12%, f1 0.8512, precision 0.8512, recall 0.8512, auc 0.8512
epoch 9101, loss 0.4024, train acc 85.08%, f1 0.8507, precision 0.8510, recall 0.8504, auc 0.8508
epoch 9201, loss 0.3213, train acc 85.14%, f1 0.8524, precision 0.8470, recall 0.8577, auc 0.8514
epoch 9301, loss 0.4673, train acc 85.18%, f1 0.8518, precision 0.8518, recall 0.8519, auc 0.8518
epoch 9401, loss 0.2840, train acc 85.15%, f1 0.8520, precision 0.8492, recall 0.8547, auc 0.8515
epoch 9501, loss 0.4127, train acc 85.15%, f1 0.8516, precision 0.8508, recall 0.8524, auc 0.8515
epoch 9601, loss 0.1863, train acc 85.20%, f1 0.8522, precision 0.8512, recall 0.8531, auc 0.8520
epoch 9701, loss 0.3388, train acc 85.18%, f1 0.8520, precision 0.8509, recall 0.8530, auc 0.8518
epoch 9801, loss 0.3513, train acc 85.19%, f1 0.8518, precision 0.8520, recall 0.8516, auc 0.8519
epoch 9901, loss 0.4106, train acc 85.16%, f1 0.8520, precision 0.8496, recall 0.8544, auc 0.8516
epoch 10001, loss 0.3778, train acc 85.21%, f1 0.8523, precision 0.8511, recall 0.8534, auc 0.8521
epoch 10101, loss 0.3659, train acc 85.21%, f1 0.8526, precision 0.8494, recall 0.8558, auc 0.8521
epoch 10201, loss 0.2880, train acc 85.23%, f1 0.8521, precision 0.8530, recall 0.8512, auc 0.8523
epoch 10301, loss 0.3492, train acc 85.24%, f1 0.8526, precision 0.8516, recall 0.8536, auc 0.8524
epoch 10401, loss 0.1978, train acc 85.26%, f1 0.8534, precision 0.8491, recall 0.8577, auc 0.8526
epoch 10501, loss 0.3810, train acc 85.29%, f1 0.8528, precision 0.8531, recall 0.8526, auc 0.8529
epoch 10601, loss 0.3174, train acc 85.22%, f1 0.8525, precision 0.8508, recall 0.8542, auc 0.8522
epoch 10701, loss 0.2920, train acc 85.26%, f1 0.8528, precision 0.8516, recall 0.8541, auc 0.8526
epoch 10801, loss 0.2789, train acc 85.29%, f1 0.8529, precision 0.8527, recall 0.8531, auc 0.8529
epoch 10901, loss 0.3355, train acc 85.28%, f1 0.8529, precision 0.8520, recall 0.8537, auc 0.8528
epoch 11001, loss 0.3095, train acc 85.32%, f1 0.8533, precision 0.8529, recall 0.8537, auc 0.8532
epoch 11101, loss 0.4422, train acc 85.30%, f1 0.8532, precision 0.8520, recall 0.8544, auc 0.8530
epoch 11201, loss 0.3058, train acc 85.30%, f1 0.8533, precision 0.8512, recall 0.8554, auc 0.8530
epoch 11301, loss 0.3242, train acc 85.32%, f1 0.8533, precision 0.8530, recall 0.8536, auc 0.8532
epoch 11401, loss 0.3743, train acc 85.36%, f1 0.8536, precision 0.8534, recall 0.8538, auc 0.8536
epoch 11501, loss 0.4920, train acc 85.37%, f1 0.8537, precision 0.8534, recall 0.8540, auc 0.8537
epoch 11601, loss 0.3537, train acc 85.41%, f1 0.8539, precision 0.8551, recall 0.8528, auc 0.8541
epoch 11701, loss 0.3249, train acc 85.39%, f1 0.8540, precision 0.8529, recall 0.8552, auc 0.8539
epoch 11801, loss 0.3311, train acc 85.42%, f1 0.8540, precision 0.8547, recall 0.8534, auc 0.8542
epoch 11901, loss 0.2381, train acc 85.43%, f1 0.8545, precision 0.8531, recall 0.8560, auc 0.8543
epoch 12001, loss 0.3735, train acc 85.41%, f1 0.8544, precision 0.8525, recall 0.8562, auc 0.8541
epoch 12101, loss 0.3299, train acc 85.41%, f1 0.8542, precision 0.8535, recall 0.8549, auc 0.8541
epoch 12201, loss 0.3264, train acc 85.43%, f1 0.8547, precision 0.8523, recall 0.8572, auc 0.8543
epoch 12301, loss 0.3529, train acc 85.42%, f1 0.8544, precision 0.8531, recall 0.8557, auc 0.8542
epoch 12401, loss 0.4772, train acc 85.44%, f1 0.8546, precision 0.8533, recall 0.8559, auc 0.8544
epoch 12501, loss 0.3245, train acc 85.43%, f1 0.8546, precision 0.8532, recall 0.8560, auc 0.8543
epoch 12601, loss 0.2821, train acc 85.48%, f1 0.8550, precision 0.8538, recall 0.8563, auc 0.8548
epoch 12701, loss 0.2582, train acc 85.48%, f1 0.8549, precision 0.8547, recall 0.8551, auc 0.8548
epoch 12801, loss 0.4365, train acc 85.52%, f1 0.8554, precision 0.8540, recall 0.8569, auc 0.8552
epoch 12901, loss 0.3511, train acc 85.50%, f1 0.8551, precision 0.8542, recall 0.8561, auc 0.8550
epoch 13001, loss 0.4035, train acc 85.51%, f1 0.8555, precision 0.8531, recall 0.8579, auc 0.8551
epoch 13101, loss 0.2368, train acc 85.53%, f1 0.8555, precision 0.8542, recall 0.8567, auc 0.8553
epoch 13201, loss 0.3365, train acc 85.52%, f1 0.8555, precision 0.8540, recall 0.8570, auc 0.8552
epoch 13301, loss 0.3509, train acc 85.54%, f1 0.8552, precision 0.8564, recall 0.8540, auc 0.8554
epoch 13401, loss 0.3129, train acc 85.57%, f1 0.8561, precision 0.8535, recall 0.8587, auc 0.8557
epoch 13501, loss 0.3176, train acc 85.57%, f1 0.8557, precision 0.8558, recall 0.8555, auc 0.8557
epoch 13601, loss 0.3677, train acc 85.57%, f1 0.8559, precision 0.8543, recall 0.8576, auc 0.8557
epoch 13701, loss 0.2664, train acc 85.57%, f1 0.8560, precision 0.8543, recall 0.8577, auc 0.8557
epoch 13801, loss 0.3147, train acc 85.57%, f1 0.8557, precision 0.8554, recall 0.8561, auc 0.8557
epoch 13901, loss 0.3162, train acc 85.56%, f1 0.8556, precision 0.8556, recall 0.8555, auc 0.8556
epoch 14001, loss 0.3718, train acc 85.50%, f1 0.8557, precision 0.8517, recall 0.8597, auc 0.8550
epoch 14101, loss 0.2751, train acc 85.62%, f1 0.8562, precision 0.8566, recall 0.8557, auc 0.8562
epoch 14201, loss 0.3304, train acc 85.58%, f1 0.8559, precision 0.8554, recall 0.8563, auc 0.8558
epoch 14301, loss 0.3594, train acc 85.64%, f1 0.8562, precision 0.8571, recall 0.8552, auc 0.8564
epoch 14401, loss 0.2956, train acc 85.65%, f1 0.8567, precision 0.8552, recall 0.8582, auc 0.8565
epoch 14501, loss 0.3007, train acc 85.61%, f1 0.8562, precision 0.8557, recall 0.8567, auc 0.8561
epoch 14601, loss 0.2830, train acc 85.59%, f1 0.8559, precision 0.8560, recall 0.8558, auc 0.8559
epoch 14701, loss 0.2733, train acc 85.64%, f1 0.8565, precision 0.8558, recall 0.8571, auc 0.8564
epoch 14801, loss 0.2887, train acc 85.67%, f1 0.8564, precision 0.8582, recall 0.8547, auc 0.8567
epoch 14901, loss 0.3235, train acc 85.60%, f1 0.8560, precision 0.8559, recall 0.8562, auc 0.8560
epoch 15001, loss 0.3287, train acc 85.61%, f1 0.8563, precision 0.8552, recall 0.8574, auc 0.8561
epoch 15101, loss 0.3093, train acc 85.68%, f1 0.8571, precision 0.8553, recall 0.8590, auc 0.8568
epoch 15201, loss 0.3201, train acc 85.70%, f1 0.8567, precision 0.8581, recall 0.8554, auc 0.8570
epoch 15301, loss 0.3632, train acc 85.68%, f1 0.8570, precision 0.8561, recall 0.8579, auc 0.8568
epoch 15401, loss 0.2816, train acc 85.68%, f1 0.8565, precision 0.8583, recall 0.8548, auc 0.8568
epoch 15501, loss 0.2687, train acc 85.68%, f1 0.8571, precision 0.8556, recall 0.8585, auc 0.8568
epoch 15601, loss 0.2753, train acc 85.70%, f1 0.8572, precision 0.8558, recall 0.8586, auc 0.8570
epoch 15701, loss 0.2754, train acc 85.65%, f1 0.8564, precision 0.8568, recall 0.8560, auc 0.8565
epoch 15801, loss 0.4334, train acc 85.65%, f1 0.8568, precision 0.8547, recall 0.8589, auc 0.8565
epoch 15901, loss 0.2438, train acc 85.71%, f1 0.8577, precision 0.8540, recall 0.8614, auc 0.8571
epoch 16001, loss 0.3483, train acc 85.75%, f1 0.8575, precision 0.8575, recall 0.8575, auc 0.8575
epoch 16101, loss 0.3818, train acc 85.65%, f1 0.8562, precision 0.8577, recall 0.8548, auc 0.8565
epoch 16201, loss 0.2262, train acc 85.79%, f1 0.8577, precision 0.8585, recall 0.8570, auc 0.8579
epoch 16301, loss 0.3583, train acc 85.72%, f1 0.8572, precision 0.8568, recall 0.8577, auc 0.8572
epoch 16401, loss 0.3986, train acc 85.69%, f1 0.8571, precision 0.8559, recall 0.8584, auc 0.8569
epoch 16501, loss 0.2566, train acc 85.69%, f1 0.8570, precision 0.8565, recall 0.8575, auc 0.8569/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.1634, train acc 85.67%, f1 0.8571, precision 0.8547, recall 0.8595, auc 0.8567
epoch 16701, loss 0.2539, train acc 85.71%, f1 0.8572, precision 0.8569, recall 0.8575, auc 0.8571
epoch 16801, loss 0.3404, train acc 85.66%, f1 0.8571, precision 0.8538, recall 0.8605, auc 0.8566
epoch 16901, loss 0.4426, train acc 85.74%, f1 0.8574, precision 0.8573, recall 0.8576, auc 0.8574
epoch 17001, loss 0.2933, train acc 85.71%, f1 0.8574, precision 0.8553, recall 0.8594, auc 0.8571
epoch 17101, loss 0.3411, train acc 85.71%, f1 0.8570, precision 0.8577, recall 0.8563, auc 0.8571
epoch 17201, loss 0.3299, train acc 85.72%, f1 0.8573, precision 0.8566, recall 0.8580, auc 0.8572
epoch 17301, loss 0.2777, train acc 85.78%, f1 0.8581, precision 0.8561, recall 0.8602, auc 0.8578
epoch 17401, loss 0.3042, train acc 85.74%, f1 0.8575, precision 0.8564, recall 0.8587, auc 0.8574
epoch 17501, loss 0.2965, train acc 85.77%, f1 0.8581, precision 0.8556, recall 0.8607, auc 0.8577
epoch 17601, loss 0.2827, train acc 85.76%, f1 0.8580, precision 0.8556, recall 0.8603, auc 0.8576
epoch 17701, loss 0.2818, train acc 85.81%, f1 0.8583, precision 0.8572, recall 0.8593, auc 0.8581
epoch 17801, loss 0.3395, train acc 85.81%, f1 0.8578, precision 0.8597, recall 0.8559, auc 0.8581
epoch 17901, loss 0.3182, train acc 85.80%, f1 0.8582, precision 0.8566, recall 0.8598, auc 0.8580
epoch 18001, loss 0.2482, train acc 85.80%, f1 0.8582, precision 0.8567, recall 0.8597, auc 0.8580
epoch 18101, loss 0.5056, train acc 85.86%, f1 0.8585, precision 0.8587, recall 0.8583, auc 0.8586
epoch 18201, loss 0.2960, train acc 85.86%, f1 0.8588, precision 0.8578, recall 0.8597, auc 0.8586
epoch 18301, loss 0.3413, train acc 85.85%, f1 0.8589, precision 0.8560, recall 0.8619, auc 0.8585
epoch 18401, loss 0.3786, train acc 85.83%, f1 0.8583, precision 0.8585, recall 0.8580, auc 0.8583
epoch 18501, loss 0.4094, train acc 85.83%, f1 0.8585, precision 0.8573, recall 0.8596, auc 0.8583
epoch 18601, loss 0.2912, train acc 85.86%, f1 0.8585, precision 0.8593, recall 0.8577, auc 0.8586
epoch 18701, loss 0.4256, train acc 85.82%, f1 0.8584, precision 0.8574, recall 0.8593, auc 0.8582
epoch 18801, loss 0.2847, train acc 85.85%, f1 0.8590, precision 0.8557, recall 0.8623, auc 0.8585
epoch 18901, loss 0.3718, train acc 85.89%, f1 0.8591, precision 0.8578, recall 0.8603, auc 0.8589
epoch 19001, loss 0.3356, train acc 85.84%, f1 0.8584, precision 0.8582, recall 0.8586, auc 0.8584
epoch 19101, loss 0.3825, train acc 85.86%, f1 0.8586, precision 0.8584, recall 0.8588, auc 0.8586
epoch 19201, loss 0.2029, train acc 85.81%, f1 0.8585, precision 0.8558, recall 0.8613, auc 0.8581
epoch 19301, loss 0.2694, train acc 85.92%, f1 0.8596, precision 0.8571, recall 0.8621, auc 0.8592
epoch 19401, loss 0.3561, train acc 85.89%, f1 0.8592, precision 0.8576, recall 0.8608, auc 0.8589
epoch 19501, loss 0.2707, train acc 85.90%, f1 0.8589, precision 0.8594, recall 0.8584, auc 0.8590
epoch 19601, loss 0.4214, train acc 85.93%, f1 0.8600, precision 0.8556, recall 0.8644, auc 0.8593
epoch 19701, loss 0.4097, train acc 85.91%, f1 0.8589, precision 0.8602, recall 0.8576, auc 0.8591
epoch 19801, loss 0.2549, train acc 85.85%, f1 0.8584, precision 0.8587, recall 0.8582, auc 0.8585
epoch 19901, loss 0.3364, train acc 85.92%, f1 0.8596, precision 0.8571, recall 0.8620, auc 0.8592
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_2
./test_pima/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6207407407407408

the Fscore is 0.585635359116022

the precision is 0.41732283464566927

the recall is 0.9814814814814815

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_2
----------------------



epoch 1, loss 0.6931, train acc 49.69%, f1 0.6639, precision 0.4969, recall 1.0000, auc 0.5000
epoch 101, loss 0.6191, train acc 77.83%, f1 0.7682, precision 0.7995, recall 0.7392, auc 0.7781
epoch 201, loss 0.4924, train acc 80.31%, f1 0.8006, precision 0.8057, recall 0.7956, auc 0.8031
epoch 301, loss 0.4316, train acc 81.84%, f1 0.8176, precision 0.8161, recall 0.8191, auc 0.8184
epoch 401, loss 0.2994, train acc 82.87%, f1 0.8282, precision 0.8255, recall 0.8308, auc 0.8287
epoch 501, loss 0.3650, train acc 83.37%, f1 0.8332, precision 0.8307, recall 0.8358, auc 0.8338
epoch 601, loss 0.3610, train acc 83.76%, f1 0.8369, precision 0.8349, recall 0.8390, auc 0.8376
epoch 701, loss 0.4536, train acc 83.91%, f1 0.8387, precision 0.8359, recall 0.8414, auc 0.8392
epoch 801, loss 0.4848, train acc 84.02%, f1 0.8394, precision 0.8381, recall 0.8408, auc 0.8402
epoch 901, loss 0.3119, train acc 84.12%, f1 0.8406, precision 0.8384, recall 0.8428, auc 0.8412
epoch 1001, loss 0.4193, train acc 84.14%, f1 0.8407, precision 0.8394, recall 0.8419, auc 0.8414
epoch 1101, loss 0.4749, train acc 84.12%, f1 0.8408, precision 0.8376, recall 0.8441, auc 0.8412
epoch 1201, loss 0.3357, train acc 84.17%, f1 0.8412, precision 0.8387, recall 0.8436, auc 0.8417
epoch 1301, loss 0.3387, train acc 84.12%, f1 0.8409, precision 0.8376, recall 0.8441, auc 0.8413
epoch 1401, loss 0.3744, train acc 84.18%, f1 0.8412, precision 0.8390, recall 0.8435, auc 0.8418
epoch 1501, loss 0.3472, train acc 84.21%, f1 0.8415, precision 0.8394, recall 0.8435, auc 0.8421
epoch 1601, loss 0.3323, train acc 84.20%, f1 0.8414, precision 0.8393, recall 0.8436, auc 0.8420
epoch 1701, loss 0.2867, train acc 84.22%, f1 0.8415, precision 0.8401, recall 0.8428, auc 0.8422
epoch 1801, loss 0.4528, train acc 84.18%, f1 0.8413, precision 0.8388, recall 0.8439, auc 0.8418
epoch 1901, loss 0.2716, train acc 84.19%, f1 0.8409, precision 0.8409, recall 0.8410, auc 0.8419
epoch 2001, loss 0.4239, train acc 84.16%, f1 0.8410, precision 0.8390, recall 0.8429, auc 0.8416
epoch 2101, loss 0.4355, train acc 84.19%, f1 0.8413, precision 0.8391, recall 0.8436, auc 0.8419
epoch 2201, loss 0.3000, train acc 84.18%, f1 0.8410, precision 0.8402, recall 0.8418, auc 0.8418
epoch 2301, loss 0.2765, train acc 84.14%, f1 0.8407, precision 0.8391, recall 0.8424, auc 0.8414
epoch 2401, loss 0.4667, train acc 84.15%, f1 0.8406, precision 0.8400, recall 0.8413, auc 0.8415
epoch 2501, loss 0.4135, train acc 84.20%, f1 0.8410, precision 0.8408, recall 0.8412, auc 0.8420
epoch 2601, loss 0.5216, train acc 84.22%, f1 0.8417, precision 0.8388, recall 0.8446, auc 0.8422
epoch 2701, loss 0.2671, train acc 84.18%, f1 0.8407, precision 0.8414, recall 0.8401, auc 0.8418
epoch 2801, loss 0.4064, train acc 84.23%, f1 0.8417, precision 0.8398, recall 0.8437, auc 0.8424
epoch 2901, loss 0.2691, train acc 84.13%, f1 0.8408, precision 0.8383, recall 0.8433, auc 0.8413
epoch 3001, loss 0.4104, train acc 84.18%, f1 0.8412, precision 0.8391, recall 0.8434, auc 0.8418
epoch 3101, loss 0.3771, train acc 84.21%, f1 0.8417, precision 0.8387, recall 0.8446, auc 0.8421
epoch 3201, loss 0.3186, train acc 84.25%, f1 0.8417, precision 0.8409, recall 0.8424, auc 0.8425
epoch 3301, loss 0.4172, train acc 84.23%, f1 0.8416, precision 0.8401, recall 0.8430, auc 0.8423
epoch 3401, loss 0.3031, train acc 84.22%, f1 0.8413, precision 0.8408, recall 0.8419, auc 0.8422
epoch 3501, loss 0.3625, train acc 84.23%, f1 0.8412, precision 0.8419, recall 0.8405, auc 0.8423
epoch 3601, loss 0.3982, train acc 84.19%, f1 0.8410, precision 0.8408, recall 0.8412, auc 0.8419
epoch 3701, loss 0.3255, train acc 84.20%, f1 0.8416, precision 0.8388, recall 0.8444, auc 0.8420
epoch 3801, loss 0.3522, train acc 84.21%, f1 0.8411, precision 0.8409, recall 0.8413, auc 0.8421
epoch 3901, loss 0.3282, train acc 84.19%, f1 0.8411, precision 0.8400, recall 0.8423, auc 0.8419
epoch 4001, loss 0.4411, train acc 84.21%, f1 0.8408, precision 0.8425, recall 0.8392, auc 0.8421
epoch 4101, loss 0.4359, train acc 84.23%, f1 0.8413, precision 0.8417, recall 0.8408, auc 0.8423
epoch 4201, loss 0.3383, train acc 84.23%, f1 0.8415, precision 0.8406, recall 0.8425, auc 0.8423
epoch 4301, loss 0.3020, train acc 84.21%, f1 0.8410, precision 0.8414, recall 0.8406, auc 0.8421
epoch 4401, loss 0.4027, train acc 84.22%, f1 0.8411, precision 0.8415, recall 0.8408, auc 0.8422
epoch 4501, loss 0.4166, train acc 84.27%, f1 0.8417, precision 0.8414, recall 0.8421, auc 0.8426
epoch 4601, loss 0.3906, train acc 84.27%, f1 0.8412, precision 0.8441, recall 0.8382, auc 0.8427
epoch 4701, loss 0.3428, train acc 84.32%, f1 0.8418, precision 0.8439, recall 0.8397, auc 0.8431
epoch 4801, loss 0.4012, train acc 84.30%, f1 0.8422, precision 0.8415, recall 0.8429, auc 0.8430
epoch 4901, loss 0.4198, train acc 84.34%, f1 0.8424, precision 0.8423, recall 0.8425, auc 0.8434
epoch 5001, loss 0.2082, train acc 84.30%, f1 0.8419, precision 0.8424, recall 0.8414, auc 0.8430
epoch 5101, loss 0.3322, train acc 84.31%, f1 0.8422, precision 0.8418, recall 0.8425, auc 0.8431
epoch 5201, loss 0.4245, train acc 84.33%, f1 0.8423, precision 0.8423, recall 0.8423, auc 0.8433
epoch 5301, loss 0.2155, train acc 84.33%, f1 0.8423, precision 0.8421, recall 0.8425, auc 0.8433
epoch 5401, loss 0.3039, train acc 84.39%, f1 0.8429, precision 0.8431, recall 0.8427, auc 0.8439
epoch 5501, loss 0.2663, train acc 84.38%, f1 0.8428, precision 0.8430, recall 0.8427, auc 0.8438
epoch 5601, loss 0.2811, train acc 84.38%, f1 0.8430, precision 0.8420, recall 0.8441, auc 0.8438
epoch 5701, loss 0.2794, train acc 84.41%, f1 0.8429, precision 0.8441, recall 0.8417, auc 0.8441
epoch 5801, loss 0.3730, train acc 84.45%, f1 0.8437, precision 0.8429, recall 0.8446, auc 0.8445
epoch 5901, loss 0.2864, train acc 84.44%, f1 0.8434, precision 0.8432, recall 0.8436, auc 0.8444
epoch 6001, loss 0.3861, train acc 84.51%, f1 0.8443, precision 0.8437, recall 0.8448, auc 0.8451
epoch 6101, loss 0.3859, train acc 84.45%, f1 0.8438, precision 0.8422, recall 0.8455, auc 0.8445
epoch 6201, loss 0.2980, train acc 84.52%, f1 0.8443, precision 0.8436, recall 0.8450, auc 0.8452
epoch 6301, loss 0.3671, train acc 84.56%, f1 0.8443, precision 0.8459, recall 0.8427, auc 0.8455
epoch 6401, loss 0.4046, train acc 84.55%, f1 0.8446, precision 0.8442, recall 0.8450, auc 0.8455
epoch 6501, loss 0.4445, train acc 84.62%, f1 0.8454, precision 0.8447, recall 0.8461, auc 0.8462
epoch 6601, loss 0.3396, train acc 84.60%, f1 0.8449, precision 0.8455, recall 0.8442, auc 0.8459
epoch 6701, loss 0.3408, train acc 84.57%, f1 0.8450, precision 0.8438, recall 0.8462, auc 0.8457
epoch 6801, loss 0.3895, train acc 84.65%, f1 0.8459, precision 0.8439, recall 0.8479, auc 0.8465
epoch 6901, loss 0.4014, train acc 84.73%, f1 0.8461, precision 0.8470, recall 0.8453, auc 0.8472
epoch 7001, loss 0.3803, train acc 84.69%, f1 0.8458, precision 0.8463, recall 0.8453, auc 0.8468
epoch 7101, loss 0.2471, train acc 84.71%, f1 0.8459, precision 0.8472, recall 0.8446, auc 0.8471
epoch 7201, loss 0.3253, train acc 84.75%, f1 0.8465, precision 0.8464, recall 0.8467, auc 0.8474
epoch 7301, loss 0.3221, train acc 84.77%, f1 0.8466, precision 0.8476, recall 0.8455, auc 0.8477
epoch 7401, loss 0.3112, train acc 84.80%, f1 0.8471, precision 0.8470, recall 0.8471, auc 0.8480
epoch 7501, loss 0.2459, train acc 84.83%, f1 0.8476, precision 0.8461, recall 0.8491, auc 0.8483
epoch 7601, loss 0.2238, train acc 84.84%, f1 0.8476, precision 0.8471, recall 0.8481, auc 0.8484
epoch 7701, loss 0.4546, train acc 84.89%, f1 0.8483, precision 0.8461, recall 0.8505, auc 0.8489
epoch 7801, loss 0.3393, train acc 84.90%, f1 0.8478, precision 0.8494, recall 0.8462, auc 0.8490
epoch 7901, loss 0.2341, train acc 84.83%, f1 0.8473, precision 0.8476, recall 0.8470, auc 0.8483
epoch 8001, loss 0.3646, train acc 84.90%, f1 0.8483, precision 0.8468, recall 0.8498, auc 0.8490
epoch 8101, loss 0.2706, train acc 84.96%, f1 0.8489, precision 0.8475, recall 0.8504, auc 0.8496
epoch 8201, loss 0.4782, train acc 84.94%, f1 0.8491, precision 0.8458, recall 0.8524, auc 0.8495/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.3025, train acc 84.95%, f1 0.8488, precision 0.8476, recall 0.8499, auc 0.8495
epoch 8401, loss 0.2525, train acc 84.96%, f1 0.8488, precision 0.8483, recall 0.8493, auc 0.8496
epoch 8501, loss 0.2826, train acc 84.91%, f1 0.8481, precision 0.8485, recall 0.8477, auc 0.8491
epoch 8601, loss 0.3344, train acc 85.01%, f1 0.8493, precision 0.8487, recall 0.8498, auc 0.8501
epoch 8701, loss 0.3519, train acc 85.04%, f1 0.8496, precision 0.8489, recall 0.8502, auc 0.8504
epoch 8801, loss 0.2805, train acc 85.01%, f1 0.8492, precision 0.8489, recall 0.8495, auc 0.8501
epoch 8901, loss 0.2302, train acc 85.00%, f1 0.8491, precision 0.8489, recall 0.8493, auc 0.8500
epoch 9001, loss 0.3988, train acc 85.06%, f1 0.8497, precision 0.8495, recall 0.8498, auc 0.8506
epoch 9101, loss 0.3720, train acc 85.09%, f1 0.8499, precision 0.8500, recall 0.8498, auc 0.8508
epoch 9201, loss 0.3438, train acc 85.08%, f1 0.8497, precision 0.8504, recall 0.8491, auc 0.8508
epoch 9301, loss 0.4535, train acc 85.13%, f1 0.8504, precision 0.8502, recall 0.8507, auc 0.8513
epoch 9401, loss 0.3695, train acc 85.13%, f1 0.8504, precision 0.8503, recall 0.8504, auc 0.8513
epoch 9501, loss 0.3550, train acc 85.14%, f1 0.8509, precision 0.8485, recall 0.8533, auc 0.8514
epoch 9601, loss 0.3417, train acc 85.23%, f1 0.8512, precision 0.8523, recall 0.8502, auc 0.8523
epoch 9701, loss 0.3103, train acc 85.13%, f1 0.8506, precision 0.8493, recall 0.8519, auc 0.8513
epoch 9801, loss 0.3276, train acc 85.14%, f1 0.8505, precision 0.8503, recall 0.8507, auc 0.8514
epoch 9901, loss 0.2848, train acc 85.25%, f1 0.8520, precision 0.8495, recall 0.8544, auc 0.8525
epoch 10001, loss 0.3447, train acc 85.17%, f1 0.8510, precision 0.8498, recall 0.8522, auc 0.8517
epoch 10101, loss 0.3125, train acc 85.14%, f1 0.8502, precision 0.8516, recall 0.8489, auc 0.8514
epoch 10201, loss 0.4137, train acc 85.21%, f1 0.8509, precision 0.8522, recall 0.8496, auc 0.8521
epoch 10301, loss 0.2935, train acc 85.23%, f1 0.8521, precision 0.8480, recall 0.8561, auc 0.8523
epoch 10401, loss 0.3002, train acc 85.22%, f1 0.8512, precision 0.8518, recall 0.8506, auc 0.8522
epoch 10501, loss 0.3033, train acc 85.26%, f1 0.8517, precision 0.8515, recall 0.8520, auc 0.8526
epoch 10601, loss 0.2909, train acc 85.24%, f1 0.8517, precision 0.8507, recall 0.8527, auc 0.8524
epoch 10701, loss 0.3855, train acc 85.29%, f1 0.8524, precision 0.8499, recall 0.8549, auc 0.8529
epoch 10801, loss 0.3380, train acc 85.32%, f1 0.8521, precision 0.8534, recall 0.8508, auc 0.8532
epoch 10901, loss 0.3046, train acc 85.38%, f1 0.8532, precision 0.8512, recall 0.8551, auc 0.8538
epoch 11001, loss 0.4199, train acc 85.39%, f1 0.8528, precision 0.8536, recall 0.8520, auc 0.8538
epoch 11101, loss 0.4242, train acc 85.39%, f1 0.8531, precision 0.8522, recall 0.8541, auc 0.8539
epoch 11201, loss 0.3310, train acc 85.45%, f1 0.8534, precision 0.8543, recall 0.8525, auc 0.8545
epoch 11301, loss 0.3783, train acc 85.36%, f1 0.8525, precision 0.8535, recall 0.8515, auc 0.8536
epoch 11401, loss 0.3340, train acc 85.38%, f1 0.8530, precision 0.8526, recall 0.8534, auc 0.8538
epoch 11501, loss 0.4439, train acc 85.42%, f1 0.8534, precision 0.8524, recall 0.8545, auc 0.8542
epoch 11601, loss 0.4686, train acc 85.40%, f1 0.8534, precision 0.8514, recall 0.8555, auc 0.8540
epoch 11701, loss 0.2836, train acc 85.47%, f1 0.8539, precision 0.8534, recall 0.8543, auc 0.8547
epoch 11801, loss 0.3627, train acc 85.48%, f1 0.8540, precision 0.8535, recall 0.8545, auc 0.8548
epoch 11901, loss 0.3348, train acc 85.43%, f1 0.8537, precision 0.8522, recall 0.8552, auc 0.8544
epoch 12001, loss 0.3406, train acc 85.47%, f1 0.8539, precision 0.8534, recall 0.8544, auc 0.8547
epoch 12101, loss 0.3687, train acc 85.45%, f1 0.8537, precision 0.8530, recall 0.8544, auc 0.8545
epoch 12201, loss 0.2847, train acc 85.43%, f1 0.8536, precision 0.8522, recall 0.8551, auc 0.8543
epoch 12301, loss 0.3184, train acc 85.48%, f1 0.8539, precision 0.8540, recall 0.8538, auc 0.8548
epoch 12401, loss 0.4093, train acc 85.56%, f1 0.8547, precision 0.8546, recall 0.8548, auc 0.8556
epoch 12501, loss 0.3251, train acc 85.58%, f1 0.8553, precision 0.8530, recall 0.8576, auc 0.8558
epoch 12601, loss 0.2823, train acc 85.52%, f1 0.8545, precision 0.8531, recall 0.8560, auc 0.8552
epoch 12701, loss 0.2452, train acc 85.56%, f1 0.8548, precision 0.8546, recall 0.8549, auc 0.8556
epoch 12801, loss 0.2599, train acc 85.59%, f1 0.8551, precision 0.8547, recall 0.8556, auc 0.8559
epoch 12901, loss 0.4547, train acc 85.56%, f1 0.8549, precision 0.8534, recall 0.8564, auc 0.8556
epoch 13001, loss 0.2631, train acc 85.58%, f1 0.8549, precision 0.8546, recall 0.8552, auc 0.8558
epoch 13101, loss 0.3036, train acc 85.59%, f1 0.8551, precision 0.8546, recall 0.8556, auc 0.8559
epoch 13201, loss 0.3164, train acc 85.61%, f1 0.8553, precision 0.8547, recall 0.8558, auc 0.8561
epoch 13301, loss 0.2606, train acc 85.62%, f1 0.8550, precision 0.8565, recall 0.8536, auc 0.8562
epoch 13401, loss 0.3069, train acc 85.63%, f1 0.8556, precision 0.8543, recall 0.8569, auc 0.8563
epoch 13501, loss 0.2129, train acc 85.64%, f1 0.8551, precision 0.8576, recall 0.8527, auc 0.8564
epoch 13601, loss 0.3394, train acc 85.67%, f1 0.8561, precision 0.8543, recall 0.8579, auc 0.8567
epoch 13701, loss 0.3349, train acc 85.65%, f1 0.8555, precision 0.8563, recall 0.8548, auc 0.8565
epoch 13801, loss 0.4173, train acc 85.63%, f1 0.8556, precision 0.8545, recall 0.8568, auc 0.8563
epoch 13901, loss 0.3388, train acc 85.66%, f1 0.8556, precision 0.8564, recall 0.8548, auc 0.8566
epoch 14001, loss 0.3979, train acc 85.67%, f1 0.8561, precision 0.8544, recall 0.8579, auc 0.8567
epoch 14101, loss 0.3718, train acc 85.68%, f1 0.8560, precision 0.8553, recall 0.8567, auc 0.8568
epoch 14201, loss 0.3362, train acc 85.64%, f1 0.8559, precision 0.8534, recall 0.8584, auc 0.8564
epoch 14301, loss 0.3057, train acc 85.68%, f1 0.8560, precision 0.8551, recall 0.8570, auc 0.8568
epoch 14401, loss 0.3549, train acc 85.69%, f1 0.8562, precision 0.8551, recall 0.8572, auc 0.8569
epoch 14501, loss 0.3413, train acc 85.69%, f1 0.8563, precision 0.8548, recall 0.8578, auc 0.8569
epoch 14601, loss 0.3387, train acc 85.68%, f1 0.8562, precision 0.8544, recall 0.8580, auc 0.8568
epoch 14701, loss 0.3237, train acc 85.71%, f1 0.8563, precision 0.8557, recall 0.8568, auc 0.8571
epoch 14801, loss 0.2908, train acc 85.74%, f1 0.8567, precision 0.8554, recall 0.8579, auc 0.8574
epoch 14901, loss 0.3460, train acc 85.69%, f1 0.8562, precision 0.8552, recall 0.8572, auc 0.8569
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_2
./test_pima/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.635

the Fscore is 0.5966850828729281

the precision is 0.4251968503937008

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_2
----------------------



epoch 1, loss 0.6933, train acc 49.87%, f1 0.0000, precision 0.0833, recall 0.0000, auc 0.4999
epoch 101, loss 0.6121, train acc 77.97%, f1 0.7803, precision 0.7801, recall 0.7805, auc 0.7797
epoch 201, loss 0.4775, train acc 80.28%, f1 0.8032, precision 0.8032, recall 0.8033, auc 0.8028
epoch 301, loss 0.3726, train acc 82.02%, f1 0.8207, precision 0.8205, recall 0.8209, auc 0.8202
epoch 401, loss 0.3705, train acc 82.86%, f1 0.8293, precision 0.8276, recall 0.8311, auc 0.8286
epoch 501, loss 0.3676, train acc 83.40%, f1 0.8344, precision 0.8345, recall 0.8342, auc 0.8340
epoch 601, loss 0.3732, train acc 83.71%, f1 0.8378, precision 0.8363, recall 0.8394, auc 0.8371
epoch 701, loss 0.3516, train acc 83.94%, f1 0.8397, precision 0.8403, recall 0.8390, auc 0.8394
epoch 801, loss 0.3852, train acc 84.06%, f1 0.8412, precision 0.8401, recall 0.8423, auc 0.8406
epoch 901, loss 0.3654, train acc 84.08%, f1 0.8413, precision 0.8410, recall 0.8416, auc 0.8408
epoch 1001, loss 0.3008, train acc 84.01%, f1 0.8401, precision 0.8424, recall 0.8377, auc 0.8401
epoch 1101, loss 0.4051, train acc 84.12%, f1 0.8412, precision 0.8433, recall 0.8392, auc 0.8412
epoch 1201, loss 0.3832, train acc 84.17%, f1 0.8424, precision 0.8408, recall 0.8441, auc 0.8417
epoch 1301, loss 0.3944, train acc 84.21%, f1 0.8425, precision 0.8424, recall 0.8426, auc 0.8421
epoch 1401, loss 0.3589, train acc 84.12%, f1 0.8416, precision 0.8419, recall 0.8413, auc 0.8412
epoch 1501, loss 0.2019, train acc 84.14%, f1 0.8412, precision 0.8440, recall 0.8384, auc 0.8414
epoch 1601, loss 0.3916, train acc 84.13%, f1 0.8419, precision 0.8412, recall 0.8425, auc 0.8413
epoch 1701, loss 0.3963, train acc 84.16%, f1 0.8421, precision 0.8415, recall 0.8427, auc 0.8416
epoch 1801, loss 0.3050, train acc 84.15%, f1 0.8422, precision 0.8408, recall 0.8436, auc 0.8415
epoch 1901, loss 0.3884, train acc 84.21%, f1 0.8425, precision 0.8424, recall 0.8425, auc 0.8421
epoch 2001, loss 0.2650, train acc 84.17%, f1 0.8420, precision 0.8426, recall 0.8414, auc 0.8417
epoch 2101, loss 0.2936, train acc 84.17%, f1 0.8420, precision 0.8424, recall 0.8415, auc 0.8417
epoch 2201, loss 0.3938, train acc 84.19%, f1 0.8421, precision 0.8434, recall 0.8408, auc 0.8419
epoch 2301, loss 0.2629, train acc 84.14%, f1 0.8420, precision 0.8410, recall 0.8430, auc 0.8414
epoch 2401, loss 0.4803, train acc 84.16%, f1 0.8416, precision 0.8438, recall 0.8394, auc 0.8416
epoch 2501, loss 0.3262, train acc 84.16%, f1 0.8422, precision 0.8410, recall 0.8435, auc 0.8416
epoch 2601, loss 0.4746, train acc 84.21%, f1 0.8424, precision 0.8427, recall 0.8421, auc 0.8421
epoch 2701, loss 0.3604, train acc 84.20%, f1 0.8423, precision 0.8429, recall 0.8416, auc 0.8420
epoch 2801, loss 0.2918, train acc 84.22%, f1 0.8422, precision 0.8443, recall 0.8400, auc 0.8422
epoch 2901, loss 0.4481, train acc 84.18%, f1 0.8420, precision 0.8431, recall 0.8408, auc 0.8418
epoch 3001, loss 0.4099, train acc 84.16%, f1 0.8416, precision 0.8438, recall 0.8394, auc 0.8416
epoch 3101, loss 0.4516, train acc 84.18%, f1 0.8420, precision 0.8433, recall 0.8407, auc 0.8418
epoch 3201, loss 0.3442, train acc 84.17%, f1 0.8424, precision 0.8406, recall 0.8442, auc 0.8417
epoch 3301, loss 0.4629, train acc 84.16%, f1 0.8416, precision 0.8439, recall 0.8393, auc 0.8417
epoch 3401, loss 0.3875, train acc 84.16%, f1 0.8420, precision 0.8417, recall 0.8423, auc 0.8416
epoch 3501, loss 0.4549, train acc 84.20%, f1 0.8427, precision 0.8411, recall 0.8443, auc 0.8420
epoch 3601, loss 0.4176, train acc 84.26%, f1 0.8425, precision 0.8450, recall 0.8400, auc 0.8426
epoch 3701, loss 0.2958, train acc 84.23%, f1 0.8426, precision 0.8431, recall 0.8420, auc 0.8423
epoch 3801, loss 0.2630, train acc 84.23%, f1 0.8424, precision 0.8438, recall 0.8411, auc 0.8423
epoch 3901, loss 0.3563, train acc 84.17%, f1 0.8420, precision 0.8426, recall 0.8414, auc 0.8417
epoch 4001, loss 0.3518, train acc 84.19%, f1 0.8423, precision 0.8421, recall 0.8425, auc 0.8419
epoch 4101, loss 0.4618, train acc 84.14%, f1 0.8418, precision 0.8416, recall 0.8421, auc 0.8414
epoch 4201, loss 0.4305, train acc 84.14%, f1 0.8420, precision 0.8409, recall 0.8431, auc 0.8414
epoch 4301, loss 0.4835, train acc 84.16%, f1 0.8416, precision 0.8441, recall 0.8391, auc 0.8417
epoch 4401, loss 0.3624, train acc 84.21%, f1 0.8424, precision 0.8429, recall 0.8420, auc 0.8421
epoch 4501, loss 0.2775, train acc 84.22%, f1 0.8425, precision 0.8430, recall 0.8420, auc 0.8422
epoch 4601, loss 0.4441, train acc 84.23%, f1 0.8425, precision 0.8435, recall 0.8414, auc 0.8423
epoch 4701, loss 0.5055, train acc 84.26%, f1 0.8435, precision 0.8411, recall 0.8458, auc 0.8426
epoch 4801, loss 0.2560, train acc 84.25%, f1 0.8425, precision 0.8444, recall 0.8405, auc 0.8425
epoch 4901, loss 0.3095, train acc 84.27%, f1 0.8431, precision 0.8430, recall 0.8432, auc 0.8427
epoch 5001, loss 0.3198, train acc 84.31%, f1 0.8433, precision 0.8446, recall 0.8420, auc 0.8431
epoch 5101, loss 0.2738, train acc 84.28%, f1 0.8429, precision 0.8448, recall 0.8410, auc 0.8428
epoch 5201, loss 0.4274, train acc 84.30%, f1 0.8432, precision 0.8443, recall 0.8422, auc 0.8430
epoch 5301, loss 0.3708, train acc 84.26%, f1 0.8431, precision 0.8424, recall 0.8437, auc 0.8426
epoch 5401, loss 0.3657, train acc 84.32%, f1 0.8439, precision 0.8422, recall 0.8455, auc 0.8432
epoch 5501, loss 0.3621, train acc 84.36%, f1 0.8445, precision 0.8418, recall 0.8473, auc 0.8436
epoch 5601, loss 0.3081, train acc 84.34%, f1 0.8433, precision 0.8458, recall 0.8409, auc 0.8434
epoch 5701, loss 0.3599, train acc 84.40%, f1 0.8444, precision 0.8445, recall 0.8443, auc 0.8440
epoch 5801, loss 0.3951, train acc 84.44%, f1 0.8449, precision 0.8443, recall 0.8455, auc 0.8444
epoch 5901, loss 0.3453, train acc 84.43%, f1 0.8447, precision 0.8446, recall 0.8448, auc 0.8443
epoch 6001, loss 0.3473, train acc 84.41%, f1 0.8444, precision 0.8450, recall 0.8439, auc 0.8441
epoch 6101, loss 0.2925, train acc 84.47%, f1 0.8451, precision 0.8454, recall 0.8447, auc 0.8447
epoch 6201, loss 0.4154, train acc 84.47%, f1 0.8451, precision 0.8451, recall 0.8451, auc 0.8447
epoch 6301, loss 0.3293, train acc 84.47%, f1 0.8450, precision 0.8455, recall 0.8446, auc 0.8447
epoch 6401, loss 0.3518, train acc 84.51%, f1 0.8455, precision 0.8453, recall 0.8457, auc 0.8451
epoch 6501, loss 0.2676, train acc 84.55%, f1 0.8459, precision 0.8459, recall 0.8460, auc 0.8455
epoch 6601, loss 0.2644, train acc 84.54%, f1 0.8459, precision 0.8456, recall 0.8461, auc 0.8454
epoch 6701, loss 0.2978, train acc 84.56%, f1 0.8461, precision 0.8451, recall 0.8472, auc 0.8456
epoch 6801, loss 0.4006, train acc 84.59%, f1 0.8465, precision 0.8453, recall 0.8476, auc 0.8459
epoch 6901, loss 0.4378, train acc 84.59%, f1 0.8467, precision 0.8444, recall 0.8490, auc 0.8459
epoch 7001, loss 0.4777, train acc 84.62%, f1 0.8469, precision 0.8451, recall 0.8486, auc 0.8462
epoch 7101, loss 0.3249, train acc 84.60%, f1 0.8464, precision 0.8466, recall 0.8461, auc 0.8460
epoch 7201, loss 0.2906, train acc 84.61%, f1 0.8461, precision 0.8480, recall 0.8443, auc 0.8461
epoch 7301, loss 0.2829, train acc 84.69%, f1 0.8474, precision 0.8471, recall 0.8476, auc 0.8469
epoch 7401, loss 0.3514, train acc 84.71%, f1 0.8477, precision 0.8465, recall 0.8488, auc 0.8471
epoch 7501, loss 0.3941, train acc 84.72%, f1 0.8476, precision 0.8474, recall 0.8477, auc 0.8472
epoch 7601, loss 0.3124, train acc 84.75%, f1 0.8486, precision 0.8445, recall 0.8526, auc 0.8474
epoch 7701, loss 0.2980, train acc 84.74%, f1 0.8475, precision 0.8489, recall 0.8462, auc 0.8474
epoch 7801, loss 0.3097, train acc 84.78%, f1 0.8483, precision 0.8475, recall 0.8491, auc 0.8478
epoch 7901, loss 0.3368, train acc 84.80%, f1 0.8481, precision 0.8498, recall 0.8464, auc 0.8481
epoch 8001, loss 0.3139, train acc 84.82%, f1 0.8486, precision 0.8482, recall 0.8491, auc 0.8482
epoch 8101, loss 0.3874, train acc 84.87%, f1 0.8491, precision 0.8487, recall 0.8495, auc 0.8487
epoch 8201, loss 0.2801, train acc 84.85%, f1 0.8488, precision 0.8494, recall 0.8482, auc 0.8485/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2130, train acc 84.86%, f1 0.8491, precision 0.8483, recall 0.8499, auc 0.8486
epoch 8401, loss 0.5329, train acc 84.85%, f1 0.8485, precision 0.8504, recall 0.8467, auc 0.8485
epoch 8501, loss 0.3493, train acc 84.88%, f1 0.8491, precision 0.8497, recall 0.8485, auc 0.8488
epoch 8601, loss 0.4792, train acc 84.90%, f1 0.8491, precision 0.8504, recall 0.8479, auc 0.8490
epoch 8701, loss 0.2108, train acc 84.91%, f1 0.8493, precision 0.8503, recall 0.8484, auc 0.8491
epoch 8801, loss 0.3089, train acc 84.91%, f1 0.8496, precision 0.8493, recall 0.8498, auc 0.8491
epoch 8901, loss 0.3470, train acc 84.94%, f1 0.8498, precision 0.8498, recall 0.8497, auc 0.8494
epoch 9001, loss 0.2749, train acc 84.89%, f1 0.8492, precision 0.8495, recall 0.8489, auc 0.8489
epoch 9101, loss 0.3195, train acc 85.00%, f1 0.8502, precision 0.8511, recall 0.8493, auc 0.8500
epoch 9201, loss 0.3224, train acc 84.98%, f1 0.8500, precision 0.8505, recall 0.8496, auc 0.8498
epoch 9301, loss 0.3555, train acc 85.00%, f1 0.8498, precision 0.8528, recall 0.8468, auc 0.8500
epoch 9401, loss 0.3203, train acc 85.02%, f1 0.8507, precision 0.8502, recall 0.8512, auc 0.8502
epoch 9501, loss 0.3486, train acc 85.00%, f1 0.8500, precision 0.8519, recall 0.8481, auc 0.8500
epoch 9601, loss 0.4370, train acc 85.03%, f1 0.8506, precision 0.8511, recall 0.8500, auc 0.8503
epoch 9701, loss 0.4092, train acc 85.08%, f1 0.8511, precision 0.8515, recall 0.8506, auc 0.8508
epoch 9801, loss 0.5147, train acc 85.06%, f1 0.8511, precision 0.8504, recall 0.8519, auc 0.8506
epoch 9901, loss 0.3648, train acc 85.08%, f1 0.8509, precision 0.8524, recall 0.8495, auc 0.8508
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_2
./test_pima/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.65

the Fscore is 0.6067415730337078

the precision is 0.43548387096774194

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_2
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6248, train acc 78.20%, f1 0.7890, precision 0.7644, recall 0.8151, auc 0.7820
epoch 201, loss 0.5322, train acc 80.39%, f1 0.8045, precision 0.8019, recall 0.8071, auc 0.8039
epoch 301, loss 0.3957, train acc 82.05%, f1 0.8208, precision 0.8194, recall 0.8221, auc 0.8205
epoch 401, loss 0.4405, train acc 82.87%, f1 0.8288, precision 0.8281, recall 0.8296, auc 0.8287
epoch 501, loss 0.3852, train acc 83.40%, f1 0.8339, precision 0.8347, recall 0.8330, auc 0.8340
epoch 601, loss 0.4132, train acc 83.70%, f1 0.8369, precision 0.8375, recall 0.8362, auc 0.8370
epoch 701, loss 0.3846, train acc 83.94%, f1 0.8392, precision 0.8405, recall 0.8379, auc 0.8394
epoch 801, loss 0.4709, train acc 83.94%, f1 0.8392, precision 0.8403, recall 0.8380, auc 0.8394
epoch 901, loss 0.4222, train acc 84.00%, f1 0.8402, precision 0.8392, recall 0.8412, auc 0.8400
epoch 1001, loss 0.3168, train acc 84.10%, f1 0.8413, precision 0.8395, recall 0.8432, auc 0.8410
epoch 1101, loss 0.3655, train acc 84.13%, f1 0.8412, precision 0.8416, recall 0.8408, auc 0.8413
epoch 1201, loss 0.2715, train acc 84.13%, f1 0.8409, precision 0.8427, recall 0.8392, auc 0.8413
epoch 1301, loss 0.3209, train acc 84.14%, f1 0.8414, precision 0.8413, recall 0.8415, auc 0.8414
epoch 1401, loss 0.3293, train acc 84.16%, f1 0.8419, precision 0.8402, recall 0.8437, auc 0.8416
epoch 1501, loss 0.2834, train acc 84.15%, f1 0.8409, precision 0.8440, recall 0.8379, auc 0.8415
epoch 1601, loss 0.4182, train acc 84.10%, f1 0.8410, precision 0.8409, recall 0.8412, auc 0.8410
epoch 1701, loss 0.3419, train acc 84.15%, f1 0.8418, precision 0.8398, recall 0.8438, auc 0.8415
epoch 1801, loss 0.4083, train acc 84.13%, f1 0.8416, precision 0.8403, recall 0.8428, auc 0.8413
epoch 1901, loss 0.4092, train acc 84.16%, f1 0.8416, precision 0.8417, recall 0.8415, auc 0.8416
epoch 2001, loss 0.4699, train acc 84.16%, f1 0.8417, precision 0.8412, recall 0.8422, auc 0.8416
epoch 2101, loss 0.3632, train acc 84.16%, f1 0.8419, precision 0.8399, recall 0.8440, auc 0.8416
epoch 2201, loss 0.3609, train acc 84.20%, f1 0.8421, precision 0.8416, recall 0.8426, auc 0.8420
epoch 2301, loss 0.2746, train acc 84.20%, f1 0.8417, precision 0.8432, recall 0.8402, auc 0.8420
epoch 2401, loss 0.2750, train acc 84.20%, f1 0.8420, precision 0.8422, recall 0.8417, auc 0.8420
epoch 2501, loss 0.2689, train acc 84.15%, f1 0.8412, precision 0.8425, recall 0.8400, auc 0.8415
epoch 2601, loss 0.3839, train acc 84.16%, f1 0.8417, precision 0.8412, recall 0.8422, auc 0.8416
epoch 2701, loss 0.3939, train acc 84.12%, f1 0.8416, precision 0.8393, recall 0.8440, auc 0.8412
epoch 2801, loss 0.3187, train acc 84.15%, f1 0.8412, precision 0.8426, recall 0.8398, auc 0.8415
epoch 2901, loss 0.3571, train acc 84.15%, f1 0.8413, precision 0.8423, recall 0.8402, auc 0.8415
epoch 3001, loss 0.3965, train acc 84.18%, f1 0.8423, precision 0.8396, recall 0.8450, auc 0.8418
epoch 3101, loss 0.3488, train acc 84.17%, f1 0.8423, precision 0.8395, recall 0.8450, auc 0.8417
epoch 3201, loss 0.3488, train acc 84.12%, f1 0.8419, precision 0.8382, recall 0.8457, auc 0.8412
epoch 3301, loss 0.3408, train acc 84.15%, f1 0.8419, precision 0.8396, recall 0.8442, auc 0.8415
epoch 3401, loss 0.3986, train acc 84.19%, f1 0.8419, precision 0.8415, recall 0.8424, auc 0.8419
epoch 3501, loss 0.4332, train acc 84.19%, f1 0.8419, precision 0.8416, recall 0.8423, auc 0.8419
epoch 3601, loss 0.2938, train acc 84.22%, f1 0.8426, precision 0.8403, recall 0.8450, auc 0.8422
epoch 3701, loss 0.2696, train acc 84.18%, f1 0.8422, precision 0.8401, recall 0.8444, auc 0.8418
epoch 3801, loss 0.3428, train acc 84.21%, f1 0.8422, precision 0.8416, recall 0.8428, auc 0.8421
epoch 3901, loss 0.2758, train acc 84.22%, f1 0.8425, precision 0.8411, recall 0.8438, auc 0.8422
epoch 4001, loss 0.3938, train acc 84.18%, f1 0.8417, precision 0.8419, recall 0.8415, auc 0.8418
epoch 4101, loss 0.3714, train acc 84.24%, f1 0.8425, precision 0.8416, recall 0.8435, auc 0.8424
epoch 4201, loss 0.3446, train acc 84.23%, f1 0.8425, precision 0.8412, recall 0.8439, auc 0.8423
epoch 4301, loss 0.3424, train acc 84.26%, f1 0.8429, precision 0.8416, recall 0.8441, auc 0.8426
epoch 4401, loss 0.2769, train acc 84.25%, f1 0.8427, precision 0.8415, recall 0.8440, auc 0.8425
epoch 4501, loss 0.3702, train acc 84.31%, f1 0.8432, precision 0.8425, recall 0.8439, auc 0.8431
epoch 4601, loss 0.4370, train acc 84.33%, f1 0.8438, precision 0.8414, recall 0.8462, auc 0.8433
epoch 4701, loss 0.3545, train acc 84.30%, f1 0.8435, precision 0.8408, recall 0.8462, auc 0.8430
epoch 4801, loss 0.4636, train acc 84.31%, f1 0.8432, precision 0.8424, recall 0.8440, auc 0.8431
epoch 4901, loss 0.4875, train acc 84.29%, f1 0.8432, precision 0.8417, recall 0.8447, auc 0.8429
epoch 5001, loss 0.3110, train acc 84.34%, f1 0.8436, precision 0.8422, recall 0.8449, auc 0.8434
epoch 5101, loss 0.3692, train acc 84.34%, f1 0.8436, precision 0.8425, recall 0.8447, auc 0.8434
epoch 5201, loss 0.3311, train acc 84.35%, f1 0.8438, precision 0.8420, recall 0.8457, auc 0.8435
epoch 5301, loss 0.3662, train acc 84.40%, f1 0.8443, precision 0.8425, recall 0.8462, auc 0.8440
epoch 5401, loss 0.3613, train acc 84.44%, f1 0.8447, precision 0.8431, recall 0.8463, auc 0.8444
epoch 5501, loss 0.3500, train acc 84.44%, f1 0.8446, precision 0.8434, recall 0.8459, auc 0.8444
epoch 5601, loss 0.3216, train acc 84.48%, f1 0.8452, precision 0.8432, recall 0.8471, auc 0.8448
epoch 5701, loss 0.3918, train acc 84.53%, f1 0.8457, precision 0.8434, recall 0.8480, auc 0.8453
epoch 5801, loss 0.4286, train acc 84.50%, f1 0.8453, precision 0.8434, recall 0.8473, auc 0.8450
epoch 5901, loss 0.3851, train acc 84.50%, f1 0.8453, precision 0.8436, recall 0.8471, auc 0.8450
epoch 6001, loss 0.4328, train acc 84.52%, f1 0.8455, precision 0.8437, recall 0.8473, auc 0.8452
epoch 6101, loss 0.3841, train acc 84.53%, f1 0.8461, precision 0.8417, recall 0.8505, auc 0.8453
epoch 6201, loss 0.4160, train acc 84.57%, f1 0.8461, precision 0.8439, recall 0.8482, auc 0.8457
epoch 6301, loss 0.2980, train acc 84.61%, f1 0.8460, precision 0.8463, recall 0.8458, auc 0.8461
epoch 6401, loss 0.4240, train acc 84.57%, f1 0.8459, precision 0.8447, recall 0.8472, auc 0.8457
epoch 6501, loss 0.3240, train acc 84.62%, f1 0.8462, precision 0.8459, recall 0.8465, auc 0.8462
epoch 6601, loss 0.4845, train acc 84.64%, f1 0.8465, precision 0.8455, recall 0.8476, auc 0.8464
epoch 6701, loss 0.3461, train acc 84.66%, f1 0.8469, precision 0.8451, recall 0.8488, auc 0.8466
epoch 6801, loss 0.3011, train acc 84.66%, f1 0.8469, precision 0.8453, recall 0.8485, auc 0.8466
epoch 6901, loss 0.2128, train acc 84.70%, f1 0.8475, precision 0.8446, recall 0.8504, auc 0.8470
epoch 7001, loss 0.3093, train acc 84.69%, f1 0.8473, precision 0.8451, recall 0.8495, auc 0.8469
epoch 7101, loss 0.3384, train acc 84.72%, f1 0.8478, precision 0.8446, recall 0.8509, auc 0.8472
epoch 7201, loss 0.3388, train acc 84.76%, f1 0.8477, precision 0.8471, recall 0.8484, auc 0.8476
epoch 7301, loss 0.4077, train acc 84.77%, f1 0.8479, precision 0.8469, recall 0.8490, auc 0.8477
epoch 7401, loss 0.3557, train acc 84.77%, f1 0.8476, precision 0.8482, recall 0.8470, auc 0.8477
epoch 7501, loss 0.5014, train acc 84.77%, f1 0.8480, precision 0.8461, recall 0.8500, auc 0.8477
epoch 7601, loss 0.4392, train acc 84.87%, f1 0.8488, precision 0.8483, recall 0.8493, auc 0.8487
epoch 7701, loss 0.3556, train acc 84.86%, f1 0.8489, precision 0.8472, recall 0.8507, auc 0.8486
epoch 7801, loss 0.3634, train acc 84.87%, f1 0.8486, precision 0.8491, recall 0.8481, auc 0.8487
epoch 7901, loss 0.3905, train acc 84.87%, f1 0.8490, precision 0.8473, recall 0.8508, auc 0.8487
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_2
./test_pima/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.5914814814814814

the Fscore is 0.5652173913043479

the precision is 0.4

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_2
----------------------



epoch 1, loss 0.6930, train acc 50.11%, f1 0.6677, precision 0.5011, recall 1.0000, auc 0.5000
epoch 101, loss 0.6103, train acc 78.92%, f1 0.7913, precision 0.7852, recall 0.7975, auc 0.7892
epoch 201, loss 0.4660, train acc 80.85%, f1 0.8088, precision 0.8092, recall 0.8084, auc 0.8085
epoch 301, loss 0.5121, train acc 82.01%, f1 0.8203, precision 0.8213, recall 0.8193, auc 0.8201
epoch 401, loss 0.3426, train acc 82.94%, f1 0.8296, precision 0.8307, recall 0.8284, auc 0.8294
epoch 501, loss 0.3248, train acc 83.41%, f1 0.8341, precision 0.8358, recall 0.8325, auc 0.8341
epoch 601, loss 0.4951, train acc 83.75%, f1 0.8377, precision 0.8389, recall 0.8364, auc 0.8375
epoch 701, loss 0.2975, train acc 83.95%, f1 0.8395, precision 0.8416, recall 0.8373, auc 0.8395
epoch 801, loss 0.3882, train acc 83.99%, f1 0.8401, precision 0.8409, recall 0.8394, auc 0.8399
epoch 901, loss 0.3280, train acc 84.10%, f1 0.8413, precision 0.8415, recall 0.8411, auc 0.8410
epoch 1001, loss 0.4044, train acc 84.12%, f1 0.8413, precision 0.8426, recall 0.8400, auc 0.8412
epoch 1101, loss 0.4506, train acc 84.12%, f1 0.8412, precision 0.8432, recall 0.8391, auc 0.8412
epoch 1201, loss 0.4231, train acc 84.12%, f1 0.8415, precision 0.8417, recall 0.8414, auc 0.8412
epoch 1301, loss 0.4475, train acc 84.19%, f1 0.8419, precision 0.8437, recall 0.8402, auc 0.8419
epoch 1401, loss 0.2887, train acc 84.19%, f1 0.8423, precision 0.8421, recall 0.8426, auc 0.8419
epoch 1501, loss 0.3192, train acc 84.12%, f1 0.8411, precision 0.8435, recall 0.8387, auc 0.8412
epoch 1601, loss 0.4083, train acc 84.21%, f1 0.8422, precision 0.8436, recall 0.8409, auc 0.8421
epoch 1701, loss 0.3781, train acc 84.17%, f1 0.8420, precision 0.8425, recall 0.8414, auc 0.8417
epoch 1801, loss 0.2403, train acc 84.18%, f1 0.8418, precision 0.8434, recall 0.8402, auc 0.8418
epoch 1901, loss 0.2638, train acc 84.18%, f1 0.8416, precision 0.8445, recall 0.8388, auc 0.8418
epoch 2001, loss 0.4216, train acc 84.18%, f1 0.8418, precision 0.8435, recall 0.8401, auc 0.8418
epoch 2101, loss 0.2674, train acc 84.14%, f1 0.8417, precision 0.8421, recall 0.8413, auc 0.8414
epoch 2201, loss 0.4523, train acc 84.14%, f1 0.8416, precision 0.8420, recall 0.8413, auc 0.8414
epoch 2301, loss 0.4159, train acc 84.16%, f1 0.8414, precision 0.8447, recall 0.8381, auc 0.8416
epoch 2401, loss 0.3313, train acc 84.14%, f1 0.8412, precision 0.8441, recall 0.8384, auc 0.8414
epoch 2501, loss 0.3856, train acc 84.13%, f1 0.8417, precision 0.8415, recall 0.8420, auc 0.8413
epoch 2601, loss 0.4722, train acc 84.22%, f1 0.8423, precision 0.8433, recall 0.8414, auc 0.8422
epoch 2701, loss 0.2960, train acc 84.21%, f1 0.8418, precision 0.8455, recall 0.8382, auc 0.8422
epoch 2801, loss 0.3714, train acc 84.20%, f1 0.8424, precision 0.8424, recall 0.8423, auc 0.8420
epoch 2901, loss 0.4555, train acc 84.18%, f1 0.8415, precision 0.8454, recall 0.8376, auc 0.8419
epoch 3001, loss 0.3411, train acc 84.19%, f1 0.8422, precision 0.8427, recall 0.8417, auc 0.8419
epoch 3101, loss 0.2738, train acc 84.13%, f1 0.8415, precision 0.8419, recall 0.8411, auc 0.8413
epoch 3201, loss 0.2580, train acc 84.18%, f1 0.8420, precision 0.8428, recall 0.8411, auc 0.8418
epoch 3301, loss 0.2734, train acc 84.16%, f1 0.8418, precision 0.8428, recall 0.8407, auc 0.8416
epoch 3401, loss 0.3133, train acc 84.19%, f1 0.8419, precision 0.8439, recall 0.8399, auc 0.8419
epoch 3501, loss 0.4283, train acc 84.15%, f1 0.8414, precision 0.8436, recall 0.8392, auc 0.8415
epoch 3601, loss 0.4583, train acc 84.14%, f1 0.8411, precision 0.8448, recall 0.8375, auc 0.8414
epoch 3701, loss 0.4135, train acc 84.13%, f1 0.8415, precision 0.8423, recall 0.8408, auc 0.8413
epoch 3801, loss 0.2997, train acc 84.19%, f1 0.8416, precision 0.8451, recall 0.8381, auc 0.8419
epoch 3901, loss 0.4527, train acc 84.21%, f1 0.8422, precision 0.8435, recall 0.8410, auc 0.8421
epoch 4001, loss 0.3576, train acc 84.25%, f1 0.8426, precision 0.8439, recall 0.8413, auc 0.8425
epoch 4101, loss 0.3296, train acc 84.27%, f1 0.8426, precision 0.8451, recall 0.8402, auc 0.8428
epoch 4201, loss 0.4558, train acc 84.28%, f1 0.8424, precision 0.8464, recall 0.8385, auc 0.8428
epoch 4301, loss 0.4016, train acc 84.27%, f1 0.8426, precision 0.8449, recall 0.8403, auc 0.8427
epoch 4401, loss 0.4548, train acc 84.27%, f1 0.8429, precision 0.8434, recall 0.8424, auc 0.8427
epoch 4501, loss 0.3028, train acc 84.30%, f1 0.8430, precision 0.8446, recall 0.8414, auc 0.8430
epoch 4601, loss 0.3291, train acc 84.30%, f1 0.8432, precision 0.8444, recall 0.8419, auc 0.8430
epoch 4701, loss 0.3793, train acc 84.33%, f1 0.8433, precision 0.8449, recall 0.8418, auc 0.8433
epoch 4801, loss 0.3289, train acc 84.33%, f1 0.8433, precision 0.8452, recall 0.8415, auc 0.8433
epoch 4901, loss 0.3822, train acc 84.36%, f1 0.8437, precision 0.8449, recall 0.8425, auc 0.8436
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_2
./test_pima/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.585

the Fscore is 0.5654450261780105

the precision is 0.39416058394160586

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_2
----------------------



epoch 1, loss 0.6932, train acc 49.90%, f1 0.6658, precision 0.4990, recall 1.0000, auc 0.5000
epoch 101, loss 0.6054, train acc 78.33%, f1 0.7718, precision 0.8134, recall 0.7343, auc 0.7832
epoch 201, loss 0.5179, train acc 80.64%, f1 0.8052, precision 0.8085, recall 0.8019, auc 0.8064
epoch 301, loss 0.3470, train acc 82.11%, f1 0.8210, precision 0.8199, recall 0.8221, auc 0.8211
epoch 401, loss 0.3204, train acc 82.94%, f1 0.8295, precision 0.8274, recall 0.8315, auc 0.8294
epoch 501, loss 0.4688, train acc 83.50%, f1 0.8351, precision 0.8331, recall 0.8372, auc 0.8350
epoch 601, loss 0.5055, train acc 83.69%, f1 0.8369, precision 0.8354, recall 0.8383, auc 0.8369
epoch 701, loss 0.3743, train acc 83.88%, f1 0.8390, precision 0.8364, recall 0.8416, auc 0.8388
epoch 801, loss 0.4355, train acc 84.00%, f1 0.8401, precision 0.8381, recall 0.8421, auc 0.8401
epoch 901, loss 0.4495, train acc 84.03%, f1 0.8402, precision 0.8392, recall 0.8412, auc 0.8403
epoch 1001, loss 0.3288, train acc 84.09%, f1 0.8409, precision 0.8391, recall 0.8428, auc 0.8409
epoch 1101, loss 0.3578, train acc 84.13%, f1 0.8416, precision 0.8384, recall 0.8448, auc 0.8413
epoch 1201, loss 0.4784, train acc 84.18%, f1 0.8420, precision 0.8393, recall 0.8447, auc 0.8418
epoch 1301, loss 0.3841, train acc 84.21%, f1 0.8424, precision 0.8390, recall 0.8458, auc 0.8421
epoch 1401, loss 0.4961, train acc 84.15%, f1 0.8415, precision 0.8400, recall 0.8429, auc 0.8415
epoch 1501, loss 0.3294, train acc 84.22%, f1 0.8422, precision 0.8407, recall 0.8437, auc 0.8422
epoch 1601, loss 0.4604, train acc 84.17%, f1 0.8415, precision 0.8411, recall 0.8418, auc 0.8417
epoch 1701, loss 0.4663, train acc 84.11%, f1 0.8412, precision 0.8394, recall 0.8429, auc 0.8411
epoch 1801, loss 0.3314, train acc 84.14%, f1 0.8413, precision 0.8400, recall 0.8425, auc 0.8414
epoch 1901, loss 0.3879, train acc 84.13%, f1 0.8406, precision 0.8426, recall 0.8387, auc 0.8413
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_2
./test_pima/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.575

the Fscore is 0.5595854922279793

the precision is 0.38848920863309355

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_2
----------------------



epoch 1, loss 0.6953, train acc 34.85%, f1 0.5169, precision 0.3485, recall 1.0000, auc 0.5000
epoch 101, loss 0.6239, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5960, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5699, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5467, train acc 66.94%, f1 0.1362, precision 0.7619, recall 0.0748, auc 0.5311
epoch 501, loss 0.5281, train acc 70.68%, f1 0.3382, precision 0.7931, recall 0.2150, auc 0.5925
epoch 601, loss 0.5139, train acc 73.94%, f1 0.5031, precision 0.7500, recall 0.3785, auc 0.6555
epoch 701, loss 0.5033, train acc 74.92%, f1 0.5575, precision 0.7239, recall 0.4533, auc 0.6804
epoch 801, loss 0.4953, train acc 74.76%, f1 0.5753, precision 0.6954, recall 0.4907, auc 0.6878
epoch 901, loss 0.4894, train acc 75.90%, f1 0.6126, precision 0.6964, recall 0.5467, auc 0.7096
epoch 1001, loss 0.4848, train acc 76.06%, f1 0.6240, precision 0.6893, recall 0.5701, auc 0.7163
epoch 1101, loss 0.4810, train acc 76.22%, f1 0.6294, precision 0.6889, recall 0.5794, auc 0.7197
epoch 1201, loss 0.4774, train acc 76.38%, f1 0.6329, precision 0.6906, recall 0.5841, auc 0.7221
epoch 1301, loss 0.4735, train acc 77.04%, f1 0.6448, precision 0.6995, recall 0.5981, auc 0.7303
epoch 1401, loss 0.4690, train acc 78.01%, f1 0.6547, precision 0.7232, recall 0.5981, auc 0.7378
epoch 1501, loss 0.4646, train acc 78.01%, f1 0.6565, precision 0.7207, recall 0.6028, auc 0.7389
epoch 1601, loss 0.4605, train acc 78.18%, f1 0.6564, precision 0.7273, recall 0.5981, auc 0.7391
epoch 1701, loss 0.4569, train acc 78.01%, f1 0.6565, precision 0.7207, recall 0.6028, auc 0.7389
epoch 1801, loss 0.4538, train acc 78.34%, f1 0.6633, precision 0.7238, recall 0.6121, auc 0.7436
epoch 1901, loss 0.4512, train acc 78.01%, f1 0.6599, precision 0.7158, recall 0.6121, auc 0.7411
epoch 2001, loss 0.4489, train acc 78.34%, f1 0.6667, precision 0.7189, recall 0.6215, auc 0.7457
epoch 2101, loss 0.4467, train acc 78.50%, f1 0.6683, precision 0.7228, recall 0.6215, auc 0.7470
epoch 2201, loss 0.4444, train acc 78.34%, f1 0.6650, precision 0.7213, recall 0.6168, auc 0.7447
epoch 2301, loss 0.4423, train acc 78.66%, f1 0.6700, precision 0.7268, recall 0.6215, auc 0.7482
epoch 2401, loss 0.4402, train acc 78.99%, f1 0.6767, precision 0.7297, recall 0.6308, auc 0.7529
epoch 2501, loss 0.4383, train acc 78.83%, f1 0.6750, precision 0.7258, recall 0.6308, auc 0.7517
epoch 2601, loss 0.4366, train acc 78.99%, f1 0.6783, precision 0.7273, recall 0.6355, auc 0.7540
epoch 2701, loss 0.4349, train acc 78.83%, f1 0.6750, precision 0.7258, recall 0.6308, auc 0.7517
epoch 2801, loss 0.4334, train acc 79.15%, f1 0.6784, precision 0.7337, recall 0.6308, auc 0.7542
epoch 2901, loss 0.4318, train acc 79.15%, f1 0.6784, precision 0.7337, recall 0.6308, auc 0.7542
epoch 3001, loss 0.4302, train acc 79.48%, f1 0.6818, precision 0.7418, recall 0.6308, auc 0.7567
epoch 3101, loss 0.4286, train acc 79.32%, f1 0.6817, precision 0.7351, recall 0.6355, auc 0.7565
epoch 3201, loss 0.4269, train acc 79.48%, f1 0.6850, precision 0.7366, recall 0.6402, auc 0.7588
epoch 3301, loss 0.4251, train acc 79.48%, f1 0.6850, precision 0.7366, recall 0.6402, auc 0.7588
epoch 3401, loss 0.4232, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 3501, loss 0.4214, train acc 79.80%, f1 0.6931, precision 0.7368, recall 0.6542, auc 0.7646
epoch 3601, loss 0.4194, train acc 79.64%, f1 0.6898, precision 0.7354, recall 0.6495, auc 0.7623
epoch 3701, loss 0.4170, train acc 79.64%, f1 0.6898, precision 0.7354, recall 0.6495, auc 0.7623
epoch 3801, loss 0.4144, train acc 79.32%, f1 0.6849, precision 0.7302, recall 0.6449, auc 0.7587
epoch 3901, loss 0.4115, train acc 79.48%, f1 0.6881, precision 0.7316, recall 0.6495, auc 0.7610
epoch 4001, loss 0.4088, train acc 79.80%, f1 0.6931, precision 0.7368, recall 0.6542, auc 0.7646
epoch 4101, loss 0.4061, train acc 80.29%, f1 0.7027, precision 0.7409, recall 0.6682, auc 0.7716
epoch 4201, loss 0.4034, train acc 80.46%, f1 0.7059, precision 0.7423, recall 0.6729, auc 0.7739
epoch 4301, loss 0.4007, train acc 80.78%, f1 0.7094, precision 0.7500, recall 0.6729, auc 0.7764
epoch 4401, loss 0.3981, train acc 80.94%, f1 0.7125, precision 0.7513, recall 0.6776, auc 0.7788
epoch 4501, loss 0.3955, train acc 80.94%, f1 0.7139, precision 0.7487, recall 0.6822, auc 0.7799
epoch 4601, loss 0.3927, train acc 81.11%, f1 0.7171, precision 0.7500, recall 0.6869, auc 0.7822
epoch 4701, loss 0.3900, train acc 81.11%, f1 0.7171, precision 0.7500, recall 0.6869, auc 0.7822
epoch 4801, loss 0.3870, train acc 81.27%, f1 0.7215, precision 0.7487, recall 0.6963, auc 0.7856
epoch 4901, loss 0.3844, train acc 81.60%, f1 0.7251, precision 0.7563, recall 0.6963, auc 0.7881
epoch 5001, loss 0.3820, train acc 81.43%, f1 0.7220, precision 0.7551, recall 0.6916, auc 0.7858
epoch 5101, loss 0.3798, train acc 81.60%, f1 0.7237, precision 0.7590, recall 0.6916, auc 0.7870
epoch 5201, loss 0.3778, train acc 81.92%, f1 0.7286, precision 0.7641, recall 0.6963, auc 0.7906
epoch 5301, loss 0.3759, train acc 81.92%, f1 0.7286, precision 0.7641, recall 0.6963, auc 0.7906
epoch 5401, loss 0.3742, train acc 81.92%, f1 0.7286, precision 0.7641, recall 0.6963, auc 0.7906
epoch 5501, loss 0.3726, train acc 82.08%, f1 0.7304, precision 0.7680, recall 0.6963, auc 0.7919
epoch 5601, loss 0.3710, train acc 82.08%, f1 0.7317, precision 0.7653, recall 0.7009, auc 0.7930
epoch 5701, loss 0.3692, train acc 82.25%, f1 0.7335, precision 0.7692, recall 0.7009, auc 0.7942
epoch 5801, loss 0.3669, train acc 82.25%, f1 0.7335, precision 0.7692, recall 0.7009, auc 0.7942
epoch 5901, loss 0.3639, train acc 82.41%, f1 0.7366, precision 0.7704, recall 0.7056, auc 0.7966
epoch 6001, loss 0.3613, train acc 82.41%, f1 0.7366, precision 0.7704, recall 0.7056, auc 0.7966
epoch 6101, loss 0.3587, train acc 83.06%, f1 0.7463, precision 0.7806, recall 0.7150, auc 0.8037
epoch 6201, loss 0.3553, train acc 83.39%, f1 0.7512, precision 0.7857, recall 0.7196, auc 0.8073
epoch 6301, loss 0.3511, train acc 83.88%, f1 0.7568, precision 0.7979, recall 0.7196, auc 0.8111
epoch 6401, loss 0.3465, train acc 84.04%, f1 0.7598, precision 0.7990, recall 0.7243, auc 0.8134
epoch 6501, loss 0.3414, train acc 84.36%, f1 0.7647, precision 0.8041, recall 0.7290, auc 0.8170
epoch 6601, loss 0.3365, train acc 84.85%, f1 0.7748, precision 0.8040, recall 0.7477, auc 0.8251
epoch 6701, loss 0.3323, train acc 85.02%, f1 0.7778, precision 0.8050, recall 0.7523, auc 0.8274
epoch 6801, loss 0.3285, train acc 85.50%, f1 0.7855, precision 0.8109, recall 0.7617, auc 0.8333
epoch 6901, loss 0.3248, train acc 85.50%, f1 0.7866, precision 0.8079, recall 0.7664, auc 0.8344
epoch 7001, loss 0.3211, train acc 85.67%, f1 0.7885, precision 0.8119, recall 0.7664, auc 0.8357
epoch 7101, loss 0.3176, train acc 86.16%, f1 0.7952, precision 0.8209, recall 0.7710, auc 0.8405
epoch 7201, loss 0.3144, train acc 86.48%, f1 0.8010, precision 0.8227, recall 0.7804, auc 0.8452
epoch 7301, loss 0.3113, train acc 86.81%, f1 0.8058, precision 0.8276, recall 0.7850, auc 0.8488
epoch 7401, loss 0.3085, train acc 86.97%, f1 0.8068, precision 0.8350, recall 0.7804, auc 0.8489
epoch 7501, loss 0.3060, train acc 86.81%, f1 0.8039, precision 0.8342, recall 0.7757, auc 0.8466
epoch 7601, loss 0.3034, train acc 86.81%, f1 0.8048, precision 0.8308, recall 0.7804, auc 0.8477
epoch 7701, loss 0.3009, train acc 87.30%, f1 0.8125, precision 0.8366, recall 0.7897, auc 0.8536
epoch 7801, loss 0.2985, train acc 87.13%, f1 0.8106, precision 0.8325, recall 0.7897, auc 0.8524
epoch 7901, loss 0.2961, train acc 87.30%, f1 0.8134, precision 0.8333, recall 0.7944, auc 0.8547
epoch 8001, loss 0.2936, train acc 87.46%, f1 0.8162, precision 0.8341, recall 0.7991, auc 0.8570
epoch 8101, loss 0.2909, train acc 87.46%, f1 0.8171, precision 0.8309, recall 0.8037, auc 0.8581
epoch 8201, loss 0.2885, train acc 87.95%, f1 0.8255, precision 0.8333, recall 0.8178, auc 0.8651
epoch 8301, loss 0.2860, train acc 87.79%, f1 0.8219, precision 0.8357, recall 0.8084, auc 0.8617
epoch 8401, loss 0.2834, train acc 88.11%, f1 0.8258, precision 0.8439, recall 0.8084, auc 0.8642
epoch 8501, loss 0.2811, train acc 88.11%, f1 0.8258, precision 0.8439, recall 0.8084, auc 0.8642
epoch 8601, loss 0.2789, train acc 88.11%, f1 0.8258, precision 0.8439, recall 0.8084, auc 0.8642
epoch 8701, loss 0.2766, train acc 88.44%, f1 0.8305, precision 0.8488, recall 0.8131, auc 0.8678
epoch 8801, loss 0.2745, train acc 88.60%, f1 0.8333, precision 0.8495, recall 0.8178, auc 0.8701
epoch 8901, loss 0.2727, train acc 88.93%, f1 0.8389, precision 0.8510, recall 0.8271, auc 0.8748
epoch 9001, loss 0.2710, train acc 89.09%, f1 0.8416, precision 0.8517, recall 0.8318, auc 0.8771
epoch 9101, loss 0.2693, train acc 89.09%, f1 0.8416, precision 0.8517, recall 0.8318, auc 0.8771
epoch 9201, loss 0.2677, train acc 89.25%, f1 0.8443, precision 0.8524, recall 0.8364, auc 0.8795
epoch 9301, loss 0.2661, train acc 89.41%, f1 0.8471, precision 0.8531, recall 0.8411, auc 0.8818
epoch 9401, loss 0.2646, train acc 89.41%, f1 0.8471, precision 0.8531, recall 0.8411, auc 0.8818
epoch 9501, loss 0.2622, train acc 89.58%, f1 0.8498, precision 0.8538, recall 0.8458, auc 0.8841
epoch 9601, loss 0.2605, train acc 89.74%, f1 0.8525, precision 0.8545, recall 0.8505, auc 0.8865
epoch 9701, loss 0.2589, train acc 89.74%, f1 0.8525, precision 0.8545, recall 0.8505, auc 0.8865
epoch 9801, loss 0.2574, train acc 89.74%, f1 0.8525, precision 0.8545, recall 0.8505, auc 0.8865
epoch 9901, loss 0.2558, train acc 89.90%, f1 0.8551, precision 0.8551, recall 0.8551, auc 0.8888
epoch 10001, loss 0.2544, train acc 90.39%, f1 0.8618, precision 0.8638, recall 0.8598, auc 0.8937
epoch 10101, loss 0.2528, train acc 90.55%, f1 0.8638, precision 0.8679, recall 0.8598, auc 0.8949
epoch 10201, loss 0.2512, train acc 90.72%, f1 0.8659, precision 0.8720, recall 0.8598, auc 0.8962
epoch 10301, loss 0.2497, train acc 90.88%, f1 0.8679, precision 0.8762, recall 0.8598, auc 0.8974
epoch 10401, loss 0.2483, train acc 91.21%, f1 0.8720, precision 0.8846, recall 0.8598, auc 0.8999
epoch 10501, loss 0.2468, train acc 91.21%, f1 0.8720, precision 0.8846, recall 0.8598, auc 0.8999
epoch 10601, loss 0.2454, train acc 91.37%, f1 0.8747, precision 0.8852, recall 0.8645, auc 0.9022
epoch 10701, loss 0.2440, train acc 91.69%, f1 0.8806, precision 0.8826, recall 0.8785, auc 0.9080
epoch 10801, loss 0.2426, train acc 91.69%, f1 0.8806, precision 0.8826, recall 0.8785, auc 0.9080
epoch 10901, loss 0.2413, train acc 91.86%, f1 0.8832, precision 0.8832, recall 0.8832, auc 0.9103
epoch 11001, loss 0.2401, train acc 91.86%, f1 0.8832, precision 0.8832, recall 0.8832, auc 0.9103
epoch 11101, loss 0.2388, train acc 91.86%, f1 0.8832, precision 0.8832, recall 0.8832, auc 0.9103
epoch 11201, loss 0.2376, train acc 91.86%, f1 0.8832, precision 0.8832, recall 0.8832, auc 0.9103
epoch 11301, loss 0.2365, train acc 91.69%, f1 0.8811, precision 0.8791, recall 0.8832, auc 0.9091
epoch 11401, loss 0.2353, train acc 91.69%, f1 0.8811, precision 0.8791, recall 0.8832, auc 0.9091
epoch 11501, loss 0.2342, train acc 91.69%, f1 0.8811, precision 0.8791, recall 0.8832, auc 0.9091
epoch 11601, loss 0.2331, train acc 91.69%, f1 0.8811, precision 0.8791, recall 0.8832, auc 0.9091
epoch 11701, loss 0.2321, train acc 91.69%, f1 0.8811, precision 0.8791, recall 0.8832, auc 0.9091
epoch 11801, loss 0.2310, train acc 91.53%, f1 0.8791, precision 0.8750, recall 0.8832, auc 0.9078
epoch 11901, loss 0.2300, train acc 91.69%, f1 0.8811, precision 0.8791, recall 0.8832, auc 0.9091
epoch 12001, loss 0.2290, train acc 91.86%, f1 0.8837, precision 0.8796, recall 0.8879, auc 0.9114
epoch 12101, loss 0.2280, train acc 91.69%, f1 0.8817, precision 0.8756, recall 0.8879, auc 0.9102
epoch 12201, loss 0.2271, train acc 92.02%, f1 0.8868, precision 0.8767, recall 0.8972, auc 0.9148
epoch 12301, loss 0.2262, train acc 92.02%, f1 0.8868, precision 0.8767, recall 0.8972, auc 0.9148
epoch 12401, loss 0.2253, train acc 91.86%, f1 0.8848, precision 0.8727, recall 0.8972, auc 0.9136
epoch 12501, loss 0.2244, train acc 91.86%, f1 0.8848, precision 0.8727, recall 0.8972, auc 0.9136
epoch 12601, loss 0.2235, train acc 91.86%, f1 0.8848, precision 0.8727, recall 0.8972, auc 0.9136
epoch 12701, loss 0.2227, train acc 91.86%, f1 0.8848, precision 0.8727, recall 0.8972, auc 0.9136
epoch 12801, loss 0.2218, train acc 92.18%, f1 0.8894, precision 0.8773, recall 0.9019, auc 0.9172
epoch 12901, loss 0.2210, train acc 92.18%, f1 0.8894, precision 0.8773, recall 0.9019, auc 0.9172
epoch 13001, loss 0.2202, train acc 92.18%, f1 0.8894, precision 0.8773, recall 0.9019, auc 0.9172
epoch 13101, loss 0.2193, train acc 92.35%, f1 0.8920, precision 0.8778, recall 0.9065, auc 0.9195
epoch 13201, loss 0.2185, train acc 92.35%, f1 0.8920, precision 0.8778, recall 0.9065, auc 0.9195
epoch 13301, loss 0.2177, train acc 92.35%, f1 0.8920, precision 0.8778, recall 0.9065, auc 0.9195
epoch 13401, loss 0.2169, train acc 92.35%, f1 0.8920, precision 0.8778, recall 0.9065, auc 0.9195
epoch 13501, loss 0.2161, train acc 92.35%, f1 0.8920, precision 0.8778, recall 0.9065, auc 0.9195
epoch 13601, loss 0.2154, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 13701, loss 0.2146, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 13801, loss 0.2139, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 13901, loss 0.2132, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 14001, loss 0.2125, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 14101, loss 0.2118, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 14201, loss 0.2111, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 14301, loss 0.2105, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 14401, loss 0.2098, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 14501, loss 0.2092, train acc 92.51%, f1 0.8940, precision 0.8818, recall 0.9065, auc 0.9208
epoch 14601, loss 0.2085, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 14701, loss 0.2079, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 14801, loss 0.2073, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 14901, loss 0.2067, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 15001, loss 0.2061, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 15101, loss 0.2055, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 15201, loss 0.2049, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 15301, loss 0.2044, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 15401, loss 0.2038, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 15501, loss 0.2033, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 15601, loss 0.2027, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 15701, loss 0.2022, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 15801, loss 0.2017, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 15901, loss 0.2012, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 16001, loss 0.2007, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 16101, loss 0.2002, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 16201, loss 0.1997, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 16301, loss 0.1992, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 16401, loss 0.1987, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 16501, loss 0.1983, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.1978, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 16701, loss 0.1973, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 16801, loss 0.1969, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 16901, loss 0.1965, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 17001, loss 0.1960, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 17101, loss 0.1956, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 17201, loss 0.1952, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 17301, loss 0.1948, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 17401, loss 0.1943, train acc 93.00%, f1 0.9002, precision 0.8940, recall 0.9065, auc 0.9245
epoch 17501, loss 0.1939, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 17601, loss 0.1935, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 17701, loss 0.1931, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 17801, loss 0.1928, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 17901, loss 0.1924, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 18001, loss 0.1920, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 18101, loss 0.1916, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 18201, loss 0.1912, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 18301, loss 0.1909, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 18401, loss 0.1905, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 18501, loss 0.1901, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 18601, loss 0.1898, train acc 93.00%, f1 0.9002, precision 0.8940, recall 0.9065, auc 0.9245
epoch 18701, loss 0.1894, train acc 93.00%, f1 0.9002, precision 0.8940, recall 0.9065, auc 0.9245
epoch 18801, loss 0.1891, train acc 93.00%, f1 0.9002, precision 0.8940, recall 0.9065, auc 0.9245
epoch 18901, loss 0.1887, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 19001, loss 0.1884, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 19101, loss 0.1881, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 19201, loss 0.1877, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 19301, loss 0.1874, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 19401, loss 0.1871, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 19501, loss 0.1868, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 19601, loss 0.1865, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 19701, loss 0.1862, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 19801, loss 0.1859, train acc 93.32%, f1 0.9053, precision 0.8950, recall 0.9159, auc 0.9292
epoch 19901, loss 0.1856, train acc 93.32%, f1 0.9053, precision 0.8950, recall 0.9159, auc 0.9292
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_normal_20000
normal
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_2
./test_pima/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.6727777777777778

the Fscore is 0.5714285714285715

the precision is 0.5882352941176471

the recall is 0.5555555555555556

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_2
----------------------



epoch 1, loss 0.6958, train acc 34.85%, f1 0.5169, precision 0.3485, recall 1.0000, auc 0.5000
epoch 101, loss 0.6237, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5958, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5699, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5467, train acc 66.94%, f1 0.1362, precision 0.7619, recall 0.0748, auc 0.5311
epoch 501, loss 0.5282, train acc 70.68%, f1 0.3382, precision 0.7931, recall 0.2150, auc 0.5925
epoch 601, loss 0.5140, train acc 73.94%, f1 0.5031, precision 0.7500, recall 0.3785, auc 0.6555
epoch 701, loss 0.5033, train acc 74.92%, f1 0.5575, precision 0.7239, recall 0.4533, auc 0.6804
epoch 801, loss 0.4954, train acc 74.92%, f1 0.5769, precision 0.7000, recall 0.4907, auc 0.6891
epoch 901, loss 0.4894, train acc 75.73%, f1 0.6110, precision 0.6923, recall 0.5467, auc 0.7084
epoch 1001, loss 0.4848, train acc 76.06%, f1 0.6240, precision 0.6893, recall 0.5701, auc 0.7163
epoch 1101, loss 0.4811, train acc 76.22%, f1 0.6294, precision 0.6889, recall 0.5794, auc 0.7197
epoch 1201, loss 0.4775, train acc 76.38%, f1 0.6329, precision 0.6906, recall 0.5841, auc 0.7221
epoch 1301, loss 0.4736, train acc 77.04%, f1 0.6448, precision 0.6995, recall 0.5981, auc 0.7303
epoch 1401, loss 0.4691, train acc 77.85%, f1 0.6531, precision 0.7191, recall 0.5981, auc 0.7366
epoch 1501, loss 0.4647, train acc 77.85%, f1 0.6531, precision 0.7191, recall 0.5981, auc 0.7366
epoch 1601, loss 0.4606, train acc 78.18%, f1 0.6564, precision 0.7273, recall 0.5981, auc 0.7391
epoch 1701, loss 0.4571, train acc 78.01%, f1 0.6565, precision 0.7207, recall 0.6028, auc 0.7389
epoch 1801, loss 0.4541, train acc 78.18%, f1 0.6616, precision 0.7198, recall 0.6121, auc 0.7423
epoch 1901, loss 0.4515, train acc 78.18%, f1 0.6616, precision 0.7198, recall 0.6121, auc 0.7423
epoch 2001, loss 0.4493, train acc 78.34%, f1 0.6667, precision 0.7189, recall 0.6215, auc 0.7457
epoch 2101, loss 0.4472, train acc 78.66%, f1 0.6717, precision 0.7243, recall 0.6262, auc 0.7493
epoch 2201, loss 0.4451, train acc 78.50%, f1 0.6683, precision 0.7228, recall 0.6215, auc 0.7470
epoch 2301, loss 0.4429, train acc 78.66%, f1 0.6700, precision 0.7268, recall 0.6215, auc 0.7482
epoch 2401, loss 0.4408, train acc 78.83%, f1 0.6734, precision 0.7283, recall 0.6262, auc 0.7506
epoch 2501, loss 0.4387, train acc 78.99%, f1 0.6767, precision 0.7297, recall 0.6308, auc 0.7529
epoch 2601, loss 0.4367, train acc 79.15%, f1 0.6800, precision 0.7312, recall 0.6355, auc 0.7553
epoch 2701, loss 0.4350, train acc 78.99%, f1 0.6767, precision 0.7297, recall 0.6308, auc 0.7529
epoch 2801, loss 0.4335, train acc 78.99%, f1 0.6767, precision 0.7297, recall 0.6308, auc 0.7529
epoch 2901, loss 0.4321, train acc 78.99%, f1 0.6767, precision 0.7297, recall 0.6308, auc 0.7529
epoch 3001, loss 0.4307, train acc 78.99%, f1 0.6767, precision 0.7297, recall 0.6308, auc 0.7529
epoch 3101, loss 0.4294, train acc 79.48%, f1 0.6834, precision 0.7391, recall 0.6355, auc 0.7578
epoch 3201, loss 0.4281, train acc 79.32%, f1 0.6833, precision 0.7326, recall 0.6402, auc 0.7576
epoch 3301, loss 0.4268, train acc 79.48%, f1 0.6866, precision 0.7340, recall 0.6449, auc 0.7599
epoch 3401, loss 0.4255, train acc 79.32%, f1 0.6849, precision 0.7302, recall 0.6449, auc 0.7587
epoch 3501, loss 0.4241, train acc 79.32%, f1 0.6849, precision 0.7302, recall 0.6449, auc 0.7587
epoch 3601, loss 0.4226, train acc 79.32%, f1 0.6849, precision 0.7302, recall 0.6449, auc 0.7587
epoch 3701, loss 0.4209, train acc 79.64%, f1 0.6914, precision 0.7330, recall 0.6542, auc 0.7634
epoch 3801, loss 0.4192, train acc 79.48%, f1 0.6866, precision 0.7340, recall 0.6449, auc 0.7599
epoch 3901, loss 0.4176, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 4001, loss 0.4159, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 4101, loss 0.4142, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 4201, loss 0.4125, train acc 79.32%, f1 0.6833, precision 0.7326, recall 0.6402, auc 0.7576
epoch 4301, loss 0.4109, train acc 79.48%, f1 0.6850, precision 0.7366, recall 0.6402, auc 0.7588
epoch 4401, loss 0.4091, train acc 79.64%, f1 0.6898, precision 0.7354, recall 0.6495, auc 0.7623
epoch 4501, loss 0.4073, train acc 79.97%, f1 0.6948, precision 0.7407, recall 0.6542, auc 0.7659
epoch 4601, loss 0.4055, train acc 80.13%, f1 0.6980, precision 0.7421, recall 0.6589, auc 0.7682
epoch 4701, loss 0.4032, train acc 80.46%, f1 0.7044, precision 0.7448, recall 0.6682, auc 0.7729
epoch 4801, loss 0.4000, train acc 80.46%, f1 0.7059, precision 0.7423, recall 0.6729, auc 0.7739
epoch 4901, loss 0.3964, train acc 80.62%, f1 0.7090, precision 0.7436, recall 0.6776, auc 0.7763
epoch 5001, loss 0.3926, train acc 80.78%, f1 0.7122, precision 0.7449, recall 0.6822, auc 0.7786
epoch 5101, loss 0.3890, train acc 81.27%, f1 0.7202, precision 0.7513, recall 0.6916, auc 0.7845
epoch 5201, loss 0.3851, train acc 81.11%, f1 0.7184, precision 0.7475, recall 0.6916, auc 0.7833
epoch 5301, loss 0.3813, train acc 81.76%, f1 0.7268, precision 0.7602, recall 0.6963, auc 0.7894
epoch 5401, loss 0.3776, train acc 82.25%, f1 0.7335, precision 0.7692, recall 0.7009, auc 0.7942
epoch 5501, loss 0.3739, train acc 82.57%, f1 0.7409, precision 0.7688, recall 0.7150, auc 0.8000
epoch 5601, loss 0.3703, train acc 82.90%, f1 0.7470, precision 0.7711, recall 0.7243, auc 0.8046
epoch 5701, loss 0.3665, train acc 83.71%, f1 0.7585, precision 0.7850, recall 0.7336, auc 0.8131
epoch 5801, loss 0.3628, train acc 84.04%, f1 0.7633, precision 0.7900, recall 0.7383, auc 0.8167
epoch 5901, loss 0.3595, train acc 84.36%, f1 0.7692, precision 0.7921, recall 0.7477, auc 0.8213
epoch 6001, loss 0.3565, train acc 84.36%, f1 0.7681, precision 0.7950, recall 0.7430, auc 0.8202
epoch 6101, loss 0.3529, train acc 84.20%, f1 0.7651, precision 0.7940, recall 0.7383, auc 0.8179
epoch 6201, loss 0.3486, train acc 83.88%, f1 0.7591, precision 0.7919, recall 0.7290, auc 0.8132
epoch 6301, loss 0.3442, train acc 84.53%, f1 0.7700, precision 0.7990, recall 0.7430, auc 0.8215
epoch 6401, loss 0.3387, train acc 85.50%, f1 0.7845, precision 0.8141, recall 0.7570, auc 0.8323
epoch 6501, loss 0.3323, train acc 85.50%, f1 0.7835, precision 0.8173, recall 0.7523, auc 0.8312
epoch 6601, loss 0.3266, train acc 85.50%, f1 0.7845, precision 0.8141, recall 0.7570, auc 0.8323
epoch 6701, loss 0.3213, train acc 85.99%, f1 0.7913, precision 0.8232, recall 0.7617, auc 0.8371
epoch 6801, loss 0.3154, train acc 85.83%, f1 0.7883, precision 0.8223, recall 0.7570, auc 0.8348
epoch 6901, loss 0.3093, train acc 86.32%, f1 0.7971, precision 0.8250, recall 0.7710, auc 0.8418
epoch 7001, loss 0.3036, train acc 86.97%, f1 0.8086, precision 0.8284, recall 0.7897, auc 0.8511
epoch 7101, loss 0.2984, train acc 86.97%, f1 0.8095, precision 0.8252, recall 0.7944, auc 0.8522
epoch 7201, loss 0.2936, train acc 87.62%, f1 0.8190, precision 0.8350, recall 0.8037, auc 0.8594
epoch 7301, loss 0.2888, train acc 88.27%, f1 0.8278, precision 0.8480, recall 0.8084, auc 0.8655
epoch 7401, loss 0.2850, train acc 88.60%, f1 0.8325, precision 0.8529, recall 0.8131, auc 0.8690
epoch 7501, loss 0.2817, train acc 88.60%, f1 0.8333, precision 0.8495, recall 0.8178, auc 0.8701
epoch 7601, loss 0.2788, train acc 88.76%, f1 0.8361, precision 0.8502, recall 0.8224, auc 0.8725
epoch 7701, loss 0.2761, train acc 88.93%, f1 0.8381, precision 0.8544, recall 0.8224, auc 0.8737
epoch 7801, loss 0.2735, train acc 89.09%, f1 0.8401, precision 0.8585, recall 0.8224, auc 0.8750
epoch 7901, loss 0.2711, train acc 88.76%, f1 0.8345, precision 0.8571, recall 0.8131, auc 0.8703
epoch 8001, loss 0.2687, train acc 88.76%, f1 0.8345, precision 0.8571, recall 0.8131, auc 0.8703
epoch 8101, loss 0.2662, train acc 89.09%, f1 0.8393, precision 0.8621, recall 0.8178, auc 0.8739
epoch 8201, loss 0.2638, train acc 89.25%, f1 0.8413, precision 0.8663, recall 0.8178, auc 0.8751/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2616, train acc 89.41%, f1 0.8434, precision 0.8706, recall 0.8178, auc 0.8764
epoch 8401, loss 0.2594, train acc 89.41%, f1 0.8434, precision 0.8706, recall 0.8178, auc 0.8764
epoch 8501, loss 0.2574, train acc 89.58%, f1 0.8462, precision 0.8713, recall 0.8224, auc 0.8787
epoch 8601, loss 0.2554, train acc 90.39%, f1 0.8585, precision 0.8818, recall 0.8364, auc 0.8882
epoch 8701, loss 0.2535, train acc 90.07%, f1 0.8537, precision 0.8768, recall 0.8318, auc 0.8846
epoch 8801, loss 0.2516, train acc 90.07%, f1 0.8537, precision 0.8768, recall 0.8318, auc 0.8846
epoch 8901, loss 0.2498, train acc 90.39%, f1 0.8578, precision 0.8856, recall 0.8318, auc 0.8871
epoch 9001, loss 0.2480, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 9101, loss 0.2463, train acc 90.72%, f1 0.8640, precision 0.8829, recall 0.8458, auc 0.8929
epoch 9201, loss 0.2446, train acc 90.55%, f1 0.8612, precision 0.8824, recall 0.8411, auc 0.8906
epoch 9301, loss 0.2430, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 9401, loss 0.2413, train acc 90.39%, f1 0.8578, precision 0.8856, recall 0.8318, auc 0.8871
epoch 9501, loss 0.2397, train acc 90.55%, f1 0.8606, precision 0.8861, recall 0.8364, auc 0.8895
epoch 9601, loss 0.2381, train acc 90.55%, f1 0.8606, precision 0.8861, recall 0.8364, auc 0.8895
epoch 9701, loss 0.2365, train acc 90.39%, f1 0.8585, precision 0.8818, recall 0.8364, auc 0.8882
epoch 9801, loss 0.2349, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 9901, loss 0.2334, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 10001, loss 0.2320, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 10101, loss 0.2306, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 10201, loss 0.2292, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 10301, loss 0.2278, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 10401, loss 0.2265, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 10501, loss 0.2252, train acc 90.72%, f1 0.8633, precision 0.8867, recall 0.8411, auc 0.8918
epoch 10601, loss 0.2240, train acc 90.88%, f1 0.8654, precision 0.8911, recall 0.8411, auc 0.8931
epoch 10701, loss 0.2227, train acc 90.88%, f1 0.8654, precision 0.8911, recall 0.8411, auc 0.8931
epoch 10801, loss 0.2215, train acc 91.04%, f1 0.8675, precision 0.8955, recall 0.8411, auc 0.8943
epoch 10901, loss 0.2203, train acc 90.88%, f1 0.8654, precision 0.8911, recall 0.8411, auc 0.8931
epoch 11001, loss 0.2192, train acc 91.21%, f1 0.8702, precision 0.8960, recall 0.8458, auc 0.8966
epoch 11101, loss 0.2181, train acc 91.37%, f1 0.8729, precision 0.8966, recall 0.8505, auc 0.8990
epoch 11201, loss 0.2169, train acc 91.37%, f1 0.8729, precision 0.8966, recall 0.8505, auc 0.8990
epoch 11301, loss 0.2159, train acc 91.53%, f1 0.8756, precision 0.8971, recall 0.8551, auc 0.9013
epoch 11401, loss 0.2148, train acc 91.69%, f1 0.8783, precision 0.8976, recall 0.8598, auc 0.9037
epoch 11501, loss 0.2137, train acc 91.69%, f1 0.8783, precision 0.8976, recall 0.8598, auc 0.9037
epoch 11601, loss 0.2127, train acc 91.69%, f1 0.8789, precision 0.8937, recall 0.8645, auc 0.9047
epoch 11701, loss 0.2117, train acc 91.69%, f1 0.8789, precision 0.8937, recall 0.8645, auc 0.9047
epoch 11801, loss 0.2107, train acc 91.69%, f1 0.8789, precision 0.8937, recall 0.8645, auc 0.9047
epoch 11901, loss 0.2097, train acc 91.86%, f1 0.8810, precision 0.8981, recall 0.8645, auc 0.9060
epoch 12001, loss 0.2085, train acc 91.69%, f1 0.8789, precision 0.8937, recall 0.8645, auc 0.9047
epoch 12101, loss 0.2068, train acc 91.37%, f1 0.8747, precision 0.8852, recall 0.8645, auc 0.9022
epoch 12201, loss 0.2053, train acc 92.18%, f1 0.8863, precision 0.8990, recall 0.8738, auc 0.9107
epoch 12301, loss 0.2041, train acc 92.35%, f1 0.8884, precision 0.9034, recall 0.8738, auc 0.9119
epoch 12401, loss 0.2030, train acc 92.18%, f1 0.8863, precision 0.8990, recall 0.8738, auc 0.9107
epoch 12501, loss 0.2019, train acc 92.35%, f1 0.8889, precision 0.8995, recall 0.8785, auc 0.9130
epoch 12601, loss 0.2009, train acc 92.51%, f1 0.8915, precision 0.9000, recall 0.8832, auc 0.9153
epoch 12701, loss 0.1999, train acc 92.51%, f1 0.8915, precision 0.9000, recall 0.8832, auc 0.9153
epoch 12801, loss 0.1989, train acc 92.51%, f1 0.8915, precision 0.9000, recall 0.8832, auc 0.9153
epoch 12901, loss 0.1979, train acc 92.67%, f1 0.8941, precision 0.9005, recall 0.8879, auc 0.9177
epoch 13001, loss 0.1969, train acc 92.67%, f1 0.8941, precision 0.9005, recall 0.8879, auc 0.9177
epoch 13101, loss 0.1958, train acc 92.83%, f1 0.8967, precision 0.9009, recall 0.8925, auc 0.9200
epoch 13201, loss 0.1948, train acc 92.83%, f1 0.8967, precision 0.9009, recall 0.8925, auc 0.9200
epoch 13301, loss 0.1937, train acc 93.00%, f1 0.8988, precision 0.9052, recall 0.8925, auc 0.9213
epoch 13401, loss 0.1926, train acc 93.32%, f1 0.9035, precision 0.9100, recall 0.8972, auc 0.9248
epoch 13501, loss 0.1914, train acc 93.65%, f1 0.9087, precision 0.9108, recall 0.9065, auc 0.9295
epoch 13601, loss 0.1903, train acc 93.65%, f1 0.9087, precision 0.9108, recall 0.9065, auc 0.9295
epoch 13701, loss 0.1893, train acc 93.81%, f1 0.9112, precision 0.9112, recall 0.9112, auc 0.9319
epoch 13801, loss 0.1883, train acc 93.81%, f1 0.9112, precision 0.9112, recall 0.9112, auc 0.9319
epoch 13901, loss 0.1873, train acc 93.49%, f1 0.9061, precision 0.9104, recall 0.9019, auc 0.9272
epoch 14001, loss 0.1863, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 14101, loss 0.1848, train acc 93.00%, f1 0.8983, precision 0.9091, recall 0.8879, auc 0.9202
epoch 14201, loss 0.1832, train acc 93.16%, f1 0.9009, precision 0.9095, recall 0.8925, auc 0.9225
epoch 14301, loss 0.1820, train acc 93.16%, f1 0.9009, precision 0.9095, recall 0.8925, auc 0.9225
epoch 14401, loss 0.1808, train acc 93.32%, f1 0.9035, precision 0.9100, recall 0.8972, auc 0.9248
epoch 14501, loss 0.1797, train acc 93.16%, f1 0.9009, precision 0.9095, recall 0.8925, auc 0.9225
epoch 14601, loss 0.1782, train acc 93.49%, f1 0.9061, precision 0.9104, recall 0.9019, auc 0.9272
epoch 14701, loss 0.1758, train acc 93.65%, f1 0.9087, precision 0.9108, recall 0.9065, auc 0.9295
epoch 14801, loss 0.1731, train acc 93.81%, f1 0.9116, precision 0.9074, recall 0.9159, auc 0.9329
epoch 14901, loss 0.1710, train acc 93.81%, f1 0.9116, precision 0.9074, recall 0.9159, auc 0.9329
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_normal_15000
normal
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_2
./test_pima/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.6677777777777778

the Fscore is 0.5660377358490566

the precision is 0.5769230769230769

the recall is 0.5555555555555556

Done
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_2
----------------------



epoch 1, loss 0.6924, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6220, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5943, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5684, train acc 65.15%, f1 0.0093, precision 0.5000, recall 0.0047, auc 0.5011
epoch 401, loss 0.5453, train acc 67.10%, f1 0.1441, precision 0.7727, recall 0.0794, auc 0.5335
epoch 501, loss 0.5270, train acc 71.01%, f1 0.3504, precision 0.8000, recall 0.2243, auc 0.5971
epoch 601, loss 0.5130, train acc 74.43%, f1 0.5169, precision 0.7568, recall 0.3925, auc 0.6625
epoch 701, loss 0.5026, train acc 74.92%, f1 0.5600, precision 0.7206, recall 0.4579, auc 0.6815
epoch 801, loss 0.4948, train acc 74.59%, f1 0.5738, precision 0.6908, recall 0.4907, auc 0.6866
epoch 901, loss 0.4889, train acc 76.06%, f1 0.6182, precision 0.6959, recall 0.5561, auc 0.7130
epoch 1001, loss 0.4843, train acc 76.06%, f1 0.6240, precision 0.6893, recall 0.5701, auc 0.7163
epoch 1101, loss 0.4805, train acc 76.06%, f1 0.6260, precision 0.6872, recall 0.5748, auc 0.7174
epoch 1201, loss 0.4769, train acc 76.55%, f1 0.6345, precision 0.6944, recall 0.5841, auc 0.7233
epoch 1301, loss 0.4729, train acc 77.04%, f1 0.6448, precision 0.6995, recall 0.5981, auc 0.7303
epoch 1401, loss 0.4686, train acc 78.01%, f1 0.6547, precision 0.7232, recall 0.5981, auc 0.7378
epoch 1501, loss 0.4641, train acc 77.85%, f1 0.6531, precision 0.7191, recall 0.5981, auc 0.7366
epoch 1601, loss 0.4599, train acc 78.18%, f1 0.6564, precision 0.7273, recall 0.5981, auc 0.7391
epoch 1701, loss 0.4562, train acc 78.18%, f1 0.6582, precision 0.7247, recall 0.6028, auc 0.7402
epoch 1801, loss 0.4531, train acc 78.01%, f1 0.6599, precision 0.7158, recall 0.6121, auc 0.7411
epoch 1901, loss 0.4503, train acc 78.01%, f1 0.6599, precision 0.7158, recall 0.6121, auc 0.7411
epoch 2001, loss 0.4478, train acc 78.18%, f1 0.6633, precision 0.7174, recall 0.6168, auc 0.7434
epoch 2101, loss 0.4453, train acc 78.18%, f1 0.6633, precision 0.7174, recall 0.6168, auc 0.7434
epoch 2201, loss 0.4430, train acc 78.34%, f1 0.6650, precision 0.7213, recall 0.6168, auc 0.7447
epoch 2301, loss 0.4409, train acc 78.66%, f1 0.6700, precision 0.7268, recall 0.6215, auc 0.7482
epoch 2401, loss 0.4390, train acc 78.66%, f1 0.6717, precision 0.7243, recall 0.6262, auc 0.7493
epoch 2501, loss 0.4372, train acc 78.99%, f1 0.6783, precision 0.7273, recall 0.6355, auc 0.7540
epoch 2601, loss 0.4355, train acc 79.15%, f1 0.6784, precision 0.7337, recall 0.6308, auc 0.7542
epoch 2701, loss 0.4339, train acc 79.32%, f1 0.6801, precision 0.7377, recall 0.6308, auc 0.7554
epoch 2801, loss 0.4324, train acc 79.64%, f1 0.6851, precision 0.7432, recall 0.6355, auc 0.7590
epoch 2901, loss 0.4310, train acc 79.64%, f1 0.6851, precision 0.7432, recall 0.6355, auc 0.7590
epoch 3001, loss 0.4296, train acc 79.80%, f1 0.6884, precision 0.7446, recall 0.6402, auc 0.7613
epoch 3101, loss 0.4283, train acc 79.64%, f1 0.6867, precision 0.7405, recall 0.6402, auc 0.7601
epoch 3201, loss 0.4270, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 3301, loss 0.4256, train acc 79.32%, f1 0.6833, precision 0.7326, recall 0.6402, auc 0.7576
epoch 3401, loss 0.4244, train acc 79.48%, f1 0.6866, precision 0.7340, recall 0.6449, auc 0.7599
epoch 3501, loss 0.4231, train acc 79.48%, f1 0.6850, precision 0.7366, recall 0.6402, auc 0.7588
epoch 3601, loss 0.4218, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 3701, loss 0.4206, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 3801, loss 0.4192, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 3901, loss 0.4176, train acc 79.80%, f1 0.6900, precision 0.7419, recall 0.6449, auc 0.7624
epoch 4001, loss 0.4159, train acc 79.80%, f1 0.6900, precision 0.7419, recall 0.6449, auc 0.7624
epoch 4101, loss 0.4141, train acc 80.29%, f1 0.6983, precision 0.7487, recall 0.6542, auc 0.7684
epoch 4201, loss 0.4124, train acc 80.29%, f1 0.6998, precision 0.7460, recall 0.6589, auc 0.7694
epoch 4301, loss 0.4107, train acc 81.11%, f1 0.7129, precision 0.7579, recall 0.6729, auc 0.7789
epoch 4401, loss 0.4090, train acc 81.27%, f1 0.7146, precision 0.7619, recall 0.6729, auc 0.7802
epoch 4501, loss 0.4075, train acc 81.43%, f1 0.7178, precision 0.7632, recall 0.6776, auc 0.7825
epoch 4601, loss 0.4060, train acc 81.76%, f1 0.7241, precision 0.7656, recall 0.6869, auc 0.7872
epoch 4701, loss 0.4044, train acc 81.76%, f1 0.7241, precision 0.7656, recall 0.6869, auc 0.7872
epoch 4801, loss 0.4021, train acc 81.92%, f1 0.7273, precision 0.7668, recall 0.6916, auc 0.7895
epoch 4901, loss 0.3995, train acc 81.92%, f1 0.7286, precision 0.7641, recall 0.6963, auc 0.7906
epoch 5001, loss 0.3967, train acc 81.92%, f1 0.7286, precision 0.7641, recall 0.6963, auc 0.7906
epoch 5101, loss 0.3929, train acc 82.08%, f1 0.7317, precision 0.7653, recall 0.7009, auc 0.7930
epoch 5201, loss 0.3879, train acc 82.25%, f1 0.7335, precision 0.7692, recall 0.7009, auc 0.7942
epoch 5301, loss 0.3832, train acc 82.57%, f1 0.7384, precision 0.7744, recall 0.7056, auc 0.7978
epoch 5401, loss 0.3788, train acc 82.25%, f1 0.7335, precision 0.7692, recall 0.7009, auc 0.7942
epoch 5501, loss 0.3746, train acc 82.74%, f1 0.7402, precision 0.7784, recall 0.7056, auc 0.7991
epoch 5601, loss 0.3708, train acc 82.90%, f1 0.7420, precision 0.7824, recall 0.7056, auc 0.8003
epoch 5701, loss 0.3671, train acc 82.57%, f1 0.7358, precision 0.7801, recall 0.6963, auc 0.7956
epoch 5801, loss 0.3635, train acc 82.41%, f1 0.7340, precision 0.7760, recall 0.6963, auc 0.7944
epoch 5901, loss 0.3600, train acc 82.57%, f1 0.7384, precision 0.7744, recall 0.7056, auc 0.7978
epoch 6001, loss 0.3564, train acc 82.57%, f1 0.7371, precision 0.7772, recall 0.7009, auc 0.7967
epoch 6101, loss 0.3527, train acc 83.06%, f1 0.7426, precision 0.7895, recall 0.7009, auc 0.8005
epoch 6201, loss 0.3488, train acc 83.71%, f1 0.7549, precision 0.7938, recall 0.7196, auc 0.8098
epoch 6301, loss 0.3449, train acc 84.20%, f1 0.7640, precision 0.7970, recall 0.7336, auc 0.8168
epoch 6401, loss 0.3408, train acc 84.69%, f1 0.7718, precision 0.8030, recall 0.7430, auc 0.8227
epoch 6501, loss 0.3364, train acc 85.34%, f1 0.7816, precision 0.8131, recall 0.7523, auc 0.8299
epoch 6601, loss 0.3322, train acc 85.50%, f1 0.7845, precision 0.8141, recall 0.7570, auc 0.8323
epoch 6701, loss 0.3283, train acc 86.32%, f1 0.7971, precision 0.8250, recall 0.7710, auc 0.8418
epoch 6801, loss 0.3249, train acc 86.48%, f1 0.8010, precision 0.8227, recall 0.7804, auc 0.8452
epoch 6901, loss 0.3217, train acc 86.64%, f1 0.8038, precision 0.8235, recall 0.7850, auc 0.8475
epoch 7001, loss 0.3183, train acc 86.81%, f1 0.8076, precision 0.8213, recall 0.7944, auc 0.8509
epoch 7101, loss 0.3152, train acc 86.81%, f1 0.8085, precision 0.8182, recall 0.7991, auc 0.8520
epoch 7201, loss 0.3122, train acc 86.64%, f1 0.8066, precision 0.8143, recall 0.7991, auc 0.8508
epoch 7301, loss 0.3091, train acc 86.81%, f1 0.8103, precision 0.8122, recall 0.8084, auc 0.8542
epoch 7401, loss 0.3065, train acc 86.97%, f1 0.8131, precision 0.8131, recall 0.8131, auc 0.8565
epoch 7501, loss 0.3041, train acc 87.13%, f1 0.8150, precision 0.8169, recall 0.8131, auc 0.8578
epoch 7601, loss 0.3018, train acc 87.13%, f1 0.8150, precision 0.8169, recall 0.8131, auc 0.8578
epoch 7701, loss 0.2996, train acc 87.30%, f1 0.8178, precision 0.8178, recall 0.8178, auc 0.8601
epoch 7801, loss 0.2974, train acc 87.30%, f1 0.8178, precision 0.8178, recall 0.8178, auc 0.8601
epoch 7901, loss 0.2953, train acc 87.30%, f1 0.8160, precision 0.8238, recall 0.8084, auc 0.8580
epoch 8001, loss 0.2930, train acc 87.30%, f1 0.8169, precision 0.8208, recall 0.8131, auc 0.8590
epoch 8101, loss 0.2899, train acc 87.95%, f1 0.8255, precision 0.8333, recall 0.8178, auc 0.8651
epoch 8201, loss 0.2849, train acc 87.95%, f1 0.8271, precision 0.8271, recall 0.8271, auc 0.8673/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2811, train acc 88.27%, f1 0.8310, precision 0.8349, recall 0.8271, auc 0.8698
epoch 8401, loss 0.2768, train acc 88.93%, f1 0.8411, precision 0.8411, recall 0.8411, auc 0.8781
epoch 8501, loss 0.2721, train acc 89.09%, f1 0.8438, precision 0.8419, recall 0.8458, auc 0.8804
epoch 8601, loss 0.2691, train acc 89.25%, f1 0.8458, precision 0.8458, recall 0.8458, auc 0.8816
epoch 8701, loss 0.2661, train acc 89.41%, f1 0.8485, precision 0.8465, recall 0.8505, auc 0.8840
epoch 8801, loss 0.2634, train acc 89.58%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8852
epoch 8901, loss 0.2610, train acc 89.74%, f1 0.8531, precision 0.8512, recall 0.8551, auc 0.8876
epoch 9001, loss 0.2587, train acc 89.74%, f1 0.8538, precision 0.8479, recall 0.8598, auc 0.8887
epoch 9101, loss 0.2567, train acc 89.74%, f1 0.8531, precision 0.8512, recall 0.8551, auc 0.8876
epoch 9201, loss 0.2547, train acc 89.74%, f1 0.8531, precision 0.8512, recall 0.8551, auc 0.8876
epoch 9301, loss 0.2529, train acc 89.74%, f1 0.8531, precision 0.8512, recall 0.8551, auc 0.8876
epoch 9401, loss 0.2511, train acc 89.90%, f1 0.8551, precision 0.8551, recall 0.8551, auc 0.8888
epoch 9501, loss 0.2494, train acc 89.90%, f1 0.8551, precision 0.8551, recall 0.8551, auc 0.8888
epoch 9601, loss 0.2478, train acc 89.90%, f1 0.8551, precision 0.8551, recall 0.8551, auc 0.8888
epoch 9701, loss 0.2460, train acc 89.90%, f1 0.8551, precision 0.8551, recall 0.8551, auc 0.8888
epoch 9801, loss 0.2437, train acc 90.39%, f1 0.8625, precision 0.8605, recall 0.8645, auc 0.8947
epoch 9901, loss 0.2410, train acc 90.88%, f1 0.8685, precision 0.8726, recall 0.8645, auc 0.8985
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_normal_10000
normal
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_2
./test_pima/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.7098148148148149

the Fscore is 0.6238532110091743

the precision is 0.6181818181818182

the recall is 0.6296296296296297

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_2
----------------------



epoch 1, loss 0.6861, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6212, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5939, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5674, train acc 65.31%, f1 0.0184, precision 0.6667, recall 0.0093, auc 0.5034
epoch 401, loss 0.5440, train acc 67.75%, f1 0.1818, precision 0.7857, recall 0.1028, auc 0.5439
epoch 501, loss 0.5256, train acc 71.01%, f1 0.3597, precision 0.7812, recall 0.2336, auc 0.5993
epoch 601, loss 0.5117, train acc 74.59%, f1 0.5215, precision 0.7589, recall 0.3972, auc 0.6648
epoch 701, loss 0.5014, train acc 74.92%, f1 0.5600, precision 0.7206, recall 0.4579, auc 0.6815
epoch 801, loss 0.4937, train acc 74.76%, f1 0.5777, precision 0.6928, recall 0.4953, auc 0.6889
epoch 901, loss 0.4880, train acc 76.06%, f1 0.6182, precision 0.6959, recall 0.5561, auc 0.7130
epoch 1001, loss 0.4835, train acc 76.06%, f1 0.6240, precision 0.6893, recall 0.5701, auc 0.7163
epoch 1101, loss 0.4799, train acc 76.06%, f1 0.6260, precision 0.6872, recall 0.5748, auc 0.7174
epoch 1201, loss 0.4765, train acc 76.55%, f1 0.6364, precision 0.6923, recall 0.5888, auc 0.7244
epoch 1301, loss 0.4729, train acc 77.04%, f1 0.6448, precision 0.6995, recall 0.5981, auc 0.7303
epoch 1401, loss 0.4689, train acc 77.52%, f1 0.6497, precision 0.7111, recall 0.5981, auc 0.7341
epoch 1501, loss 0.4646, train acc 78.01%, f1 0.6565, precision 0.7207, recall 0.6028, auc 0.7389
epoch 1601, loss 0.4603, train acc 78.50%, f1 0.6633, precision 0.7303, recall 0.6075, auc 0.7437
epoch 1701, loss 0.4564, train acc 78.34%, f1 0.6633, precision 0.7238, recall 0.6121, auc 0.7436
epoch 1801, loss 0.4530, train acc 78.34%, f1 0.6616, precision 0.7263, recall 0.6075, auc 0.7425
epoch 1901, loss 0.4499, train acc 78.01%, f1 0.6599, precision 0.7158, recall 0.6121, auc 0.7411
epoch 2001, loss 0.4471, train acc 78.18%, f1 0.6633, precision 0.7174, recall 0.6168, auc 0.7434
epoch 2101, loss 0.4446, train acc 78.18%, f1 0.6633, precision 0.7174, recall 0.6168, auc 0.7434
epoch 2201, loss 0.4424, train acc 78.34%, f1 0.6650, precision 0.7213, recall 0.6168, auc 0.7447
epoch 2301, loss 0.4404, train acc 78.50%, f1 0.6683, precision 0.7228, recall 0.6215, auc 0.7470
epoch 2401, loss 0.4387, train acc 78.99%, f1 0.6783, precision 0.7273, recall 0.6355, auc 0.7540
epoch 2501, loss 0.4372, train acc 78.99%, f1 0.6783, precision 0.7273, recall 0.6355, auc 0.7540
epoch 2601, loss 0.4359, train acc 78.99%, f1 0.6783, precision 0.7273, recall 0.6355, auc 0.7540
epoch 2701, loss 0.4347, train acc 78.99%, f1 0.6783, precision 0.7273, recall 0.6355, auc 0.7540
epoch 2801, loss 0.4335, train acc 78.83%, f1 0.6766, precision 0.7234, recall 0.6355, auc 0.7528
epoch 2901, loss 0.4324, train acc 78.83%, f1 0.6766, precision 0.7234, recall 0.6355, auc 0.7528
epoch 3001, loss 0.4313, train acc 78.99%, f1 0.6783, precision 0.7273, recall 0.6355, auc 0.7540
epoch 3101, loss 0.4301, train acc 78.50%, f1 0.6700, precision 0.7204, recall 0.6262, auc 0.7481
epoch 3201, loss 0.4288, train acc 78.99%, f1 0.6783, precision 0.7273, recall 0.6355, auc 0.7540
epoch 3301, loss 0.4273, train acc 79.32%, f1 0.6817, precision 0.7351, recall 0.6355, auc 0.7565
epoch 3401, loss 0.4258, train acc 79.80%, f1 0.6900, precision 0.7419, recall 0.6449, auc 0.7624
epoch 3501, loss 0.4244, train acc 79.80%, f1 0.6900, precision 0.7419, recall 0.6449, auc 0.7624
epoch 3601, loss 0.4231, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 3701, loss 0.4219, train acc 79.97%, f1 0.6933, precision 0.7433, recall 0.6495, auc 0.7648
epoch 3801, loss 0.4206, train acc 79.97%, f1 0.6933, precision 0.7433, recall 0.6495, auc 0.7648
epoch 3901, loss 0.4193, train acc 79.97%, f1 0.6933, precision 0.7433, recall 0.6495, auc 0.7648
epoch 4001, loss 0.4179, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 4101, loss 0.4158, train acc 79.48%, f1 0.6866, precision 0.7340, recall 0.6449, auc 0.7599
epoch 4201, loss 0.4134, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 4301, loss 0.4107, train acc 79.48%, f1 0.6866, precision 0.7340, recall 0.6449, auc 0.7599
epoch 4401, loss 0.4077, train acc 79.97%, f1 0.6948, precision 0.7407, recall 0.6542, auc 0.7659
epoch 4501, loss 0.4043, train acc 80.29%, f1 0.7012, precision 0.7435, recall 0.6636, auc 0.7705
epoch 4601, loss 0.4006, train acc 80.62%, f1 0.7076, precision 0.7461, recall 0.6729, auc 0.7752
epoch 4701, loss 0.3966, train acc 80.94%, f1 0.7139, precision 0.7487, recall 0.6822, auc 0.7799
epoch 4801, loss 0.3928, train acc 81.11%, f1 0.7157, precision 0.7526, recall 0.6822, auc 0.7811
epoch 4901, loss 0.3883, train acc 80.62%, f1 0.7090, precision 0.7436, recall 0.6776, auc 0.7763
epoch 5001, loss 0.3834, train acc 81.11%, f1 0.7143, precision 0.7552, recall 0.6776, auc 0.7800
epoch 5101, loss 0.3792, train acc 81.43%, f1 0.7206, precision 0.7577, recall 0.6869, auc 0.7847
epoch 5201, loss 0.3752, train acc 81.60%, f1 0.7237, precision 0.7590, recall 0.6916, auc 0.7870
epoch 5301, loss 0.3710, train acc 81.92%, f1 0.7286, precision 0.7641, recall 0.6963, auc 0.7906
epoch 5401, loss 0.3666, train acc 82.41%, f1 0.7353, precision 0.7732, recall 0.7009, auc 0.7955
epoch 5501, loss 0.3622, train acc 82.57%, f1 0.7384, precision 0.7744, recall 0.7056, auc 0.7978
epoch 5601, loss 0.3576, train acc 83.22%, f1 0.7494, precision 0.7817, recall 0.7196, auc 0.8061
epoch 5701, loss 0.3529, train acc 83.39%, f1 0.7524, precision 0.7828, recall 0.7243, auc 0.8084
epoch 5801, loss 0.3481, train acc 83.39%, f1 0.7536, precision 0.7800, recall 0.7290, auc 0.8095
epoch 5901, loss 0.3428, train acc 83.88%, f1 0.7603, precision 0.7889, recall 0.7336, auc 0.8143
epoch 6001, loss 0.3373, train acc 84.53%, f1 0.7700, precision 0.7990, recall 0.7430, auc 0.8215
epoch 6101, loss 0.3320, train acc 85.83%, f1 0.7893, precision 0.8191, recall 0.7617, auc 0.8358
epoch 6201, loss 0.3266, train acc 86.16%, f1 0.7942, precision 0.8241, recall 0.7664, auc 0.8394
epoch 6301, loss 0.3205, train acc 86.48%, f1 0.8000, precision 0.8259, recall 0.7757, auc 0.8441
epoch 6401, loss 0.3137, train acc 86.16%, f1 0.7962, precision 0.8177, recall 0.7757, auc 0.8416
epoch 6501, loss 0.3069, train acc 86.81%, f1 0.8048, precision 0.8308, recall 0.7804, auc 0.8477
epoch 6601, loss 0.2997, train acc 87.13%, f1 0.8106, precision 0.8325, recall 0.7897, auc 0.8524
epoch 6701, loss 0.2932, train acc 87.13%, f1 0.8106, precision 0.8325, recall 0.7897, auc 0.8524
epoch 6801, loss 0.2875, train acc 87.62%, f1 0.8190, precision 0.8350, recall 0.8037, auc 0.8594
epoch 6901, loss 0.2821, train acc 88.27%, f1 0.8294, precision 0.8413, recall 0.8178, auc 0.8676
epoch 7001, loss 0.2771, train acc 88.44%, f1 0.8314, precision 0.8454, recall 0.8178, auc 0.8689
epoch 7101, loss 0.2724, train acc 88.44%, f1 0.8314, precision 0.8454, recall 0.8178, auc 0.8689
epoch 7201, loss 0.2680, train acc 88.27%, f1 0.8302, precision 0.8381, recall 0.8224, auc 0.8687
epoch 7301, loss 0.2639, train acc 88.44%, f1 0.8322, precision 0.8421, recall 0.8224, auc 0.8700
epoch 7401, loss 0.2600, train acc 89.09%, f1 0.8416, precision 0.8517, recall 0.8318, auc 0.8771
epoch 7501, loss 0.2558, train acc 89.74%, f1 0.8511, precision 0.8612, recall 0.8411, auc 0.8843
epoch 7601, loss 0.2520, train acc 90.07%, f1 0.8558, precision 0.8660, recall 0.8458, auc 0.8879
epoch 7701, loss 0.2487, train acc 89.74%, f1 0.8518, precision 0.8578, recall 0.8458, auc 0.8854
epoch 7801, loss 0.2455, train acc 89.41%, f1 0.8471, precision 0.8531, recall 0.8411, auc 0.8818
epoch 7901, loss 0.2424, train acc 89.58%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8852
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_normal_8000
normal
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_2
./test_pima/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.7098148148148149

the Fscore is 0.6238532110091743

the precision is 0.6181818181818182

the recall is 0.6296296296296297

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_2
----------------------



epoch 1, loss 0.6905, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6225, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5950, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5688, train acc 65.15%, f1 0.0093, precision 0.5000, recall 0.0047, auc 0.5011
epoch 401, loss 0.5455, train acc 67.10%, f1 0.1441, precision 0.7727, recall 0.0794, auc 0.5335
epoch 501, loss 0.5269, train acc 70.85%, f1 0.3491, precision 0.7869, recall 0.2243, auc 0.5959
epoch 601, loss 0.5128, train acc 74.43%, f1 0.5169, precision 0.7568, recall 0.3925, auc 0.6625
epoch 701, loss 0.5023, train acc 74.92%, f1 0.5600, precision 0.7206, recall 0.4579, auc 0.6815
epoch 801, loss 0.4945, train acc 74.59%, f1 0.5738, precision 0.6908, recall 0.4907, auc 0.6866
epoch 901, loss 0.4887, train acc 76.06%, f1 0.6182, precision 0.6959, recall 0.5561, auc 0.7130
epoch 1001, loss 0.4841, train acc 76.06%, f1 0.6240, precision 0.6893, recall 0.5701, auc 0.7163
epoch 1101, loss 0.4803, train acc 76.06%, f1 0.6260, precision 0.6872, recall 0.5748, auc 0.7174
epoch 1201, loss 0.4767, train acc 76.71%, f1 0.6361, precision 0.6983, recall 0.5841, auc 0.7246
epoch 1301, loss 0.4728, train acc 77.04%, f1 0.6448, precision 0.6995, recall 0.5981, auc 0.7303
epoch 1401, loss 0.4685, train acc 78.01%, f1 0.6547, precision 0.7232, recall 0.5981, auc 0.7378
epoch 1501, loss 0.4642, train acc 77.85%, f1 0.6531, precision 0.7191, recall 0.5981, auc 0.7366
epoch 1601, loss 0.4600, train acc 78.50%, f1 0.6633, precision 0.7303, recall 0.6075, auc 0.7437
epoch 1701, loss 0.4564, train acc 78.18%, f1 0.6599, precision 0.7222, recall 0.6075, auc 0.7412
epoch 1801, loss 0.4531, train acc 78.18%, f1 0.6633, precision 0.7174, recall 0.6168, auc 0.7434
epoch 1901, loss 0.4502, train acc 78.18%, f1 0.6633, precision 0.7174, recall 0.6168, auc 0.7434
epoch 2001, loss 0.4475, train acc 78.66%, f1 0.6733, precision 0.7219, recall 0.6308, auc 0.7504
epoch 2101, loss 0.4451, train acc 78.18%, f1 0.6650, precision 0.7151, recall 0.6215, auc 0.7445
epoch 2201, loss 0.4428, train acc 78.66%, f1 0.6717, precision 0.7243, recall 0.6262, auc 0.7493
epoch 2301, loss 0.4407, train acc 78.83%, f1 0.6734, precision 0.7283, recall 0.6262, auc 0.7506
epoch 2401, loss 0.4387, train acc 78.99%, f1 0.6767, precision 0.7297, recall 0.6308, auc 0.7529
epoch 2501, loss 0.4369, train acc 79.15%, f1 0.6800, precision 0.7312, recall 0.6355, auc 0.7553
epoch 2601, loss 0.4352, train acc 79.15%, f1 0.6800, precision 0.7312, recall 0.6355, auc 0.7553
epoch 2701, loss 0.4336, train acc 79.15%, f1 0.6800, precision 0.7312, recall 0.6355, auc 0.7553
epoch 2801, loss 0.4320, train acc 79.15%, f1 0.6800, precision 0.7312, recall 0.6355, auc 0.7553
epoch 2901, loss 0.4304, train acc 79.15%, f1 0.6784, precision 0.7337, recall 0.6308, auc 0.7542
epoch 3001, loss 0.4287, train acc 79.32%, f1 0.6817, precision 0.7351, recall 0.6355, auc 0.7565
epoch 3101, loss 0.4271, train acc 79.48%, f1 0.6850, precision 0.7366, recall 0.6402, auc 0.7588
epoch 3201, loss 0.4255, train acc 79.48%, f1 0.6850, precision 0.7366, recall 0.6402, auc 0.7588
epoch 3301, loss 0.4240, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 3401, loss 0.4225, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 3501, loss 0.4211, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 3601, loss 0.4197, train acc 79.15%, f1 0.6800, precision 0.7312, recall 0.6355, auc 0.7553
epoch 3701, loss 0.4184, train acc 78.99%, f1 0.6767, precision 0.7297, recall 0.6308, auc 0.7529
epoch 3801, loss 0.4170, train acc 78.99%, f1 0.6751, precision 0.7322, recall 0.6262, auc 0.7518
epoch 3901, loss 0.4157, train acc 79.32%, f1 0.6801, precision 0.7377, recall 0.6308, auc 0.7554
epoch 4001, loss 0.4142, train acc 79.32%, f1 0.6817, precision 0.7351, recall 0.6355, auc 0.7565
epoch 4101, loss 0.4121, train acc 79.48%, f1 0.6850, precision 0.7366, recall 0.6402, auc 0.7588
epoch 4201, loss 0.4093, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 4301, loss 0.4064, train acc 80.46%, f1 0.7030, precision 0.7474, recall 0.6636, auc 0.7718
epoch 4401, loss 0.4031, train acc 80.46%, f1 0.7044, precision 0.7448, recall 0.6682, auc 0.7729
epoch 4501, loss 0.3999, train acc 80.78%, f1 0.7108, precision 0.7474, recall 0.6776, auc 0.7775
epoch 4601, loss 0.3969, train acc 81.11%, f1 0.7143, precision 0.7552, recall 0.6776, auc 0.7800
epoch 4701, loss 0.3940, train acc 81.27%, f1 0.7160, precision 0.7592, recall 0.6776, auc 0.7813
epoch 4801, loss 0.3912, train acc 81.27%, f1 0.7160, precision 0.7592, recall 0.6776, auc 0.7813
epoch 4901, loss 0.3881, train acc 81.27%, f1 0.7160, precision 0.7592, recall 0.6776, auc 0.7813
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_normal_5000
normal
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_2
./test_pima/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.7255555555555555

the Fscore is 0.6407766990291262

the precision is 0.673469387755102

the recall is 0.6111111111111112

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_2
----------------------



epoch 1, loss 0.6953, train acc 34.85%, f1 0.5169, precision 0.3485, recall 1.0000, auc 0.5000
epoch 101, loss 0.6238, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5958, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5697, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5465, train acc 66.94%, f1 0.1362, precision 0.7619, recall 0.0748, auc 0.5311
epoch 501, loss 0.5279, train acc 70.68%, f1 0.3382, precision 0.7931, recall 0.2150, auc 0.5925
epoch 601, loss 0.5138, train acc 73.94%, f1 0.5031, precision 0.7500, recall 0.3785, auc 0.6555
epoch 701, loss 0.5031, train acc 74.92%, f1 0.5575, precision 0.7239, recall 0.4533, auc 0.6804
epoch 801, loss 0.4952, train acc 74.76%, f1 0.5753, precision 0.6954, recall 0.4907, auc 0.6878
epoch 901, loss 0.4893, train acc 75.90%, f1 0.6126, precision 0.6964, recall 0.5467, auc 0.7096
epoch 1001, loss 0.4847, train acc 76.06%, f1 0.6240, precision 0.6893, recall 0.5701, auc 0.7163
epoch 1101, loss 0.4809, train acc 76.06%, f1 0.6260, precision 0.6872, recall 0.5748, auc 0.7174
epoch 1201, loss 0.4772, train acc 76.38%, f1 0.6329, precision 0.6906, recall 0.5841, auc 0.7221
epoch 1301, loss 0.4732, train acc 77.04%, f1 0.6448, precision 0.6995, recall 0.5981, auc 0.7303
epoch 1401, loss 0.4687, train acc 78.01%, f1 0.6547, precision 0.7232, recall 0.5981, auc 0.7378
epoch 1501, loss 0.4642, train acc 77.85%, f1 0.6531, precision 0.7191, recall 0.5981, auc 0.7366
epoch 1601, loss 0.4599, train acc 78.18%, f1 0.6564, precision 0.7273, recall 0.5981, auc 0.7391
epoch 1701, loss 0.4563, train acc 78.01%, f1 0.6565, precision 0.7207, recall 0.6028, auc 0.7389
epoch 1801, loss 0.4533, train acc 78.34%, f1 0.6633, precision 0.7238, recall 0.6121, auc 0.7436
epoch 1901, loss 0.4508, train acc 78.18%, f1 0.6633, precision 0.7174, recall 0.6168, auc 0.7434
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_2.csv
./test_pima/standlization_data/pima_std_test_2.csv
MLP_normal_2000
normal
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_2
./test_pima/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.7270370370370369

the Fscore is 0.6391752577319588

the precision is 0.7209302325581395

the recall is 0.5740740740740741

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_2
----------------------



epoch 1, loss 0.6941, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.2927, train acc 86.82%, f1 0.8682, precision 0.8680, recall 0.8684, auc 0.8682
epoch 201, loss 0.1391, train acc 97.11%, f1 0.9711, precision 0.9712, recall 0.9710, auc 0.9711
epoch 301, loss 0.0646, train acc 98.34%, f1 0.9834, precision 0.9834, recall 0.9834, auc 0.9834
epoch 401, loss 0.0472, train acc 98.83%, f1 0.9883, precision 0.9883, recall 0.9884, auc 0.9883
epoch 501, loss 0.0649, train acc 98.99%, f1 0.9899, precision 0.9899, recall 0.9899, auc 0.9899
epoch 601, loss 0.0227, train acc 99.14%, f1 0.9914, precision 0.9914, recall 0.9915, auc 0.9914
epoch 701, loss 0.0263, train acc 99.24%, f1 0.9924, precision 0.9924, recall 0.9924, auc 0.9924
epoch 801, loss 0.0212, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9934, auc 0.9934
epoch 901, loss 0.0209, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9939, auc 0.9939
epoch 1001, loss 0.0241, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9946
epoch 1101, loss 0.0209, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9951, auc 0.9951
epoch 1201, loss 0.0204, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1301, loss 0.0172, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1401, loss 0.0163, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1501, loss 0.0128, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1601, loss 0.0141, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1701, loss 0.0105, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1801, loss 0.0108, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1901, loss 0.0110, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2001, loss 0.0191, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2101, loss 0.0084, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2201, loss 0.0066, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2301, loss 0.0047, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2401, loss 0.0120, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2501, loss 0.0054, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2601, loss 0.0104, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2701, loss 0.0093, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 2801, loss 0.0044, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2901, loss 0.0033, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 3001, loss 0.0066, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 3101, loss 0.0072, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9979, auc 0.9979
epoch 3201, loss 0.0045, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9979, auc 0.9979
epoch 3301, loss 0.0047, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3401, loss 0.0077, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3501, loss 0.0149, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 3601, loss 0.0105, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3701, loss 0.0015, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3801, loss 0.0102, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 3901, loss 0.0077, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 4001, loss 0.0051, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4101, loss 0.0075, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 4201, loss 0.0045, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4301, loss 0.0067, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4401, loss 0.0064, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 4501, loss 0.0011, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4601, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4701, loss 0.0042, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 4801, loss 0.0023, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4901, loss 0.0086, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 5001, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5101, loss 0.0059, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5201, loss 0.0026, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 5301, loss 0.0018, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5401, loss 0.0028, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 5501, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 5601, loss 0.0032, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 5701, loss 0.0051, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5801, loss 0.0013, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 5901, loss 0.0082, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6001, loss 0.0082, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 6101, loss 0.0025, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 6201, loss 0.0025, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 6301, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 6401, loss 0.0041, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 6501, loss 0.0023, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 6601, loss 0.0002, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6701, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 6801, loss 0.0041, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6901, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7001, loss 0.0063, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7101, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7201, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 7301, loss 0.0007, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 7401, loss 0.0051, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7501, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 7601, loss 0.0030, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7701, loss 0.0029, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 7801, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 7901, loss 0.0018, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8001, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 8101, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 8201, loss 0.0038, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 8301, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 8401, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 8501, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 8601, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 8701, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 8801, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 8901, loss 0.0030, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 9001, loss 0.0028, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 9101, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9201, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9301, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9401, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9501, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9601, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 9701, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9801, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 9901, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 10001, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10101, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10201, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10301, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10401, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 10501, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10601, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10701, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10801, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 10901, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11001, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11101, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11201, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11301, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 11401, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 11501, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 11601, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11701, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11801, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11901, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12001, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12101, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12201, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_2
./test_vehicle0/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9461240310077519

the Fscore is 0.935064935064935

the precision is 0.972972972972973

the recall is 0.9

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_2
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.2920, train acc 87.38%, f1 0.8738, precision 0.8738, recall 0.8738, auc 0.8738
epoch 201, loss 0.1605, train acc 97.23%, f1 0.9723, precision 0.9726, recall 0.9720, auc 0.9723
epoch 301, loss 0.0961, train acc 98.45%, f1 0.9845, precision 0.9845, recall 0.9845, auc 0.9845
epoch 401, loss 0.0677, train acc 98.77%, f1 0.9877, precision 0.9877, recall 0.9877, auc 0.9877
epoch 501, loss 0.0486, train acc 99.01%, f1 0.9901, precision 0.9901, recall 0.9902, auc 0.9901
epoch 601, loss 0.0310, train acc 99.15%, f1 0.9915, precision 0.9914, recall 0.9915, auc 0.9915
epoch 701, loss 0.0440, train acc 99.25%, f1 0.9925, precision 0.9924, recall 0.9925, auc 0.9925
epoch 801, loss 0.0230, train acc 99.34%, f1 0.9934, precision 0.9933, recall 0.9934, auc 0.9934
epoch 901, loss 0.0251, train acc 99.41%, f1 0.9941, precision 0.9940, recall 0.9941, auc 0.9941
epoch 1001, loss 0.0237, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9946
epoch 1101, loss 0.0131, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9953, auc 0.9953
epoch 1201, loss 0.0143, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1301, loss 0.0150, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 1401, loss 0.0097, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1501, loss 0.0138, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 1601, loss 0.0174, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 1701, loss 0.0071, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1801, loss 0.0188, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 1901, loss 0.0131, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2001, loss 0.0087, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2101, loss 0.0104, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2201, loss 0.0032, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2301, loss 0.0114, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2401, loss 0.0097, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2501, loss 0.0112, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9979, auc 0.9978
epoch 2601, loss 0.0057, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 2701, loss 0.0037, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2801, loss 0.0034, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2901, loss 0.0076, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3001, loss 0.0131, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 3101, loss 0.0058, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 3201, loss 0.0068, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3301, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3401, loss 0.0051, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3501, loss 0.0091, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 3601, loss 0.0034, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 3701, loss 0.0022, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 3801, loss 0.0117, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 3901, loss 0.0057, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 4001, loss 0.0025, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 4101, loss 0.0017, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4201, loss 0.0038, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 4301, loss 0.0027, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 4401, loss 0.0006, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 4501, loss 0.0075, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 4601, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 4701, loss 0.0015, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 4801, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4901, loss 0.0042, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 5001, loss 0.0070, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 5101, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 5201, loss 0.0022, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 5301, loss 0.0014, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 5401, loss 0.0046, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5501, loss 0.0015, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 5601, loss 0.0035, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 5701, loss 0.0058, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 5801, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5901, loss 0.0015, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 6001, loss 0.0008, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6101, loss 0.0038, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 6201, loss 0.0008, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 6301, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 6401, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 6501, loss 0.0028, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6601, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6701, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 6801, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 6901, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 7001, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 7101, loss 0.0055, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 7201, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 7301, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 7401, loss 0.0033, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7501, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 7601, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7701, loss 0.0036, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 7801, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 7901, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 8001, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8101, loss 0.0026, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8201, loss 0.0028, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 8401, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8501, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 8601, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 8701, loss 0.0032, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8801, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 8901, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 9001, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 9101, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 9201, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 9301, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 9401, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9501, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9601, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 9701, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9801, loss 0.0001, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 9901, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10001, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 10101, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10201, loss 0.0020, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10301, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10401, loss 0.0001, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10501, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10601, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10701, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10801, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 10901, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11001, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0013, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_2
./test_vehicle0/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.983624031007752

the Fscore is 0.975

the precision is 0.975

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_2
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.3402, train acc 87.40%, f1 0.8742, precision 0.8724, recall 0.8761, auc 0.8740
epoch 201, loss 0.1229, train acc 97.33%, f1 0.9733, precision 0.9730, recall 0.9736, auc 0.9733
epoch 301, loss 0.0855, train acc 98.43%, f1 0.9843, precision 0.9843, recall 0.9842, auc 0.9843
epoch 401, loss 0.0633, train acc 98.77%, f1 0.9877, precision 0.9878, recall 0.9877, auc 0.9877
epoch 501, loss 0.0343, train acc 99.02%, f1 0.9902, precision 0.9903, recall 0.9902, auc 0.9902
epoch 601, loss 0.0313, train acc 99.17%, f1 0.9917, precision 0.9918, recall 0.9916, auc 0.9917
epoch 701, loss 0.0298, train acc 99.24%, f1 0.9924, precision 0.9924, recall 0.9923, auc 0.9924
epoch 801, loss 0.0147, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9933, auc 0.9934
epoch 901, loss 0.0202, train acc 99.42%, f1 0.9942, precision 0.9943, recall 0.9941, auc 0.9942
epoch 1001, loss 0.0205, train acc 99.46%, f1 0.9946, precision 0.9947, recall 0.9945, auc 0.9946
epoch 1101, loss 0.0183, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9951, auc 0.9952
epoch 1201, loss 0.0159, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9955, auc 0.9956
epoch 1301, loss 0.0121, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9958, auc 0.9959
epoch 1401, loss 0.0098, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9961, auc 0.9962
epoch 1501, loss 0.0120, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9963, auc 0.9964
epoch 1601, loss 0.0077, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1701, loss 0.0137, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9968, auc 0.9969
epoch 1801, loss 0.0100, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9969, auc 0.9969
epoch 1901, loss 0.0073, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2001, loss 0.0126, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9971, auc 0.9972
epoch 2101, loss 0.0075, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9973, auc 0.9973
epoch 2201, loss 0.0061, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 2301, loss 0.0083, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2401, loss 0.0048, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0056, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2601, loss 0.0102, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9977, auc 0.9977
epoch 2701, loss 0.0084, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2801, loss 0.0034, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2901, loss 0.0069, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3001, loss 0.0059, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 3101, loss 0.0050, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 3201, loss 0.0021, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 3301, loss 0.0071, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3401, loss 0.0133, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 3501, loss 0.0042, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 3601, loss 0.0029, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 3701, loss 0.0057, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3801, loss 0.0055, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 3901, loss 0.0067, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4001, loss 0.0091, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 4101, loss 0.0072, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4201, loss 0.0060, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 4301, loss 0.0046, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 4401, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4501, loss 0.0010, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4601, loss 0.0057, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 4701, loss 0.0023, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4801, loss 0.0061, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4901, loss 0.0045, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5001, loss 0.0062, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9984, auc 0.9985
epoch 5101, loss 0.0018, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 5201, loss 0.0040, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 5301, loss 0.0033, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5401, loss 0.0063, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 5501, loss 0.0071, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 5601, loss 0.0020, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 5701, loss 0.0053, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 5801, loss 0.0098, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 5901, loss 0.0015, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 6001, loss 0.0019, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6101, loss 0.0016, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 6201, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 6301, loss 0.0008, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 6401, loss 0.0042, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6501, loss 0.0024, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6601, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6701, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 6801, loss 0.0016, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9989, auc 0.9991
epoch 6901, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 7001, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9992, auc 0.9991
epoch 7101, loss 0.0007, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7201, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 7301, loss 0.0009, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9990, auc 0.9992
epoch 7401, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 7501, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 7601, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9994, auc 0.9992
epoch 7701, loss 0.0028, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7801, loss 0.0035, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 7901, loss 0.0037, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 8001, loss 0.0040, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9991, auc 0.9993
epoch 8101, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8201, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 8401, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 8501, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 8601, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 8701, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 8801, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 8901, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 9001, loss 0.0048, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 9101, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 9201, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9997, auc 0.9995
epoch 9301, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9401, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 9501, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9601, loss 0.0027, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9701, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9801, loss 0.0022, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 9901, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_2
./test_vehicle0/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9258720930232558

the Fscore is 0.8974358974358975

the precision is 0.9210526315789473

the recall is 0.875

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_2
----------------------



epoch 1, loss 0.6936, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3587, train acc 86.28%, f1 0.8603, precision 0.8765, recall 0.8447, auc 0.8628
epoch 201, loss 0.1635, train acc 96.99%, f1 0.9699, precision 0.9706, recall 0.9691, auc 0.9699
epoch 301, loss 0.0890, train acc 98.22%, f1 0.9822, precision 0.9819, recall 0.9826, auc 0.9822
epoch 401, loss 0.0414, train acc 98.73%, f1 0.9873, precision 0.9869, recall 0.9877, auc 0.9873
epoch 501, loss 0.0460, train acc 98.95%, f1 0.9895, precision 0.9891, recall 0.9899, auc 0.9895
epoch 601, loss 0.0211, train acc 99.14%, f1 0.9914, precision 0.9910, recall 0.9918, auc 0.9914
epoch 701, loss 0.0225, train acc 99.24%, f1 0.9925, precision 0.9921, recall 0.9928, auc 0.9924
epoch 801, loss 0.0191, train acc 99.34%, f1 0.9934, precision 0.9931, recall 0.9937, auc 0.9934
epoch 901, loss 0.0174, train acc 99.40%, f1 0.9940, precision 0.9937, recall 0.9942, auc 0.9940
epoch 1001, loss 0.0246, train acc 99.45%, f1 0.9945, precision 0.9943, recall 0.9946, auc 0.9945
epoch 1101, loss 0.0167, train acc 99.50%, f1 0.9950, precision 0.9948, recall 0.9952, auc 0.9950
epoch 1201, loss 0.0208, train acc 99.55%, f1 0.9955, precision 0.9953, recall 0.9957, auc 0.9955
epoch 1301, loss 0.0080, train acc 99.57%, f1 0.9957, precision 0.9956, recall 0.9959, auc 0.9957
epoch 1401, loss 0.0175, train acc 99.60%, f1 0.9960, precision 0.9958, recall 0.9962, auc 0.9960
epoch 1501, loss 0.0186, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9965, auc 0.9963
epoch 1601, loss 0.0139, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9967, auc 0.9966
epoch 1701, loss 0.0111, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9969, auc 0.9968
epoch 1801, loss 0.0142, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 1901, loss 0.0083, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9971, auc 0.9970
epoch 2001, loss 0.0109, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 2101, loss 0.0128, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 2201, loss 0.0090, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2301, loss 0.0055, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2401, loss 0.0127, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 2501, loss 0.0120, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9981, auc 0.9979
epoch 2601, loss 0.0104, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2701, loss 0.0103, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 2801, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 2901, loss 0.0068, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3001, loss 0.0051, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 3101, loss 0.0059, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9985, auc 0.9983
epoch 3201, loss 0.0031, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 3301, loss 0.0072, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3401, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3501, loss 0.0029, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3601, loss 0.0055, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 3701, loss 0.0043, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 3801, loss 0.0038, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 3901, loss 0.0051, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4001, loss 0.0043, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4101, loss 0.0057, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4201, loss 0.0085, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4301, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4401, loss 0.0032, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4501, loss 0.0045, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 4601, loss 0.0083, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 4701, loss 0.0009, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 4801, loss 0.0069, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 4901, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 5001, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 5101, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 5201, loss 0.0030, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 5301, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 5401, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 5501, loss 0.0050, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 5601, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 5701, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 5801, loss 0.0002, train acc 99.91%, f1 0.9991, precision 0.9994, recall 0.9989, auc 0.9991
epoch 5901, loss 0.0033, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6001, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 6101, loss 0.0024, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9995, auc 0.9993
epoch 6201, loss 0.0047, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6301, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6401, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6501, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6601, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9992, auc 0.9994
epoch 6701, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 6801, loss 0.0027, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 6901, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 7001, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7101, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7201, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7301, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7401, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7501, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9994, auc 0.9996
epoch 7601, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 7701, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 7801, loss 0.0028, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7901, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_2
./test_vehicle0/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9797480620155039

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_2
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3102, train acc 86.73%, f1 0.8673, precision 0.8673, recall 0.8672, auc 0.8673
epoch 201, loss 0.1348, train acc 97.30%, f1 0.9730, precision 0.9727, recall 0.9733, auc 0.9730
epoch 301, loss 0.0964, train acc 98.35%, f1 0.9835, precision 0.9836, recall 0.9834, auc 0.9835
epoch 401, loss 0.0626, train acc 98.79%, f1 0.9879, precision 0.9880, recall 0.9878, auc 0.9879
epoch 501, loss 0.0389, train acc 98.98%, f1 0.9898, precision 0.9898, recall 0.9898, auc 0.9898
epoch 601, loss 0.0444, train acc 99.15%, f1 0.9915, precision 0.9915, recall 0.9914, auc 0.9915
epoch 701, loss 0.0202, train acc 99.27%, f1 0.9927, precision 0.9927, recall 0.9927, auc 0.9927
epoch 801, loss 0.0280, train acc 99.36%, f1 0.9936, precision 0.9936, recall 0.9935, auc 0.9936
epoch 901, loss 0.0191, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1001, loss 0.0175, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9946
epoch 1101, loss 0.0142, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9951, auc 0.9951
epoch 1201, loss 0.0167, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9955, auc 0.9956
epoch 1301, loss 0.0232, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 1401, loss 0.0204, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1501, loss 0.0101, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1601, loss 0.0089, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1701, loss 0.0099, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 1801, loss 0.0109, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 1901, loss 0.0105, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2001, loss 0.0105, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2101, loss 0.0068, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2201, loss 0.0084, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2301, loss 0.0089, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2401, loss 0.0042, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0074, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9976, auc 0.9977
epoch 2601, loss 0.0052, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9977, auc 0.9978
epoch 2701, loss 0.0113, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2801, loss 0.0067, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2901, loss 0.0048, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 3001, loss 0.0085, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3101, loss 0.0077, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3201, loss 0.0081, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3301, loss 0.0062, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 3401, loss 0.0159, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 3501, loss 0.0065, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3601, loss 0.0023, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3701, loss 0.0032, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3801, loss 0.0041, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3901, loss 0.0022, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4001, loss 0.0038, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4101, loss 0.0090, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 4201, loss 0.0027, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4301, loss 0.0057, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 4401, loss 0.0079, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 4501, loss 0.0022, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4601, loss 0.0021, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 4701, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 4801, loss 0.0084, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 4901, loss 0.0018, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_2
./test_vehicle0/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9594961240310077

the Fscore is 0.9268292682926829

the precision is 0.9047619047619048

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_2
----------------------



epoch 1, loss 0.6931, train acc 70.27%, f1 0.5794, precision 0.9900, recall 0.4095, auc 0.7027
epoch 101, loss 0.3461, train acc 87.20%, f1 0.8719, precision 0.8721, recall 0.8717, auc 0.8720
epoch 201, loss 0.1543, train acc 97.30%, f1 0.9730, precision 0.9731, recall 0.9729, auc 0.9730
epoch 301, loss 0.1054, train acc 98.45%, f1 0.9845, precision 0.9846, recall 0.9844, auc 0.9845
epoch 401, loss 0.0471, train acc 98.81%, f1 0.9881, precision 0.9881, recall 0.9880, auc 0.9881
epoch 501, loss 0.0282, train acc 99.00%, f1 0.9900, precision 0.9901, recall 0.9900, auc 0.9900
epoch 601, loss 0.0309, train acc 99.17%, f1 0.9917, precision 0.9917, recall 0.9917, auc 0.9917
epoch 701, loss 0.0312, train acc 99.27%, f1 0.9927, precision 0.9927, recall 0.9927, auc 0.9927
epoch 801, loss 0.0222, train acc 99.35%, f1 0.9935, precision 0.9935, recall 0.9935, auc 0.9935
epoch 901, loss 0.0316, train acc 99.42%, f1 0.9942, precision 0.9942, recall 0.9942, auc 0.9942
epoch 1001, loss 0.0262, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9948, auc 0.9949
epoch 1101, loss 0.0164, train acc 99.54%, f1 0.9954, precision 0.9955, recall 0.9954, auc 0.9954
epoch 1201, loss 0.0112, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9957, auc 0.9958
epoch 1301, loss 0.0184, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1401, loss 0.0118, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1501, loss 0.0074, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9963, auc 0.9964
epoch 1601, loss 0.0145, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 1701, loss 0.0079, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1801, loss 0.0118, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1901, loss 0.0195, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9970, auc 0.9970
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_2
./test_vehicle0/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9719961240310078

the Fscore is 0.9397590361445783

the precision is 0.9069767441860465

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_2
----------------------



epoch 1, loss 0.6941, train acc 49.99%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3815, train acc 86.33%, f1 0.8627, precision 0.8669, recall 0.8585, auc 0.8633
epoch 201, loss 0.1454, train acc 97.08%, f1 0.9708, precision 0.9719, recall 0.9697, auc 0.9708
epoch 301, loss 0.0764, train acc 98.34%, f1 0.9834, precision 0.9837, recall 0.9832, auc 0.9834
epoch 401, loss 0.0469, train acc 98.73%, f1 0.9873, precision 0.9873, recall 0.9872, auc 0.9873
epoch 501, loss 0.0286, train acc 98.96%, f1 0.9896, precision 0.9897, recall 0.9895, auc 0.9896
epoch 601, loss 0.0354, train acc 99.13%, f1 0.9913, precision 0.9915, recall 0.9912, auc 0.9913
epoch 701, loss 0.0316, train acc 99.25%, f1 0.9925, precision 0.9924, recall 0.9927, auc 0.9925
epoch 801, loss 0.0198, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9934, auc 0.9934
epoch 901, loss 0.0147, train acc 99.40%, f1 0.9940, precision 0.9940, recall 0.9940, auc 0.9940
epoch 1001, loss 0.0144, train acc 99.45%, f1 0.9945, precision 0.9943, recall 0.9947, auc 0.9945
epoch 1101, loss 0.0263, train acc 99.51%, f1 0.9951, precision 0.9948, recall 0.9954, auc 0.9951
epoch 1201, loss 0.0191, train acc 99.55%, f1 0.9955, precision 0.9958, recall 0.9952, auc 0.9955
epoch 1301, loss 0.0089, train acc 99.59%, f1 0.9959, precision 0.9960, recall 0.9958, auc 0.9959
epoch 1401, loss 0.0222, train acc 99.59%, f1 0.9959, precision 0.9957, recall 0.9961, auc 0.9959
epoch 1501, loss 0.0163, train acc 99.62%, f1 0.9962, precision 0.9963, recall 0.9961, auc 0.9962
epoch 1601, loss 0.0117, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9968, auc 0.9966
epoch 1701, loss 0.0082, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9968, auc 0.9968
epoch 1801, loss 0.0113, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9967, auc 0.9969
epoch 1901, loss 0.0031, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9972, auc 0.9970
epoch 2001, loss 0.0053, train acc 99.71%, f1 0.9971, precision 0.9969, recall 0.9974, auc 0.9971
epoch 2101, loss 0.0077, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9975, auc 0.9973
epoch 2201, loss 0.0093, train acc 99.75%, f1 0.9975, precision 0.9979, recall 0.9972, auc 0.9975
epoch 2301, loss 0.0047, train acc 99.76%, f1 0.9976, precision 0.9978, recall 0.9975, auc 0.9976
epoch 2401, loss 0.0145, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 2501, loss 0.0066, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2601, loss 0.0029, train acc 99.78%, f1 0.9978, precision 0.9975, recall 0.9981, auc 0.9978
epoch 2701, loss 0.0067, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 2801, loss 0.0050, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 2901, loss 0.0028, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 3001, loss 0.0079, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9980, auc 0.9981
epoch 3101, loss 0.0036, train acc 99.81%, f1 0.9981, precision 0.9978, recall 0.9984, auc 0.9981
epoch 3201, loss 0.0045, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 3301, loss 0.0087, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3401, loss 0.0056, train acc 99.84%, f1 0.9984, precision 0.9981, recall 0.9986, auc 0.9984
epoch 3501, loss 0.0075, train acc 99.84%, f1 0.9984, precision 0.9981, recall 0.9987, auc 0.9984
epoch 3601, loss 0.0024, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 3701, loss 0.0058, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 3801, loss 0.0026, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 3901, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9987, auc 0.9985
epoch 4001, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9989, auc 0.9986
epoch 4101, loss 0.0039, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9988, auc 0.9985
epoch 4201, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 4301, loss 0.0071, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4401, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4501, loss 0.0038, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9990, auc 0.9986
epoch 4601, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4701, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9983, auc 0.9987
epoch 4801, loss 0.0034, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4901, loss 0.0073, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 5001, loss 0.0049, train acc 99.89%, f1 0.9989, precision 0.9986, recall 0.9991, auc 0.9989
epoch 5101, loss 0.0013, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5201, loss 0.0094, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 5301, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 5401, loss 0.0043, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 5501, loss 0.0034, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 5601, loss 0.0013, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5701, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9988, auc 0.9990
epoch 5801, loss 0.0013, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9991, auc 0.9990
epoch 5901, loss 0.0045, train acc 99.90%, f1 0.9990, precision 0.9987, recall 0.9993, auc 0.9990
epoch 6001, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9988, auc 0.9990
epoch 6101, loss 0.0040, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9992, auc 0.9990
epoch 6201, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 6301, loss 0.0015, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9989, auc 0.9990
epoch 6401, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9988, recall 0.9993, auc 0.9991
epoch 6501, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 6601, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 6701, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 6801, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9994, auc 0.9992
epoch 6901, loss 0.0060, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 7001, loss 0.0020, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 7101, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 7201, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9994, auc 0.9992
epoch 7301, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9995, auc 0.9992
epoch 7401, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 7501, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9995, recall 0.9991, auc 0.9993
epoch 7601, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9995, auc 0.9993
epoch 7701, loss 0.0048, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 7801, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 7901, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 8001, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 8101, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 8201, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8301, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8401, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 8501, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 8601, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8701, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8801, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 8901, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 9001, loss 0.0025, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9998, auc 0.9996
epoch 9101, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9995, recall 0.9998, auc 0.9997
epoch 9201, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 9301, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9995, auc 0.9996
epoch 9401, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9995, auc 0.9996
epoch 9501, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9601, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 9701, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9996, recall 0.9999, auc 0.9998
epoch 9801, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 9901, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10001, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 10101, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9997, recall 1.0000, auc 0.9998
epoch 10201, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 10301, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10401, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 10501, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9996, recall 1.0000, auc 0.9998
epoch 10601, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 10701, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9997, recall 1.0000, auc 0.9998
epoch 10801, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10901, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11001, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 11101, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 11201, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 11301, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 11401, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11501, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11601, loss 0.0007, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 11701, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 12001, loss 0.0007, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 12201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 12501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_2
./test_vehicle0/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.983624031007752

the Fscore is 0.975

the precision is 0.975

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_2
----------------------



epoch 1, loss 0.6934, train acc 50.07%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.2931, train acc 86.50%, f1 0.8642, precision 0.8679, recall 0.8605, auc 0.8650
epoch 201, loss 0.1396, train acc 97.28%, f1 0.9728, precision 0.9730, recall 0.9725, auc 0.9728
epoch 301, loss 0.0625, train acc 98.43%, f1 0.9843, precision 0.9840, recall 0.9845, auc 0.9843
epoch 401, loss 0.0649, train acc 98.74%, f1 0.9874, precision 0.9868, recall 0.9880, auc 0.9874
epoch 501, loss 0.0544, train acc 98.95%, f1 0.9895, precision 0.9890, recall 0.9900, auc 0.9895
epoch 601, loss 0.0302, train acc 99.13%, f1 0.9913, precision 0.9909, recall 0.9917, auc 0.9913
epoch 701, loss 0.0282, train acc 99.24%, f1 0.9924, precision 0.9925, recall 0.9923, auc 0.9924
epoch 801, loss 0.0340, train acc 99.33%, f1 0.9933, precision 0.9930, recall 0.9937, auc 0.9933
epoch 901, loss 0.0224, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9938, auc 0.9939
epoch 1001, loss 0.0158, train acc 99.46%, f1 0.9946, precision 0.9945, recall 0.9947, auc 0.9946
epoch 1101, loss 0.0198, train acc 99.50%, f1 0.9950, precision 0.9949, recall 0.9951, auc 0.9950
epoch 1201, loss 0.0132, train acc 99.54%, f1 0.9954, precision 0.9956, recall 0.9953, auc 0.9954
epoch 1301, loss 0.0191, train acc 99.58%, f1 0.9957, precision 0.9954, recall 0.9961, auc 0.9958
epoch 1401, loss 0.0097, train acc 99.61%, f1 0.9961, precision 0.9965, recall 0.9956, auc 0.9961
epoch 1501, loss 0.0090, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1601, loss 0.0103, train acc 99.65%, f1 0.9965, precision 0.9968, recall 0.9962, auc 0.9965
epoch 1701, loss 0.0207, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9967, auc 0.9967
epoch 1801, loss 0.0062, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9967, auc 0.9969
epoch 1901, loss 0.0068, train acc 99.70%, f1 0.9970, precision 0.9973, recall 0.9967, auc 0.9970
epoch 2001, loss 0.0128, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9972
epoch 2101, loss 0.0111, train acc 99.74%, f1 0.9974, precision 0.9972, recall 0.9975, auc 0.9974
epoch 2201, loss 0.0068, train acc 99.76%, f1 0.9976, precision 0.9978, recall 0.9973, auc 0.9976
epoch 2301, loss 0.0068, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 2401, loss 0.0081, train acc 99.77%, f1 0.9977, precision 0.9979, recall 0.9975, auc 0.9977
epoch 2501, loss 0.0065, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9979, auc 0.9978
epoch 2601, loss 0.0081, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2701, loss 0.0034, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2801, loss 0.0055, train acc 99.78%, f1 0.9978, precision 0.9973, recall 0.9983, auc 0.9978
epoch 2901, loss 0.0066, train acc 99.79%, f1 0.9979, precision 0.9976, recall 0.9982, auc 0.9979
epoch 3001, loss 0.0053, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3101, loss 0.0063, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 3201, loss 0.0053, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 3301, loss 0.0049, train acc 99.81%, f1 0.9981, precision 0.9978, recall 0.9984, auc 0.9981
epoch 3401, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 3501, loss 0.0040, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 3601, loss 0.0036, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 3701, loss 0.0066, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 3801, loss 0.0037, train acc 99.84%, f1 0.9984, precision 0.9981, recall 0.9987, auc 0.9984
epoch 3901, loss 0.0072, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 4001, loss 0.0080, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 4101, loss 0.0018, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4201, loss 0.0053, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 4301, loss 0.0045, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 4401, loss 0.0057, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4501, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 4601, loss 0.0032, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9989, auc 0.9986
epoch 4701, loss 0.0049, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4801, loss 0.0033, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9984, auc 0.9987
epoch 4901, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 5001, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9984, recall 0.9991, auc 0.9987
epoch 5101, loss 0.0068, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 5201, loss 0.0018, train acc 99.88%, f1 0.9988, precision 0.9992, recall 0.9984, auc 0.9988
epoch 5301, loss 0.0018, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 5401, loss 0.0013, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9987, auc 0.9988
epoch 5501, loss 0.0057, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9987, auc 0.9988
epoch 5601, loss 0.0031, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 5701, loss 0.0011, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5801, loss 0.0022, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 5901, loss 0.0069, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 6001, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9985, recall 0.9993, auc 0.9989
epoch 6101, loss 0.0059, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 6201, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 6301, loss 0.0043, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9987, auc 0.9990
epoch 6401, loss 0.0005, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 6501, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9989, auc 0.9991
epoch 6601, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 6701, loss 0.0023, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 6801, loss 0.0035, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 6901, loss 0.0011, train acc 99.92%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9992
epoch 7001, loss 0.0038, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 7101, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9989, auc 0.9992
epoch 7201, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 7301, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 7401, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7501, loss 0.0033, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 7601, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 7701, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 7801, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7901, loss 0.0031, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9992, auc 0.9994
epoch 8001, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9997, auc 0.9995
epoch 8101, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 8301, loss 0.0023, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 8401, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 8501, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 8601, loss 0.0021, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 8701, loss 0.0024, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 8801, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 8901, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9001, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 9101, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 9201, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 9301, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 9401, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 9501, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9601, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9701, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9801, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 9901, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10001, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10101, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10201, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 10301, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10401, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 10501, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10601, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 10701, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10801, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10901, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11001, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11101, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11201, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 11301, loss 0.0003, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 11401, loss 0.0001, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 11501, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 11601, loss 0.0010, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 11701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 11901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_2
./test_vehicle0/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.983624031007752

the Fscore is 0.975

the precision is 0.975

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_2
----------------------



epoch 1, loss 0.6929, train acc 50.14%, f1 0.6679, precision 0.5014, recall 1.0000, auc 0.5000
epoch 101, loss 0.2794, train acc 87.24%, f1 0.8720, precision 0.8774, recall 0.8665, auc 0.8724
epoch 201, loss 0.1192, train acc 97.16%, f1 0.9716, precision 0.9717, recall 0.9716, auc 0.9716
epoch 301, loss 0.0693, train acc 98.37%, f1 0.9838, precision 0.9840, recall 0.9835, auc 0.9837
epoch 401, loss 0.0669, train acc 98.74%, f1 0.9874, precision 0.9875, recall 0.9873, auc 0.9874
epoch 501, loss 0.0390, train acc 98.98%, f1 0.9898, precision 0.9904, recall 0.9893, auc 0.9898
epoch 601, loss 0.0506, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9916, auc 0.9916
epoch 701, loss 0.0331, train acc 99.29%, f1 0.9929, precision 0.9929, recall 0.9929, auc 0.9929
epoch 801, loss 0.0247, train acc 99.37%, f1 0.9937, precision 0.9938, recall 0.9936, auc 0.9937
epoch 901, loss 0.0220, train acc 99.42%, f1 0.9942, precision 0.9942, recall 0.9943, auc 0.9942
epoch 1001, loss 0.0215, train acc 99.47%, f1 0.9947, precision 0.9948, recall 0.9947, auc 0.9947
epoch 1101, loss 0.0188, train acc 99.52%, f1 0.9952, precision 0.9950, recall 0.9954, auc 0.9952
epoch 1201, loss 0.0209, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1301, loss 0.0124, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 1401, loss 0.0145, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9963, auc 0.9963
epoch 1501, loss 0.0093, train acc 99.64%, f1 0.9964, precision 0.9962, recall 0.9966, auc 0.9964
epoch 1601, loss 0.0086, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9965, auc 0.9965
epoch 1701, loss 0.0172, train acc 99.67%, f1 0.9968, precision 0.9964, recall 0.9971, auc 0.9967
epoch 1801, loss 0.0122, train acc 99.69%, f1 0.9969, precision 0.9967, recall 0.9971, auc 0.9969
epoch 1901, loss 0.0137, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2001, loss 0.0063, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9971, auc 0.9972
epoch 2101, loss 0.0147, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9973
epoch 2201, loss 0.0142, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 2301, loss 0.0117, train acc 99.75%, f1 0.9976, precision 0.9980, recall 0.9971, auc 0.9975
epoch 2401, loss 0.0061, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9975, auc 0.9976
epoch 2501, loss 0.0036, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 2601, loss 0.0016, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 2701, loss 0.0118, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9977, auc 0.9978
epoch 2801, loss 0.0046, train acc 99.78%, f1 0.9978, precision 0.9980, recall 0.9977, auc 0.9978
epoch 2901, loss 0.0092, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9977, auc 0.9979
epoch 3001, loss 0.0082, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 3101, loss 0.0029, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3201, loss 0.0028, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9982, auc 0.9980
epoch 3301, loss 0.0027, train acc 99.80%, f1 0.9980, precision 0.9982, recall 0.9978, auc 0.9980
epoch 3401, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 3501, loss 0.0048, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 3601, loss 0.0066, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 3701, loss 0.0114, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9979, auc 0.9982
epoch 3801, loss 0.0012, train acc 99.82%, f1 0.9982, precision 0.9986, recall 0.9978, auc 0.9982
epoch 3901, loss 0.0051, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 4001, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9980, auc 0.9983
epoch 4101, loss 0.0065, train acc 99.83%, f1 0.9983, precision 0.9986, recall 0.9980, auc 0.9983
epoch 4201, loss 0.0058, train acc 99.83%, f1 0.9984, precision 0.9987, recall 0.9980, auc 0.9983
epoch 4301, loss 0.0082, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9982, auc 0.9984
epoch 4401, loss 0.0083, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4501, loss 0.0084, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 4601, loss 0.0039, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 4701, loss 0.0060, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9982, auc 0.9985
epoch 4801, loss 0.0048, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 4901, loss 0.0068, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9987, auc 0.9986
epoch 5001, loss 0.0060, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 5101, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 5201, loss 0.0037, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 5301, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9989, auc 0.9986
epoch 5401, loss 0.0048, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9985, auc 0.9987
epoch 5501, loss 0.0053, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5601, loss 0.0026, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 5701, loss 0.0026, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9985, auc 0.9988
epoch 5801, loss 0.0025, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 5901, loss 0.0040, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 6001, loss 0.0028, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 6101, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 6201, loss 0.0026, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 6301, loss 0.0043, train acc 99.88%, f1 0.9988, precision 0.9991, recall 0.9985, auc 0.9988
epoch 6401, loss 0.0061, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9991, auc 0.9989
epoch 6501, loss 0.0011, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 6601, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 6701, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9992, recall 0.9987, auc 0.9989
epoch 6801, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 6901, loss 0.0022, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 7001, loss 0.0002, train acc 99.92%, f1 0.9992, precision 0.9988, recall 0.9995, auc 0.9991
epoch 7101, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 7201, loss 0.0047, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9989, auc 0.9991
epoch 7301, loss 0.0080, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9989, auc 0.9991
epoch 7401, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7501, loss 0.0059, train acc 99.91%, f1 0.9991, precision 0.9994, recall 0.9988, auc 0.9991
epoch 7601, loss 0.0008, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 7701, loss 0.0027, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 7801, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 7901, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9990, auc 0.9992
epoch 8001, loss 0.0037, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9994, auc 0.9992
epoch 8101, loss 0.0040, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0027, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 8301, loss 0.0029, train acc 99.92%, f1 0.9992, precision 0.9989, recall 0.9996, auc 0.9992
epoch 8401, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8501, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8601, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9995, auc 0.9993
epoch 8701, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 8801, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 8901, loss 0.0004, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9001, loss 0.0034, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 9101, loss 0.0001, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9996, auc 0.9994
epoch 9201, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9301, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 9401, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 9501, loss 0.0026, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 9601, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9701, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9997, auc 0.9995
epoch 9801, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9997, auc 0.9995
epoch 9901, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_2
./test_vehicle0/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9797480620155039

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_2
----------------------



epoch 1, loss 0.6935, train acc 50.02%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3160, train acc 86.22%, f1 0.8614, precision 0.8663, recall 0.8565, auc 0.8622
epoch 201, loss 0.1472, train acc 97.00%, f1 0.9700, precision 0.9698, recall 0.9702, auc 0.9700
epoch 301, loss 0.0877, train acc 98.37%, f1 0.9837, precision 0.9840, recall 0.9834, auc 0.9837
epoch 401, loss 0.0681, train acc 98.75%, f1 0.9875, precision 0.9876, recall 0.9873, auc 0.9875
epoch 501, loss 0.0507, train acc 98.95%, f1 0.9895, precision 0.9891, recall 0.9900, auc 0.9895
epoch 601, loss 0.0314, train acc 99.12%, f1 0.9912, precision 0.9916, recall 0.9908, auc 0.9912
epoch 701, loss 0.0349, train acc 99.21%, f1 0.9921, precision 0.9919, recall 0.9923, auc 0.9921
epoch 801, loss 0.0387, train acc 99.30%, f1 0.9930, precision 0.9932, recall 0.9928, auc 0.9930
epoch 901, loss 0.0322, train acc 99.38%, f1 0.9938, precision 0.9941, recall 0.9935, auc 0.9938
epoch 1001, loss 0.0183, train acc 99.44%, f1 0.9944, precision 0.9945, recall 0.9943, auc 0.9944
epoch 1101, loss 0.0108, train acc 99.50%, f1 0.9950, precision 0.9952, recall 0.9947, auc 0.9950
epoch 1201, loss 0.0188, train acc 99.53%, f1 0.9953, precision 0.9954, recall 0.9952, auc 0.9953
epoch 1301, loss 0.0121, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9957, auc 0.9958
epoch 1401, loss 0.0040, train acc 99.61%, f1 0.9961, precision 0.9963, recall 0.9958, auc 0.9961
epoch 1501, loss 0.0144, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9963, auc 0.9962
epoch 1601, loss 0.0134, train acc 99.65%, f1 0.9965, precision 0.9966, recall 0.9963, auc 0.9965
epoch 1701, loss 0.0113, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9968, auc 0.9966
epoch 1801, loss 0.0078, train acc 99.68%, f1 0.9968, precision 0.9972, recall 0.9965, auc 0.9968
epoch 1901, loss 0.0100, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9968, auc 0.9970
epoch 2001, loss 0.0082, train acc 99.72%, f1 0.9972, precision 0.9974, recall 0.9969, auc 0.9972
epoch 2101, loss 0.0060, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9971, auc 0.9973
epoch 2201, loss 0.0071, train acc 99.74%, f1 0.9974, precision 0.9979, recall 0.9970, auc 0.9974
epoch 2301, loss 0.0145, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 2401, loss 0.0178, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0062, train acc 99.76%, f1 0.9976, precision 0.9979, recall 0.9973, auc 0.9976
epoch 2601, loss 0.0080, train acc 99.77%, f1 0.9977, precision 0.9981, recall 0.9973, auc 0.9977
epoch 2701, loss 0.0100, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2801, loss 0.0048, train acc 99.79%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9979
epoch 2901, loss 0.0076, train acc 99.79%, f1 0.9979, precision 0.9982, recall 0.9977, auc 0.9979
epoch 3001, loss 0.0031, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 3101, loss 0.0054, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9977, auc 0.9979
epoch 3201, loss 0.0094, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9978, auc 0.9981
epoch 3301, loss 0.0030, train acc 99.80%, f1 0.9980, precision 0.9983, recall 0.9978, auc 0.9980
epoch 3401, loss 0.0102, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 3501, loss 0.0073, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 3601, loss 0.0057, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9984, auc 0.9982
epoch 3701, loss 0.0031, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 3801, loss 0.0017, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9980, auc 0.9982
epoch 3901, loss 0.0084, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 4001, loss 0.0032, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9983, auc 0.9984
epoch 4101, loss 0.0105, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 4201, loss 0.0074, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 4301, loss 0.0054, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9983, auc 0.9985
epoch 4401, loss 0.0094, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 4501, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9986, auc 0.9985
epoch 4601, loss 0.0070, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9983, auc 0.9985
epoch 4701, loss 0.0039, train acc 99.84%, f1 0.9984, precision 0.9987, recall 0.9981, auc 0.9984
epoch 4801, loss 0.0012, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 4901, loss 0.0034, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 5001, loss 0.0036, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5101, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5201, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 5301, loss 0.0074, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9988, auc 0.9986
epoch 5401, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 5501, loss 0.0072, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 5601, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 5701, loss 0.0111, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 5801, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9985, recall 0.9990, auc 0.9988
epoch 5901, loss 0.0060, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9987, auc 0.9988
epoch 6001, loss 0.0019, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 6101, loss 0.0012, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 6201, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9991, recall 0.9985, auc 0.9988
epoch 6301, loss 0.0015, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 6401, loss 0.0055, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 6501, loss 0.0029, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9986, auc 0.9989
epoch 6601, loss 0.0011, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 6701, loss 0.0049, train acc 99.89%, f1 0.9989, precision 0.9992, recall 0.9986, auc 0.9989
epoch 6801, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 6901, loss 0.0016, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9987, auc 0.9990
epoch 7001, loss 0.0010, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 7101, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7201, loss 0.0008, train acc 99.90%, f1 0.9990, precision 0.9993, recall 0.9987, auc 0.9990
epoch 7301, loss 0.0042, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9988, auc 0.9990
epoch 7401, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 7501, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 7601, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 7701, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7801, loss 0.0048, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7901, loss 0.0050, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9992
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_2
./test_vehicle0/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9758720930232558

the Fscore is 0.951219512195122

the precision is 0.9285714285714286

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_2
----------------------



epoch 1, loss 0.6931, train acc 56.74%, f1 0.2306, precision 0.9974, recall 0.1304, auc 0.5650
epoch 101, loss 0.3121, train acc 87.43%, f1 0.8738, precision 0.8724, recall 0.8751, auc 0.8743
epoch 201, loss 0.1229, train acc 97.40%, f1 0.9739, precision 0.9730, recall 0.9748, auc 0.9740
epoch 301, loss 0.0850, train acc 98.41%, f1 0.9840, precision 0.9839, recall 0.9841, auc 0.9841
epoch 401, loss 0.0557, train acc 98.78%, f1 0.9877, precision 0.9874, recall 0.9881, auc 0.9878
epoch 501, loss 0.0380, train acc 98.99%, f1 0.9898, precision 0.9891, recall 0.9906, auc 0.9899
epoch 601, loss 0.0418, train acc 99.14%, f1 0.9914, precision 0.9913, recall 0.9915, auc 0.9914
epoch 701, loss 0.0218, train acc 99.25%, f1 0.9924, precision 0.9922, recall 0.9926, auc 0.9925
epoch 801, loss 0.0315, train acc 99.34%, f1 0.9933, precision 0.9930, recall 0.9937, auc 0.9934
epoch 901, loss 0.0353, train acc 99.40%, f1 0.9939, precision 0.9935, recall 0.9943, auc 0.9940
epoch 1001, loss 0.0210, train acc 99.45%, f1 0.9945, precision 0.9948, recall 0.9942, auc 0.9945
epoch 1101, loss 0.0129, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9951, auc 0.9951
epoch 1201, loss 0.0141, train acc 99.55%, f1 0.9955, precision 0.9954, recall 0.9955, auc 0.9955
epoch 1301, loss 0.0135, train acc 99.59%, f1 0.9959, precision 0.9958, recall 0.9959, auc 0.9959
epoch 1401, loss 0.0099, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9962, auc 0.9961
epoch 1501, loss 0.0167, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9964, auc 0.9963
epoch 1601, loss 0.0100, train acc 99.66%, f1 0.9965, precision 0.9964, recall 0.9967, auc 0.9966
epoch 1701, loss 0.0088, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9966, auc 0.9967
epoch 1801, loss 0.0084, train acc 99.67%, f1 0.9967, precision 0.9966, recall 0.9968, auc 0.9967
epoch 1901, loss 0.0128, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9968, auc 0.9970
epoch 2001, loss 0.0112, train acc 99.72%, f1 0.9971, precision 0.9974, recall 0.9969, auc 0.9972
epoch 2101, loss 0.0075, train acc 99.72%, f1 0.9972, precision 0.9974, recall 0.9970, auc 0.9972
epoch 2201, loss 0.0087, train acc 99.74%, f1 0.9974, precision 0.9971, recall 0.9976, auc 0.9974
epoch 2301, loss 0.0148, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9973
epoch 2401, loss 0.0134, train acc 99.76%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0102, train acc 99.76%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9976
epoch 2601, loss 0.0100, train acc 99.76%, f1 0.9976, precision 0.9974, recall 0.9979, auc 0.9976
epoch 2701, loss 0.0046, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2801, loss 0.0092, train acc 99.78%, f1 0.9978, precision 0.9975, recall 0.9980, auc 0.9978
epoch 2901, loss 0.0100, train acc 99.77%, f1 0.9977, precision 0.9972, recall 0.9982, auc 0.9977
epoch 3001, loss 0.0062, train acc 99.78%, f1 0.9977, precision 0.9975, recall 0.9979, auc 0.9978
epoch 3101, loss 0.0066, train acc 99.79%, f1 0.9978, precision 0.9973, recall 0.9983, auc 0.9979
epoch 3201, loss 0.0143, train acc 99.80%, f1 0.9979, precision 0.9977, recall 0.9982, auc 0.9980
epoch 3301, loss 0.0080, train acc 99.80%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9980
epoch 3401, loss 0.0096, train acc 99.80%, f1 0.9980, precision 0.9976, recall 0.9984, auc 0.9980
epoch 3501, loss 0.0079, train acc 99.80%, f1 0.9980, precision 0.9975, recall 0.9985, auc 0.9980
epoch 3601, loss 0.0082, train acc 99.80%, f1 0.9980, precision 0.9976, recall 0.9984, auc 0.9980
epoch 3701, loss 0.0045, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 3801, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9979, recall 0.9985, auc 0.9982
epoch 3901, loss 0.0024, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 4001, loss 0.0045, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9985, auc 0.9983
epoch 4101, loss 0.0065, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 4201, loss 0.0100, train acc 99.82%, f1 0.9982, precision 0.9976, recall 0.9988, auc 0.9982
epoch 4301, loss 0.0084, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9985, auc 0.9983
epoch 4401, loss 0.0047, train acc 99.83%, f1 0.9982, precision 0.9980, recall 0.9985, auc 0.9983
epoch 4501, loss 0.0053, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4601, loss 0.0072, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4701, loss 0.0020, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9986, auc 0.9983
epoch 4801, loss 0.0090, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9982, auc 0.9984
epoch 4901, loss 0.0020, train acc 99.83%, f1 0.9983, precision 0.9978, recall 0.9988, auc 0.9983
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_2
./test_vehicle0/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9556201550387597

the Fscore is 0.9156626506024096

the precision is 0.8837209302325582

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_2
----------------------



epoch 1, loss 0.6924, train acc 57.42%, f1 0.7007, precision 0.5393, recall 1.0000, auc 0.5755
epoch 101, loss 0.3456, train acc 87.21%, f1 0.8725, precision 0.8671, recall 0.8779, auc 0.8721
epoch 201, loss 0.1527, train acc 97.22%, f1 0.9721, precision 0.9720, recall 0.9723, auc 0.9722
epoch 301, loss 0.0775, train acc 98.44%, f1 0.9843, precision 0.9850, recall 0.9836, auc 0.9844
epoch 401, loss 0.0571, train acc 98.72%, f1 0.9872, precision 0.9874, recall 0.9869, auc 0.9872
epoch 501, loss 0.0271, train acc 99.02%, f1 0.9901, precision 0.9908, recall 0.9895, auc 0.9902
epoch 601, loss 0.0319, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9915, auc 0.9916
epoch 701, loss 0.0189, train acc 99.26%, f1 0.9926, precision 0.9930, recall 0.9922, auc 0.9926
epoch 801, loss 0.0289, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9933, auc 0.9934
epoch 901, loss 0.0203, train acc 99.41%, f1 0.9940, precision 0.9944, recall 0.9937, auc 0.9940
epoch 1001, loss 0.0224, train acc 99.45%, f1 0.9945, precision 0.9949, recall 0.9941, auc 0.9945
epoch 1101, loss 0.0168, train acc 99.51%, f1 0.9951, precision 0.9949, recall 0.9953, auc 0.9951
epoch 1201, loss 0.0142, train acc 99.56%, f1 0.9956, precision 0.9958, recall 0.9953, auc 0.9956
epoch 1301, loss 0.0157, train acc 99.56%, f1 0.9956, precision 0.9957, recall 0.9955, auc 0.9956
epoch 1401, loss 0.0177, train acc 99.60%, f1 0.9960, precision 0.9964, recall 0.9956, auc 0.9960
epoch 1501, loss 0.0160, train acc 99.63%, f1 0.9963, precision 0.9961, recall 0.9964, auc 0.9963
epoch 1601, loss 0.0087, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9962, auc 0.9964
epoch 1701, loss 0.0090, train acc 99.66%, f1 0.9966, precision 0.9969, recall 0.9964, auc 0.9966
epoch 1801, loss 0.0086, train acc 99.69%, f1 0.9969, precision 0.9971, recall 0.9967, auc 0.9969
epoch 1901, loss 0.0073, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9969, auc 0.9970
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_2
./test_vehicle0/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9478682170542636

the Fscore is 0.8941176470588236

the precision is 0.8444444444444444

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_2
----------------------



epoch 1, loss 0.6931, train acc 57.34%, f1 0.7000, precision 0.5398, recall 0.9952, auc 0.5734
epoch 101, loss 0.4120, train acc 82.55%, f1 0.8254, precision 0.8259, recall 0.8248, auc 0.8255
epoch 201, loss 0.2474, train acc 95.00%, f1 0.9500, precision 0.9501, recall 0.9498, auc 0.9500
epoch 301, loss 0.1208, train acc 97.91%, f1 0.9791, precision 0.9791, recall 0.9791, auc 0.9791
epoch 401, loss 0.0902, train acc 98.24%, f1 0.9824, precision 0.9823, recall 0.9824, auc 0.9824
epoch 501, loss 0.0691, train acc 98.70%, f1 0.9870, precision 0.9870, recall 0.9870, auc 0.9870
epoch 601, loss 0.0703, train acc 98.88%, f1 0.9888, precision 0.9888, recall 0.9888, auc 0.9888
epoch 701, loss 0.0329, train acc 98.99%, f1 0.9899, precision 0.9899, recall 0.9899, auc 0.9899
epoch 801, loss 0.0312, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9916, auc 0.9916
epoch 901, loss 0.0244, train acc 99.23%, f1 0.9923, precision 0.9923, recall 0.9923, auc 0.9923
epoch 1001, loss 0.0348, train acc 99.30%, f1 0.9930, precision 0.9930, recall 0.9930, auc 0.9930
epoch 1101, loss 0.0326, train acc 99.34%, f1 0.9934, precision 0.9933, recall 0.9934, auc 0.9934
epoch 1201, loss 0.0315, train acc 99.38%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9938
epoch 1301, loss 0.0181, train acc 99.44%, f1 0.9944, precision 0.9944, recall 0.9944, auc 0.9944
epoch 1401, loss 0.0122, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9948, auc 0.9947
epoch 1501, loss 0.0184, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1601, loss 0.0092, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1701, loss 0.0167, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9959, auc 0.9958
epoch 1801, loss 0.0180, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1901, loss 0.0097, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2001, loss 0.0165, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 2101, loss 0.0151, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2201, loss 0.0179, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2301, loss 0.0207, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2401, loss 0.0142, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2501, loss 0.0108, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2601, loss 0.0068, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9972, auc 0.9971
epoch 2701, loss 0.0114, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2801, loss 0.0153, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2901, loss 0.0141, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3001, loss 0.0102, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3101, loss 0.0082, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3201, loss 0.0093, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3301, loss 0.0102, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3401, loss 0.0087, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 3501, loss 0.0052, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3601, loss 0.0032, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3701, loss 0.0089, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3801, loss 0.0106, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3901, loss 0.0091, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4001, loss 0.0076, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4101, loss 0.0063, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4201, loss 0.0079, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4301, loss 0.0026, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4401, loss 0.0116, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4501, loss 0.0027, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4601, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4701, loss 0.0058, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4801, loss 0.0027, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 4901, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5001, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5101, loss 0.0054, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5201, loss 0.0035, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5301, loss 0.0070, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5401, loss 0.0060, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5501, loss 0.0032, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5601, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5701, loss 0.0038, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5801, loss 0.0023, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5901, loss 0.0038, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6001, loss 0.0054, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6101, loss 0.0062, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 6201, loss 0.0060, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6301, loss 0.0075, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 6401, loss 0.0039, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6501, loss 0.0076, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6601, loss 0.0059, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6701, loss 0.0014, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6801, loss 0.0038, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6901, loss 0.0066, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 7001, loss 0.0061, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7101, loss 0.0041, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7201, loss 0.0033, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7301, loss 0.0061, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7401, loss 0.0043, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 7501, loss 0.0050, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7601, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7701, loss 0.0044, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7801, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7901, loss 0.0025, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8001, loss 0.0037, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8101, loss 0.0058, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8201, loss 0.0064, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8301, loss 0.0042, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8401, loss 0.0019, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8501, loss 0.0049, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8601, loss 0.0043, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8701, loss 0.0085, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 8801, loss 0.0027, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 8901, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 9001, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9101, loss 0.0033, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9201, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9301, loss 0.0009, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9401, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9501, loss 0.0015, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9601, loss 0.0003, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9701, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 9801, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9901, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10001, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10101, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10201, loss 0.0008, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10301, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10401, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10501, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10601, loss 0.0009, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10701, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10801, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10901, loss 0.0034, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11001, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11101, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11201, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11301, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11401, loss 0.0011, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11501, loss 0.0003, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11601, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11701, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 11801, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11901, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12001, loss 0.0003, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12101, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12201, loss 0.0018, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 12301, loss 0.0028, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12401, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 12501, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12601, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12701, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12801, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12901, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13001, loss 0.0028, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13101, loss 0.0032, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 13201, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13301, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13401, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13501, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 13601, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13701, loss 0.0024, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13801, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13901, loss 0.0001, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14001, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14101, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14201, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14301, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14401, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 14501, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 14601, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14701, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14801, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14901, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15001, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15101, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 15201, loss 0.0001, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15301, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15401, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15501, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15601, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15701, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15801, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15901, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16001, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16101, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16201, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16301, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16401, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16501, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16701, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 16801, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16901, loss 0.0034, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17001, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17101, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17201, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17301, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17401, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17501, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17601, loss 0.0024, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17701, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17801, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17901, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18001, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18101, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18201, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18301, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18401, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18501, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18601, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18701, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18801, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18901, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19001, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19101, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19201, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19301, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19401, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19501, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19601, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19701, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19801, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19901, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_2
./test_vehicle0/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9797480620155039

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_2
----------------------



epoch 1, loss 0.6931, train acc 58.03%, f1 0.2861, precision 0.9567, recall 0.1682, auc 0.5803
epoch 101, loss 0.4264, train acc 82.34%, f1 0.8227, precision 0.8259, recall 0.8195, auc 0.8234
epoch 201, loss 0.2144, train acc 95.14%, f1 0.9513, precision 0.9521, recall 0.9506, auc 0.9514
epoch 301, loss 0.1204, train acc 97.89%, f1 0.9789, precision 0.9789, recall 0.9790, auc 0.9789
epoch 401, loss 0.0902, train acc 98.44%, f1 0.9844, precision 0.9843, recall 0.9845, auc 0.9844
epoch 501, loss 0.0606, train acc 98.72%, f1 0.9872, precision 0.9871, recall 0.9873, auc 0.9872
epoch 601, loss 0.0411, train acc 98.91%, f1 0.9891, precision 0.9890, recall 0.9893, auc 0.9891
epoch 701, loss 0.0317, train acc 99.05%, f1 0.9905, precision 0.9903, recall 0.9907, auc 0.9905
epoch 801, loss 0.0222, train acc 99.13%, f1 0.9913, precision 0.9911, recall 0.9914, auc 0.9913
epoch 901, loss 0.0268, train acc 99.24%, f1 0.9924, precision 0.9922, recall 0.9925, auc 0.9924
epoch 1001, loss 0.0316, train acc 99.30%, f1 0.9930, precision 0.9929, recall 0.9932, auc 0.9930
epoch 1101, loss 0.0209, train acc 99.35%, f1 0.9935, precision 0.9934, recall 0.9937, auc 0.9935
epoch 1201, loss 0.0193, train acc 99.39%, f1 0.9939, precision 0.9938, recall 0.9941, auc 0.9939
epoch 1301, loss 0.0178, train acc 99.45%, f1 0.9945, precision 0.9944, recall 0.9945, auc 0.9945
epoch 1401, loss 0.0097, train acc 99.49%, f1 0.9949, precision 0.9947, recall 0.9950, auc 0.9949
epoch 1501, loss 0.0221, train acc 99.52%, f1 0.9952, precision 0.9951, recall 0.9953, auc 0.9952
epoch 1601, loss 0.0119, train acc 99.55%, f1 0.9955, precision 0.9954, recall 0.9956, auc 0.9955
epoch 1701, loss 0.0058, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9958, auc 0.9958
epoch 1801, loss 0.0142, train acc 99.60%, f1 0.9960, precision 0.9959, recall 0.9961, auc 0.9960
epoch 1901, loss 0.0206, train acc 99.62%, f1 0.9962, precision 0.9961, recall 0.9962, auc 0.9962
epoch 2001, loss 0.0131, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9964, auc 0.9963
epoch 2101, loss 0.0064, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9966, auc 0.9966
epoch 2201, loss 0.0043, train acc 99.67%, f1 0.9967, precision 0.9966, recall 0.9968, auc 0.9967
epoch 2301, loss 0.0049, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9968, auc 0.9967
epoch 2401, loss 0.0154, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 2501, loss 0.0165, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9971, auc 0.9970
epoch 2601, loss 0.0124, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9972, auc 0.9971
epoch 2701, loss 0.0075, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2801, loss 0.0097, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9974, auc 0.9974
epoch 2901, loss 0.0077, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 3001, loss 0.0116, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9975, auc 0.9975
epoch 3101, loss 0.0068, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3201, loss 0.0041, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3301, loss 0.0109, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 3401, loss 0.0075, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 3501, loss 0.0098, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 3601, loss 0.0076, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3701, loss 0.0037, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3801, loss 0.0098, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3901, loss 0.0076, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 4001, loss 0.0088, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 4101, loss 0.0067, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 4201, loss 0.0096, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4301, loss 0.0062, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4401, loss 0.0092, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4501, loss 0.0027, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 4601, loss 0.0086, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 4701, loss 0.0038, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 4801, loss 0.0027, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4901, loss 0.0035, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5001, loss 0.0027, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 5101, loss 0.0012, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5201, loss 0.0023, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 5301, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 5401, loss 0.0032, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5501, loss 0.0134, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5601, loss 0.0047, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5701, loss 0.0065, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5801, loss 0.0015, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 5901, loss 0.0070, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6001, loss 0.0044, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6101, loss 0.0056, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6201, loss 0.0045, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 6301, loss 0.0039, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6401, loss 0.0022, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6501, loss 0.0073, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6601, loss 0.0038, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6701, loss 0.0026, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6801, loss 0.0033, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6901, loss 0.0027, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7001, loss 0.0090, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7101, loss 0.0079, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7201, loss 0.0027, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 7301, loss 0.0045, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7401, loss 0.0003, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7501, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7601, loss 0.0012, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7701, loss 0.0018, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 7801, loss 0.0019, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 7901, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8001, loss 0.0045, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8101, loss 0.0036, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 8201, loss 0.0009, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0061, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8401, loss 0.0037, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 8501, loss 0.0028, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8601, loss 0.0051, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8701, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8801, loss 0.0013, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8901, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 9001, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9101, loss 0.0019, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9201, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 9301, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9401, loss 0.0012, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9501, loss 0.0033, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9601, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9701, loss 0.0004, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9801, loss 0.0065, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9901, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10001, loss 0.0031, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10101, loss 0.0020, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 10201, loss 0.0012, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 10301, loss 0.0031, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 10401, loss 0.0034, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 10501, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10601, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10701, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10801, loss 0.0006, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10901, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11001, loss 0.0032, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11101, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11201, loss 0.0004, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11301, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11401, loss 0.0031, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 11501, loss 0.0036, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 11601, loss 0.0035, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11701, loss 0.0007, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 11801, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 11901, loss 0.0006, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12001, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12101, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 12201, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12301, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12401, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12501, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12601, loss 0.0034, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12701, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12801, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 12901, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13001, loss 0.0033, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13101, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 13201, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13301, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 13401, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13501, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 13601, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13701, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13801, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13901, loss 0.0036, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14001, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14101, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14201, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14301, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14401, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14501, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 14601, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14701, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14801, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14901, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_2
./test_vehicle0/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9797480620155039

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_2
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4196, train acc 82.74%, f1 0.8272, precision 0.8278, recall 0.8267, auc 0.8274
epoch 201, loss 0.2202, train acc 95.22%, f1 0.9522, precision 0.9524, recall 0.9521, auc 0.9522
epoch 301, loss 0.1160, train acc 97.92%, f1 0.9792, precision 0.9792, recall 0.9793, auc 0.9792
epoch 401, loss 0.0764, train acc 98.34%, f1 0.9834, precision 0.9834, recall 0.9834, auc 0.9834
epoch 501, loss 0.0786, train acc 98.71%, f1 0.9871, precision 0.9871, recall 0.9871, auc 0.9871
epoch 601, loss 0.0469, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9890, auc 0.9890
epoch 701, loss 0.0347, train acc 99.03%, f1 0.9903, precision 0.9903, recall 0.9903, auc 0.9903
epoch 801, loss 0.0465, train acc 99.14%, f1 0.9914, precision 0.9914, recall 0.9914, auc 0.9914
epoch 901, loss 0.0316, train acc 99.23%, f1 0.9923, precision 0.9923, recall 0.9923, auc 0.9923
epoch 1001, loss 0.0236, train acc 99.28%, f1 0.9928, precision 0.9928, recall 0.9929, auc 0.9928
epoch 1101, loss 0.0443, train acc 99.35%, f1 0.9935, precision 0.9935, recall 0.9935, auc 0.9935
epoch 1201, loss 0.0104, train acc 99.40%, f1 0.9940, precision 0.9940, recall 0.9940, auc 0.9940
epoch 1301, loss 0.0232, train acc 99.44%, f1 0.9944, precision 0.9943, recall 0.9944, auc 0.9944
epoch 1401, loss 0.0167, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1501, loss 0.0124, train acc 99.51%, f1 0.9951, precision 0.9950, recall 0.9951, auc 0.9951
epoch 1601, loss 0.0141, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9953, auc 0.9953
epoch 1701, loss 0.0099, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1801, loss 0.0041, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1901, loss 0.0098, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 2001, loss 0.0135, train acc 99.64%, f1 0.9964, precision 0.9963, recall 0.9964, auc 0.9964
epoch 2101, loss 0.0129, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 2201, loss 0.0073, train acc 99.67%, f1 0.9967, precision 0.9966, recall 0.9967, auc 0.9967
epoch 2301, loss 0.0167, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2401, loss 0.0119, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2501, loss 0.0057, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2601, loss 0.0089, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2701, loss 0.0041, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2801, loss 0.0100, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2901, loss 0.0090, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3001, loss 0.0070, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3101, loss 0.0066, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3201, loss 0.0120, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3301, loss 0.0087, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9976
epoch 3401, loss 0.0119, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3501, loss 0.0034, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3601, loss 0.0060, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3701, loss 0.0057, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3801, loss 0.0066, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3901, loss 0.0076, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4001, loss 0.0093, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4101, loss 0.0084, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4201, loss 0.0063, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4301, loss 0.0051, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4401, loss 0.0059, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4501, loss 0.0131, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4601, loss 0.0046, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4701, loss 0.0042, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4801, loss 0.0021, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4901, loss 0.0040, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5001, loss 0.0032, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5101, loss 0.0077, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5201, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5301, loss 0.0075, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5401, loss 0.0051, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5501, loss 0.0017, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5601, loss 0.0081, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5701, loss 0.0032, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5801, loss 0.0048, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5901, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6001, loss 0.0052, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6101, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6201, loss 0.0059, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6301, loss 0.0011, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6401, loss 0.0049, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6501, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6601, loss 0.0082, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6701, loss 0.0032, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6801, loss 0.0013, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6901, loss 0.0045, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7001, loss 0.0041, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7101, loss 0.0056, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7201, loss 0.0025, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7301, loss 0.0025, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7401, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7501, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 7601, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7701, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7801, loss 0.0031, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7901, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 8001, loss 0.0045, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8101, loss 0.0050, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8201, loss 0.0051, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 8401, loss 0.0063, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8501, loss 0.0028, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8601, loss 0.0035, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8701, loss 0.0014, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8801, loss 0.0038, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8901, loss 0.0040, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9001, loss 0.0013, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9101, loss 0.0018, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 9201, loss 0.0008, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9301, loss 0.0046, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 9401, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9501, loss 0.0020, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9601, loss 0.0006, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9701, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9801, loss 0.0015, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9901, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_2
./test_vehicle0/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9633720930232557

the Fscore is 0.9382716049382716

the precision is 0.926829268292683

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_2
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4074, train acc 82.50%, f1 0.8253, precision 0.8241, recall 0.8266, auc 0.8250
epoch 201, loss 0.2250, train acc 95.32%, f1 0.9532, precision 0.9529, recall 0.9535, auc 0.9532
epoch 301, loss 0.1427, train acc 97.89%, f1 0.9789, precision 0.9789, recall 0.9789, auc 0.9789
epoch 401, loss 0.0733, train acc 98.40%, f1 0.9840, precision 0.9841, recall 0.9839, auc 0.9840
epoch 501, loss 0.0580, train acc 98.69%, f1 0.9869, precision 0.9870, recall 0.9868, auc 0.9869
epoch 601, loss 0.0545, train acc 98.90%, f1 0.9890, precision 0.9891, recall 0.9889, auc 0.9890
epoch 701, loss 0.0440, train acc 99.02%, f1 0.9902, precision 0.9903, recall 0.9902, auc 0.9902
epoch 801, loss 0.0533, train acc 99.14%, f1 0.9914, precision 0.9914, recall 0.9914, auc 0.9914
epoch 901, loss 0.0568, train acc 99.23%, f1 0.9923, precision 0.9923, recall 0.9922, auc 0.9923
epoch 1001, loss 0.0299, train acc 99.29%, f1 0.9929, precision 0.9930, recall 0.9929, auc 0.9929
epoch 1101, loss 0.0270, train acc 99.37%, f1 0.9937, precision 0.9937, recall 0.9936, auc 0.9937
epoch 1201, loss 0.0210, train acc 99.39%, f1 0.9939, precision 0.9940, recall 0.9938, auc 0.9939
epoch 1301, loss 0.0218, train acc 99.44%, f1 0.9944, precision 0.9944, recall 0.9944, auc 0.9944
epoch 1401, loss 0.0108, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9947, auc 0.9948
epoch 1501, loss 0.0195, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9952, auc 0.9953
epoch 1601, loss 0.0100, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9955, auc 0.9956
epoch 1701, loss 0.0117, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9957, auc 0.9958
epoch 1801, loss 0.0085, train acc 99.60%, f1 0.9960, precision 0.9961, recall 0.9960, auc 0.9960
epoch 1901, loss 0.0084, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2001, loss 0.0127, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9963, auc 0.9964
epoch 2101, loss 0.0114, train acc 99.65%, f1 0.9965, precision 0.9966, recall 0.9965, auc 0.9965
epoch 2201, loss 0.0179, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2301, loss 0.0092, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2401, loss 0.0146, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2501, loss 0.0077, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2601, loss 0.0117, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2701, loss 0.0090, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2801, loss 0.0062, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2901, loss 0.0109, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3001, loss 0.0059, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3101, loss 0.0078, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 3201, loss 0.0067, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 3301, loss 0.0063, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3401, loss 0.0063, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9977, auc 0.9977
epoch 3501, loss 0.0106, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3601, loss 0.0083, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9976, auc 0.9977
epoch 3701, loss 0.0038, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9977, auc 0.9977
epoch 3801, loss 0.0071, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3901, loss 0.0047, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4001, loss 0.0068, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 4101, loss 0.0110, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4201, loss 0.0049, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4301, loss 0.0064, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4401, loss 0.0080, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4501, loss 0.0087, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4601, loss 0.0113, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4701, loss 0.0092, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4801, loss 0.0050, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4901, loss 0.0064, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5001, loss 0.0065, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5101, loss 0.0077, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5201, loss 0.0040, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5301, loss 0.0041, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5401, loss 0.0080, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5501, loss 0.0019, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5601, loss 0.0106, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5701, loss 0.0090, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5801, loss 0.0018, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5901, loss 0.0049, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6001, loss 0.0069, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6101, loss 0.0025, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6201, loss 0.0069, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6301, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6401, loss 0.0064, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6501, loss 0.0045, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6601, loss 0.0056, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6701, loss 0.0033, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6801, loss 0.0077, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6901, loss 0.0109, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 7001, loss 0.0039, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 7101, loss 0.0084, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 7201, loss 0.0064, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 7301, loss 0.0059, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 7401, loss 0.0044, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 7501, loss 0.0028, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 7601, loss 0.0006, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 7701, loss 0.0026, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 7801, loss 0.0025, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 7901, loss 0.0048, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_2
./test_vehicle0/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9719961240310078

the Fscore is 0.9397590361445783

the precision is 0.9069767441860465

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_2
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3973, train acc 82.59%, f1 0.8259, precision 0.8260, recall 0.8259, auc 0.8259
epoch 201, loss 0.2493, train acc 95.12%, f1 0.9512, precision 0.9512, recall 0.9512, auc 0.9512
epoch 301, loss 0.1195, train acc 97.88%, f1 0.9788, precision 0.9788, recall 0.9788, auc 0.9788
epoch 401, loss 0.0608, train acc 98.37%, f1 0.9837, precision 0.9837, recall 0.9837, auc 0.9837
epoch 501, loss 0.0589, train acc 98.70%, f1 0.9870, precision 0.9870, recall 0.9870, auc 0.9870
epoch 601, loss 0.0425, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9892, auc 0.9892
epoch 701, loss 0.0348, train acc 99.06%, f1 0.9906, precision 0.9906, recall 0.9906, auc 0.9906
epoch 801, loss 0.0304, train acc 99.14%, f1 0.9914, precision 0.9914, recall 0.9914, auc 0.9914
epoch 901, loss 0.0200, train acc 99.24%, f1 0.9924, precision 0.9924, recall 0.9924, auc 0.9924
epoch 1001, loss 0.0288, train acc 99.31%, f1 0.9931, precision 0.9931, recall 0.9931, auc 0.9931
epoch 1101, loss 0.0180, train acc 99.36%, f1 0.9936, precision 0.9936, recall 0.9936, auc 0.9936
epoch 1201, loss 0.0171, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1301, loss 0.0115, train acc 99.44%, f1 0.9944, precision 0.9944, recall 0.9944, auc 0.9944
epoch 1401, loss 0.0094, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9947, auc 0.9947
epoch 1501, loss 0.0212, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1601, loss 0.0090, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1701, loss 0.0147, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1801, loss 0.0140, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1901, loss 0.0152, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2001, loss 0.0150, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 2101, loss 0.0184, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2201, loss 0.0120, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 2301, loss 0.0105, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2401, loss 0.0120, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2501, loss 0.0064, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2601, loss 0.0065, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2701, loss 0.0134, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2801, loss 0.0085, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2901, loss 0.0094, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3001, loss 0.0075, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3101, loss 0.0103, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3201, loss 0.0144, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3301, loss 0.0067, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3401, loss 0.0055, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3501, loss 0.0079, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3601, loss 0.0055, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3701, loss 0.0130, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3801, loss 0.0067, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3901, loss 0.0024, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 4001, loss 0.0081, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9979, auc 0.9978
epoch 4101, loss 0.0020, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4201, loss 0.0066, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4301, loss 0.0097, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4401, loss 0.0076, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4501, loss 0.0034, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4601, loss 0.0086, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4701, loss 0.0147, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4801, loss 0.0040, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4901, loss 0.0091, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_2
./test_vehicle0/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9594961240310077

the Fscore is 0.9268292682926829

the precision is 0.9047619047619048

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_2
----------------------



epoch 1, loss 0.6935, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4032, train acc 82.99%, f1 0.8300, precision 0.8299, recall 0.8301, auc 0.8299
epoch 201, loss 0.1939, train acc 95.14%, f1 0.9514, precision 0.9514, recall 0.9514, auc 0.9514
epoch 301, loss 0.1194, train acc 97.78%, f1 0.9778, precision 0.9778, recall 0.9778, auc 0.9778
epoch 401, loss 0.0674, train acc 98.51%, f1 0.9851, precision 0.9851, recall 0.9851, auc 0.9851
epoch 501, loss 0.0795, train acc 98.71%, f1 0.9871, precision 0.9871, recall 0.9871, auc 0.9871
epoch 601, loss 0.0567, train acc 98.87%, f1 0.9887, precision 0.9887, recall 0.9887, auc 0.9887
epoch 701, loss 0.0450, train acc 99.03%, f1 0.9903, precision 0.9903, recall 0.9902, auc 0.9903
epoch 801, loss 0.0376, train acc 99.15%, f1 0.9915, precision 0.9915, recall 0.9915, auc 0.9915
epoch 901, loss 0.0325, train acc 99.24%, f1 0.9924, precision 0.9924, recall 0.9924, auc 0.9924
epoch 1001, loss 0.0147, train acc 99.30%, f1 0.9930, precision 0.9930, recall 0.9930, auc 0.9930
epoch 1101, loss 0.0308, train acc 99.35%, f1 0.9935, precision 0.9935, recall 0.9935, auc 0.9935
epoch 1201, loss 0.0367, train acc 99.40%, f1 0.9940, precision 0.9940, recall 0.9940, auc 0.9940
epoch 1301, loss 0.0168, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9946
epoch 1401, loss 0.0223, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 1501, loss 0.0109, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9953, auc 0.9953
epoch 1601, loss 0.0244, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1701, loss 0.0227, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 1801, loss 0.0074, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1901, loss 0.0152, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_2
./test_vehicle0/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9603682170542637

the Fscore is 0.9069767441860466

the precision is 0.8478260869565217

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_2
----------------------



epoch 1, loss 0.6933, train acc 50.02%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4610, train acc 82.39%, f1 0.8231, precision 0.8262, recall 0.8201, auc 0.8239
epoch 201, loss 0.2180, train acc 95.07%, f1 0.9507, precision 0.9504, recall 0.9510, auc 0.9507
epoch 301, loss 0.1141, train acc 97.73%, f1 0.9773, precision 0.9774, recall 0.9772, auc 0.9773
epoch 401, loss 0.0759, train acc 98.30%, f1 0.9830, precision 0.9831, recall 0.9829, auc 0.9830
epoch 501, loss 0.0793, train acc 98.68%, f1 0.9868, precision 0.9866, recall 0.9870, auc 0.9868
epoch 601, loss 0.0532, train acc 98.90%, f1 0.9890, precision 0.9885, recall 0.9895, auc 0.9890
epoch 701, loss 0.0505, train acc 99.03%, f1 0.9903, precision 0.9902, recall 0.9904, auc 0.9903
epoch 801, loss 0.0274, train acc 99.15%, f1 0.9915, precision 0.9911, recall 0.9920, auc 0.9915
epoch 901, loss 0.0375, train acc 99.21%, f1 0.9921, precision 0.9922, recall 0.9921, auc 0.9921
epoch 1001, loss 0.0195, train acc 99.29%, f1 0.9929, precision 0.9925, recall 0.9933, auc 0.9929
epoch 1101, loss 0.0267, train acc 99.36%, f1 0.9936, precision 0.9933, recall 0.9938, auc 0.9936
epoch 1201, loss 0.0230, train acc 99.39%, f1 0.9939, precision 0.9937, recall 0.9942, auc 0.9939
epoch 1301, loss 0.0109, train acc 99.44%, f1 0.9944, precision 0.9941, recall 0.9946, auc 0.9944
epoch 1401, loss 0.0126, train acc 99.48%, f1 0.9948, precision 0.9946, recall 0.9950, auc 0.9948
epoch 1501, loss 0.0186, train acc 99.52%, f1 0.9952, precision 0.9951, recall 0.9954, auc 0.9952
epoch 1601, loss 0.0152, train acc 99.55%, f1 0.9955, precision 0.9953, recall 0.9957, auc 0.9955
epoch 1701, loss 0.0135, train acc 99.58%, f1 0.9958, precision 0.9956, recall 0.9961, auc 0.9958
epoch 1801, loss 0.0224, train acc 99.60%, f1 0.9960, precision 0.9958, recall 0.9962, auc 0.9960
epoch 1901, loss 0.0104, train acc 99.63%, f1 0.9963, precision 0.9961, recall 0.9965, auc 0.9963
epoch 2001, loss 0.0135, train acc 99.64%, f1 0.9964, precision 0.9962, recall 0.9966, auc 0.9964
epoch 2101, loss 0.0092, train acc 99.66%, f1 0.9966, precision 0.9964, recall 0.9967, auc 0.9966
epoch 2201, loss 0.0171, train acc 99.66%, f1 0.9966, precision 0.9964, recall 0.9968, auc 0.9966
epoch 2301, loss 0.0159, train acc 99.68%, f1 0.9968, precision 0.9966, recall 0.9970, auc 0.9968
epoch 2401, loss 0.0037, train acc 99.69%, f1 0.9969, precision 0.9967, recall 0.9972, auc 0.9969
epoch 2501, loss 0.0100, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9972, auc 0.9971
epoch 2601, loss 0.0073, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9973, auc 0.9971
epoch 2701, loss 0.0122, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9974, auc 0.9973
epoch 2801, loss 0.0079, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9976, auc 0.9973
epoch 2901, loss 0.0089, train acc 99.74%, f1 0.9974, precision 0.9972, recall 0.9975, auc 0.9974
epoch 3001, loss 0.0041, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9976
epoch 3101, loss 0.0094, train acc 99.76%, f1 0.9976, precision 0.9974, recall 0.9977, auc 0.9976
epoch 3201, loss 0.0091, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9979, auc 0.9977
epoch 3301, loss 0.0030, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 3401, loss 0.0042, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 3501, loss 0.0085, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 3601, loss 0.0086, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9980, auc 0.9978
epoch 3701, loss 0.0060, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9981, auc 0.9979
epoch 3801, loss 0.0036, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 3901, loss 0.0034, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4001, loss 0.0079, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 4101, loss 0.0088, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 4201, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 4301, loss 0.0038, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 4401, loss 0.0040, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 4501, loss 0.0035, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 4601, loss 0.0032, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9985, auc 0.9983
epoch 4701, loss 0.0023, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 4801, loss 0.0024, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 4901, loss 0.0070, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 5001, loss 0.0049, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9986, auc 0.9985
epoch 5101, loss 0.0034, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 5201, loss 0.0038, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 5301, loss 0.0050, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 5401, loss 0.0025, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 5501, loss 0.0027, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 5601, loss 0.0056, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9988, auc 0.9986
epoch 5701, loss 0.0027, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9987, auc 0.9986
epoch 5801, loss 0.0031, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9988, auc 0.9986
epoch 5901, loss 0.0039, train acc 99.87%, f1 0.9987, precision 0.9984, recall 0.9989, auc 0.9987
epoch 6001, loss 0.0035, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9989, auc 0.9986
epoch 6101, loss 0.0094, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9988, auc 0.9987
epoch 6201, loss 0.0056, train acc 99.87%, f1 0.9987, precision 0.9984, recall 0.9989, auc 0.9987
epoch 6301, loss 0.0055, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 6401, loss 0.0037, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 6501, loss 0.0019, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 6601, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 6701, loss 0.0046, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9991, auc 0.9988
epoch 6801, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 6901, loss 0.0042, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 7001, loss 0.0039, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 7101, loss 0.0019, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9990, auc 0.9989
epoch 7201, loss 0.0010, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 7301, loss 0.0053, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9991, auc 0.9989
epoch 7401, loss 0.0030, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 7501, loss 0.0054, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 7601, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 7701, loss 0.0045, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 7801, loss 0.0053, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 7901, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 8001, loss 0.0056, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 8101, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8201, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 8301, loss 0.0025, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 8401, loss 0.0028, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 8501, loss 0.0037, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 8601, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8701, loss 0.0005, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8801, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 8901, loss 0.0027, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 9001, loss 0.0032, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 9101, loss 0.0024, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9201, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9301, loss 0.0020, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 9401, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 9501, loss 0.0018, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 9601, loss 0.0003, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9701, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9801, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9901, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 10001, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 10101, loss 0.0058, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 10201, loss 0.0019, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 10301, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 10401, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 10501, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 10601, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10701, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 10801, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 10901, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 11001, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 11101, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 11201, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9996, auc 0.9994
epoch 11301, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 11401, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 11501, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 11601, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 11701, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 11801, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 11901, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 12001, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12101, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12201, loss 0.0028, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12301, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 12401, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 12501, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9997, auc 0.9995
epoch 12601, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 12701, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 12801, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12901, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13001, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 13101, loss 0.0024, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9997, auc 0.9996
epoch 13201, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13301, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 13401, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13501, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 13601, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 13701, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13801, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13901, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 14001, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 14101, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 14201, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 14301, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 14401, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14501, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14601, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14701, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 14801, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 14901, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15001, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15101, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15201, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15301, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15401, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15501, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 15601, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 15701, loss 0.0027, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15801, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15901, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 16001, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 16101, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 16201, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 16301, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 16401, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 16601, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 16701, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 16801, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 16901, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 17001, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 17101, loss 0.0001, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 17201, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 17301, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 17401, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 17501, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 17601, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 17701, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 17801, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17901, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18001, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 18101, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18201, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18301, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18401, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18501, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 18601, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 18701, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 18801, loss 0.0001, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 18901, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19001, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19101, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19201, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 19301, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 19401, loss 0.0025, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19501, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19601, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19701, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19801, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 19901, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_2
./test_vehicle0/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9797480620155039

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_2
----------------------



epoch 1, loss 0.6931, train acc 79.74%, f1 0.7999, precision 0.7887, recall 0.8114, auc 0.7974
epoch 101, loss 0.4299, train acc 82.80%, f1 0.8288, precision 0.8237, recall 0.8339, auc 0.8280
epoch 201, loss 0.2194, train acc 95.20%, f1 0.9521, precision 0.9488, recall 0.9554, auc 0.9520
epoch 301, loss 0.1569, train acc 97.76%, f1 0.9775, precision 0.9776, recall 0.9774, auc 0.9776
epoch 401, loss 0.0804, train acc 98.43%, f1 0.9843, precision 0.9841, recall 0.9845, auc 0.9843
epoch 501, loss 0.0578, train acc 98.70%, f1 0.9870, precision 0.9869, recall 0.9872, auc 0.9870
epoch 601, loss 0.0449, train acc 98.89%, f1 0.9889, precision 0.9890, recall 0.9887, auc 0.9889
epoch 701, loss 0.0433, train acc 99.05%, f1 0.9905, precision 0.9905, recall 0.9904, auc 0.9905
epoch 801, loss 0.0252, train acc 99.15%, f1 0.9915, precision 0.9915, recall 0.9916, auc 0.9915
epoch 901, loss 0.0248, train acc 99.24%, f1 0.9924, precision 0.9922, recall 0.9926, auc 0.9924
epoch 1001, loss 0.0208, train acc 99.31%, f1 0.9930, precision 0.9930, recall 0.9931, auc 0.9931
epoch 1101, loss 0.0254, train acc 99.35%, f1 0.9935, precision 0.9937, recall 0.9932, auc 0.9935
epoch 1201, loss 0.0247, train acc 99.40%, f1 0.9940, precision 0.9943, recall 0.9937, auc 0.9940
epoch 1301, loss 0.0214, train acc 99.45%, f1 0.9945, precision 0.9947, recall 0.9943, auc 0.9945
epoch 1401, loss 0.0198, train acc 99.49%, f1 0.9949, precision 0.9951, recall 0.9947, auc 0.9949
epoch 1501, loss 0.0236, train acc 99.52%, f1 0.9952, precision 0.9951, recall 0.9952, auc 0.9952
epoch 1601, loss 0.0125, train acc 99.55%, f1 0.9955, precision 0.9957, recall 0.9954, auc 0.9955
epoch 1701, loss 0.0122, train acc 99.58%, f1 0.9958, precision 0.9960, recall 0.9956, auc 0.9958
epoch 1801, loss 0.0211, train acc 99.59%, f1 0.9959, precision 0.9961, recall 0.9958, auc 0.9959
epoch 1901, loss 0.0134, train acc 99.63%, f1 0.9963, precision 0.9964, recall 0.9962, auc 0.9963
epoch 2001, loss 0.0065, train acc 99.64%, f1 0.9964, precision 0.9966, recall 0.9962, auc 0.9964
epoch 2101, loss 0.0053, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9965, auc 0.9965
epoch 2201, loss 0.0117, train acc 99.67%, f1 0.9967, precision 0.9969, recall 0.9966, auc 0.9967
epoch 2301, loss 0.0119, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9968, auc 0.9968
epoch 2401, loss 0.0053, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2501, loss 0.0097, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9968, auc 0.9970
epoch 2601, loss 0.0122, train acc 99.71%, f1 0.9971, precision 0.9972, recall 0.9970, auc 0.9971
epoch 2701, loss 0.0201, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2801, loss 0.0127, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 2901, loss 0.0043, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3001, loss 0.0056, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 3101, loss 0.0135, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3201, loss 0.0097, train acc 99.76%, f1 0.9976, precision 0.9974, recall 0.9977, auc 0.9976
epoch 3301, loss 0.0093, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3401, loss 0.0076, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3501, loss 0.0095, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9976, auc 0.9977
epoch 3601, loss 0.0099, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 3701, loss 0.0034, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9979, auc 0.9978
epoch 3801, loss 0.0105, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3901, loss 0.0035, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9981, auc 0.9979
epoch 4001, loss 0.0048, train acc 99.79%, f1 0.9978, precision 0.9978, recall 0.9979, auc 0.9979
epoch 4101, loss 0.0046, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 4201, loss 0.0050, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 4301, loss 0.0124, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 4401, loss 0.0040, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 4501, loss 0.0113, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 4601, loss 0.0055, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 4701, loss 0.0034, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9980
epoch 4801, loss 0.0048, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 4901, loss 0.0023, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 5001, loss 0.0061, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5101, loss 0.0091, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 5201, loss 0.0060, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 5301, loss 0.0079, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 5401, loss 0.0072, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 5501, loss 0.0058, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9983, auc 0.9981
epoch 5601, loss 0.0017, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9980, auc 0.9982
epoch 5701, loss 0.0075, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 5801, loss 0.0033, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 5901, loss 0.0038, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 6001, loss 0.0036, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 6101, loss 0.0053, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 6201, loss 0.0003, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 6301, loss 0.0026, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6401, loss 0.0063, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6501, loss 0.0050, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 6601, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 6701, loss 0.0024, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 6801, loss 0.0022, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 6901, loss 0.0035, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 7001, loss 0.0023, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 7101, loss 0.0067, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 7201, loss 0.0058, train acc 99.83%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9983
epoch 7301, loss 0.0043, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 7401, loss 0.0025, train acc 99.83%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9983
epoch 7501, loss 0.0064, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 7601, loss 0.0057, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 7701, loss 0.0031, train acc 99.83%, f1 0.9982, precision 0.9981, recall 0.9984, auc 0.9983
epoch 7801, loss 0.0067, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 7901, loss 0.0015, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 8001, loss 0.0036, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 8101, loss 0.0017, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0045, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 8301, loss 0.0018, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 8401, loss 0.0034, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 8501, loss 0.0014, train acc 99.83%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9983
epoch 8601, loss 0.0036, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 8701, loss 0.0047, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 8801, loss 0.0066, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 8901, loss 0.0014, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 9001, loss 0.0011, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 9101, loss 0.0070, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 9201, loss 0.0020, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 9301, loss 0.0037, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 9401, loss 0.0029, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 9501, loss 0.0053, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 9601, loss 0.0035, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 9701, loss 0.0054, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 9801, loss 0.0044, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 9901, loss 0.0052, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 10001, loss 0.0016, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9984, auc 0.9986
epoch 10101, loss 0.0054, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9984, auc 0.9986
epoch 10201, loss 0.0084, train acc 99.87%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9987
epoch 10301, loss 0.0101, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 10401, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 10501, loss 0.0039, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 10601, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 10701, loss 0.0047, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 10801, loss 0.0015, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9986, auc 0.9988
epoch 10901, loss 0.0043, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 11001, loss 0.0032, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 11101, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 11201, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 11301, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 11401, loss 0.0041, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 11501, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 11601, loss 0.0047, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 11701, loss 0.0020, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9988, auc 0.9989
epoch 11801, loss 0.0009, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 11901, loss 0.0012, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12001, loss 0.0072, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 12101, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12201, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12301, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 12401, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 12501, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9989, auc 0.9991
epoch 12601, loss 0.0034, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 12701, loss 0.0005, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 12801, loss 0.0005, train acc 99.92%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9992
epoch 12901, loss 0.0047, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 13001, loss 0.0003, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 13101, loss 0.0057, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 13201, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 13301, loss 0.0012, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 13401, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 13501, loss 0.0023, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9994, auc 0.9992
epoch 13601, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 13701, loss 0.0018, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 13801, loss 0.0043, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 13901, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14001, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 14101, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 14201, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14301, loss 0.0041, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14401, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 14501, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 14601, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 14701, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 14801, loss 0.0005, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 14901, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_2
./test_vehicle0/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9508720930232558

the Fscore is 0.925

the precision is 0.925

the recall is 0.925

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_2
----------------------



epoch 1, loss 0.6933, train acc 50.16%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4451, train acc 83.19%, f1 0.8319, precision 0.8294, recall 0.8344, auc 0.8319
epoch 201, loss 0.2155, train acc 95.33%, f1 0.9532, precision 0.9527, recall 0.9537, auc 0.9533
epoch 301, loss 0.1130, train acc 97.97%, f1 0.9796, precision 0.9797, recall 0.9794, auc 0.9797
epoch 401, loss 0.0887, train acc 98.43%, f1 0.9843, precision 0.9848, recall 0.9838, auc 0.9843
epoch 501, loss 0.0564, train acc 98.76%, f1 0.9875, precision 0.9878, recall 0.9873, auc 0.9876
epoch 601, loss 0.0518, train acc 98.94%, f1 0.9893, precision 0.9896, recall 0.9891, auc 0.9894
epoch 701, loss 0.0310, train acc 99.06%, f1 0.9906, precision 0.9911, recall 0.9901, auc 0.9906
epoch 801, loss 0.0345, train acc 99.17%, f1 0.9917, precision 0.9921, recall 0.9913, auc 0.9917
epoch 901, loss 0.0349, train acc 99.24%, f1 0.9924, precision 0.9926, recall 0.9921, auc 0.9924
epoch 1001, loss 0.0271, train acc 99.31%, f1 0.9931, precision 0.9931, recall 0.9930, auc 0.9931
epoch 1101, loss 0.0142, train acc 99.36%, f1 0.9936, precision 0.9939, recall 0.9933, auc 0.9936
epoch 1201, loss 0.0260, train acc 99.40%, f1 0.9939, precision 0.9940, recall 0.9939, auc 0.9940
epoch 1301, loss 0.0132, train acc 99.44%, f1 0.9943, precision 0.9946, recall 0.9941, auc 0.9944
epoch 1401, loss 0.0170, train acc 99.49%, f1 0.9949, precision 0.9950, recall 0.9948, auc 0.9949
epoch 1501, loss 0.0085, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9952, auc 0.9953
epoch 1601, loss 0.0121, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9955, auc 0.9956
epoch 1701, loss 0.0208, train acc 99.58%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9958
epoch 1801, loss 0.0109, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9961, auc 0.9961
epoch 1901, loss 0.0178, train acc 99.63%, f1 0.9963, precision 0.9961, recall 0.9964, auc 0.9963
epoch 2001, loss 0.0092, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 2101, loss 0.0101, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9967, auc 0.9966
epoch 2201, loss 0.0091, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2301, loss 0.0066, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9970, auc 0.9969
epoch 2401, loss 0.0107, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9970, auc 0.9969
epoch 2501, loss 0.0058, train acc 99.70%, f1 0.9970, precision 0.9968, recall 0.9972, auc 0.9970
epoch 2601, loss 0.0087, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9972
epoch 2701, loss 0.0113, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9975, auc 0.9973
epoch 2801, loss 0.0065, train acc 99.74%, f1 0.9974, precision 0.9972, recall 0.9976, auc 0.9974
epoch 2901, loss 0.0043, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9977, auc 0.9975
epoch 3001, loss 0.0101, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 3101, loss 0.0086, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9977, auc 0.9976
epoch 3201, loss 0.0103, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 3301, loss 0.0096, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 3401, loss 0.0068, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 3501, loss 0.0063, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 3601, loss 0.0029, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9981, auc 0.9979
epoch 3701, loss 0.0105, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 3801, loss 0.0082, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3901, loss 0.0054, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 4001, loss 0.0074, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 4101, loss 0.0038, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9982, auc 0.9980
epoch 4201, loss 0.0077, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9980
epoch 4301, loss 0.0056, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4401, loss 0.0065, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 4501, loss 0.0038, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 4601, loss 0.0078, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 4701, loss 0.0047, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9984, auc 0.9982
epoch 4801, loss 0.0027, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9984, auc 0.9982
epoch 4901, loss 0.0027, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5001, loss 0.0031, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 5101, loss 0.0030, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 5201, loss 0.0033, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 5301, loss 0.0054, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 5401, loss 0.0028, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5501, loss 0.0023, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 5601, loss 0.0048, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 5701, loss 0.0054, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5801, loss 0.0068, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 5901, loss 0.0043, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 6001, loss 0.0033, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 6101, loss 0.0042, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6201, loss 0.0041, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6301, loss 0.0053, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6401, loss 0.0017, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 6501, loss 0.0026, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 6601, loss 0.0055, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 6701, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9983, auc 0.9985
epoch 6801, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 6901, loss 0.0058, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 7001, loss 0.0045, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9985, auc 0.9986
epoch 7101, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7201, loss 0.0032, train acc 99.87%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9987
epoch 7301, loss 0.0039, train acc 99.87%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9987
epoch 7401, loss 0.0056, train acc 99.87%, f1 0.9986, precision 0.9988, recall 0.9985, auc 0.9987
epoch 7501, loss 0.0024, train acc 99.87%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9987
epoch 7601, loss 0.0070, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7701, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 7801, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 7901, loss 0.0050, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 8001, loss 0.0020, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 8101, loss 0.0012, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0009, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8301, loss 0.0021, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 8401, loss 0.0030, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 8501, loss 0.0023, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9987, auc 0.9988
epoch 8601, loss 0.0048, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 8701, loss 0.0046, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8801, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 8901, loss 0.0048, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 9001, loss 0.0059, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 9101, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 9201, loss 0.0054, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9301, loss 0.0037, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 9401, loss 0.0020, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9501, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9601, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9701, loss 0.0012, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9801, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9901, loss 0.0033, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_2
./test_vehicle0/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9633720930232557

the Fscore is 0.9382716049382716

the precision is 0.926829268292683

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_2
----------------------



epoch 1, loss 0.6934, train acc 49.80%, f1 0.6649, precision 0.4980, recall 1.0000, auc 0.5000
epoch 101, loss 0.4525, train acc 82.79%, f1 0.8279, precision 0.8247, recall 0.8311, auc 0.8279
epoch 201, loss 0.2162, train acc 94.93%, f1 0.9492, precision 0.9479, recall 0.9504, auc 0.9493
epoch 301, loss 0.1168, train acc 97.80%, f1 0.9779, precision 0.9786, recall 0.9772, auc 0.9780
epoch 401, loss 0.0849, train acc 98.37%, f1 0.9837, precision 0.9841, recall 0.9832, auc 0.9837
epoch 501, loss 0.0625, train acc 98.69%, f1 0.9869, precision 0.9875, recall 0.9862, auc 0.9869
epoch 601, loss 0.0572, train acc 98.86%, f1 0.9885, precision 0.9889, recall 0.9881, auc 0.9886
epoch 701, loss 0.0382, train acc 99.02%, f1 0.9902, precision 0.9906, recall 0.9898, auc 0.9902
epoch 801, loss 0.0511, train acc 99.14%, f1 0.9914, precision 0.9914, recall 0.9913, auc 0.9914
epoch 901, loss 0.0368, train acc 99.23%, f1 0.9923, precision 0.9925, recall 0.9921, auc 0.9923
epoch 1001, loss 0.0297, train acc 99.29%, f1 0.9928, precision 0.9930, recall 0.9927, auc 0.9929
epoch 1101, loss 0.0215, train acc 99.35%, f1 0.9935, precision 0.9934, recall 0.9935, auc 0.9935
epoch 1201, loss 0.0236, train acc 99.40%, f1 0.9940, precision 0.9939, recall 0.9940, auc 0.9940
epoch 1301, loss 0.0149, train acc 99.44%, f1 0.9944, precision 0.9945, recall 0.9943, auc 0.9944
epoch 1401, loss 0.0157, train acc 99.48%, f1 0.9948, precision 0.9949, recall 0.9946, auc 0.9948
epoch 1501, loss 0.0161, train acc 99.51%, f1 0.9951, precision 0.9953, recall 0.9950, auc 0.9951
epoch 1601, loss 0.0228, train acc 99.55%, f1 0.9954, precision 0.9956, recall 0.9953, auc 0.9955
epoch 1701, loss 0.0184, train acc 99.57%, f1 0.9956, precision 0.9958, recall 0.9955, auc 0.9957
epoch 1801, loss 0.0181, train acc 99.61%, f1 0.9960, precision 0.9961, recall 0.9960, auc 0.9961
epoch 1901, loss 0.0116, train acc 99.62%, f1 0.9961, precision 0.9962, recall 0.9961, auc 0.9962
epoch 2001, loss 0.0119, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 2101, loss 0.0132, train acc 99.65%, f1 0.9964, precision 0.9966, recall 0.9963, auc 0.9965
epoch 2201, loss 0.0140, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9966, auc 0.9967
epoch 2301, loss 0.0062, train acc 99.69%, f1 0.9968, precision 0.9968, recall 0.9969, auc 0.9969
epoch 2401, loss 0.0102, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2501, loss 0.0041, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 2601, loss 0.0100, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2701, loss 0.0061, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9972, auc 0.9973
epoch 2801, loss 0.0085, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9974
epoch 2901, loss 0.0112, train acc 99.74%, f1 0.9973, precision 0.9974, recall 0.9972, auc 0.9974
epoch 3001, loss 0.0115, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 3101, loss 0.0058, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3201, loss 0.0078, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3301, loss 0.0054, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 3401, loss 0.0102, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9979, auc 0.9978
epoch 3501, loss 0.0053, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9977, auc 0.9978
epoch 3601, loss 0.0036, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 3701, loss 0.0016, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3801, loss 0.0044, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 3901, loss 0.0103, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 4001, loss 0.0067, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4101, loss 0.0090, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 4201, loss 0.0083, train acc 99.82%, f1 0.9981, precision 0.9980, recall 0.9983, auc 0.9982
epoch 4301, loss 0.0037, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4401, loss 0.0033, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 4501, loss 0.0037, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 4601, loss 0.0043, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 4701, loss 0.0038, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 4801, loss 0.0066, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9984, auc 0.9983
epoch 4901, loss 0.0037, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 5001, loss 0.0047, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 5101, loss 0.0044, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9984, auc 0.9983
epoch 5201, loss 0.0107, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 5301, loss 0.0070, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 5401, loss 0.0045, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9985, auc 0.9984
epoch 5501, loss 0.0027, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 5601, loss 0.0062, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5701, loss 0.0037, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 5801, loss 0.0069, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 5901, loss 0.0012, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 6001, loss 0.0019, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6101, loss 0.0047, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 6201, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 6301, loss 0.0042, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 6401, loss 0.0042, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 6501, loss 0.0019, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 6601, loss 0.0023, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 6701, loss 0.0034, train acc 99.86%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9986
epoch 6801, loss 0.0105, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 6901, loss 0.0041, train acc 99.86%, f1 0.9985, precision 0.9987, recall 0.9984, auc 0.9986
epoch 7001, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 7101, loss 0.0026, train acc 99.86%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9986
epoch 7201, loss 0.0026, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7301, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7401, loss 0.0026, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7501, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 7601, loss 0.0063, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 7701, loss 0.0066, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 7801, loss 0.0055, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 7901, loss 0.0048, train acc 99.87%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9987
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_2
./test_vehicle0/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9758720930232558

the Fscore is 0.951219512195122

the precision is 0.9285714285714286

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_2
----------------------



epoch 1, loss 0.6934, train acc 49.88%, f1 0.6656, precision 0.4988, recall 1.0000, auc 0.5000
epoch 101, loss 0.3960, train acc 82.69%, f1 0.8251, precision 0.8314, recall 0.8189, auc 0.8269
epoch 201, loss 0.2242, train acc 95.04%, f1 0.9502, precision 0.9504, recall 0.9501, auc 0.9504
epoch 301, loss 0.1189, train acc 97.84%, f1 0.9783, precision 0.9781, recall 0.9785, auc 0.9784
epoch 401, loss 0.0805, train acc 98.37%, f1 0.9837, precision 0.9838, recall 0.9835, auc 0.9837
epoch 501, loss 0.0743, train acc 98.69%, f1 0.9868, precision 0.9868, recall 0.9869, auc 0.9869
epoch 601, loss 0.0516, train acc 98.89%, f1 0.9888, precision 0.9888, recall 0.9889, auc 0.9889
epoch 701, loss 0.0566, train acc 99.02%, f1 0.9902, precision 0.9904, recall 0.9900, auc 0.9902
epoch 801, loss 0.0287, train acc 99.14%, f1 0.9914, precision 0.9914, recall 0.9913, auc 0.9914
epoch 901, loss 0.0370, train acc 99.24%, f1 0.9924, precision 0.9922, recall 0.9926, auc 0.9924
epoch 1001, loss 0.0342, train acc 99.28%, f1 0.9927, precision 0.9925, recall 0.9930, auc 0.9928
epoch 1101, loss 0.0207, train acc 99.35%, f1 0.9934, precision 0.9934, recall 0.9935, auc 0.9935
epoch 1201, loss 0.0218, train acc 99.39%, f1 0.9939, precision 0.9937, recall 0.9941, auc 0.9939
epoch 1301, loss 0.0188, train acc 99.44%, f1 0.9944, precision 0.9941, recall 0.9946, auc 0.9944
epoch 1401, loss 0.0118, train acc 99.48%, f1 0.9948, precision 0.9947, recall 0.9949, auc 0.9948
epoch 1501, loss 0.0163, train acc 99.52%, f1 0.9952, precision 0.9950, recall 0.9954, auc 0.9952
epoch 1601, loss 0.0180, train acc 99.55%, f1 0.9955, precision 0.9952, recall 0.9957, auc 0.9955
epoch 1701, loss 0.0165, train acc 99.58%, f1 0.9958, precision 0.9956, recall 0.9960, auc 0.9958
epoch 1801, loss 0.0066, train acc 99.61%, f1 0.9961, precision 0.9958, recall 0.9963, auc 0.9961
epoch 1901, loss 0.0107, train acc 99.62%, f1 0.9962, precision 0.9959, recall 0.9965, auc 0.9962
epoch 2001, loss 0.0152, train acc 99.64%, f1 0.9964, precision 0.9961, recall 0.9967, auc 0.9964
epoch 2101, loss 0.0180, train acc 99.65%, f1 0.9965, precision 0.9961, recall 0.9970, auc 0.9965
epoch 2201, loss 0.0119, train acc 99.67%, f1 0.9967, precision 0.9964, recall 0.9971, auc 0.9967
epoch 2301, loss 0.0100, train acc 99.68%, f1 0.9968, precision 0.9964, recall 0.9973, auc 0.9968
epoch 2401, loss 0.0124, train acc 99.70%, f1 0.9970, precision 0.9965, recall 0.9974, auc 0.9970
epoch 2501, loss 0.0098, train acc 99.70%, f1 0.9970, precision 0.9967, recall 0.9973, auc 0.9970
epoch 2601, loss 0.0120, train acc 99.71%, f1 0.9971, precision 0.9965, recall 0.9976, auc 0.9971
epoch 2701, loss 0.0083, train acc 99.72%, f1 0.9972, precision 0.9969, recall 0.9975, auc 0.9972
epoch 2801, loss 0.0080, train acc 99.73%, f1 0.9973, precision 0.9969, recall 0.9976, auc 0.9973
epoch 2901, loss 0.0120, train acc 99.74%, f1 0.9974, precision 0.9971, recall 0.9977, auc 0.9974
epoch 3001, loss 0.0065, train acc 99.75%, f1 0.9975, precision 0.9973, recall 0.9977, auc 0.9975
epoch 3101, loss 0.0094, train acc 99.76%, f1 0.9976, precision 0.9972, recall 0.9981, auc 0.9976
epoch 3201, loss 0.0069, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9978, auc 0.9976
epoch 3301, loss 0.0057, train acc 99.77%, f1 0.9977, precision 0.9975, recall 0.9979, auc 0.9977
epoch 3401, loss 0.0060, train acc 99.78%, f1 0.9978, precision 0.9975, recall 0.9981, auc 0.9978
epoch 3501, loss 0.0062, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 3601, loss 0.0055, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9981, auc 0.9978
epoch 3701, loss 0.0058, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9981, auc 0.9979
epoch 3801, loss 0.0070, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9982, auc 0.9980
epoch 3901, loss 0.0037, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9981, auc 0.9979
epoch 4001, loss 0.0057, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 4101, loss 0.0046, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 4201, loss 0.0134, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9982, auc 0.9981
epoch 4301, loss 0.0047, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9982, auc 0.9980
epoch 4401, loss 0.0053, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9983, auc 0.9981
epoch 4501, loss 0.0057, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 4601, loss 0.0050, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9984, auc 0.9981
epoch 4701, loss 0.0075, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 4801, loss 0.0068, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 4901, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_2
./test_vehicle0/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9594961240310077

the Fscore is 0.9268292682926829

the precision is 0.9047619047619048

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_2
----------------------



epoch 1, loss 0.6933, train acc 50.31%, f1 0.6694, precision 0.5031, recall 1.0000, auc 0.5000
epoch 101, loss 0.4087, train acc 82.53%, f1 0.8274, precision 0.8230, recall 0.8318, auc 0.8253
epoch 201, loss 0.2295, train acc 95.31%, f1 0.9534, precision 0.9539, recall 0.9529, auc 0.9531
epoch 301, loss 0.1238, train acc 97.86%, f1 0.9787, precision 0.9796, recall 0.9779, auc 0.9786
epoch 401, loss 0.0846, train acc 98.45%, f1 0.9846, precision 0.9857, recall 0.9836, auc 0.9845
epoch 501, loss 0.0719, train acc 98.69%, f1 0.9869, precision 0.9872, recall 0.9867, auc 0.9869
epoch 601, loss 0.0609, train acc 98.90%, f1 0.9890, precision 0.9895, recall 0.9886, auc 0.9890
epoch 701, loss 0.0445, train acc 99.04%, f1 0.9905, precision 0.9908, recall 0.9901, auc 0.9904
epoch 801, loss 0.0397, train acc 99.15%, f1 0.9916, precision 0.9919, recall 0.9913, auc 0.9915
epoch 901, loss 0.0406, train acc 99.24%, f1 0.9925, precision 0.9929, recall 0.9920, auc 0.9924
epoch 1001, loss 0.0313, train acc 99.31%, f1 0.9931, precision 0.9938, recall 0.9924, auc 0.9931
epoch 1101, loss 0.0289, train acc 99.35%, f1 0.9936, precision 0.9943, recall 0.9929, auc 0.9935
epoch 1201, loss 0.0182, train acc 99.40%, f1 0.9940, precision 0.9949, recall 0.9932, auc 0.9940
epoch 1301, loss 0.0203, train acc 99.45%, f1 0.9946, precision 0.9950, recall 0.9941, auc 0.9945
epoch 1401, loss 0.0156, train acc 99.50%, f1 0.9950, precision 0.9955, recall 0.9945, auc 0.9950
epoch 1501, loss 0.0212, train acc 99.53%, f1 0.9953, precision 0.9959, recall 0.9946, auc 0.9953
epoch 1601, loss 0.0105, train acc 99.56%, f1 0.9956, precision 0.9964, recall 0.9948, auc 0.9956
epoch 1701, loss 0.0121, train acc 99.57%, f1 0.9958, precision 0.9964, recall 0.9951, auc 0.9957
epoch 1801, loss 0.0101, train acc 99.60%, f1 0.9960, precision 0.9966, recall 0.9954, auc 0.9960
epoch 1901, loss 0.0095, train acc 99.62%, f1 0.9963, precision 0.9969, recall 0.9956, auc 0.9963
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_2
./test_vehicle0/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9681201550387598

the Fscore is 0.9285714285714285

the precision is 0.8863636363636364

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_2
----------------------



epoch 1, loss 0.6971, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4709, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4131, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3700, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3335, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3021, train acc 76.81%, f1 0.0485, precision 0.6667, recall 0.0252, auc 0.5106
epoch 601, loss 0.2751, train acc 77.84%, f1 0.1279, precision 0.8462, recall 0.0692, auc 0.5327
epoch 701, loss 0.2516, train acc 83.46%, f1 0.4667, precision 0.9608, recall 0.3082, auc 0.6522
epoch 801, loss 0.2310, train acc 87.59%, f1 0.6471, precision 0.9747, recall 0.4843, auc 0.7402
epoch 901, loss 0.2127, train acc 89.96%, f1 0.7385, precision 0.9505, recall 0.6038, auc 0.7971
epoch 1001, loss 0.1966, train acc 92.91%, f1 0.8286, precision 0.9587, recall 0.7296, auc 0.8600
epoch 1101, loss 0.1824, train acc 94.53%, f1 0.8737, precision 0.9552, recall 0.8050, auc 0.8967
epoch 1201, loss 0.1698, train acc 96.31%, f1 0.9180, precision 0.9589, recall 0.8805, auc 0.9345
epoch 1301, loss 0.1583, train acc 97.19%, f1 0.9385, precision 0.9667, recall 0.9119, auc 0.9511
epoch 1401, loss 0.1478, train acc 97.34%, f1 0.9416, precision 0.9732, recall 0.9119, auc 0.9521
epoch 1501, loss 0.1380, train acc 97.93%, f1 0.9551, precision 0.9739, recall 0.9371, auc 0.9647
epoch 1601, loss 0.1290, train acc 98.23%, f1 0.9618, precision 0.9742, recall 0.9497, auc 0.9710
epoch 1701, loss 0.1206, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1801, loss 0.1130, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 1901, loss 0.1059, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2001, loss 0.0993, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2101, loss 0.0933, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2201, loss 0.0876, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2301, loss 0.0824, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2401, loss 0.0776, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2501, loss 0.0731, train acc 99.41%, f1 0.9874, precision 0.9874, recall 0.9874, auc 0.9918
epoch 2601, loss 0.0690, train acc 99.41%, f1 0.9874, precision 0.9874, recall 0.9874, auc 0.9918
epoch 2701, loss 0.0651, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 2801, loss 0.0615, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 2901, loss 0.0582, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3001, loss 0.0550, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3101, loss 0.0521, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3201, loss 0.0494, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3301, loss 0.0468, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3401, loss 0.0444, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3501, loss 0.0421, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3601, loss 0.0400, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3701, loss 0.0380, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3801, loss 0.0360, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3901, loss 0.0342, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4001, loss 0.0325, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4101, loss 0.0309, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4201, loss 0.0293, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0278, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0264, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0251, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0239, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0227, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0215, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0204, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0194, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0184, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0175, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0166, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0158, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0150, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0142, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0135, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0128, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0122, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0116, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0110, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0104, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0099, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0094, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0089, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0085, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0081, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0077, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0073, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0069, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0066, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0062, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0059, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0054, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0051, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0044, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0042, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0040, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8201, loss 0.0038, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0036, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0033, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0031, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_normal_20000
normal
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_2
./test_vehicle0/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.9469961240310077

the Fscore is 0.9135802469135802

the precision is 0.9024390243902439

the recall is 0.925

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_2
----------------------



epoch 1, loss 0.6844, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4654, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4094, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3665, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3299, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.2986, train acc 76.96%, f1 0.0602, precision 0.7143, recall 0.0314, auc 0.5138
epoch 601, loss 0.2718, train acc 78.29%, f1 0.1600, precision 0.8750, recall 0.0881, auc 0.5421
epoch 701, loss 0.2487, train acc 84.34%, f1 0.5093, precision 0.9649, recall 0.3459, auc 0.6710
epoch 801, loss 0.2283, train acc 88.33%, f1 0.6749, precision 0.9762, recall 0.5157, auc 0.7559
epoch 901, loss 0.2103, train acc 90.55%, f1 0.7576, precision 0.9524, recall 0.6289, auc 0.8096
epoch 1001, loss 0.1945, train acc 93.35%, f1 0.8410, precision 0.9597, recall 0.7484, auc 0.8694
epoch 1101, loss 0.1804, train acc 94.83%, f1 0.8814, precision 0.9559, recall 0.8176, auc 0.9030
epoch 1201, loss 0.1679, train acc 96.31%, f1 0.9180, precision 0.9589, recall 0.8805, auc 0.9345
epoch 1301, loss 0.1566, train acc 97.34%, f1 0.9416, precision 0.9732, recall 0.9119, auc 0.9521
epoch 1401, loss 0.1462, train acc 97.34%, f1 0.9416, precision 0.9732, recall 0.9119, auc 0.9521
epoch 1501, loss 0.1367, train acc 97.64%, f1 0.9484, precision 0.9735, recall 0.9245, auc 0.9584
epoch 1601, loss 0.1277, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1701, loss 0.1194, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1801, loss 0.1118, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 1901, loss 0.1047, train acc 99.11%, f1 0.9811, precision 0.9811, recall 0.9811, auc 0.9877
epoch 2001, loss 0.0982, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2101, loss 0.0922, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2201, loss 0.0866, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2301, loss 0.0815, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2401, loss 0.0767, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2501, loss 0.0723, train acc 99.41%, f1 0.9874, precision 0.9874, recall 0.9874, auc 0.9918
epoch 2601, loss 0.0682, train acc 99.41%, f1 0.9874, precision 0.9874, recall 0.9874, auc 0.9918
epoch 2701, loss 0.0644, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 2801, loss 0.0608, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 2901, loss 0.0575, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3001, loss 0.0544, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3101, loss 0.0515, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3201, loss 0.0488, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3301, loss 0.0463, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3401, loss 0.0439, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3501, loss 0.0417, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3601, loss 0.0395, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3701, loss 0.0375, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3801, loss 0.0356, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3901, loss 0.0338, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4001, loss 0.0321, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4101, loss 0.0305, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4201, loss 0.0290, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0275, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0261, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0248, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0236, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0224, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0212, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0202, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0191, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0182, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0172, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0164, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0155, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0147, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0140, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0133, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0126, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0120, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0114, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0108, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0102, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0097, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0092, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0087, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0083, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0079, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0075, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0071, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0068, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0064, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0061, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0058, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0055, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0052, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0050, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0047, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0045, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0043, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0035, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0033, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0030, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_normal_15000
normal
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_2
./test_vehicle0/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.9508720930232558

the Fscore is 0.925

the precision is 0.925

the recall is 0.925

Done
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_2
----------------------



epoch 1, loss 0.6920, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4685, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4114, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3684, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3319, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3006, train acc 76.81%, f1 0.0485, precision 0.6667, recall 0.0252, auc 0.5106
epoch 601, loss 0.2738, train acc 78.14%, f1 0.1494, precision 0.8667, recall 0.0818, auc 0.5390
epoch 701, loss 0.2505, train acc 83.90%, f1 0.4883, precision 0.9630, recall 0.3270, auc 0.6616
epoch 801, loss 0.2301, train acc 87.89%, f1 0.6583, precision 0.9753, recall 0.4969, auc 0.7465
epoch 901, loss 0.2119, train acc 90.25%, f1 0.7481, precision 0.9515, recall 0.6164, auc 0.8033
epoch 1001, loss 0.1959, train acc 93.21%, f1 0.8369, precision 0.9593, recall 0.7421, auc 0.8662
epoch 1101, loss 0.1818, train acc 94.53%, f1 0.8737, precision 0.9552, recall 0.8050, auc 0.8967
epoch 1201, loss 0.1693, train acc 96.31%, f1 0.9180, precision 0.9589, recall 0.8805, auc 0.9345
epoch 1301, loss 0.1580, train acc 97.05%, f1 0.9355, precision 0.9603, recall 0.9119, auc 0.9502
epoch 1401, loss 0.1476, train acc 97.34%, f1 0.9416, precision 0.9732, recall 0.9119, auc 0.9521
epoch 1501, loss 0.1379, train acc 97.78%, f1 0.9518, precision 0.9737, recall 0.9308, auc 0.9615
epoch 1601, loss 0.1289, train acc 98.08%, f1 0.9585, precision 0.9740, recall 0.9434, auc 0.9678
epoch 1701, loss 0.1205, train acc 98.67%, f1 0.9716, precision 0.9747, recall 0.9686, auc 0.9804
epoch 1801, loss 0.1129, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 1901, loss 0.1058, train acc 99.11%, f1 0.9811, precision 0.9811, recall 0.9811, auc 0.9877
epoch 2001, loss 0.0993, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2101, loss 0.0932, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2201, loss 0.0876, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2301, loss 0.0824, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2401, loss 0.0775, train acc 99.41%, f1 0.9874, precision 0.9874, recall 0.9874, auc 0.9918
epoch 2501, loss 0.0730, train acc 99.41%, f1 0.9874, precision 0.9874, recall 0.9874, auc 0.9918
epoch 2601, loss 0.0689, train acc 99.41%, f1 0.9874, precision 0.9874, recall 0.9874, auc 0.9918
epoch 2701, loss 0.0650, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 2801, loss 0.0614, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 2901, loss 0.0580, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3001, loss 0.0549, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3101, loss 0.0520, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3201, loss 0.0493, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3301, loss 0.0467, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3401, loss 0.0443, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3501, loss 0.0421, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3601, loss 0.0399, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3701, loss 0.0379, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3801, loss 0.0360, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3901, loss 0.0342, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4001, loss 0.0325, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4101, loss 0.0309, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4201, loss 0.0293, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0279, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0265, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0251, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0239, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0227, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0215, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0204, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0194, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0184, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0174, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0166, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0157, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0149, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0141, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0134, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0127, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0121, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0115, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0109, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0103, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0098, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0093, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0088, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0084, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0080, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0076, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0072, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0068, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0065, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0061, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0058, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0050, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0045, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0043, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0035, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0031, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_normal_10000
normal
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_2
./test_vehicle0/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.9383720930232557

the Fscore is 0.9113924050632911

the precision is 0.9230769230769231

the recall is 0.9

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_2
----------------------



epoch 1, loss 0.6789, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4663, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4104, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3674, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3305, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.2988, train acc 76.81%, f1 0.0485, precision 0.6667, recall 0.0252, auc 0.5106
epoch 601, loss 0.2716, train acc 78.58%, f1 0.1808, precision 0.8889, recall 0.1006, auc 0.5484
epoch 701, loss 0.2481, train acc 84.19%, f1 0.5023, precision 0.9643, recall 0.3396, auc 0.6679
epoch 801, loss 0.2275, train acc 88.63%, f1 0.6883, precision 0.9659, recall 0.5346, auc 0.7644
epoch 901, loss 0.2093, train acc 90.84%, f1 0.7669, precision 0.9533, recall 0.6415, auc 0.8159
epoch 1001, loss 0.1933, train acc 93.50%, f1 0.8451, precision 0.9600, recall 0.7547, auc 0.8725
epoch 1101, loss 0.1792, train acc 94.98%, f1 0.8851, precision 0.9562, recall 0.8239, auc 0.9062
epoch 1201, loss 0.1666, train acc 96.60%, f1 0.9251, precision 0.9595, recall 0.8931, auc 0.9407
epoch 1301, loss 0.1553, train acc 97.19%, f1 0.9385, precision 0.9667, recall 0.9119, auc 0.9511
epoch 1401, loss 0.1449, train acc 97.64%, f1 0.9484, precision 0.9735, recall 0.9245, auc 0.9584
epoch 1501, loss 0.1354, train acc 97.93%, f1 0.9551, precision 0.9739, recall 0.9371, auc 0.9647
epoch 1601, loss 0.1265, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1701, loss 0.1184, train acc 98.52%, f1 0.9686, precision 0.9686, recall 0.9686, auc 0.9795
epoch 1801, loss 0.1109, train acc 98.67%, f1 0.9720, precision 0.9630, recall 0.9811, auc 0.9848
epoch 1901, loss 0.1040, train acc 98.67%, f1 0.9720, precision 0.9630, recall 0.9811, auc 0.9848
epoch 2001, loss 0.0976, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2101, loss 0.0916, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2201, loss 0.0861, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2301, loss 0.0810, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2401, loss 0.0762, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2501, loss 0.0718, train acc 99.41%, f1 0.9874, precision 0.9874, recall 0.9874, auc 0.9918
epoch 2601, loss 0.0677, train acc 99.41%, f1 0.9874, precision 0.9874, recall 0.9874, auc 0.9918
epoch 2701, loss 0.0639, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 2801, loss 0.0604, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 2901, loss 0.0571, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3001, loss 0.0540, train acc 99.56%, f1 0.9906, precision 0.9875, recall 0.9937, auc 0.9949
epoch 3101, loss 0.0512, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3201, loss 0.0485, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3301, loss 0.0460, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3401, loss 0.0436, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3501, loss 0.0414, train acc 99.85%, f1 0.9968, precision 1.0000, recall 0.9937, auc 0.9969
epoch 3601, loss 0.0393, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3701, loss 0.0373, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3801, loss 0.0354, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3901, loss 0.0336, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4001, loss 0.0319, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4101, loss 0.0303, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4201, loss 0.0288, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0273, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0260, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0247, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0234, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0223, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0212, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0201, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0191, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0181, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0172, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0164, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0155, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0148, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0140, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0133, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0126, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0120, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0114, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0108, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0103, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0098, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0093, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0088, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0083, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0079, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0075, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0071, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0068, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0064, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0061, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0058, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0055, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0050, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0047, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0045, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0043, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_normal_8000
normal
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_2
./test_vehicle0/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.9547480620155038

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_2
----------------------



epoch 1, loss 0.6920, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4697, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4124, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3693, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3326, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3011, train acc 76.81%, f1 0.0485, precision 0.6667, recall 0.0252, auc 0.5106
epoch 601, loss 0.2740, train acc 77.99%, f1 0.1387, precision 0.8571, recall 0.0755, auc 0.5358
epoch 701, loss 0.2506, train acc 83.75%, f1 0.4811, precision 0.9623, recall 0.3208, auc 0.6584
epoch 801, loss 0.2299, train acc 87.89%, f1 0.6583, precision 0.9753, recall 0.4969, auc 0.7465
epoch 901, loss 0.2117, train acc 90.25%, f1 0.7481, precision 0.9515, recall 0.6164, auc 0.8033
epoch 1001, loss 0.1957, train acc 93.06%, f1 0.8327, precision 0.9590, recall 0.7358, auc 0.8631
epoch 1101, loss 0.1815, train acc 94.53%, f1 0.8737, precision 0.9552, recall 0.8050, auc 0.8967
epoch 1201, loss 0.1689, train acc 96.31%, f1 0.9180, precision 0.9589, recall 0.8805, auc 0.9345
epoch 1301, loss 0.1575, train acc 97.34%, f1 0.9416, precision 0.9732, recall 0.9119, auc 0.9521
epoch 1401, loss 0.1471, train acc 97.34%, f1 0.9416, precision 0.9732, recall 0.9119, auc 0.9521
epoch 1501, loss 0.1374, train acc 97.64%, f1 0.9484, precision 0.9735, recall 0.9245, auc 0.9584
epoch 1601, loss 0.1283, train acc 98.23%, f1 0.9618, precision 0.9742, recall 0.9497, auc 0.9710
epoch 1701, loss 0.1200, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1801, loss 0.1123, train acc 99.11%, f1 0.9811, precision 0.9811, recall 0.9811, auc 0.9877
epoch 1901, loss 0.1053, train acc 99.11%, f1 0.9811, precision 0.9811, recall 0.9811, auc 0.9877
epoch 2001, loss 0.0989, train acc 98.97%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2101, loss 0.0929, train acc 98.82%, f1 0.9752, precision 0.9632, recall 0.9874, auc 0.9879
epoch 2201, loss 0.0873, train acc 98.82%, f1 0.9752, precision 0.9632, recall 0.9874, auc 0.9879
epoch 2301, loss 0.0822, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2401, loss 0.0774, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2501, loss 0.0730, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2601, loss 0.0688, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2701, loss 0.0650, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 2801, loss 0.0614, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 2901, loss 0.0580, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3001, loss 0.0549, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3101, loss 0.0520, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3201, loss 0.0492, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3301, loss 0.0467, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3401, loss 0.0442, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3501, loss 0.0420, train acc 99.70%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9959
epoch 3601, loss 0.0398, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3701, loss 0.0378, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3801, loss 0.0359, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3901, loss 0.0341, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4001, loss 0.0323, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4101, loss 0.0307, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4201, loss 0.0292, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0277, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0263, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0250, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0237, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0225, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0214, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0203, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_normal_5000
normal
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_2
./test_vehicle0/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.9547480620155038

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_2
----------------------



epoch 1, loss 0.6909, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4678, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4110, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3680, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3314, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.3000, train acc 76.81%, f1 0.0485, precision 0.6667, recall 0.0252, auc 0.5106
epoch 601, loss 0.2731, train acc 78.14%, f1 0.1494, precision 0.8667, recall 0.0818, auc 0.5390
epoch 701, loss 0.2499, train acc 83.90%, f1 0.4883, precision 0.9630, recall 0.3270, auc 0.6616
epoch 801, loss 0.2295, train acc 88.04%, f1 0.6639, precision 0.9756, recall 0.5031, auc 0.7496
epoch 901, loss 0.2114, train acc 90.40%, f1 0.7529, precision 0.9519, recall 0.6226, auc 0.8065
epoch 1001, loss 0.1954, train acc 93.21%, f1 0.8369, precision 0.9593, recall 0.7421, auc 0.8662
epoch 1101, loss 0.1813, train acc 94.68%, f1 0.8776, precision 0.9556, recall 0.8113, auc 0.8999
epoch 1201, loss 0.1688, train acc 96.45%, f1 0.9216, precision 0.9592, recall 0.8868, auc 0.9376
epoch 1301, loss 0.1576, train acc 97.05%, f1 0.9355, precision 0.9603, recall 0.9119, auc 0.9502
epoch 1401, loss 0.1472, train acc 97.34%, f1 0.9416, precision 0.9732, recall 0.9119, auc 0.9521
epoch 1501, loss 0.1376, train acc 97.49%, f1 0.9450, precision 0.9733, recall 0.9182, auc 0.9553
epoch 1601, loss 0.1285, train acc 98.08%, f1 0.9585, precision 0.9740, recall 0.9434, auc 0.9678
epoch 1701, loss 0.1201, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1801, loss 0.1125, train acc 99.11%, f1 0.9811, precision 0.9811, recall 0.9811, auc 0.9877
epoch 1901, loss 0.1054, train acc 99.11%, f1 0.9811, precision 0.9811, recall 0.9811, auc 0.9877
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_2.csv
./test_vehicle0/standlization_data/vehicle0_std_test_2.csv
MLP_normal_2000
normal
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_2
./test_vehicle0/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.9672480620155038

the Fscore is 0.9500000000000001

the precision is 0.95

the recall is 0.95

Done
./test_yeast3/standlization_data/yeast3_std_train_2.csv
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_2
----------------------



epoch 1, loss 0.6936, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.3886, train acc 97.78%, f1 0.9779, precision 0.9738, recall 0.9820, auc 0.9778
epoch 201, loss 0.1473, train acc 98.37%, f1 0.9837, precision 0.9834, recall 0.9839, auc 0.9837
epoch 301, loss 0.0847, train acc 98.64%, f1 0.9864, precision 0.9866, recall 0.9863, auc 0.9864
epoch 401, loss 0.0601, train acc 98.74%, f1 0.9874, precision 0.9876, recall 0.9873, auc 0.9874
epoch 501, loss 0.0479, train acc 98.77%, f1 0.9877, precision 0.9879, recall 0.9875, auc 0.9877
epoch 601, loss 0.0434, train acc 98.80%, f1 0.9880, precision 0.9882, recall 0.9879, auc 0.9880
epoch 701, loss 0.0386, train acc 98.83%, f1 0.9883, precision 0.9884, recall 0.9881, auc 0.9883
epoch 801, loss 0.0372, train acc 98.83%, f1 0.9883, precision 0.9885, recall 0.9881, auc 0.9883
epoch 901, loss 0.0350, train acc 98.84%, f1 0.9884, precision 0.9885, recall 0.9882, auc 0.9884
epoch 1001, loss 0.0341, train acc 98.84%, f1 0.9884, precision 0.9885, recall 0.9882, auc 0.9884
epoch 1101, loss 0.0326, train acc 98.83%, f1 0.9883, precision 0.9885, recall 0.9882, auc 0.9883
epoch 1201, loss 0.0321, train acc 98.83%, f1 0.9883, precision 0.9884, recall 0.9881, auc 0.9883
epoch 1301, loss 0.0318, train acc 98.83%, f1 0.9883, precision 0.9885, recall 0.9882, auc 0.9883
epoch 1401, loss 0.0291, train acc 98.83%, f1 0.9883, precision 0.9884, recall 0.9881, auc 0.9883
epoch 1501, loss 0.0311, train acc 98.82%, f1 0.9882, precision 0.9884, recall 0.9880, auc 0.9882
epoch 1601, loss 0.0297, train acc 98.82%, f1 0.9882, precision 0.9884, recall 0.9880, auc 0.9882
epoch 1701, loss 0.0298, train acc 98.83%, f1 0.9883, precision 0.9884, recall 0.9881, auc 0.9883
epoch 1801, loss 0.0303, train acc 98.83%, f1 0.9883, precision 0.9884, recall 0.9882, auc 0.9883
epoch 1901, loss 0.0298, train acc 98.82%, f1 0.9882, precision 0.9883, recall 0.9881, auc 0.9882
epoch 2001, loss 0.0284, train acc 98.82%, f1 0.9882, precision 0.9883, recall 0.9881, auc 0.9882
epoch 2101, loss 0.0279, train acc 98.82%, f1 0.9882, precision 0.9883, recall 0.9882, auc 0.9882
epoch 2201, loss 0.0291, train acc 98.82%, f1 0.9882, precision 0.9883, recall 0.9881, auc 0.9882
epoch 2301, loss 0.0276, train acc 98.85%, f1 0.9885, precision 0.9885, recall 0.9884, auc 0.9885
epoch 2401, loss 0.0277, train acc 98.85%, f1 0.9885, precision 0.9886, recall 0.9884, auc 0.9885
epoch 2501, loss 0.0269, train acc 98.87%, f1 0.9887, precision 0.9887, recall 0.9886, auc 0.9887
epoch 2601, loss 0.0265, train acc 98.88%, f1 0.9888, precision 0.9887, recall 0.9888, auc 0.9888
epoch 2701, loss 0.0272, train acc 98.88%, f1 0.9888, precision 0.9888, recall 0.9888, auc 0.9888
epoch 2801, loss 0.0265, train acc 98.89%, f1 0.9889, precision 0.9889, recall 0.9889, auc 0.9889
epoch 2901, loss 0.0255, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9889, auc 0.9890
epoch 3001, loss 0.0248, train acc 98.91%, f1 0.9891, precision 0.9891, recall 0.9891, auc 0.9891
epoch 3101, loss 0.0250, train acc 98.93%, f1 0.9893, precision 0.9893, recall 0.9892, auc 0.9893
epoch 3201, loss 0.0252, train acc 98.95%, f1 0.9895, precision 0.9895, recall 0.9895, auc 0.9895
epoch 3301, loss 0.0245, train acc 98.97%, f1 0.9897, precision 0.9898, recall 0.9897, auc 0.9897
epoch 3401, loss 0.0249, train acc 99.00%, f1 0.9900, precision 0.9900, recall 0.9900, auc 0.9900
epoch 3501, loss 0.0244, train acc 99.03%, f1 0.9903, precision 0.9903, recall 0.9902, auc 0.9903
epoch 3601, loss 0.0237, train acc 99.05%, f1 0.9905, precision 0.9905, recall 0.9904, auc 0.9905
epoch 3701, loss 0.0231, train acc 99.06%, f1 0.9906, precision 0.9907, recall 0.9905, auc 0.9906
epoch 3801, loss 0.0235, train acc 99.08%, f1 0.9908, precision 0.9908, recall 0.9907, auc 0.9908
epoch 3901, loss 0.0226, train acc 99.09%, f1 0.9909, precision 0.9909, recall 0.9909, auc 0.9909
epoch 4001, loss 0.0225, train acc 99.10%, f1 0.9910, precision 0.9910, recall 0.9909, auc 0.9910
epoch 4101, loss 0.0226, train acc 99.12%, f1 0.9912, precision 0.9912, recall 0.9911, auc 0.9912
epoch 4201, loss 0.0222, train acc 99.14%, f1 0.9914, precision 0.9914, recall 0.9913, auc 0.9914
epoch 4301, loss 0.0221, train acc 99.14%, f1 0.9914, precision 0.9915, recall 0.9914, auc 0.9914
epoch 4401, loss 0.0214, train acc 99.16%, f1 0.9916, precision 0.9915, recall 0.9916, auc 0.9916
epoch 4501, loss 0.0215, train acc 99.18%, f1 0.9918, precision 0.9918, recall 0.9918, auc 0.9918
epoch 4601, loss 0.0208, train acc 99.20%, f1 0.9920, precision 0.9919, recall 0.9920, auc 0.9920
epoch 4701, loss 0.0208, train acc 99.21%, f1 0.9921, precision 0.9920, recall 0.9921, auc 0.9921
epoch 4801, loss 0.0203, train acc 99.22%, f1 0.9922, precision 0.9922, recall 0.9922, auc 0.9922
epoch 4901, loss 0.0163, train acc 99.23%, f1 0.9923, precision 0.9923, recall 0.9924, auc 0.9923
epoch 5001, loss 0.0190, train acc 99.25%, f1 0.9925, precision 0.9926, recall 0.9925, auc 0.9925
epoch 5101, loss 0.0182, train acc 99.28%, f1 0.9928, precision 0.9928, recall 0.9928, auc 0.9928
epoch 5201, loss 0.0176, train acc 99.28%, f1 0.9928, precision 0.9929, recall 0.9928, auc 0.9928
epoch 5301, loss 0.0185, train acc 99.30%, f1 0.9930, precision 0.9930, recall 0.9931, auc 0.9930
epoch 5401, loss 0.0174, train acc 99.32%, f1 0.9932, precision 0.9932, recall 0.9933, auc 0.9932
epoch 5501, loss 0.0175, train acc 99.33%, f1 0.9933, precision 0.9932, recall 0.9933, auc 0.9933
epoch 5601, loss 0.0171, train acc 99.35%, f1 0.9935, precision 0.9935, recall 0.9935, auc 0.9935
epoch 5701, loss 0.0162, train acc 99.37%, f1 0.9937, precision 0.9936, recall 0.9938, auc 0.9937
epoch 5801, loss 0.0161, train acc 99.39%, f1 0.9939, precision 0.9938, recall 0.9939, auc 0.9939
epoch 5901, loss 0.0150, train acc 99.40%, f1 0.9940, precision 0.9939, recall 0.9941, auc 0.9940
epoch 6001, loss 0.0130, train acc 99.42%, f1 0.9942, precision 0.9941, recall 0.9943, auc 0.9942
epoch 6101, loss 0.0152, train acc 99.45%, f1 0.9945, precision 0.9945, recall 0.9944, auc 0.9945
epoch 6201, loss 0.0146, train acc 99.48%, f1 0.9948, precision 0.9947, recall 0.9948, auc 0.9948
epoch 6301, loss 0.0140, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9950, auc 0.9949
epoch 6401, loss 0.0135, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 6501, loss 0.0129, train acc 99.54%, f1 0.9954, precision 0.9955, recall 0.9954, auc 0.9954
epoch 6601, loss 0.0119, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 6701, loss 0.0122, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9958, auc 0.9958
epoch 6801, loss 0.0117, train acc 99.59%, f1 0.9959, precision 0.9958, recall 0.9960, auc 0.9959
epoch 6901, loss 0.0112, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9963, auc 0.9961
epoch 7001, loss 0.0106, train acc 99.63%, f1 0.9963, precision 0.9961, recall 0.9964, auc 0.9963
epoch 7101, loss 0.0103, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9966, auc 0.9965
epoch 7201, loss 0.0090, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 7301, loss 0.0090, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 7401, loss 0.0087, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9972
epoch 7501, loss 0.0083, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 7601, loss 0.0081, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9976
epoch 7701, loss 0.0074, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 7801, loss 0.0071, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9981, auc 0.9980
epoch 7901, loss 0.0069, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 8001, loss 0.0062, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 8101, loss 0.0062, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 8201, loss 0.0058, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 8301, loss 0.0055, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 8401, loss 0.0053, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 8501, loss 0.0049, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 8601, loss 0.0047, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 8701, loss 0.0045, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 8801, loss 0.0044, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 8901, loss 0.0041, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 9001, loss 0.0039, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9101, loss 0.0035, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9201, loss 0.0036, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9301, loss 0.0029, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9401, loss 0.0032, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9501, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9601, loss 0.0029, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 9701, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9801, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 9901, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 10001, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 10101, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 10201, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 10301, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 10401, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 10501, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 10601, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 10701, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 10801, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 10901, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 11001, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 11101, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 11201, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 11301, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 11401, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 11501, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 11601, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 11701, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 11801, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 11901, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 12001, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 12101, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 12201, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 12301, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 12401, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 12501, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12601, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12701, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 12801, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 12901, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 13001, loss 0.0006, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_yeast3/standlization_data/yeast3_std_train_2.csv
./test_yeast3/standlization_data/yeast3_std_test_2.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_2
./test_yeast3/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6631944444444444

the Fscore is 0.42857142857142855

the precision is 0.6

the recall is 0.3333333333333333

Done
train_mlp_3_2.sh: line 433: 22258 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=yeast3 dataset_index=2 record_index=1 device_id=3 train_method=MLP_concat_Mirror_15000
