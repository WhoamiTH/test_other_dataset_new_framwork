nohup: ignoring input
./test_abalone19/standlization_data/abalone19_std_train_4.csv
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_4
----------------------



epoch 1, loss 0.6933, train acc 50.94%, f1 0.6687, precision 0.5048, recall 0.9904, auc 0.5094
epoch 101, loss 0.5162, train acc 75.50%, f1 0.7548, precision 0.7554, recall 0.7541, auc 0.7550
epoch 201, loss 0.4164, train acc 81.24%, f1 0.8124, precision 0.8125, recall 0.8122, auc 0.8124
epoch 301, loss 0.3065, train acc 89.97%, f1 0.8996, precision 0.8999, recall 0.8994, auc 0.8997
epoch 401, loss 0.2298, train acc 93.35%, f1 0.9335, precision 0.9336, recall 0.9334, auc 0.9335
epoch 501, loss 0.1937, train acc 94.15%, f1 0.9415, precision 0.9416, recall 0.9414, auc 0.9415
epoch 601, loss 0.1757, train acc 94.45%, f1 0.9445, precision 0.9445, recall 0.9445, auc 0.9445
epoch 701, loss 0.1672, train acc 94.60%, f1 0.9460, precision 0.9460, recall 0.9460, auc 0.9460
epoch 801, loss 0.1619, train acc 94.72%, f1 0.9472, precision 0.9472, recall 0.9471, auc 0.9472
epoch 901, loss 0.1582, train acc 94.80%, f1 0.9479, precision 0.9480, recall 0.9479, auc 0.9480
epoch 1001, loss 0.1553, train acc 94.85%, f1 0.9485, precision 0.9486, recall 0.9484, auc 0.9485
epoch 1101, loss 0.1514, train acc 94.91%, f1 0.9491, precision 0.9492, recall 0.9490, auc 0.9491
epoch 1201, loss 0.1475, train acc 94.95%, f1 0.9495, precision 0.9496, recall 0.9494, auc 0.9495
epoch 1301, loss 0.1435, train acc 95.01%, f1 0.9501, precision 0.9502, recall 0.9499, auc 0.9501
epoch 1401, loss 0.1398, train acc 95.06%, f1 0.9506, precision 0.9507, recall 0.9505, auc 0.9506
epoch 1501, loss 0.1367, train acc 95.13%, f1 0.9513, precision 0.9514, recall 0.9512, auc 0.9513
epoch 1601, loss 0.1333, train acc 95.21%, f1 0.9521, precision 0.9522, recall 0.9519, auc 0.9521
epoch 1701, loss 0.1298, train acc 95.29%, f1 0.9529, precision 0.9531, recall 0.9526, auc 0.9529
epoch 1801, loss 0.1261, train acc 95.36%, f1 0.9536, precision 0.9538, recall 0.9533, auc 0.9536
epoch 1901, loss 0.1211, train acc 95.43%, f1 0.9543, precision 0.9546, recall 0.9541, auc 0.9543
epoch 2001, loss 0.1177, train acc 95.51%, f1 0.9551, precision 0.9554, recall 0.9548, auc 0.9551
epoch 2101, loss 0.1133, train acc 95.58%, f1 0.9557, precision 0.9562, recall 0.9553, auc 0.9558
epoch 2201, loss 0.1089, train acc 95.65%, f1 0.9564, precision 0.9569, recall 0.9560, auc 0.9565
epoch 2301, loss 0.1065, train acc 95.71%, f1 0.9570, precision 0.9576, recall 0.9565, auc 0.9571
epoch 2401, loss 0.1032, train acc 95.83%, f1 0.9582, precision 0.9587, recall 0.9577, auc 0.9583
epoch 2501, loss 0.1002, train acc 95.91%, f1 0.9590, precision 0.9595, recall 0.9586, auc 0.9591
epoch 2601, loss 0.0966, train acc 95.96%, f1 0.9596, precision 0.9601, recall 0.9591, auc 0.9596
epoch 2701, loss 0.0952, train acc 96.02%, f1 0.9602, precision 0.9606, recall 0.9597, auc 0.9602
epoch 2801, loss 0.0923, train acc 96.08%, f1 0.9608, precision 0.9611, recall 0.9605, auc 0.9608
epoch 2901, loss 0.0905, train acc 96.14%, f1 0.9614, precision 0.9617, recall 0.9610, auc 0.9614
epoch 3001, loss 0.0886, train acc 96.21%, f1 0.9621, precision 0.9625, recall 0.9617, auc 0.9621
epoch 3101, loss 0.0876, train acc 96.28%, f1 0.9628, precision 0.9630, recall 0.9625, auc 0.9628
epoch 3201, loss 0.0858, train acc 96.35%, f1 0.9634, precision 0.9639, recall 0.9630, auc 0.9635
epoch 3301, loss 0.0835, train acc 96.43%, f1 0.9643, precision 0.9647, recall 0.9639, auc 0.9643
epoch 3401, loss 0.0823, train acc 96.50%, f1 0.9649, precision 0.9654, recall 0.9645, auc 0.9650
epoch 3501, loss 0.0806, train acc 96.56%, f1 0.9656, precision 0.9659, recall 0.9653, auc 0.9656
epoch 3601, loss 0.0798, train acc 96.62%, f1 0.9662, precision 0.9664, recall 0.9659, auc 0.9662
epoch 3701, loss 0.0778, train acc 96.65%, f1 0.9665, precision 0.9668, recall 0.9661, auc 0.9665
epoch 3801, loss 0.0764, train acc 96.70%, f1 0.9670, precision 0.9673, recall 0.9668, auc 0.9670
epoch 3901, loss 0.0758, train acc 96.74%, f1 0.9674, precision 0.9676, recall 0.9672, auc 0.9674
epoch 4001, loss 0.0746, train acc 96.80%, f1 0.9680, precision 0.9681, recall 0.9679, auc 0.9680
epoch 4101, loss 0.0733, train acc 96.86%, f1 0.9686, precision 0.9687, recall 0.9685, auc 0.9686
epoch 4201, loss 0.0716, train acc 96.90%, f1 0.9690, precision 0.9691, recall 0.9690, auc 0.9690
epoch 4301, loss 0.0706, train acc 96.96%, f1 0.9696, precision 0.9698, recall 0.9694, auc 0.9696
epoch 4401, loss 0.0696, train acc 97.01%, f1 0.9701, precision 0.9703, recall 0.9698, auc 0.9701
epoch 4501, loss 0.0681, train acc 97.07%, f1 0.9707, precision 0.9709, recall 0.9704, auc 0.9707
epoch 4601, loss 0.0672, train acc 97.13%, f1 0.9712, precision 0.9716, recall 0.9709, auc 0.9713
epoch 4701, loss 0.0655, train acc 97.19%, f1 0.9718, precision 0.9722, recall 0.9715, auc 0.9719
epoch 4801, loss 0.0641, train acc 97.24%, f1 0.9724, precision 0.9727, recall 0.9721, auc 0.9724
epoch 4901, loss 0.0632, train acc 97.30%, f1 0.9730, precision 0.9734, recall 0.9727, auc 0.9730
epoch 5001, loss 0.0618, train acc 97.37%, f1 0.9737, precision 0.9741, recall 0.9733, auc 0.9737
epoch 5101, loss 0.0605, train acc 97.44%, f1 0.9744, precision 0.9748, recall 0.9740, auc 0.9744
epoch 5201, loss 0.0591, train acc 97.50%, f1 0.9750, precision 0.9754, recall 0.9746, auc 0.9750
epoch 5301, loss 0.0575, train acc 97.59%, f1 0.9759, precision 0.9763, recall 0.9755, auc 0.9759
epoch 5401, loss 0.0560, train acc 97.66%, f1 0.9766, precision 0.9768, recall 0.9763, auc 0.9766
epoch 5501, loss 0.0546, train acc 97.74%, f1 0.9774, precision 0.9774, recall 0.9773, auc 0.9774
epoch 5601, loss 0.0534, train acc 97.79%, f1 0.9779, precision 0.9779, recall 0.9779, auc 0.9779
epoch 5701, loss 0.0516, train acc 97.86%, f1 0.9786, precision 0.9786, recall 0.9786, auc 0.9786
epoch 5801, loss 0.0502, train acc 97.93%, f1 0.9793, precision 0.9793, recall 0.9793, auc 0.9793
epoch 5901, loss 0.0490, train acc 97.99%, f1 0.9799, precision 0.9799, recall 0.9799, auc 0.9799
epoch 6001, loss 0.0472, train acc 98.07%, f1 0.9807, precision 0.9807, recall 0.9806, auc 0.9807
epoch 6101, loss 0.0460, train acc 98.13%, f1 0.9813, precision 0.9813, recall 0.9813, auc 0.9813
epoch 6201, loss 0.0445, train acc 98.19%, f1 0.9819, precision 0.9819, recall 0.9818, auc 0.9819
epoch 6301, loss 0.0434, train acc 98.25%, f1 0.9825, precision 0.9825, recall 0.9825, auc 0.9825
epoch 6401, loss 0.0419, train acc 98.32%, f1 0.9832, precision 0.9834, recall 0.9830, auc 0.9832
epoch 6501, loss 0.0406, train acc 98.39%, f1 0.9839, precision 0.9842, recall 0.9836, auc 0.9839
epoch 6601, loss 0.0392, train acc 98.48%, f1 0.9848, precision 0.9851, recall 0.9845, auc 0.9848
epoch 6701, loss 0.0378, train acc 98.55%, f1 0.9855, precision 0.9859, recall 0.9850, auc 0.9855
epoch 6801, loss 0.0363, train acc 98.62%, f1 0.9862, precision 0.9866, recall 0.9857, auc 0.9862
epoch 6901, loss 0.0353, train acc 98.68%, f1 0.9868, precision 0.9872, recall 0.9863, auc 0.9868
epoch 7001, loss 0.0335, train acc 98.74%, f1 0.9874, precision 0.9879, recall 0.9868, auc 0.9874
epoch 7101, loss 0.0327, train acc 98.80%, f1 0.9880, precision 0.9886, recall 0.9875, auc 0.9880
epoch 7201, loss 0.0317, train acc 98.85%, f1 0.9885, precision 0.9890, recall 0.9880, auc 0.9885
epoch 7301, loss 0.0304, train acc 98.90%, f1 0.9890, precision 0.9893, recall 0.9886, auc 0.9890
epoch 7401, loss 0.0295, train acc 98.95%, f1 0.9895, precision 0.9898, recall 0.9892, auc 0.9895
epoch 7501, loss 0.0283, train acc 98.99%, f1 0.9899, precision 0.9902, recall 0.9896, auc 0.9899
epoch 7601, loss 0.0274, train acc 99.04%, f1 0.9904, precision 0.9906, recall 0.9902, auc 0.9904
epoch 7701, loss 0.0264, train acc 99.09%, f1 0.9909, precision 0.9910, recall 0.9908, auc 0.9909
epoch 7801, loss 0.0254, train acc 99.14%, f1 0.9914, precision 0.9915, recall 0.9912, auc 0.9914
epoch 7901, loss 0.0245, train acc 99.18%, f1 0.9918, precision 0.9919, recall 0.9918, auc 0.9918
epoch 8001, loss 0.0236, train acc 99.22%, f1 0.9922, precision 0.9924, recall 0.9920, auc 0.9922
epoch 8101, loss 0.0225, train acc 99.26%, f1 0.9926, precision 0.9928, recall 0.9924, auc 0.9926
epoch 8201, loss 0.0216, train acc 99.30%, f1 0.9930, precision 0.9933, recall 0.9927, auc 0.9930
epoch 8301, loss 0.0210, train acc 99.33%, f1 0.9933, precision 0.9936, recall 0.9929, auc 0.9933
epoch 8401, loss 0.0201, train acc 99.36%, f1 0.9936, precision 0.9938, recall 0.9933, auc 0.9936
epoch 8501, loss 0.0195, train acc 99.38%, f1 0.9938, precision 0.9941, recall 0.9935, auc 0.9938
epoch 8601, loss 0.0187, train acc 99.41%, f1 0.9941, precision 0.9945, recall 0.9937, auc 0.9941
epoch 8701, loss 0.0180, train acc 99.44%, f1 0.9944, precision 0.9947, recall 0.9940, auc 0.9944
epoch 8801, loss 0.0171, train acc 99.45%, f1 0.9945, precision 0.9949, recall 0.9941, auc 0.9945
epoch 8901, loss 0.0162, train acc 99.49%, f1 0.9949, precision 0.9954, recall 0.9944, auc 0.9949
epoch 9001, loss 0.0157, train acc 99.53%, f1 0.9953, precision 0.9958, recall 0.9947, auc 0.9953
epoch 9101, loss 0.0151, train acc 99.55%, f1 0.9955, precision 0.9961, recall 0.9949, auc 0.9955
epoch 9201, loss 0.0144, train acc 99.58%, f1 0.9958, precision 0.9964, recall 0.9952, auc 0.9958
epoch 9301, loss 0.0138, train acc 99.61%, f1 0.9961, precision 0.9966, recall 0.9955, auc 0.9961
epoch 9401, loss 0.0130, train acc 99.63%, f1 0.9963, precision 0.9969, recall 0.9957, auc 0.9963
epoch 9501, loss 0.0125, train acc 99.65%, f1 0.9965, precision 0.9972, recall 0.9959, auc 0.9965
epoch 9601, loss 0.0120, train acc 99.67%, f1 0.9967, precision 0.9974, recall 0.9961, auc 0.9967
epoch 9701, loss 0.0115, train acc 99.69%, f1 0.9969, precision 0.9975, recall 0.9963, auc 0.9969
epoch 9801, loss 0.0109, train acc 99.71%, f1 0.9971, precision 0.9977, recall 0.9966, auc 0.9971
epoch 9901, loss 0.0105, train acc 99.73%, f1 0.9973, precision 0.9978, recall 0.9968, auc 0.9973
epoch 10001, loss 0.0099, train acc 99.75%, f1 0.9975, precision 0.9979, recall 0.9972, auc 0.9975
epoch 10101, loss 0.0095, train acc 99.76%, f1 0.9976, precision 0.9980, recall 0.9973, auc 0.9976
epoch 10201, loss 0.0091, train acc 99.78%, f1 0.9978, precision 0.9982, recall 0.9974, auc 0.9978
epoch 10301, loss 0.0087, train acc 99.79%, f1 0.9979, precision 0.9983, recall 0.9976, auc 0.9979
epoch 10401, loss 0.0083, train acc 99.81%, f1 0.9981, precision 0.9984, recall 0.9978, auc 0.9981
epoch 10501, loss 0.0078, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9980, auc 0.9982
epoch 10601, loss 0.0076, train acc 99.84%, f1 0.9984, precision 0.9987, recall 0.9981, auc 0.9984
epoch 10701, loss 0.0072, train acc 99.85%, f1 0.9985, precision 0.9988, recall 0.9983, auc 0.9985
epoch 10801, loss 0.0069, train acc 99.86%, f1 0.9986, precision 0.9989, recall 0.9984, auc 0.9986
epoch 10901, loss 0.0065, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9985, auc 0.9987
epoch 11001, loss 0.0062, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 11101, loss 0.0060, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9987, auc 0.9989
epoch 11201, loss 0.0056, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9988, auc 0.9990
epoch 11301, loss 0.0054, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9990, auc 0.9992
epoch 11401, loss 0.0051, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9992
epoch 11501, loss 0.0049, train acc 99.93%, f1 0.9993, precision 0.9995, recall 0.9992, auc 0.9993
epoch 11601, loss 0.0047, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9992, auc 0.9994
epoch 11701, loss 0.0044, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9993, auc 0.9994
epoch 11801, loss 0.0042, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9993, auc 0.9995
epoch 11901, loss 0.0040, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9994, auc 0.9996
epoch 12001, loss 0.0038, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9994, auc 0.9996
epoch 12101, loss 0.0037, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9995, auc 0.9997
epoch 12201, loss 0.0036, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9996, auc 0.9997
epoch 12301, loss 0.0034, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9996, auc 0.9997
epoch 12401, loss 0.0032, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9996, auc 0.9998
epoch 12501, loss 0.0031, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 12601, loss 0.0030, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 12701, loss 0.0028, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 12801, loss 0.0027, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 12901, loss 0.0026, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 13001, loss 0.0025, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 13101, loss 0.0023, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13201, loss 0.0022, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 13301, loss 0.0021, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 13401, loss 0.0020, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 13501, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_4.csv
./test_abalone19/standlization_data/abalone19_std_test_4.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_4
./test_abalone19/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.4987937273823884

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
./test_abalone19/standlization_data/abalone19_std_train_4.csv
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_4
----------------------



epoch 1, loss 0.6931, train acc 53.26%, f1 0.6813, precision 0.5169, recall 0.9990, auc 0.5326
epoch 101, loss 0.5150, train acc 75.57%, f1 0.7561, precision 0.7549, recall 0.7573, auc 0.7557
epoch 201, loss 0.4155, train acc 81.29%, f1 0.8124, precision 0.8147, recall 0.8100, auc 0.8129
epoch 301, loss 0.3062, train acc 89.96%, f1 0.8996, precision 0.8996, recall 0.8995, auc 0.8996
epoch 401, loss 0.2301, train acc 93.36%, f1 0.9336, precision 0.9336, recall 0.9335, auc 0.9336
epoch 501, loss 0.1934, train acc 94.15%, f1 0.9414, precision 0.9418, recall 0.9410, auc 0.9415
epoch 601, loss 0.1764, train acc 94.45%, f1 0.9444, precision 0.9450, recall 0.9438, auc 0.9445
epoch 701, loss 0.1668, train acc 94.60%, f1 0.9460, precision 0.9467, recall 0.9452, auc 0.9460
epoch 801, loss 0.1605, train acc 94.70%, f1 0.9469, precision 0.9479, recall 0.9460, auc 0.9470
epoch 901, loss 0.1584, train acc 94.79%, f1 0.9479, precision 0.9487, recall 0.9470, auc 0.9479
epoch 1001, loss 0.1554, train acc 94.86%, f1 0.9485, precision 0.9493, recall 0.9478, auc 0.9486
epoch 1101, loss 0.1513, train acc 94.89%, f1 0.9489, precision 0.9496, recall 0.9482, auc 0.9489
epoch 1201, loss 0.1467, train acc 94.95%, f1 0.9494, precision 0.9501, recall 0.9488, auc 0.9495
epoch 1301, loss 0.1440, train acc 95.02%, f1 0.9501, precision 0.9508, recall 0.9494, auc 0.9502
epoch 1401, loss 0.1399, train acc 95.07%, f1 0.9507, precision 0.9512, recall 0.9501, auc 0.9507
epoch 1501, loss 0.1362, train acc 95.12%, f1 0.9512, precision 0.9516, recall 0.9508, auc 0.9512
epoch 1601, loss 0.1332, train acc 95.20%, f1 0.9520, precision 0.9523, recall 0.9517, auc 0.9520
epoch 1701, loss 0.1295, train acc 95.28%, f1 0.9528, precision 0.9530, recall 0.9526, auc 0.9528
epoch 1801, loss 0.1260, train acc 95.36%, f1 0.9535, precision 0.9538, recall 0.9533, auc 0.9536
epoch 1901, loss 0.1219, train acc 95.43%, f1 0.9543, precision 0.9547, recall 0.9539, auc 0.9543
epoch 2001, loss 0.1171, train acc 95.50%, f1 0.9550, precision 0.9552, recall 0.9548, auc 0.9550
epoch 2101, loss 0.1138, train acc 95.59%, f1 0.9558, precision 0.9560, recall 0.9556, auc 0.9559
epoch 2201, loss 0.1095, train acc 95.64%, f1 0.9564, precision 0.9565, recall 0.9563, auc 0.9564
epoch 2301, loss 0.1062, train acc 95.72%, f1 0.9572, precision 0.9571, recall 0.9573, auc 0.9572
epoch 2401, loss 0.1029, train acc 95.83%, f1 0.9583, precision 0.9581, recall 0.9585, auc 0.9583
epoch 2501, loss 0.1009, train acc 95.90%, f1 0.9590, precision 0.9589, recall 0.9591, auc 0.9590
epoch 2601, loss 0.0981, train acc 95.96%, f1 0.9596, precision 0.9596, recall 0.9597, auc 0.9596
epoch 2701, loss 0.0955, train acc 96.02%, f1 0.9602, precision 0.9603, recall 0.9602, auc 0.9602
epoch 2801, loss 0.0935, train acc 96.07%, f1 0.9607, precision 0.9607, recall 0.9606, auc 0.9607
epoch 2901, loss 0.0906, train acc 96.15%, f1 0.9615, precision 0.9617, recall 0.9612, auc 0.9615
epoch 3001, loss 0.0881, train acc 96.22%, f1 0.9622, precision 0.9623, recall 0.9620, auc 0.9622
epoch 3101, loss 0.0875, train acc 96.29%, f1 0.9629, precision 0.9631, recall 0.9626, auc 0.9629
epoch 3201, loss 0.0858, train acc 96.36%, f1 0.9636, precision 0.9638, recall 0.9635, auc 0.9636
epoch 3301, loss 0.0840, train acc 96.43%, f1 0.9643, precision 0.9646, recall 0.9641, auc 0.9643
epoch 3401, loss 0.0825, train acc 96.48%, f1 0.9648, precision 0.9651, recall 0.9646, auc 0.9648
epoch 3501, loss 0.0801, train acc 96.56%, f1 0.9656, precision 0.9658, recall 0.9654, auc 0.9656
epoch 3601, loss 0.0799, train acc 96.60%, f1 0.9660, precision 0.9662, recall 0.9659, auc 0.9660
epoch 3701, loss 0.0782, train acc 96.64%, f1 0.9664, precision 0.9665, recall 0.9663, auc 0.9664
epoch 3801, loss 0.0772, train acc 96.67%, f1 0.9667, precision 0.9668, recall 0.9666, auc 0.9667
epoch 3901, loss 0.0756, train acc 96.73%, f1 0.9673, precision 0.9674, recall 0.9673, auc 0.9673
epoch 4001, loss 0.0747, train acc 96.79%, f1 0.9679, precision 0.9680, recall 0.9677, auc 0.9679
epoch 4101, loss 0.0734, train acc 96.84%, f1 0.9684, precision 0.9684, recall 0.9684, auc 0.9684
epoch 4201, loss 0.0725, train acc 96.89%, f1 0.9689, precision 0.9689, recall 0.9689, auc 0.9689
epoch 4301, loss 0.0715, train acc 96.93%, f1 0.9693, precision 0.9695, recall 0.9691, auc 0.9693
epoch 4401, loss 0.0705, train acc 96.97%, f1 0.9697, precision 0.9698, recall 0.9695, auc 0.9697
epoch 4501, loss 0.0688, train acc 97.02%, f1 0.9702, precision 0.9704, recall 0.9699, auc 0.9702
epoch 4601, loss 0.0680, train acc 97.07%, f1 0.9707, precision 0.9709, recall 0.9704, auc 0.9707
epoch 4701, loss 0.0670, train acc 97.12%, f1 0.9711, precision 0.9714, recall 0.9709, auc 0.9712
epoch 4801, loss 0.0658, train acc 97.16%, f1 0.9716, precision 0.9718, recall 0.9714, auc 0.9716
epoch 4901, loss 0.0645, train acc 97.22%, f1 0.9721, precision 0.9724, recall 0.9719, auc 0.9722
epoch 5001, loss 0.0629, train acc 97.28%, f1 0.9728, precision 0.9730, recall 0.9725, auc 0.9728
epoch 5101, loss 0.0623, train acc 97.34%, f1 0.9734, precision 0.9736, recall 0.9732, auc 0.9734
epoch 5201, loss 0.0607, train acc 97.39%, f1 0.9739, precision 0.9742, recall 0.9737, auc 0.9739
epoch 5301, loss 0.0594, train acc 97.46%, f1 0.9745, precision 0.9748, recall 0.9743, auc 0.9746
epoch 5401, loss 0.0582, train acc 97.52%, f1 0.9751, precision 0.9753, recall 0.9749, auc 0.9752
epoch 5501, loss 0.0571, train acc 97.58%, f1 0.9757, precision 0.9760, recall 0.9755, auc 0.9758
epoch 5601, loss 0.0555, train acc 97.64%, f1 0.9764, precision 0.9766, recall 0.9762, auc 0.9764
epoch 5701, loss 0.0539, train acc 97.71%, f1 0.9771, precision 0.9772, recall 0.9770, auc 0.9771
epoch 5801, loss 0.0521, train acc 97.75%, f1 0.9775, precision 0.9777, recall 0.9773, auc 0.9775
epoch 5901, loss 0.0508, train acc 97.82%, f1 0.9782, precision 0.9784, recall 0.9780, auc 0.9782
epoch 6001, loss 0.0498, train acc 97.89%, f1 0.9789, precision 0.9790, recall 0.9789, auc 0.9789
epoch 6101, loss 0.0485, train acc 97.98%, f1 0.9798, precision 0.9798, recall 0.9798, auc 0.9798
epoch 6201, loss 0.0471, train acc 98.05%, f1 0.9805, precision 0.9806, recall 0.9804, auc 0.9805
epoch 6301, loss 0.0455, train acc 98.12%, f1 0.9812, precision 0.9815, recall 0.9808, auc 0.9812
epoch 6401, loss 0.0440, train acc 98.20%, f1 0.9820, precision 0.9824, recall 0.9815, auc 0.9820
epoch 6501, loss 0.0422, train acc 98.29%, f1 0.9829, precision 0.9834, recall 0.9823, auc 0.9829
epoch 6601, loss 0.0408, train acc 98.38%, f1 0.9838, precision 0.9842, recall 0.9834, auc 0.9838
epoch 6701, loss 0.0391, train acc 98.46%, f1 0.9846, precision 0.9851, recall 0.9841, auc 0.9846
epoch 6801, loss 0.0376, train acc 98.54%, f1 0.9854, precision 0.9860, recall 0.9847, auc 0.9854
epoch 6901, loss 0.0360, train acc 98.62%, f1 0.9862, precision 0.9868, recall 0.9856, auc 0.9862
epoch 7001, loss 0.0342, train acc 98.71%, f1 0.9871, precision 0.9880, recall 0.9863, auc 0.9871
epoch 7101, loss 0.0329, train acc 98.81%, f1 0.9881, precision 0.9888, recall 0.9873, auc 0.9881
epoch 7201, loss 0.0315, train acc 98.87%, f1 0.9887, precision 0.9895, recall 0.9880, auc 0.9887
epoch 7301, loss 0.0295, train acc 98.94%, f1 0.9893, precision 0.9900, recall 0.9887, auc 0.9894
epoch 7401, loss 0.0286, train acc 99.00%, f1 0.9899, precision 0.9905, recall 0.9894, auc 0.9900
epoch 7501, loss 0.0273, train acc 99.04%, f1 0.9904, precision 0.9909, recall 0.9900, auc 0.9904
epoch 7601, loss 0.0263, train acc 99.10%, f1 0.9910, precision 0.9915, recall 0.9905, auc 0.9910
epoch 7701, loss 0.0251, train acc 99.15%, f1 0.9915, precision 0.9921, recall 0.9909, auc 0.9915
epoch 7801, loss 0.0242, train acc 99.20%, f1 0.9920, precision 0.9926, recall 0.9913, auc 0.9920
epoch 7901, loss 0.0232, train acc 99.24%, f1 0.9924, precision 0.9930, recall 0.9918, auc 0.9924
epoch 8001, loss 0.0220, train acc 99.27%, f1 0.9927, precision 0.9933, recall 0.9921, auc 0.9927
epoch 8101, loss 0.0211, train acc 99.32%, f1 0.9932, precision 0.9937, recall 0.9926, auc 0.9932/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0204, train acc 99.36%, f1 0.9936, precision 0.9940, recall 0.9931, auc 0.9936
epoch 8301, loss 0.0195, train acc 99.39%, f1 0.9939, precision 0.9943, recall 0.9935, auc 0.9939
epoch 8401, loss 0.0186, train acc 99.42%, f1 0.9942, precision 0.9946, recall 0.9938, auc 0.9942
epoch 8501, loss 0.0179, train acc 99.45%, f1 0.9945, precision 0.9949, recall 0.9941, auc 0.9945
epoch 8601, loss 0.0171, train acc 99.48%, f1 0.9948, precision 0.9952, recall 0.9944, auc 0.9948
epoch 8701, loss 0.0164, train acc 99.51%, f1 0.9951, precision 0.9954, recall 0.9947, auc 0.9951
epoch 8801, loss 0.0154, train acc 99.54%, f1 0.9954, precision 0.9956, recall 0.9952, auc 0.9954
epoch 8901, loss 0.0150, train acc 99.57%, f1 0.9957, precision 0.9959, recall 0.9955, auc 0.9957
epoch 9001, loss 0.0141, train acc 99.59%, f1 0.9959, precision 0.9962, recall 0.9957, auc 0.9959
epoch 9101, loss 0.0137, train acc 99.62%, f1 0.9962, precision 0.9964, recall 0.9960, auc 0.9962
epoch 9201, loss 0.0131, train acc 99.64%, f1 0.9964, precision 0.9966, recall 0.9963, auc 0.9964
epoch 9301, loss 0.0125, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9966, auc 0.9967
epoch 9401, loss 0.0120, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 9501, loss 0.0115, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9970, auc 0.9971
epoch 9601, loss 0.0110, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 9701, loss 0.0104, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 9801, loss 0.0100, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 9901, loss 0.0096, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 10001, loss 0.0092, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9977, auc 0.9978
epoch 10101, loss 0.0087, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 10201, loss 0.0084, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 10301, loss 0.0081, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9982
epoch 10401, loss 0.0077, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 10501, loss 0.0074, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 10601, loss 0.0071, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 10701, loss 0.0068, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 10801, loss 0.0065, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 10901, loss 0.0062, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 11001, loss 0.0059, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 11101, loss 0.0056, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 11201, loss 0.0054, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 11301, loss 0.0051, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 11401, loss 0.0048, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 11501, loss 0.0047, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 11601, loss 0.0045, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 11701, loss 0.0043, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 11801, loss 0.0041, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11901, loss 0.0039, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12001, loss 0.0038, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 12101, loss 0.0036, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 12201, loss 0.0034, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 12301, loss 0.0033, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 12401, loss 0.0031, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 12501, loss 0.0030, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12601, loss 0.0028, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12701, loss 0.0027, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12801, loss 0.0026, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 12901, loss 0.0025, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 13001, loss 0.0024, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 13101, loss 0.0022, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 13201, loss 0.0021, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 13301, loss 0.0020, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 13401, loss 0.0019, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13501, loss 0.0018, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13601, loss 0.0017, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13701, loss 0.0017, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13801, loss 0.0016, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13901, loss 0.0015, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 14001, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_4.csv
./test_abalone19/standlization_data/abalone19_std_test_4.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_4
./test_abalone19/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.49638118214716526

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
./test_abalone19/standlization_data/abalone19_std_train_4.csv
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_4
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5157, train acc 75.55%, f1 0.7554, precision 0.7555, recall 0.7554, auc 0.7555
epoch 201, loss 0.4159, train acc 81.29%, f1 0.8131, precision 0.8124, recall 0.8137, auc 0.8129
epoch 301, loss 0.3053, train acc 89.99%, f1 0.8999, precision 0.8999, recall 0.8999, auc 0.8999
epoch 401, loss 0.2296, train acc 93.35%, f1 0.9335, precision 0.9335, recall 0.9335, auc 0.9335
epoch 501, loss 0.1931, train acc 94.15%, f1 0.9415, precision 0.9414, recall 0.9416, auc 0.9415
epoch 601, loss 0.1761, train acc 94.45%, f1 0.9445, precision 0.9444, recall 0.9447, auc 0.9445
epoch 701, loss 0.1674, train acc 94.60%, f1 0.9460, precision 0.9458, recall 0.9463, auc 0.9460
epoch 801, loss 0.1612, train acc 94.71%, f1 0.9472, precision 0.9469, recall 0.9474, auc 0.9471
epoch 901, loss 0.1583, train acc 94.79%, f1 0.9479, precision 0.9476, recall 0.9482, auc 0.9479
epoch 1001, loss 0.1539, train acc 94.86%, f1 0.9486, precision 0.9483, recall 0.9489, auc 0.9486
epoch 1101, loss 0.1523, train acc 94.90%, f1 0.9490, precision 0.9487, recall 0.9493, auc 0.9490
epoch 1201, loss 0.1474, train acc 94.94%, f1 0.9494, precision 0.9491, recall 0.9497, auc 0.9494
epoch 1301, loss 0.1440, train acc 94.99%, f1 0.9499, precision 0.9497, recall 0.9501, auc 0.9499
epoch 1401, loss 0.1404, train acc 95.06%, f1 0.9506, precision 0.9503, recall 0.9509, auc 0.9506
epoch 1501, loss 0.1353, train acc 95.13%, f1 0.9513, precision 0.9511, recall 0.9515, auc 0.9513
epoch 1601, loss 0.1331, train acc 95.20%, f1 0.9520, precision 0.9520, recall 0.9521, auc 0.9520
epoch 1701, loss 0.1295, train acc 95.27%, f1 0.9527, precision 0.9526, recall 0.9529, auc 0.9527
epoch 1801, loss 0.1264, train acc 95.35%, f1 0.9535, precision 0.9534, recall 0.9536, auc 0.9535
epoch 1901, loss 0.1224, train acc 95.42%, f1 0.9543, precision 0.9541, recall 0.9544, auc 0.9542
epoch 2001, loss 0.1164, train acc 95.50%, f1 0.9550, precision 0.9549, recall 0.9551, auc 0.9550
epoch 2101, loss 0.1139, train acc 95.57%, f1 0.9557, precision 0.9556, recall 0.9557, auc 0.9557
epoch 2201, loss 0.1101, train acc 95.64%, f1 0.9564, precision 0.9564, recall 0.9565, auc 0.9564
epoch 2301, loss 0.1054, train acc 95.73%, f1 0.9573, precision 0.9573, recall 0.9573, auc 0.9573
epoch 2401, loss 0.1032, train acc 95.82%, f1 0.9582, precision 0.9583, recall 0.9581, auc 0.9582
epoch 2501, loss 0.1006, train acc 95.91%, f1 0.9591, precision 0.9591, recall 0.9591, auc 0.9591
epoch 2601, loss 0.0978, train acc 95.97%, f1 0.9597, precision 0.9597, recall 0.9596, auc 0.9597
epoch 2701, loss 0.0945, train acc 96.04%, f1 0.9604, precision 0.9605, recall 0.9604, auc 0.9604
epoch 2801, loss 0.0921, train acc 96.10%, f1 0.9610, precision 0.9611, recall 0.9609, auc 0.9610
epoch 2901, loss 0.0908, train acc 96.16%, f1 0.9616, precision 0.9617, recall 0.9614, auc 0.9616
epoch 3001, loss 0.0889, train acc 96.23%, f1 0.9623, precision 0.9623, recall 0.9623, auc 0.9623
epoch 3101, loss 0.0870, train acc 96.31%, f1 0.9631, precision 0.9630, recall 0.9632, auc 0.9631
epoch 3201, loss 0.0852, train acc 96.38%, f1 0.9638, precision 0.9639, recall 0.9637, auc 0.9638
epoch 3301, loss 0.0838, train acc 96.45%, f1 0.9645, precision 0.9645, recall 0.9646, auc 0.9645
epoch 3401, loss 0.0816, train acc 96.50%, f1 0.9650, precision 0.9648, recall 0.9652, auc 0.9650
epoch 3501, loss 0.0804, train acc 96.57%, f1 0.9657, precision 0.9656, recall 0.9658, auc 0.9657
epoch 3601, loss 0.0789, train acc 96.62%, f1 0.9663, precision 0.9661, recall 0.9664, auc 0.9662
epoch 3701, loss 0.0769, train acc 96.65%, f1 0.9665, precision 0.9664, recall 0.9666, auc 0.9665
epoch 3801, loss 0.0764, train acc 96.70%, f1 0.9670, precision 0.9668, recall 0.9672, auc 0.9670
epoch 3901, loss 0.0749, train acc 96.76%, f1 0.9676, precision 0.9674, recall 0.9677, auc 0.9676
epoch 4001, loss 0.0741, train acc 96.82%, f1 0.9682, precision 0.9680, recall 0.9683, auc 0.9682
epoch 4101, loss 0.0729, train acc 96.87%, f1 0.9687, precision 0.9684, recall 0.9689, auc 0.9687
epoch 4201, loss 0.0721, train acc 96.90%, f1 0.9690, precision 0.9690, recall 0.9691, auc 0.9690
epoch 4301, loss 0.0702, train acc 96.95%, f1 0.9695, precision 0.9695, recall 0.9695, auc 0.9695
epoch 4401, loss 0.0697, train acc 97.02%, f1 0.9702, precision 0.9701, recall 0.9702, auc 0.9702
epoch 4501, loss 0.0683, train acc 97.07%, f1 0.9707, precision 0.9707, recall 0.9707, auc 0.9707
epoch 4601, loss 0.0655, train acc 97.12%, f1 0.9712, precision 0.9713, recall 0.9711, auc 0.9712
epoch 4701, loss 0.0660, train acc 97.17%, f1 0.9717, precision 0.9720, recall 0.9715, auc 0.9717
epoch 4801, loss 0.0645, train acc 97.23%, f1 0.9723, precision 0.9725, recall 0.9720, auc 0.9723
epoch 4901, loss 0.0622, train acc 97.29%, f1 0.9729, precision 0.9733, recall 0.9725, auc 0.9729
epoch 5001, loss 0.0622, train acc 97.35%, f1 0.9735, precision 0.9739, recall 0.9731, auc 0.9735
epoch 5101, loss 0.0607, train acc 97.40%, f1 0.9740, precision 0.9743, recall 0.9738, auc 0.9740
epoch 5201, loss 0.0595, train acc 97.48%, f1 0.9748, precision 0.9750, recall 0.9745, auc 0.9748
epoch 5301, loss 0.0582, train acc 97.53%, f1 0.9753, precision 0.9756, recall 0.9750, auc 0.9753
epoch 5401, loss 0.0567, train acc 97.62%, f1 0.9762, precision 0.9765, recall 0.9758, auc 0.9762
epoch 5501, loss 0.0554, train acc 97.68%, f1 0.9768, precision 0.9771, recall 0.9764, auc 0.9768
epoch 5601, loss 0.0534, train acc 97.73%, f1 0.9772, precision 0.9775, recall 0.9770, auc 0.9773
epoch 5701, loss 0.0526, train acc 97.79%, f1 0.9779, precision 0.9781, recall 0.9776, auc 0.9779
epoch 5801, loss 0.0511, train acc 97.85%, f1 0.9785, precision 0.9789, recall 0.9782, auc 0.9785
epoch 5901, loss 0.0500, train acc 97.90%, f1 0.9790, precision 0.9792, recall 0.9788, auc 0.9790
epoch 6001, loss 0.0484, train acc 97.97%, f1 0.9797, precision 0.9798, recall 0.9796, auc 0.9797
epoch 6101, loss 0.0471, train acc 98.02%, f1 0.9802, precision 0.9805, recall 0.9800, auc 0.9802
epoch 6201, loss 0.0460, train acc 98.09%, f1 0.9809, precision 0.9811, recall 0.9807, auc 0.9809
epoch 6301, loss 0.0445, train acc 98.16%, f1 0.9816, precision 0.9817, recall 0.9814, auc 0.9816
epoch 6401, loss 0.0432, train acc 98.22%, f1 0.9822, precision 0.9824, recall 0.9821, auc 0.9822
epoch 6501, loss 0.0420, train acc 98.29%, f1 0.9829, precision 0.9831, recall 0.9826, auc 0.9829
epoch 6601, loss 0.0405, train acc 98.36%, f1 0.9836, precision 0.9839, recall 0.9834, auc 0.9836
epoch 6701, loss 0.0394, train acc 98.44%, f1 0.9844, precision 0.9846, recall 0.9842, auc 0.9844
epoch 6801, loss 0.0378, train acc 98.51%, f1 0.9851, precision 0.9852, recall 0.9850, auc 0.9851
epoch 6901, loss 0.0366, train acc 98.58%, f1 0.9858, precision 0.9859, recall 0.9857, auc 0.9858
epoch 7001, loss 0.0348, train acc 98.64%, f1 0.9864, precision 0.9867, recall 0.9862, auc 0.9864
epoch 7101, loss 0.0340, train acc 98.70%, f1 0.9870, precision 0.9872, recall 0.9868, auc 0.9870
epoch 7201, loss 0.0328, train acc 98.77%, f1 0.9877, precision 0.9880, recall 0.9873, auc 0.9877
epoch 7301, loss 0.0317, train acc 98.82%, f1 0.9882, precision 0.9884, recall 0.9880, auc 0.9882
epoch 7401, loss 0.0304, train acc 98.87%, f1 0.9887, precision 0.9890, recall 0.9883, auc 0.9887
epoch 7501, loss 0.0293, train acc 98.92%, f1 0.9892, precision 0.9894, recall 0.9889, auc 0.9892
epoch 7601, loss 0.0283, train acc 98.97%, f1 0.9897, precision 0.9899, recall 0.9895, auc 0.9897
epoch 7701, loss 0.0271, train acc 99.02%, f1 0.9902, precision 0.9903, recall 0.9901, auc 0.9902
epoch 7801, loss 0.0262, train acc 99.07%, f1 0.9907, precision 0.9906, recall 0.9908, auc 0.9907
epoch 7901, loss 0.0250, train acc 99.12%, f1 0.9912, precision 0.9910, recall 0.9913, auc 0.9912
epoch 8001, loss 0.0242, train acc 99.16%, f1 0.9916, precision 0.9915, recall 0.9918, auc 0.9916
epoch 8101, loss 0.0230, train acc 99.21%, f1 0.9921, precision 0.9919, recall 0.9923, auc 0.9921/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0223, train acc 99.25%, f1 0.9925, precision 0.9923, recall 0.9927, auc 0.9925
epoch 8301, loss 0.0214, train acc 99.29%, f1 0.9929, precision 0.9927, recall 0.9931, auc 0.9929
epoch 8401, loss 0.0205, train acc 99.33%, f1 0.9933, precision 0.9931, recall 0.9934, auc 0.9933
epoch 8501, loss 0.0196, train acc 99.36%, f1 0.9936, precision 0.9934, recall 0.9938, auc 0.9936
epoch 8601, loss 0.0187, train acc 99.41%, f1 0.9941, precision 0.9940, recall 0.9942, auc 0.9941
epoch 8701, loss 0.0178, train acc 99.45%, f1 0.9945, precision 0.9943, recall 0.9948, auc 0.9945
epoch 8801, loss 0.0170, train acc 99.50%, f1 0.9950, precision 0.9948, recall 0.9951, auc 0.9950
epoch 8901, loss 0.0161, train acc 99.53%, f1 0.9953, precision 0.9951, recall 0.9955, auc 0.9953
epoch 9001, loss 0.0155, train acc 99.56%, f1 0.9956, precision 0.9953, recall 0.9958, auc 0.9956
epoch 9101, loss 0.0148, train acc 99.58%, f1 0.9958, precision 0.9955, recall 0.9961, auc 0.9958
epoch 9201, loss 0.0141, train acc 99.60%, f1 0.9960, precision 0.9957, recall 0.9964, auc 0.9960
epoch 9301, loss 0.0134, train acc 99.63%, f1 0.9963, precision 0.9959, recall 0.9966, auc 0.9963
epoch 9401, loss 0.0127, train acc 99.65%, f1 0.9965, precision 0.9961, recall 0.9968, auc 0.9965
epoch 9501, loss 0.0124, train acc 99.66%, f1 0.9966, precision 0.9962, recall 0.9970, auc 0.9966
epoch 9601, loss 0.0118, train acc 99.68%, f1 0.9968, precision 0.9964, recall 0.9972, auc 0.9968
epoch 9701, loss 0.0113, train acc 99.70%, f1 0.9970, precision 0.9965, recall 0.9974, auc 0.9970
epoch 9801, loss 0.0107, train acc 99.71%, f1 0.9971, precision 0.9968, recall 0.9975, auc 0.9971
epoch 9901, loss 0.0103, train acc 99.73%, f1 0.9973, precision 0.9969, recall 0.9978, auc 0.9973
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_4.csv
./test_abalone19/standlization_data/abalone19_std_test_4.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_4
./test_abalone19/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6552070767993566

the Fscore is 0.14814814814814814

the precision is 0.09523809523809523

the recall is 0.3333333333333333

Done
train_mlp_5_1.sh: line 27: 17873 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=abalone19 dataset_index=4 record_index=1 device_id=5 train_method=MLP_concat_Mirror_8000
