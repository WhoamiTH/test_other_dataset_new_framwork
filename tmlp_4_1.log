nohup: ignoring input
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_3
----------------------



epoch 1, loss 0.6932, train acc 49.99%, f1 0.6665, precision 0.4999, recall 0.9997, auc 0.4999
epoch 101, loss 0.5296, train acc 73.96%, f1 0.7396, precision 0.7396, recall 0.7395, auc 0.7396
epoch 201, loss 0.4471, train acc 78.63%, f1 0.7863, precision 0.7863, recall 0.7864, auc 0.7863
epoch 301, loss 0.3542, train acc 86.59%, f1 0.8659, precision 0.8659, recall 0.8659, auc 0.8659
epoch 401, loss 0.2826, train acc 90.61%, f1 0.9061, precision 0.9061, recall 0.9061, auc 0.9061
epoch 501, loss 0.2456, train acc 91.79%, f1 0.9179, precision 0.9180, recall 0.9179, auc 0.9179
epoch 601, loss 0.2264, train acc 92.34%, f1 0.9234, precision 0.9234, recall 0.9233, auc 0.9234
epoch 701, loss 0.2153, train acc 92.67%, f1 0.9267, precision 0.9267, recall 0.9267, auc 0.9267
epoch 801, loss 0.2085, train acc 92.87%, f1 0.9287, precision 0.9286, recall 0.9287, auc 0.9287
epoch 901, loss 0.2036, train acc 93.00%, f1 0.9300, precision 0.9300, recall 0.9300, auc 0.9300
epoch 1001, loss 0.1981, train acc 93.08%, f1 0.9308, precision 0.9307, recall 0.9309, auc 0.9308
epoch 1101, loss 0.1912, train acc 93.19%, f1 0.9319, precision 0.9318, recall 0.9319, auc 0.9319
epoch 1201, loss 0.1844, train acc 93.37%, f1 0.9337, precision 0.9337, recall 0.9337, auc 0.9337
epoch 1301, loss 0.1780, train acc 93.56%, f1 0.9356, precision 0.9356, recall 0.9356, auc 0.9356
epoch 1401, loss 0.1724, train acc 93.78%, f1 0.9378, precision 0.9378, recall 0.9377, auc 0.9378
epoch 1501, loss 0.1673, train acc 93.93%, f1 0.9393, precision 0.9393, recall 0.9394, auc 0.9393
epoch 1601, loss 0.1626, train acc 94.02%, f1 0.9402, precision 0.9402, recall 0.9402, auc 0.9402
epoch 1701, loss 0.1581, train acc 94.15%, f1 0.9415, precision 0.9416, recall 0.9414, auc 0.9415
epoch 1801, loss 0.1534, train acc 94.28%, f1 0.9428, precision 0.9429, recall 0.9426, auc 0.9428
epoch 1901, loss 0.1487, train acc 94.38%, f1 0.9438, precision 0.9439, recall 0.9438, auc 0.9438
epoch 2001, loss 0.1441, train acc 94.49%, f1 0.9449, precision 0.9449, recall 0.9448, auc 0.9449
epoch 2101, loss 0.1399, train acc 94.61%, f1 0.9460, precision 0.9463, recall 0.9458, auc 0.9461
epoch 2201, loss 0.1358, train acc 94.74%, f1 0.9473, precision 0.9475, recall 0.9472, auc 0.9474
epoch 2301, loss 0.1319, train acc 94.86%, f1 0.9486, precision 0.9489, recall 0.9483, auc 0.9486
epoch 2401, loss 0.1280, train acc 94.99%, f1 0.9499, precision 0.9503, recall 0.9496, auc 0.9499
epoch 2501, loss 0.1243, train acc 95.16%, f1 0.9516, precision 0.9519, recall 0.9512, auc 0.9516
epoch 2601, loss 0.1207, train acc 95.28%, f1 0.9528, precision 0.9531, recall 0.9524, auc 0.9528
epoch 2701, loss 0.1172, train acc 95.39%, f1 0.9539, precision 0.9543, recall 0.9534, auc 0.9539
epoch 2801, loss 0.1139, train acc 95.51%, f1 0.9551, precision 0.9554, recall 0.9548, auc 0.9551
epoch 2901, loss 0.1108, train acc 95.60%, f1 0.9559, precision 0.9563, recall 0.9556, auc 0.9560
epoch 3001, loss 0.1079, train acc 95.71%, f1 0.9571, precision 0.9574, recall 0.9568, auc 0.9571
epoch 3101, loss 0.1051, train acc 95.80%, f1 0.9580, precision 0.9583, recall 0.9576, auc 0.9580
epoch 3201, loss 0.1025, train acc 95.88%, f1 0.9588, precision 0.9591, recall 0.9585, auc 0.9588
epoch 3301, loss 0.0999, train acc 95.97%, f1 0.9596, precision 0.9601, recall 0.9591, auc 0.9597
epoch 3401, loss 0.0975, train acc 96.05%, f1 0.9605, precision 0.9610, recall 0.9599, auc 0.9605
epoch 3501, loss 0.0951, train acc 96.12%, f1 0.9612, precision 0.9617, recall 0.9608, auc 0.9612
epoch 3601, loss 0.0929, train acc 96.21%, f1 0.9621, precision 0.9626, recall 0.9616, auc 0.9621
epoch 3701, loss 0.0907, train acc 96.29%, f1 0.9629, precision 0.9633, recall 0.9625, auc 0.9629
epoch 3801, loss 0.0886, train acc 96.38%, f1 0.9637, precision 0.9642, recall 0.9632, auc 0.9638
epoch 3901, loss 0.0866, train acc 96.44%, f1 0.9644, precision 0.9649, recall 0.9639, auc 0.9644
epoch 4001, loss 0.0847, train acc 96.51%, f1 0.9650, precision 0.9655, recall 0.9646, auc 0.9651
epoch 4101, loss 0.0828, train acc 96.57%, f1 0.9656, precision 0.9662, recall 0.9651, auc 0.9657
epoch 4201, loss 0.0809, train acc 96.63%, f1 0.9662, precision 0.9667, recall 0.9658, auc 0.9663
epoch 4301, loss 0.0791, train acc 96.70%, f1 0.9670, precision 0.9674, recall 0.9665, auc 0.9670
epoch 4401, loss 0.0773, train acc 96.77%, f1 0.9677, precision 0.9682, recall 0.9672, auc 0.9677
epoch 4501, loss 0.0755, train acc 96.85%, f1 0.9685, precision 0.9690, recall 0.9679, auc 0.9685
epoch 4601, loss 0.0737, train acc 96.92%, f1 0.9692, precision 0.9698, recall 0.9686, auc 0.9692
epoch 4701, loss 0.0720, train acc 97.00%, f1 0.9699, precision 0.9705, recall 0.9694, auc 0.9700
epoch 4801, loss 0.0702, train acc 97.07%, f1 0.9707, precision 0.9712, recall 0.9702, auc 0.9707
epoch 4901, loss 0.0685, train acc 97.17%, f1 0.9717, precision 0.9721, recall 0.9713, auc 0.9717
epoch 5001, loss 0.0667, train acc 97.24%, f1 0.9724, precision 0.9727, recall 0.9721, auc 0.9724
epoch 5101, loss 0.0650, train acc 97.31%, f1 0.9731, precision 0.9734, recall 0.9729, auc 0.9731
epoch 5201, loss 0.0633, train acc 97.38%, f1 0.9738, precision 0.9740, recall 0.9737, auc 0.9738
epoch 5301, loss 0.0616, train acc 97.45%, f1 0.9745, precision 0.9747, recall 0.9744, auc 0.9745
epoch 5401, loss 0.0599, train acc 97.53%, f1 0.9753, precision 0.9753, recall 0.9752, auc 0.9753
epoch 5501, loss 0.0582, train acc 97.60%, f1 0.9760, precision 0.9761, recall 0.9759, auc 0.9760
epoch 5601, loss 0.0565, train acc 97.67%, f1 0.9767, precision 0.9769, recall 0.9766, auc 0.9767
epoch 5701, loss 0.0548, train acc 97.76%, f1 0.9776, precision 0.9776, recall 0.9776, auc 0.9776
epoch 5801, loss 0.0531, train acc 97.84%, f1 0.9784, precision 0.9783, recall 0.9784, auc 0.9784
epoch 5901, loss 0.0513, train acc 97.93%, f1 0.9793, precision 0.9792, recall 0.9793, auc 0.9793
epoch 6001, loss 0.0496, train acc 98.03%, f1 0.9803, precision 0.9802, recall 0.9803, auc 0.9803
epoch 6101, loss 0.0478, train acc 98.13%, f1 0.9813, precision 0.9812, recall 0.9814, auc 0.9813
epoch 6201, loss 0.0461, train acc 98.23%, f1 0.9823, precision 0.9821, recall 0.9824, auc 0.9823
epoch 6301, loss 0.0444, train acc 98.33%, f1 0.9833, precision 0.9829, recall 0.9838, auc 0.9833
epoch 6401, loss 0.0426, train acc 98.43%, f1 0.9843, precision 0.9839, recall 0.9846, auc 0.9843
epoch 6501, loss 0.0406, train acc 98.53%, f1 0.9853, precision 0.9848, recall 0.9858, auc 0.9853
epoch 6601, loss 0.0386, train acc 98.64%, f1 0.9864, precision 0.9860, recall 0.9867, auc 0.9864
epoch 6701, loss 0.0368, train acc 98.72%, f1 0.9872, precision 0.9869, recall 0.9875, auc 0.9872
epoch 6801, loss 0.0351, train acc 98.80%, f1 0.9880, precision 0.9878, recall 0.9882, auc 0.9880
epoch 6901, loss 0.0336, train acc 98.87%, f1 0.9887, precision 0.9885, recall 0.9888, auc 0.9887
epoch 7001, loss 0.0321, train acc 98.94%, f1 0.9894, precision 0.9894, recall 0.9895, auc 0.9894
epoch 7101, loss 0.0307, train acc 99.01%, f1 0.9901, precision 0.9899, recall 0.9902, auc 0.9901
epoch 7201, loss 0.0293, train acc 99.07%, f1 0.9907, precision 0.9905, recall 0.9909, auc 0.9907
epoch 7301, loss 0.0281, train acc 99.12%, f1 0.9912, precision 0.9910, recall 0.9915, auc 0.9912
epoch 7401, loss 0.0269, train acc 99.17%, f1 0.9917, precision 0.9915, recall 0.9920, auc 0.9917
epoch 7501, loss 0.0257, train acc 99.23%, f1 0.9923, precision 0.9921, recall 0.9924, auc 0.9923
epoch 7601, loss 0.0247, train acc 99.27%, f1 0.9927, precision 0.9926, recall 0.9928, auc 0.9927
epoch 7701, loss 0.0236, train acc 99.30%, f1 0.9930, precision 0.9928, recall 0.9932, auc 0.9930
epoch 7801, loss 0.0226, train acc 99.34%, f1 0.9934, precision 0.9932, recall 0.9935, auc 0.9934
epoch 7901, loss 0.0217, train acc 99.36%, f1 0.9936, precision 0.9935, recall 0.9938, auc 0.9936
epoch 8001, loss 0.0208, train acc 99.40%, f1 0.9940, precision 0.9938, recall 0.9941, auc 0.9940
epoch 8101, loss 0.0200, train acc 99.43%, f1 0.9943, precision 0.9941, recall 0.9945, auc 0.9943
epoch 8201, loss 0.0191, train acc 99.45%, f1 0.9945, precision 0.9944, recall 0.9946, auc 0.9945
epoch 8301, loss 0.0184, train acc 99.48%, f1 0.9948, precision 0.9947, recall 0.9948, auc 0.9948
epoch 8401, loss 0.0176, train acc 99.51%, f1 0.9951, precision 0.9950, recall 0.9952, auc 0.9951
epoch 8501, loss 0.0169, train acc 99.54%, f1 0.9954, precision 0.9953, recall 0.9954, auc 0.9954
epoch 8601, loss 0.0163, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9957, auc 0.9956
epoch 8701, loss 0.0156, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9959, auc 0.9958
epoch 8801, loss 0.0150, train acc 99.60%, f1 0.9960, precision 0.9959, recall 0.9961, auc 0.9960
epoch 8901, loss 0.0144, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9963, auc 0.9962
epoch 9001, loss 0.0138, train acc 99.64%, f1 0.9964, precision 0.9963, recall 0.9965, auc 0.9964
epoch 9101, loss 0.0133, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9967, auc 0.9966
epoch 9201, loss 0.0128, train acc 99.68%, f1 0.9968, precision 0.9967, recall 0.9969, auc 0.9968
epoch 9301, loss 0.0123, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 9401, loss 0.0118, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9972
epoch 9501, loss 0.0113, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9973
epoch 9601, loss 0.0109, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 9701, loss 0.0105, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 9801, loss 0.0101, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 9901, loss 0.0097, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 10001, loss 0.0094, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 10101, loss 0.0090, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 10201, loss 0.0087, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9981, auc 0.9979
epoch 10301, loss 0.0084, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 10401, loss 0.0081, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 10501, loss 0.0078, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 10601, loss 0.0076, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 10701, loss 0.0073, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 10801, loss 0.0070, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 10901, loss 0.0068, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 11001, loss 0.0066, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 11101, loss 0.0063, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 11201, loss 0.0061, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 11301, loss 0.0059, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 11401, loss 0.0057, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 11501, loss 0.0055, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 11601, loss 0.0053, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 11701, loss 0.0051, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 11801, loss 0.0049, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 11901, loss 0.0048, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12001, loss 0.0046, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12101, loss 0.0044, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 12201, loss 0.0043, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 12301, loss 0.0041, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 12401, loss 0.0040, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 12501, loss 0.0038, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 12601, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12701, loss 0.0035, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 12801, loss 0.0034, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 12901, loss 0.0033, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 13001, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 13101, loss 0.0030, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 13201, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 13301, loss 0.0028, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 13401, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 13501, loss 0.0025, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 13601, loss 0.0024, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 13701, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 13801, loss 0.0022, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 13901, loss 0.0021, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 14001, loss 0.0020, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 14101, loss 0.0019, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 14201, loss 0.0018, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 14301, loss 0.0017, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 14401, loss 0.0016, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14501, loss 0.0016, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14601, loss 0.0015, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14701, loss 0.0014, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14801, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14901, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 15001, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 15101, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15201, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15301, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16501, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/standlization_data/abalone19_std_test_3.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_3
./test_abalone19/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.4975903614457831

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_3
----------------------



epoch 1, loss 0.6932, train acc 50.21%, f1 0.0085, precision 1.0000, recall 0.0043, auc 0.5021
epoch 101, loss 0.5302, train acc 73.94%, f1 0.7393, precision 0.7394, recall 0.7392, auc 0.7394
epoch 201, loss 0.4474, train acc 78.59%, f1 0.7860, precision 0.7858, recall 0.7863, auc 0.7859
epoch 301, loss 0.3542, train acc 86.59%, f1 0.8659, precision 0.8658, recall 0.8659, auc 0.8659
epoch 401, loss 0.2825, train acc 90.61%, f1 0.9061, precision 0.9062, recall 0.9061, auc 0.9061
epoch 501, loss 0.2455, train acc 91.79%, f1 0.9179, precision 0.9180, recall 0.9179, auc 0.9179
epoch 601, loss 0.2263, train acc 92.34%, f1 0.9234, precision 0.9233, recall 0.9234, auc 0.9234
epoch 701, loss 0.2152, train acc 92.67%, f1 0.9267, precision 0.9267, recall 0.9267, auc 0.9267
epoch 801, loss 0.2083, train acc 92.87%, f1 0.9287, precision 0.9286, recall 0.9288, auc 0.9287
epoch 901, loss 0.2029, train acc 93.00%, f1 0.9300, precision 0.9300, recall 0.9301, auc 0.9300
epoch 1001, loss 0.1967, train acc 93.10%, f1 0.9310, precision 0.9309, recall 0.9311, auc 0.9310
epoch 1101, loss 0.1898, train acc 93.26%, f1 0.9326, precision 0.9324, recall 0.9328, auc 0.9326
epoch 1201, loss 0.1828, train acc 93.47%, f1 0.9347, precision 0.9344, recall 0.9350, auc 0.9347
epoch 1301, loss 0.1766, train acc 93.67%, f1 0.9368, precision 0.9365, recall 0.9371, auc 0.9367
epoch 1401, loss 0.1712, train acc 93.85%, f1 0.9385, precision 0.9382, recall 0.9388, auc 0.9385
epoch 1501, loss 0.1666, train acc 93.97%, f1 0.9397, precision 0.9397, recall 0.9398, auc 0.9397
epoch 1601, loss 0.1624, train acc 94.07%, f1 0.9408, precision 0.9406, recall 0.9409, auc 0.9407
epoch 1701, loss 0.1584, train acc 94.18%, f1 0.9418, precision 0.9417, recall 0.9420, auc 0.9418
epoch 1801, loss 0.1542, train acc 94.27%, f1 0.9427, precision 0.9425, recall 0.9430, auc 0.9427
epoch 1901, loss 0.1497, train acc 94.38%, f1 0.9438, precision 0.9435, recall 0.9440, auc 0.9438
epoch 2001, loss 0.1452, train acc 94.48%, f1 0.9448, precision 0.9446, recall 0.9451, auc 0.9448
epoch 2101, loss 0.1408, train acc 94.61%, f1 0.9461, precision 0.9459, recall 0.9463, auc 0.9461
epoch 2201, loss 0.1366, train acc 94.73%, f1 0.9473, precision 0.9470, recall 0.9476, auc 0.9473
epoch 2301, loss 0.1325, train acc 94.88%, f1 0.9488, precision 0.9485, recall 0.9491, auc 0.9488
epoch 2401, loss 0.1286, train acc 95.01%, f1 0.9501, precision 0.9499, recall 0.9504, auc 0.9501
epoch 2501, loss 0.1248, train acc 95.18%, f1 0.9518, precision 0.9515, recall 0.9521, auc 0.9518
epoch 2601, loss 0.1212, train acc 95.30%, f1 0.9530, precision 0.9527, recall 0.9534, auc 0.9530
epoch 2701, loss 0.1178, train acc 95.40%, f1 0.9541, precision 0.9538, recall 0.9543, auc 0.9540
epoch 2801, loss 0.1145, train acc 95.52%, f1 0.9552, precision 0.9549, recall 0.9555, auc 0.9552
epoch 2901, loss 0.1115, train acc 95.60%, f1 0.9560, precision 0.9558, recall 0.9562, auc 0.9560
epoch 3001, loss 0.1086, train acc 95.71%, f1 0.9571, precision 0.9569, recall 0.9574, auc 0.9571
epoch 3101, loss 0.1059, train acc 95.80%, f1 0.9581, precision 0.9580, recall 0.9581, auc 0.9580
epoch 3201, loss 0.1033, train acc 95.89%, f1 0.9589, precision 0.9590, recall 0.9588, auc 0.9589
epoch 3301, loss 0.1008, train acc 95.96%, f1 0.9596, precision 0.9596, recall 0.9595, auc 0.9596
epoch 3401, loss 0.0984, train acc 96.04%, f1 0.9604, precision 0.9605, recall 0.9603, auc 0.9604
epoch 3501, loss 0.0961, train acc 96.13%, f1 0.9613, precision 0.9614, recall 0.9612, auc 0.9613
epoch 3601, loss 0.0939, train acc 96.20%, f1 0.9620, precision 0.9620, recall 0.9619, auc 0.9620
epoch 3701, loss 0.0918, train acc 96.26%, f1 0.9626, precision 0.9626, recall 0.9627, auc 0.9626
epoch 3801, loss 0.0898, train acc 96.35%, f1 0.9635, precision 0.9634, recall 0.9636, auc 0.9635
epoch 3901, loss 0.0878, train acc 96.41%, f1 0.9641, precision 0.9641, recall 0.9641, auc 0.9641
epoch 4001, loss 0.0859, train acc 96.48%, f1 0.9648, precision 0.9649, recall 0.9647, auc 0.9648
epoch 4101, loss 0.0841, train acc 96.54%, f1 0.9654, precision 0.9655, recall 0.9652, auc 0.9654
epoch 4201, loss 0.0823, train acc 96.59%, f1 0.9659, precision 0.9660, recall 0.9658, auc 0.9659
epoch 4301, loss 0.0806, train acc 96.65%, f1 0.9665, precision 0.9665, recall 0.9665, auc 0.9665
epoch 4401, loss 0.0788, train acc 96.72%, f1 0.9672, precision 0.9672, recall 0.9672, auc 0.9672
epoch 4501, loss 0.0771, train acc 96.79%, f1 0.9679, precision 0.9679, recall 0.9680, auc 0.9679
epoch 4601, loss 0.0755, train acc 96.86%, f1 0.9686, precision 0.9685, recall 0.9686, auc 0.9686
epoch 4701, loss 0.0738, train acc 96.91%, f1 0.9691, precision 0.9691, recall 0.9691, auc 0.9691
epoch 4801, loss 0.0721, train acc 96.97%, f1 0.9697, precision 0.9697, recall 0.9697, auc 0.9697
epoch 4901, loss 0.0705, train acc 97.03%, f1 0.9704, precision 0.9703, recall 0.9704, auc 0.9703
epoch 5001, loss 0.0688, train acc 97.10%, f1 0.9710, precision 0.9710, recall 0.9711, auc 0.9710
epoch 5101, loss 0.0672, train acc 97.17%, f1 0.9717, precision 0.9716, recall 0.9718, auc 0.9717
epoch 5201, loss 0.0655, train acc 97.25%, f1 0.9725, precision 0.9722, recall 0.9727, auc 0.9725
epoch 5301, loss 0.0638, train acc 97.32%, f1 0.9732, precision 0.9729, recall 0.9736, auc 0.9732
epoch 5401, loss 0.0621, train acc 97.39%, f1 0.9740, precision 0.9736, recall 0.9743, auc 0.9739
epoch 5501, loss 0.0604, train acc 97.45%, f1 0.9745, precision 0.9741, recall 0.9749, auc 0.9745
epoch 5601, loss 0.0587, train acc 97.52%, f1 0.9752, precision 0.9748, recall 0.9756, auc 0.9752
epoch 5701, loss 0.0571, train acc 97.60%, f1 0.9760, precision 0.9755, recall 0.9765, auc 0.9760
epoch 5801, loss 0.0554, train acc 97.67%, f1 0.9767, precision 0.9763, recall 0.9771, auc 0.9767
epoch 5901, loss 0.0536, train acc 97.75%, f1 0.9775, precision 0.9770, recall 0.9781, auc 0.9775
epoch 6001, loss 0.0519, train acc 97.84%, f1 0.9784, precision 0.9780, recall 0.9789, auc 0.9784
epoch 6101, loss 0.0502, train acc 97.94%, f1 0.9794, precision 0.9790, recall 0.9798, auc 0.9794
epoch 6201, loss 0.0485, train acc 98.03%, f1 0.9803, precision 0.9801, recall 0.9805, auc 0.9803
epoch 6301, loss 0.0469, train acc 98.12%, f1 0.9812, precision 0.9811, recall 0.9813, auc 0.9812
epoch 6401, loss 0.0452, train acc 98.19%, f1 0.9819, precision 0.9819, recall 0.9820, auc 0.9819
epoch 6501, loss 0.0436, train acc 98.28%, f1 0.9828, precision 0.9829, recall 0.9827, auc 0.9828
epoch 6601, loss 0.0420, train acc 98.37%, f1 0.9837, precision 0.9837, recall 0.9838, auc 0.9837
epoch 6701, loss 0.0404, train acc 98.46%, f1 0.9846, precision 0.9847, recall 0.9846, auc 0.9846
epoch 6801, loss 0.0389, train acc 98.54%, f1 0.9854, precision 0.9854, recall 0.9855, auc 0.9854
epoch 6901, loss 0.0373, train acc 98.61%, f1 0.9861, precision 0.9861, recall 0.9862, auc 0.9861
epoch 7001, loss 0.0358, train acc 98.68%, f1 0.9868, precision 0.9868, recall 0.9869, auc 0.9868
epoch 7101, loss 0.0344, train acc 98.77%, f1 0.9877, precision 0.9877, recall 0.9876, auc 0.9877
epoch 7201, loss 0.0330, train acc 98.83%, f1 0.9883, precision 0.9884, recall 0.9883, auc 0.9883
epoch 7301, loss 0.0316, train acc 98.90%, f1 0.9890, precision 0.9892, recall 0.9889, auc 0.9890
epoch 7401, loss 0.0304, train acc 98.96%, f1 0.9896, precision 0.9896, recall 0.9895, auc 0.9896
epoch 7501, loss 0.0291, train acc 99.01%, f1 0.9901, precision 0.9902, recall 0.9901, auc 0.9901
epoch 7601, loss 0.0279, train acc 99.07%, f1 0.9907, precision 0.9906, recall 0.9907, auc 0.9907
epoch 7701, loss 0.0268, train acc 99.12%, f1 0.9912, precision 0.9912, recall 0.9913, auc 0.9912
epoch 7801, loss 0.0256, train acc 99.18%, f1 0.9918, precision 0.9917, recall 0.9918, auc 0.9918
epoch 7901, loss 0.0245, train acc 99.22%, f1 0.9922, precision 0.9922, recall 0.9923, auc 0.9922
epoch 8001, loss 0.0235, train acc 99.26%, f1 0.9926, precision 0.9925, recall 0.9927, auc 0.9926
epoch 8101, loss 0.0224, train acc 99.30%, f1 0.9930, precision 0.9928, recall 0.9933, auc 0.9930/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0214, train acc 99.34%, f1 0.9934, precision 0.9933, recall 0.9936, auc 0.9934
epoch 8301, loss 0.0204, train acc 99.38%, f1 0.9938, precision 0.9937, recall 0.9940, auc 0.9938
epoch 8401, loss 0.0195, train acc 99.42%, f1 0.9942, precision 0.9941, recall 0.9943, auc 0.9942
epoch 8501, loss 0.0185, train acc 99.47%, f1 0.9947, precision 0.9945, recall 0.9949, auc 0.9947
epoch 8601, loss 0.0176, train acc 99.50%, f1 0.9950, precision 0.9948, recall 0.9952, auc 0.9950
epoch 8701, loss 0.0168, train acc 99.54%, f1 0.9954, precision 0.9953, recall 0.9955, auc 0.9954
epoch 8801, loss 0.0160, train acc 99.58%, f1 0.9958, precision 0.9956, recall 0.9960, auc 0.9958
epoch 8901, loss 0.0152, train acc 99.60%, f1 0.9960, precision 0.9958, recall 0.9962, auc 0.9960
epoch 9001, loss 0.0145, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9965, auc 0.9962
epoch 9101, loss 0.0138, train acc 99.65%, f1 0.9965, precision 0.9963, recall 0.9967, auc 0.9965
epoch 9201, loss 0.0131, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9968, auc 0.9967
epoch 9301, loss 0.0125, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9970, auc 0.9969
epoch 9401, loss 0.0120, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 9501, loss 0.0114, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 9601, loss 0.0109, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 9701, loss 0.0104, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 9801, loss 0.0099, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 9901, loss 0.0095, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 10001, loss 0.0090, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9979, auc 0.9979
epoch 10101, loss 0.0086, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 10201, loss 0.0082, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 10301, loss 0.0078, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 10401, loss 0.0074, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 10501, loss 0.0071, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 10601, loss 0.0068, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 10701, loss 0.0065, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 10801, loss 0.0062, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 10901, loss 0.0059, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9987, auc 0.9988
epoch 11001, loss 0.0057, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 11101, loss 0.0055, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 11201, loss 0.0052, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9988, auc 0.9990
epoch 11301, loss 0.0050, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 11401, loss 0.0048, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 11501, loss 0.0046, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 11601, loss 0.0045, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 11701, loss 0.0043, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 11801, loss 0.0041, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 11901, loss 0.0040, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 12001, loss 0.0038, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 12101, loss 0.0037, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 12201, loss 0.0035, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 12301, loss 0.0034, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 12401, loss 0.0033, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 12501, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 12601, loss 0.0030, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 12701, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 12801, loss 0.0028, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 12901, loss 0.0027, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 13001, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 13101, loss 0.0024, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13201, loss 0.0024, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13301, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 13401, loss 0.0022, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 13501, loss 0.0021, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 13601, loss 0.0020, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 13701, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 13801, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 13901, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 14001, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 14101, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 14201, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 14301, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 14401, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 14501, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 14601, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 14701, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 14801, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 14901, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/standlization_data/abalone19_std_test_3.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_3
./test_abalone19/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.5969879518072289

the Fscore is 0.1818181818181818

the precision is 0.16666666666666666

the recall is 0.2

Done
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_3
----------------------



epoch 1, loss 0.6933, train acc 58.38%, f1 0.6617, precision 0.5574, recall 0.8139, auc 0.5838
epoch 101, loss 0.5303, train acc 73.92%, f1 0.7395, precision 0.7387, recall 0.7403, auc 0.7392
epoch 201, loss 0.4476, train acc 78.58%, f1 0.7866, precision 0.7839, recall 0.7892, auc 0.7858
epoch 301, loss 0.3546, train acc 86.56%, f1 0.8657, precision 0.8653, recall 0.8662, auc 0.8656
epoch 401, loss 0.2829, train acc 90.61%, f1 0.9061, precision 0.9058, recall 0.9065, auc 0.9061
epoch 501, loss 0.2457, train acc 91.79%, f1 0.9179, precision 0.9173, recall 0.9186, auc 0.9179
epoch 601, loss 0.2264, train acc 92.33%, f1 0.9234, precision 0.9226, recall 0.9241, auc 0.9233
epoch 701, loss 0.2154, train acc 92.68%, f1 0.9269, precision 0.9259, recall 0.9279, auc 0.9268
epoch 801, loss 0.2085, train acc 92.88%, f1 0.9288, precision 0.9280, recall 0.9297, auc 0.9288
epoch 901, loss 0.2037, train acc 93.00%, f1 0.9300, precision 0.9292, recall 0.9309, auc 0.9300
epoch 1001, loss 0.1983, train acc 93.09%, f1 0.9310, precision 0.9302, recall 0.9317, auc 0.9309
epoch 1101, loss 0.1916, train acc 93.20%, f1 0.9320, precision 0.9313, recall 0.9328, auc 0.9320
epoch 1201, loss 0.1848, train acc 93.37%, f1 0.9338, precision 0.9331, recall 0.9344, auc 0.9337
epoch 1301, loss 0.1784, train acc 93.55%, f1 0.9355, precision 0.9349, recall 0.9362, auc 0.9355
epoch 1401, loss 0.1727, train acc 93.76%, f1 0.9376, precision 0.9371, recall 0.9382, auc 0.9376
epoch 1501, loss 0.1675, train acc 93.93%, f1 0.9393, precision 0.9387, recall 0.9400, auc 0.9393
epoch 1601, loss 0.1627, train acc 94.03%, f1 0.9403, precision 0.9396, recall 0.9410, auc 0.9403
epoch 1701, loss 0.1580, train acc 94.15%, f1 0.9415, precision 0.9409, recall 0.9420, auc 0.9415
epoch 1801, loss 0.1533, train acc 94.26%, f1 0.9427, precision 0.9423, recall 0.9430, auc 0.9426
epoch 1901, loss 0.1487, train acc 94.37%, f1 0.9437, precision 0.9435, recall 0.9438, auc 0.9437
epoch 2001, loss 0.1442, train acc 94.47%, f1 0.9447, precision 0.9447, recall 0.9446, auc 0.9447
epoch 2101, loss 0.1398, train acc 94.59%, f1 0.9459, precision 0.9459, recall 0.9459, auc 0.9459
epoch 2201, loss 0.1357, train acc 94.73%, f1 0.9473, precision 0.9473, recall 0.9473, auc 0.9473
epoch 2301, loss 0.1317, train acc 94.85%, f1 0.9485, precision 0.9484, recall 0.9486, auc 0.9485
epoch 2401, loss 0.1279, train acc 94.98%, f1 0.9499, precision 0.9495, recall 0.9502, auc 0.9498
epoch 2501, loss 0.1242, train acc 95.11%, f1 0.9511, precision 0.9508, recall 0.9515, auc 0.9511
epoch 2601, loss 0.1207, train acc 95.25%, f1 0.9525, precision 0.9521, recall 0.9529, auc 0.9525
epoch 2701, loss 0.1172, train acc 95.39%, f1 0.9539, precision 0.9532, recall 0.9546, auc 0.9539
epoch 2801, loss 0.1140, train acc 95.50%, f1 0.9550, precision 0.9545, recall 0.9556, auc 0.9550
epoch 2901, loss 0.1108, train acc 95.59%, f1 0.9559, precision 0.9556, recall 0.9562, auc 0.9559
epoch 3001, loss 0.1079, train acc 95.70%, f1 0.9570, precision 0.9567, recall 0.9573, auc 0.9570
epoch 3101, loss 0.1051, train acc 95.79%, f1 0.9579, precision 0.9577, recall 0.9582, auc 0.9579
epoch 3201, loss 0.1025, train acc 95.88%, f1 0.9588, precision 0.9586, recall 0.9590, auc 0.9588
epoch 3301, loss 0.1000, train acc 95.97%, f1 0.9597, precision 0.9595, recall 0.9598, auc 0.9597
epoch 3401, loss 0.0976, train acc 96.05%, f1 0.9605, precision 0.9603, recall 0.9607, auc 0.9605
epoch 3501, loss 0.0953, train acc 96.11%, f1 0.9611, precision 0.9608, recall 0.9614, auc 0.9611
epoch 3601, loss 0.0931, train acc 96.19%, f1 0.9619, precision 0.9616, recall 0.9623, auc 0.9619
epoch 3701, loss 0.0910, train acc 96.27%, f1 0.9627, precision 0.9627, recall 0.9628, auc 0.9627
epoch 3801, loss 0.0890, train acc 96.35%, f1 0.9635, precision 0.9635, recall 0.9635, auc 0.9635
epoch 3901, loss 0.0870, train acc 96.43%, f1 0.9643, precision 0.9642, recall 0.9645, auc 0.9643
epoch 4001, loss 0.0851, train acc 96.50%, f1 0.9650, precision 0.9649, recall 0.9651, auc 0.9650
epoch 4101, loss 0.0832, train acc 96.59%, f1 0.9659, precision 0.9657, recall 0.9660, auc 0.9659
epoch 4201, loss 0.0814, train acc 96.65%, f1 0.9665, precision 0.9663, recall 0.9666, auc 0.9665
epoch 4301, loss 0.0797, train acc 96.70%, f1 0.9670, precision 0.9668, recall 0.9673, auc 0.9670
epoch 4401, loss 0.0779, train acc 96.78%, f1 0.9678, precision 0.9676, recall 0.9679, auc 0.9678
epoch 4501, loss 0.0762, train acc 96.84%, f1 0.9684, precision 0.9682, recall 0.9687, auc 0.9684
epoch 4601, loss 0.0744, train acc 96.91%, f1 0.9692, precision 0.9689, recall 0.9694, auc 0.9691
epoch 4701, loss 0.0727, train acc 96.98%, f1 0.9698, precision 0.9694, recall 0.9702, auc 0.9698
epoch 4801, loss 0.0709, train acc 97.07%, f1 0.9707, precision 0.9702, recall 0.9712, auc 0.9707
epoch 4901, loss 0.0691, train acc 97.14%, f1 0.9714, precision 0.9710, recall 0.9718, auc 0.9714
epoch 5001, loss 0.0673, train acc 97.21%, f1 0.9721, precision 0.9717, recall 0.9726, auc 0.9721
epoch 5101, loss 0.0654, train acc 97.30%, f1 0.9730, precision 0.9726, recall 0.9734, auc 0.9730
epoch 5201, loss 0.0636, train acc 97.37%, f1 0.9737, precision 0.9733, recall 0.9742, auc 0.9737
epoch 5301, loss 0.0617, train acc 97.46%, f1 0.9746, precision 0.9742, recall 0.9750, auc 0.9746
epoch 5401, loss 0.0599, train acc 97.55%, f1 0.9755, precision 0.9751, recall 0.9760, auc 0.9755
epoch 5501, loss 0.0580, train acc 97.63%, f1 0.9763, precision 0.9758, recall 0.9767, auc 0.9763
epoch 5601, loss 0.0562, train acc 97.69%, f1 0.9769, precision 0.9765, recall 0.9774, auc 0.9769
epoch 5701, loss 0.0544, train acc 97.76%, f1 0.9776, precision 0.9771, recall 0.9780, auc 0.9776
epoch 5801, loss 0.0527, train acc 97.85%, f1 0.9785, precision 0.9780, recall 0.9791, auc 0.9785
epoch 5901, loss 0.0509, train acc 97.94%, f1 0.9794, precision 0.9788, recall 0.9799, auc 0.9794
epoch 6001, loss 0.0492, train acc 98.03%, f1 0.9803, precision 0.9798, recall 0.9808, auc 0.9803
epoch 6101, loss 0.0475, train acc 98.11%, f1 0.9811, precision 0.9805, recall 0.9817, auc 0.9811
epoch 6201, loss 0.0459, train acc 98.19%, f1 0.9819, precision 0.9813, recall 0.9825, auc 0.9819
epoch 6301, loss 0.0442, train acc 98.29%, f1 0.9829, precision 0.9823, recall 0.9835, auc 0.9829
epoch 6401, loss 0.0426, train acc 98.37%, f1 0.9837, precision 0.9831, recall 0.9843, auc 0.9837
epoch 6501, loss 0.0409, train acc 98.44%, f1 0.9845, precision 0.9839, recall 0.9850, auc 0.9844
epoch 6601, loss 0.0393, train acc 98.53%, f1 0.9853, precision 0.9849, recall 0.9858, auc 0.9853
epoch 6701, loss 0.0377, train acc 98.60%, f1 0.9860, precision 0.9856, recall 0.9864, auc 0.9860
epoch 6801, loss 0.0362, train acc 98.69%, f1 0.9869, precision 0.9865, recall 0.9873, auc 0.9869
epoch 6901, loss 0.0346, train acc 98.77%, f1 0.9877, precision 0.9873, recall 0.9881, auc 0.9877
epoch 7001, loss 0.0329, train acc 98.86%, f1 0.9886, precision 0.9882, recall 0.9889, auc 0.9886
epoch 7101, loss 0.0314, train acc 98.94%, f1 0.9894, precision 0.9891, recall 0.9898, auc 0.9894
epoch 7201, loss 0.0298, train acc 99.01%, f1 0.9901, precision 0.9898, recall 0.9904, auc 0.9901
epoch 7301, loss 0.0283, train acc 99.07%, f1 0.9907, precision 0.9905, recall 0.9910, auc 0.9907
epoch 7401, loss 0.0269, train acc 99.13%, f1 0.9913, precision 0.9910, recall 0.9915, auc 0.9913
epoch 7501, loss 0.0255, train acc 99.19%, f1 0.9919, precision 0.9918, recall 0.9921, auc 0.9919
epoch 7601, loss 0.0242, train acc 99.24%, f1 0.9924, precision 0.9923, recall 0.9926, auc 0.9924
epoch 7701, loss 0.0230, train acc 99.29%, f1 0.9929, precision 0.9927, recall 0.9932, auc 0.9929
epoch 7801, loss 0.0219, train acc 99.34%, f1 0.9934, precision 0.9931, recall 0.9936, auc 0.9934
epoch 7901, loss 0.0209, train acc 99.38%, f1 0.9938, precision 0.9936, recall 0.9940, auc 0.9938
epoch 8001, loss 0.0200, train acc 99.40%, f1 0.9940, precision 0.9939, recall 0.9942, auc 0.9940
epoch 8101, loss 0.0191, train acc 99.43%, f1 0.9943, precision 0.9942, recall 0.9944, auc 0.9943/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0183, train acc 99.46%, f1 0.9946, precision 0.9944, recall 0.9948, auc 0.9946
epoch 8301, loss 0.0175, train acc 99.50%, f1 0.9950, precision 0.9948, recall 0.9952, auc 0.9950
epoch 8401, loss 0.0168, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9954, auc 0.9953
epoch 8501, loss 0.0161, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9957, auc 0.9956
epoch 8601, loss 0.0155, train acc 99.59%, f1 0.9959, precision 0.9957, recall 0.9960, auc 0.9959
epoch 8701, loss 0.0148, train acc 99.61%, f1 0.9961, precision 0.9959, recall 0.9962, auc 0.9961
epoch 8801, loss 0.0142, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9964, auc 0.9963
epoch 8901, loss 0.0136, train acc 99.65%, f1 0.9965, precision 0.9963, recall 0.9967, auc 0.9965
epoch 9001, loss 0.0131, train acc 99.67%, f1 0.9967, precision 0.9965, recall 0.9968, auc 0.9967
epoch 9101, loss 0.0125, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9970, auc 0.9969
epoch 9201, loss 0.0120, train acc 99.71%, f1 0.9971, precision 0.9969, recall 0.9972, auc 0.9971
epoch 9301, loss 0.0115, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 9401, loss 0.0110, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9973
epoch 9501, loss 0.0105, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 9601, loss 0.0100, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9976, auc 0.9976
epoch 9701, loss 0.0096, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9977, auc 0.9978
epoch 9801, loss 0.0091, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 9901, loss 0.0087, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/standlization_data/abalone19_std_test_3.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_3
./test_abalone19/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.5909638554216867

the Fscore is 0.09523809523809523

the precision is 0.0625

the recall is 0.2

Done
train_mlp_4_1.sh: line 27: 21255 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=abalone19 dataset_index=3 record_index=1 device_id=4 train_method=MLP_concat_Mirror_8000
