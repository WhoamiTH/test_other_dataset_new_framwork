nohup: ignoring input
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_3
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
Validation loss decreased (inf --> 0.692845).  Saving model ...
Validation loss decreased (0.692845 --> 0.692655).  Saving model ...
Validation loss decreased (0.692655 --> 0.692460).  Saving model ...
Validation loss decreased (0.692460 --> 0.692243).  Saving model ...
Validation loss decreased (0.692243 --> 0.691999).  Saving model ...
Validation loss decreased (0.691999 --> 0.691725).  Saving model ...
Validation loss decreased (0.691725 --> 0.691418).  Saving model ...
Validation loss decreased (0.691418 --> 0.691077).  Saving model ...
Validation loss decreased (0.691077 --> 0.690700).  Saving model ...
Validation loss decreased (0.690700 --> 0.690285).  Saving model ...
Validation loss decreased (0.690285 --> 0.689830).  Saving model ...
Validation loss decreased (0.689830 --> 0.689334).  Saving model ...
Validation loss decreased (0.689334 --> 0.688795).  Saving model ...
Validation loss decreased (0.688795 --> 0.688212).  Saving model ...
Validation loss decreased (0.688212 --> 0.687584).  Saving model ...
Validation loss decreased (0.687584 --> 0.686911).  Saving model ...
Validation loss decreased (0.686911 --> 0.686190).  Saving model ...
Validation loss decreased (0.686190 --> 0.685422).  Saving model ...
Validation loss decreased (0.685422 --> 0.684606).  Saving model ...
Validation loss decreased (0.684606 --> 0.683740).  Saving model ...
Validation loss decreased (0.683740 --> 0.682825).  Saving model ...
Validation loss decreased (0.682825 --> 0.681859).  Saving model ...
Validation loss decreased (0.681859 --> 0.680843).  Saving model ...
Validation loss decreased (0.680843 --> 0.679775).  Saving model ...
Validation loss decreased (0.679775 --> 0.678656).  Saving model ...
Validation loss decreased (0.678656 --> 0.677487).  Saving model ...
Validation loss decreased (0.677487 --> 0.676266).  Saving model ...
Validation loss decreased (0.676266 --> 0.674995).  Saving model ...
Validation loss decreased (0.674995 --> 0.673673).  Saving model ...
Validation loss decreased (0.673673 --> 0.672301).  Saving model ...
Validation loss decreased (0.672301 --> 0.670880).  Saving model ...
Validation loss decreased (0.670880 --> 0.669410).  Saving model ...
Validation loss decreased (0.669410 --> 0.667893).  Saving model ...
Validation loss decreased (0.667893 --> 0.666330).  Saving model ...
Validation loss decreased (0.666330 --> 0.664722).  Saving model ...
Validation loss decreased (0.664722 --> 0.663070).  Saving model ...
Validation loss decreased (0.663070 --> 0.661376).  Saving model ...
Validation loss decreased (0.661376 --> 0.659641).  Saving model ...
Validation loss decreased (0.659641 --> 0.657866).  Saving model ...
Validation loss decreased (0.657866 --> 0.656054).  Saving model ...
Validation loss decreased (0.656054 --> 0.654207).  Saving model ...
Validation loss decreased (0.654207 --> 0.652326).  Saving model ...
Validation loss decreased (0.652326 --> 0.650414).  Saving model ...
Validation loss decreased (0.650414 --> 0.648473).  Saving model ...
Validation loss decreased (0.648473 --> 0.646505).  Saving model ...
Validation loss decreased (0.646505 --> 0.644512).  Saving model ...
Validation loss decreased (0.644512 --> 0.642498).  Saving model ...
Validation loss decreased (0.642498 --> 0.640465).  Saving model ...
Validation loss decreased (0.640465 --> 0.638414).  Saving model ...
Validation loss decreased (0.638414 --> 0.636349).  Saving model ...
Validation loss decreased (0.636349 --> 0.634271).  Saving model ...
Validation loss decreased (0.634271 --> 0.632184).  Saving model ...
Validation loss decreased (0.632184 --> 0.630089).  Saving model ...
Validation loss decreased (0.630089 --> 0.627989).  Saving model ...
Validation loss decreased (0.627989 --> 0.625887).  Saving model ...
Validation loss decreased (0.625887 --> 0.623784).  Saving model ...
Validation loss decreased (0.623784 --> 0.621683).  Saving model ...
Validation loss decreased (0.621683 --> 0.619588).  Saving model ...
Validation loss decreased (0.619588 --> 0.617498).  Saving model ...
Validation loss decreased (0.617498 --> 0.615418).  Saving model ...
Validation loss decreased (0.615418 --> 0.613349).  Saving model ...
Validation loss decreased (0.613349 --> 0.611294).  Saving model ...
Validation loss decreased (0.611294 --> 0.609254).  Saving model ...
Validation loss decreased (0.609254 --> 0.607232).  Saving model ...
Validation loss decreased (0.607232 --> 0.605228).  Saving model ...
Validation loss decreased (0.605228 --> 0.603246).  Saving model ...
Validation loss decreased (0.603246 --> 0.601287).  Saving model ...
Validation loss decreased (0.601287 --> 0.599352).  Saving model ...
Validation loss decreased (0.599352 --> 0.597443).  Saving model ...
Validation loss decreased (0.597443 --> 0.595560).  Saving model ...
Validation loss decreased (0.595560 --> 0.593706).  Saving model ...
Validation loss decreased (0.593706 --> 0.591882).  Saving model ...
Validation loss decreased (0.591882 --> 0.590088).  Saving model ...
Validation loss decreased (0.590088 --> 0.588326).  Saving model ...
Validation loss decreased (0.588326 --> 0.586597).  Saving model ...
Validation loss decreased (0.586597 --> 0.584901).  Saving model ...
Validation loss decreased (0.584901 --> 0.583238).  Saving model ...
Validation loss decreased (0.583238 --> 0.581610).  Saving model ...
Validation loss decreased (0.581610 --> 0.580016).  Saving model ...
Validation loss decreased (0.580016 --> 0.578457).  Saving model ...
Validation loss decreased (0.578457 --> 0.576932).  Saving model ...
Validation loss decreased (0.576932 --> 0.575443).  Saving model ...
Validation loss decreased (0.575443 --> 0.573988).  Saving model ...
Validation loss decreased (0.573988 --> 0.572569).  Saving model ...
Validation loss decreased (0.572569 --> 0.571184).  Saving model ...
Validation loss decreased (0.571184 --> 0.569835).  Saving model ...
Validation loss decreased (0.569835 --> 0.568519).  Saving model ...
Validation loss decreased (0.568519 --> 0.567237).  Saving model ...
Validation loss decreased (0.567237 --> 0.565988).  Saving model ...
Validation loss decreased (0.565988 --> 0.564772).  Saving model ...
Validation loss decreased (0.564772 --> 0.563587).  Saving model ...
Validation loss decreased (0.563587 --> 0.562434).  Saving model ...
Validation loss decreased (0.562434 --> 0.561312).  Saving model ...
Validation loss decreased (0.561312 --> 0.560221).  Saving model ...
Validation loss decreased (0.560221 --> 0.559159).  Saving model ...
Validation loss decreased (0.559159 --> 0.558126).  Saving model ...
Validation loss decreased (0.558126 --> 0.557122).  Saving model ...
Validation loss decreased (0.557122 --> 0.556145).  Saving model ...
Validation loss decreased (0.556145 --> 0.555194).  Saving model ...
Validation loss decreased (0.555194 --> 0.554269).  Saving model ...
epoch 101, loss 0.5284, train acc 73.94%, f1 0.7394, precision 0.7394, recall 0.7394, auc 0.7394
Validation loss decreased (0.554269 --> 0.553370).  Saving model ...
Validation loss decreased (0.553370 --> 0.552494).  Saving model ...
Validation loss decreased (0.552494 --> 0.551644).  Saving model ...
Validation loss decreased (0.551644 --> 0.550817).  Saving model ...
Validation loss decreased (0.550817 --> 0.550012).  Saving model ...
Validation loss decreased (0.550012 --> 0.549229).  Saving model ...
Validation loss decreased (0.549229 --> 0.548467).  Saving model ...
Validation loss decreased (0.548467 --> 0.547726).  Saving model ...
Validation loss decreased (0.547726 --> 0.547004).  Saving model ...
Validation loss decreased (0.547004 --> 0.546302).  Saving model ...
Validation loss decreased (0.546302 --> 0.545619).  Saving model ...
Validation loss decreased (0.545619 --> 0.544954).  Saving model ...
Validation loss decreased (0.544954 --> 0.544307).  Saving model ...
Validation loss decreased (0.544307 --> 0.543677).  Saving model ...
Validation loss decreased (0.543677 --> 0.543063).  Saving model ...
Validation loss decreased (0.543063 --> 0.542465).  Saving model ...
Validation loss decreased (0.542465 --> 0.541882).  Saving model ...
Validation loss decreased (0.541882 --> 0.541312).  Saving model ...
Validation loss decreased (0.541312 --> 0.540758).  Saving model ...
Validation loss decreased (0.540758 --> 0.540217).  Saving model ...
Validation loss decreased (0.540217 --> 0.539690).  Saving model ...
Validation loss decreased (0.539690 --> 0.539175).  Saving model ...
Validation loss decreased (0.539175 --> 0.538674).  Saving model ...
Validation loss decreased (0.538674 --> 0.538184).  Saving model ...
Validation loss decreased (0.538184 --> 0.537707).  Saving model ...
Validation loss decreased (0.537707 --> 0.537241).  Saving model ...
Validation loss decreased (0.537241 --> 0.536786).  Saving model ...
Validation loss decreased (0.536786 --> 0.536341).  Saving model ...
Validation loss decreased (0.536341 --> 0.535906).  Saving model ...
Validation loss decreased (0.535906 --> 0.535482).  Saving model ...
Validation loss decreased (0.535482 --> 0.535068).  Saving model ...
Validation loss decreased (0.535068 --> 0.534663).  Saving model ...
Validation loss decreased (0.534663 --> 0.534268).  Saving model ...
Validation loss decreased (0.534268 --> 0.533881).  Saving model ...
Validation loss decreased (0.533881 --> 0.533502).  Saving model ...
Validation loss decreased (0.533502 --> 0.533132).  Saving model ...
Validation loss decreased (0.533132 --> 0.532770).  Saving model ...
Validation loss decreased (0.532770 --> 0.532415).  Saving model ...
Validation loss decreased (0.532415 --> 0.532067).  Saving model ...
Validation loss decreased (0.532067 --> 0.531726).  Saving model ...
Validation loss decreased (0.531726 --> 0.531392).  Saving model ...
Validation loss decreased (0.531392 --> 0.531065).  Saving model ...
Validation loss decreased (0.531065 --> 0.530745).  Saving model ...
Validation loss decreased (0.530745 --> 0.530431).  Saving model ...
Validation loss decreased (0.530431 --> 0.530124).  Saving model ...
Validation loss decreased (0.530124 --> 0.529823).  Saving model ...
Validation loss decreased (0.529823 --> 0.529528).  Saving model ...
Validation loss decreased (0.529528 --> 0.529239).  Saving model ...
Validation loss decreased (0.529239 --> 0.528955).  Saving model ...
Validation loss decreased (0.528955 --> 0.528676).  Saving model ...
Validation loss decreased (0.528676 --> 0.528403).  Saving model ...
Validation loss decreased (0.528403 --> 0.528134).  Saving model ...
Validation loss decreased (0.528134 --> 0.527869).  Saving model ...
Validation loss decreased (0.527869 --> 0.527610).  Saving model ...
Validation loss decreased (0.527610 --> 0.527355).  Saving model ...
Validation loss decreased (0.527355 --> 0.527106).  Saving model ...
Validation loss decreased (0.527106 --> 0.526862).  Saving model ...
Validation loss decreased (0.526862 --> 0.526621).  Saving model ...
Validation loss decreased (0.526621 --> 0.526384).  Saving model ...
Validation loss decreased (0.526384 --> 0.526152).  Saving model ...
Validation loss decreased (0.526152 --> 0.525923).  Saving model ...
Validation loss decreased (0.525923 --> 0.525699).  Saving model ...
Validation loss decreased (0.525699 --> 0.525477).  Saving model ...
Validation loss decreased (0.525477 --> 0.525260).  Saving model ...
Validation loss decreased (0.525260 --> 0.525046).  Saving model ...
Validation loss decreased (0.525046 --> 0.524836).  Saving model ...
Validation loss decreased (0.524836 --> 0.524629).  Saving model ...
Validation loss decreased (0.524629 --> 0.524426).  Saving model ...
Validation loss decreased (0.524426 --> 0.524224).  Saving model ...
Validation loss decreased (0.524224 --> 0.524026).  Saving model ...
Validation loss decreased (0.524026 --> 0.523830).  Saving model ...
Validation loss decreased (0.523830 --> 0.523637).  Saving model ...
Validation loss decreased (0.523637 --> 0.523447).  Saving model ...
Validation loss decreased (0.523447 --> 0.523261).  Saving model ...
Validation loss decreased (0.523261 --> 0.523077).  Saving model ...
Validation loss decreased (0.523077 --> 0.522895).  Saving model ...
Validation loss decreased (0.522895 --> 0.522716).  Saving model ...
Validation loss decreased (0.522716 --> 0.522540).  Saving model ...
Validation loss decreased (0.522540 --> 0.522367).  Saving model ...
Validation loss decreased (0.522367 --> 0.522197).  Saving model ...
Validation loss decreased (0.522197 --> 0.522030).  Saving model ...
Validation loss decreased (0.522030 --> 0.521864).  Saving model ...
Validation loss decreased (0.521864 --> 0.521701).  Saving model ...
Validation loss decreased (0.521701 --> 0.521541).  Saving model ...
Validation loss decreased (0.521541 --> 0.521383).  Saving model ...
Validation loss decreased (0.521383 --> 0.521226).  Saving model ...
Validation loss decreased (0.521226 --> 0.521071).  Saving model ...
Validation loss decreased (0.521071 --> 0.520918).  Saving model ...
Validation loss decreased (0.520918 --> 0.520766).  Saving model ...
Validation loss decreased (0.520766 --> 0.520617).  Saving model ...
Validation loss decreased (0.520617 --> 0.520471).  Saving model ...
Validation loss decreased (0.520471 --> 0.520325).  Saving model ...
Validation loss decreased (0.520325 --> 0.520183).  Saving model ...
Validation loss decreased (0.520183 --> 0.520044).  Saving model ...
Validation loss decreased (0.520044 --> 0.519906).  Saving model ...
Validation loss decreased (0.519906 --> 0.519770).  Saving model ...
Validation loss decreased (0.519770 --> 0.519636).  Saving model ...
Validation loss decreased (0.519636 --> 0.519504).  Saving model ...
Validation loss decreased (0.519504 --> 0.519373).  Saving model ...
Validation loss decreased (0.519373 --> 0.519243).  Saving model ...
epoch 201, loss 0.4428, train acc 74.55%, f1 0.7455, precision 0.7455, recall 0.7455, auc 0.7455
Validation loss decreased (0.519243 --> 0.519115).  Saving model ...
Validation loss decreased (0.519115 --> 0.518988).  Saving model ...
Validation loss decreased (0.518988 --> 0.518861).  Saving model ...
Validation loss decreased (0.518861 --> 0.518737).  Saving model ...
Validation loss decreased (0.518737 --> 0.518614).  Saving model ...
Validation loss decreased (0.518614 --> 0.518493).  Saving model ...
Validation loss decreased (0.518493 --> 0.518374).  Saving model ...
Validation loss decreased (0.518374 --> 0.518256).  Saving model ...
Validation loss decreased (0.518256 --> 0.518139).  Saving model ...
Validation loss decreased (0.518139 --> 0.518023).  Saving model ...
Validation loss decreased (0.518023 --> 0.517908).  Saving model ...
Validation loss decreased (0.517908 --> 0.517793).  Saving model ...
Validation loss decreased (0.517793 --> 0.517680).  Saving model ...
Validation loss decreased (0.517680 --> 0.517567).  Saving model ...
Validation loss decreased (0.517567 --> 0.517456).  Saving model ...
Validation loss decreased (0.517456 --> 0.517346).  Saving model ...
Validation loss decreased (0.517346 --> 0.517236).  Saving model ...
Validation loss decreased (0.517236 --> 0.517127).  Saving model ...
Validation loss decreased (0.517127 --> 0.517018).  Saving model ...
Validation loss decreased (0.517018 --> 0.516909).  Saving model ...
Validation loss decreased (0.516909 --> 0.516801).  Saving model ...
Validation loss decreased (0.516801 --> 0.516693).  Saving model ...
Validation loss decreased (0.516693 --> 0.516585).  Saving model ...
Validation loss decreased (0.516585 --> 0.516478).  Saving model ...
Validation loss decreased (0.516478 --> 0.516372).  Saving model ...
Validation loss decreased (0.516372 --> 0.516265).  Saving model ...
Validation loss decreased (0.516265 --> 0.516160).  Saving model ...
Validation loss decreased (0.516160 --> 0.516054).  Saving model ...
Validation loss decreased (0.516054 --> 0.515948).  Saving model ...
Validation loss decreased (0.515948 --> 0.515844).  Saving model ...
Validation loss decreased (0.515844 --> 0.515742).  Saving model ...
Validation loss decreased (0.515742 --> 0.515639).  Saving model ...
Validation loss decreased (0.515639 --> 0.515537).  Saving model ...
Validation loss decreased (0.515537 --> 0.515433).  Saving model ...
Validation loss decreased (0.515433 --> 0.515329).  Saving model ...
Validation loss decreased (0.515329 --> 0.515225).  Saving model ...
Validation loss decreased (0.515225 --> 0.515122).  Saving model ...
Validation loss decreased (0.515122 --> 0.515017).  Saving model ...
Validation loss decreased (0.515017 --> 0.514911).  Saving model ...
Validation loss decreased (0.514911 --> 0.514806).  Saving model ...
Validation loss decreased (0.514806 --> 0.514701).  Saving model ...
Validation loss decreased (0.514701 --> 0.514595).  Saving model ...
Validation loss decreased (0.514595 --> 0.514490).  Saving model ...
Validation loss decreased (0.514490 --> 0.514384).  Saving model ...
Validation loss decreased (0.514384 --> 0.514280).  Saving model ...
Validation loss decreased (0.514280 --> 0.514175).  Saving model ...
Validation loss decreased (0.514175 --> 0.514069).  Saving model ...
Validation loss decreased (0.514069 --> 0.513963).  Saving model ...
Validation loss decreased (0.513963 --> 0.513856).  Saving model ...
Validation loss decreased (0.513856 --> 0.513751).  Saving model ...
Validation loss decreased (0.513751 --> 0.513645).  Saving model ...
Validation loss decreased (0.513645 --> 0.513539).  Saving model ...
Validation loss decreased (0.513539 --> 0.513433).  Saving model ...
Validation loss decreased (0.513433 --> 0.513326).  Saving model ...
Validation loss decreased (0.513326 --> 0.513218).  Saving model ...
Validation loss decreased (0.513218 --> 0.513108).  Saving model ...
Validation loss decreased (0.513108 --> 0.512997).  Saving model ...
Validation loss decreased (0.512997 --> 0.512886).  Saving model ...
Validation loss decreased (0.512886 --> 0.512775).  Saving model ...
Validation loss decreased (0.512775 --> 0.512664).  Saving model ...
Validation loss decreased (0.512664 --> 0.512551).  Saving model ...
Validation loss decreased (0.512551 --> 0.512439).  Saving model ...
Validation loss decreased (0.512439 --> 0.512326).  Saving model ...
Validation loss decreased (0.512326 --> 0.512214).  Saving model ...
Validation loss decreased (0.512214 --> 0.512101).  Saving model ...
Validation loss decreased (0.512101 --> 0.511988).  Saving model ...
Validation loss decreased (0.511988 --> 0.511873).  Saving model ...
Validation loss decreased (0.511873 --> 0.511758).  Saving model ...
Validation loss decreased (0.511758 --> 0.511642).  Saving model ...
Validation loss decreased (0.511642 --> 0.511526).  Saving model ...
Validation loss decreased (0.511526 --> 0.511408).  Saving model ...
Validation loss decreased (0.511408 --> 0.511290).  Saving model ...
Validation loss decreased (0.511290 --> 0.511171).  Saving model ...
Validation loss decreased (0.511171 --> 0.511051).  Saving model ...
Validation loss decreased (0.511051 --> 0.510930).  Saving model ...
Validation loss decreased (0.510930 --> 0.510808).  Saving model ...
Validation loss decreased (0.510808 --> 0.510686).  Saving model ...
Validation loss decreased (0.510686 --> 0.510562).  Saving model ...
Validation loss decreased (0.510562 --> 0.510437).  Saving model ...
Validation loss decreased (0.510437 --> 0.510312).  Saving model ...
Validation loss decreased (0.510312 --> 0.510184).  Saving model ...
Validation loss decreased (0.510184 --> 0.510056).  Saving model ...
Validation loss decreased (0.510056 --> 0.509927).  Saving model ...
Validation loss decreased (0.509927 --> 0.509797).  Saving model ...
Validation loss decreased (0.509797 --> 0.509666).  Saving model ...
Validation loss decreased (0.509666 --> 0.509534).  Saving model ...
Validation loss decreased (0.509534 --> 0.509400).  Saving model ...
Validation loss decreased (0.509400 --> 0.509263).  Saving model ...
Validation loss decreased (0.509263 --> 0.509125).  Saving model ...
Validation loss decreased (0.509125 --> 0.508984).  Saving model ...
Validation loss decreased (0.508984 --> 0.508844).  Saving model ...
Validation loss decreased (0.508844 --> 0.508705).  Saving model ...
Validation loss decreased (0.508705 --> 0.508565).  Saving model ...
Validation loss decreased (0.508565 --> 0.508422).  Saving model ...
Validation loss decreased (0.508422 --> 0.508277).  Saving model ...
Validation loss decreased (0.508277 --> 0.508131).  Saving model ...
Validation loss decreased (0.508131 --> 0.507986).  Saving model ...
Validation loss decreased (0.507986 --> 0.507839).  Saving model ...
Validation loss decreased (0.507839 --> 0.507691).  Saving model ...
Validation loss decreased (0.507691 --> 0.507541).  Saving model ...
epoch 301, loss 0.3446, train acc 78.18%, f1 0.7818, precision 0.7818, recall 0.7818, auc 0.7818
Validation loss decreased (0.507541 --> 0.507390).  Saving model ...
Validation loss decreased (0.507390 --> 0.507235).  Saving model ...
Validation loss decreased (0.507235 --> 0.507080).  Saving model ...
Validation loss decreased (0.507080 --> 0.506926).  Saving model ...
Validation loss decreased (0.506926 --> 0.506772).  Saving model ...
Validation loss decreased (0.506772 --> 0.506617).  Saving model ...
Validation loss decreased (0.506617 --> 0.506460).  Saving model ...
Validation loss decreased (0.506460 --> 0.506303).  Saving model ...
Validation loss decreased (0.506303 --> 0.506148).  Saving model ...
Validation loss decreased (0.506148 --> 0.505994).  Saving model ...
Validation loss decreased (0.505994 --> 0.505841).  Saving model ...
Validation loss decreased (0.505841 --> 0.505688).  Saving model ...
Validation loss decreased (0.505688 --> 0.505533).  Saving model ...
Validation loss decreased (0.505533 --> 0.505378).  Saving model ...
Validation loss decreased (0.505378 --> 0.505225).  Saving model ...
Validation loss decreased (0.505225 --> 0.505071).  Saving model ...
Validation loss decreased (0.505071 --> 0.504920).  Saving model ...
Validation loss decreased (0.504920 --> 0.504771).  Saving model ...
Validation loss decreased (0.504771 --> 0.504621).  Saving model ...
Validation loss decreased (0.504621 --> 0.504471).  Saving model ...
Validation loss decreased (0.504471 --> 0.504324).  Saving model ...
Validation loss decreased (0.504324 --> 0.504179).  Saving model ...
Validation loss decreased (0.504179 --> 0.504032).  Saving model ...
Validation loss decreased (0.504032 --> 0.503885).  Saving model ...
Validation loss decreased (0.503885 --> 0.503738).  Saving model ...
Validation loss decreased (0.503738 --> 0.503590).  Saving model ...
Validation loss decreased (0.503590 --> 0.503444).  Saving model ...
Validation loss decreased (0.503444 --> 0.503299).  Saving model ...
Validation loss decreased (0.503299 --> 0.503157).  Saving model ...
Validation loss decreased (0.503157 --> 0.503017).  Saving model ...
Validation loss decreased (0.503017 --> 0.502880).  Saving model ...
Validation loss decreased (0.502880 --> 0.502744).  Saving model ...
Validation loss decreased (0.502744 --> 0.502611).  Saving model ...
Validation loss decreased (0.502611 --> 0.502479).  Saving model ...
Validation loss decreased (0.502479 --> 0.502348).  Saving model ...
Validation loss decreased (0.502348 --> 0.502218).  Saving model ...
Validation loss decreased (0.502218 --> 0.502090).  Saving model ...
Validation loss decreased (0.502090 --> 0.501964).  Saving model ...
Validation loss decreased (0.501964 --> 0.501840).  Saving model ...
Validation loss decreased (0.501840 --> 0.501717).  Saving model ...
Validation loss decreased (0.501717 --> 0.501596).  Saving model ...
Validation loss decreased (0.501596 --> 0.501476).  Saving model ...
Validation loss decreased (0.501476 --> 0.501356).  Saving model ...
Validation loss decreased (0.501356 --> 0.501238).  Saving model ...
Validation loss decreased (0.501238 --> 0.501122).  Saving model ...
Validation loss decreased (0.501122 --> 0.501007).  Saving model ...
Validation loss decreased (0.501007 --> 0.500894).  Saving model ...
Validation loss decreased (0.500894 --> 0.500783).  Saving model ...
Validation loss decreased (0.500783 --> 0.500674).  Saving model ...
Validation loss decreased (0.500674 --> 0.500570).  Saving model .../home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

Validation loss decreased (0.500570 --> 0.500469).  Saving model ...
Validation loss decreased (0.500469 --> 0.500366).  Saving model ...
Validation loss decreased (0.500366 --> 0.500265).  Saving model ...
Validation loss decreased (0.500265 --> 0.500167).  Saving model ...
Validation loss decreased (0.500167 --> 0.500071).  Saving model ...
Validation loss decreased (0.500071 --> 0.499979).  Saving model ...
Validation loss decreased (0.499979 --> 0.499886).  Saving model ...
Validation loss decreased (0.499886 --> 0.499801).  Saving model ...
Validation loss decreased (0.499801 --> 0.499714).  Saving model ...
Validation loss decreased (0.499714 --> 0.499630).  Saving model ...
Validation loss decreased (0.499630 --> 0.499548).  Saving model ...
Validation loss decreased (0.499548 --> 0.499469).  Saving model ...
Validation loss decreased (0.499469 --> 0.499392).  Saving model ...
Validation loss decreased (0.499392 --> 0.499317).  Saving model ...
Validation loss decreased (0.499317 --> 0.499245).  Saving model ...
Validation loss decreased (0.499245 --> 0.499174).  Saving model ...
Validation loss decreased (0.499174 --> 0.499114).  Saving model ...
Validation loss decreased (0.499114 --> 0.499055).  Saving model ...
Validation loss decreased (0.499055 --> 0.498998).  Saving model ...
Validation loss decreased (0.498998 --> 0.498943).  Saving model ...
Validation loss decreased (0.498943 --> 0.498895).  Saving model ...
Validation loss decreased (0.498895 --> 0.498849).  Saving model ...
Validation loss decreased (0.498849 --> 0.498808).  Saving model ...
Validation loss decreased (0.498808 --> 0.498770).  Saving model ...
Validation loss decreased (0.498770 --> 0.498735).  Saving model ...
Validation loss decreased (0.498735 --> 0.498699).  Saving model ...
Validation loss decreased (0.498699 --> 0.498662).  Saving model ...
Validation loss decreased (0.498662 --> 0.498626).  Saving model ...
Validation loss decreased (0.498626 --> 0.498590).  Saving model ...
Validation loss decreased (0.498590 --> 0.498558).  Saving model ...
Validation loss decreased (0.498558 --> 0.498526).  Saving model ...
Validation loss decreased (0.498526 --> 0.498494).  Saving model ...
Validation loss decreased (0.498494 --> 0.498460).  Saving model ...
Validation loss decreased (0.498460 --> 0.498425).  Saving model ...
Validation loss decreased (0.498425 --> 0.498392).  Saving model ...
Validation loss decreased (0.498392 --> 0.498363).  Saving model ...
Validation loss decreased (0.498363 --> 0.498340).  Saving model ...
Validation loss decreased (0.498340 --> 0.498317).  Saving model ...
Validation loss decreased (0.498317 --> 0.498297).  Saving model ...
Validation loss decreased (0.498297 --> 0.498276).  Saving model ...
Validation loss decreased (0.498276 --> 0.498256).  Saving model ...
Validation loss decreased (0.498256 --> 0.498238).  Saving model ...
Validation loss decreased (0.498238 --> 0.498221).  Saving model ...
Validation loss decreased (0.498221 --> 0.498209).  Saving model ...
Validation loss decreased (0.498209 --> 0.498198).  Saving model ...
Validation loss decreased (0.498198 --> 0.498188).  Saving model ...
Validation loss decreased (0.498188 --> 0.498179).  Saving model ...
Validation loss decreased (0.498179 --> 0.498173).  Saving model ...
Validation loss decreased (0.498173 --> 0.498170).  Saving model ...
Validation loss decreased (0.498170 --> 0.498168).  Saving model ...
epoch 401, loss 0.2708, train acc 80.00%, f1 0.8000, precision 0.8000, recall 0.8000, auc 0.8000
Validation loss decreased (0.498168 --> 0.498167).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 421, loss 0.2609, train acc 80.00%, f1 0.8000, precision 0.8000, recall 0.8000, auc 0.8000



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/standlization_data/abalone19_std_test_3.csv
MLP_concat_Mirror_True
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_3
./test_abalone19/result_MLP_concat_Mirror_True_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.8216867469879519

the Fscore is 0.032679738562091505

the precision is 0.016611295681063124

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_3
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5329, train acc 73.87%, f1 0.7424, precision 0.7320, recall 0.7531, auc 0.7387
epoch 201, loss 0.4492, train acc 78.48%, f1 0.7837, precision 0.7878, recall 0.7796, auc 0.7848
epoch 301, loss 0.3564, train acc 86.46%, f1 0.8646, precision 0.8651, recall 0.8640, auc 0.8646
epoch 401, loss 0.2843, train acc 90.56%, f1 0.9056, precision 0.9062, recall 0.9049, auc 0.9056
epoch 501, loss 0.2468, train acc 91.77%, f1 0.9176, precision 0.9191, recall 0.9161, auc 0.9177
epoch 601, loss 0.2272, train acc 92.32%, f1 0.9231, precision 0.9249, recall 0.9212, auc 0.9232
epoch 701, loss 0.2159, train acc 92.68%, f1 0.9266, precision 0.9289, recall 0.9243, auc 0.9268
epoch 801, loss 0.2088, train acc 92.88%, f1 0.9286, precision 0.9308, recall 0.9265, auc 0.9288
epoch 901, loss 0.2037, train acc 93.00%, f1 0.9298, precision 0.9319, recall 0.9277, auc 0.9300
epoch 1001, loss 0.1983, train acc 93.09%, f1 0.9307, precision 0.9327, recall 0.9288, auc 0.9309
epoch 1101, loss 0.1916, train acc 93.20%, f1 0.9319, precision 0.9336, recall 0.9303, auc 0.9320
epoch 1201, loss 0.1849, train acc 93.36%, f1 0.9335, precision 0.9348, recall 0.9321, auc 0.9336
epoch 1301, loss 0.1786, train acc 93.54%, f1 0.9353, precision 0.9366, recall 0.9339, auc 0.9354
epoch 1401, loss 0.1729, train acc 93.73%, f1 0.9373, precision 0.9385, recall 0.9360, auc 0.9373
epoch 1501, loss 0.1677, train acc 93.90%, f1 0.9389, precision 0.9401, recall 0.9377, auc 0.9390
epoch 1601, loss 0.1629, train acc 94.02%, f1 0.9401, precision 0.9414, recall 0.9389, auc 0.9402
epoch 1701, loss 0.1582, train acc 94.12%, f1 0.9411, precision 0.9422, recall 0.9400, auc 0.9412
epoch 1801, loss 0.1537, train acc 94.23%, f1 0.9422, precision 0.9436, recall 0.9409, auc 0.9423
epoch 1901, loss 0.1494, train acc 94.33%, f1 0.9432, precision 0.9444, recall 0.9421, auc 0.9433
epoch 2001, loss 0.1453, train acc 94.41%, f1 0.9440, precision 0.9451, recall 0.9428, auc 0.9441
epoch 2101, loss 0.1413, train acc 94.49%, f1 0.9449, precision 0.9460, recall 0.9437, auc 0.9449
epoch 2201, loss 0.1374, train acc 94.61%, f1 0.9460, precision 0.9470, recall 0.9451, auc 0.9461
epoch 2301, loss 0.1337, train acc 94.73%, f1 0.9472, precision 0.9482, recall 0.9463, auc 0.9473
epoch 2401, loss 0.1301, train acc 94.85%, f1 0.9485, precision 0.9494, recall 0.9475, auc 0.9485
epoch 2501, loss 0.1266, train acc 94.97%, f1 0.9497, precision 0.9507, recall 0.9486, auc 0.9497
epoch 2601, loss 0.1232, train acc 95.08%, f1 0.9507, precision 0.9517, recall 0.9498, auc 0.9508
epoch 2701, loss 0.1198, train acc 95.18%, f1 0.9518, precision 0.9526, recall 0.9509, auc 0.9518
epoch 2801, loss 0.1164, train acc 95.32%, f1 0.9532, precision 0.9542, recall 0.9522, auc 0.9532
epoch 2901, loss 0.1132, train acc 95.45%, f1 0.9545, precision 0.9554, recall 0.9535, auc 0.9545
epoch 3001, loss 0.1101, train acc 95.58%, f1 0.9557, precision 0.9567, recall 0.9547, auc 0.9558
epoch 3101, loss 0.1072, train acc 95.68%, f1 0.9568, precision 0.9578, recall 0.9558, auc 0.9568
epoch 3201, loss 0.1044, train acc 95.78%, f1 0.9577, precision 0.9587, recall 0.9568, auc 0.9578
epoch 3301, loss 0.1018, train acc 95.86%, f1 0.9585, precision 0.9594, recall 0.9577, auc 0.9586
epoch 3401, loss 0.0993, train acc 95.95%, f1 0.9594, precision 0.9601, recall 0.9588, auc 0.9595
epoch 3501, loss 0.0969, train acc 96.02%, f1 0.9602, precision 0.9608, recall 0.9596, auc 0.9602
epoch 3601, loss 0.0946, train acc 96.10%, f1 0.9610, precision 0.9615, recall 0.9604, auc 0.9610
epoch 3701, loss 0.0925, train acc 96.19%, f1 0.9619, precision 0.9625, recall 0.9613, auc 0.9619
epoch 3801, loss 0.0904, train acc 96.26%, f1 0.9625, precision 0.9632, recall 0.9619, auc 0.9626
epoch 3901, loss 0.0884, train acc 96.35%, f1 0.9634, precision 0.9640, recall 0.9629, auc 0.9635
epoch 4001, loss 0.0865, train acc 96.41%, f1 0.9640, precision 0.9646, recall 0.9635, auc 0.9641
epoch 4101, loss 0.0846, train acc 96.48%, f1 0.9648, precision 0.9653, recall 0.9643, auc 0.9648
epoch 4201, loss 0.0828, train acc 96.55%, f1 0.9655, precision 0.9658, recall 0.9652, auc 0.9655
epoch 4301, loss 0.0810, train acc 96.61%, f1 0.9661, precision 0.9665, recall 0.9657, auc 0.9661
epoch 4401, loss 0.0792, train acc 96.66%, f1 0.9666, precision 0.9668, recall 0.9663, auc 0.9666
epoch 4501, loss 0.0775, train acc 96.72%, f1 0.9672, precision 0.9674, recall 0.9670, auc 0.9672
epoch 4601, loss 0.0758, train acc 96.79%, f1 0.9679, precision 0.9681, recall 0.9677, auc 0.9679
epoch 4701, loss 0.0740, train acc 96.88%, f1 0.9688, precision 0.9691, recall 0.9685, auc 0.9688
epoch 4801, loss 0.0723, train acc 96.96%, f1 0.9696, precision 0.9700, recall 0.9692, auc 0.9696
epoch 4901, loss 0.0705, train acc 97.03%, f1 0.9703, precision 0.9707, recall 0.9699, auc 0.9703
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/standlization_data/abalone19_std_test_3.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_3
./test_abalone19/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.760843373493976

the Fscore is 0.08219178082191782

the precision is 0.04411764705882353

the recall is 0.6

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_3
----------------------



epoch 1, loss 0.6931, train acc 53.07%, f1 0.6805, precision 0.5159, recall 0.9997, auc 0.5307
epoch 101, loss 0.5293, train acc 73.96%, f1 0.7399, precision 0.7392, recall 0.7406, auc 0.7396
epoch 201, loss 0.4473, train acc 78.60%, f1 0.7860, precision 0.7858, recall 0.7863, auc 0.7860
epoch 301, loss 0.3546, train acc 86.55%, f1 0.8655, precision 0.8653, recall 0.8658, auc 0.8655
epoch 401, loss 0.2829, train acc 90.60%, f1 0.9061, precision 0.9060, recall 0.9061, auc 0.9060
epoch 501, loss 0.2458, train acc 91.79%, f1 0.9179, precision 0.9179, recall 0.9179, auc 0.9179
epoch 601, loss 0.2265, train acc 92.33%, f1 0.9233, precision 0.9233, recall 0.9233, auc 0.9233
epoch 701, loss 0.2154, train acc 92.66%, f1 0.9266, precision 0.9266, recall 0.9266, auc 0.9266
epoch 801, loss 0.2085, train acc 92.87%, f1 0.9287, precision 0.9287, recall 0.9287, auc 0.9287
epoch 901, loss 0.2034, train acc 93.00%, f1 0.9300, precision 0.9300, recall 0.9300, auc 0.9300
epoch 1001, loss 0.1976, train acc 93.09%, f1 0.9309, precision 0.9309, recall 0.9310, auc 0.9309
epoch 1101, loss 0.1906, train acc 93.23%, f1 0.9323, precision 0.9322, recall 0.9324, auc 0.9323
epoch 1201, loss 0.1837, train acc 93.42%, f1 0.9342, precision 0.9341, recall 0.9343, auc 0.9342
epoch 1301, loss 0.1774, train acc 93.62%, f1 0.9362, precision 0.9359, recall 0.9365, auc 0.9362
epoch 1401, loss 0.1719, train acc 93.80%, f1 0.9380, precision 0.9377, recall 0.9383, auc 0.9380
epoch 1501, loss 0.1670, train acc 93.95%, f1 0.9395, precision 0.9392, recall 0.9397, auc 0.9395
epoch 1601, loss 0.1624, train acc 94.04%, f1 0.9404, precision 0.9400, recall 0.9408, auc 0.9404
epoch 1701, loss 0.1579, train acc 94.16%, f1 0.9416, precision 0.9413, recall 0.9420, auc 0.9416
epoch 1801, loss 0.1532, train acc 94.29%, f1 0.9429, precision 0.9424, recall 0.9433, auc 0.9429
epoch 1901, loss 0.1483, train acc 94.40%, f1 0.9440, precision 0.9435, recall 0.9446, auc 0.9440
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/standlization_data/abalone19_std_test_3.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_3
./test_abalone19/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.924698795180723

the Fscore is 0.07407407407407407

the precision is 0.038461538461538464

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_3.csv
./test_abalone19/model_MLP_concat_notMirror_True/record_1/MLP_concat_notMirror_True_3
----------------------



Traceback (most recent call last):
  File "./classifier_MLP/train_MLP.py", line 312, in <module>
    transformed_valid_data, transformed_valid_label = transform_data_to_train_form(transform_method, mirror_type, valid_pos_data, valid_neg_data)
  File "./classifier_MLP/train_MLP.py", line 192, in transform_data_to_train_form
    trans_pos_data, trans_pos_label, trans_neg_data, trans_neg_label = handleData_extend_not_mirror(positive_repeat_data, negetive_tile_data)
  File "./classifier_MLP/train_MLP.py", line 154, in handleData_extend_not_mirror
    transfrom_positive_data = transfrom_positive_data[positive_index[0]]
UnboundLocalError: local variable 'transfrom_positive_data' referenced before assignment
