nohup: ignoring input
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_1
----------------------



epoch 1, loss 0.6931, train acc 67.10%, f1 0.5791, precision 0.8034, recall 0.4527, auc 0.6710
epoch 101, loss 0.5767, train acc 78.36%, f1 0.7822, precision 0.7872, recall 0.7773, auc 0.7836
epoch 201, loss 0.4390, train acc 81.79%, f1 0.8181, precision 0.8172, recall 0.8190, auc 0.8179
epoch 301, loss 0.4906, train acc 82.89%, f1 0.8291, precision 0.8278, recall 0.8305, auc 0.8289
epoch 401, loss 0.3059, train acc 83.26%, f1 0.8330, precision 0.8314, recall 0.8346, auc 0.8326
epoch 501, loss 0.3669, train acc 83.43%, f1 0.8346, precision 0.8333, recall 0.8359, auc 0.8343
epoch 601, loss 0.4088, train acc 83.42%, f1 0.8346, precision 0.8328, recall 0.8364, auc 0.8342
epoch 701, loss 0.3116, train acc 83.43%, f1 0.8346, precision 0.8333, recall 0.8358, auc 0.8343
epoch 801, loss 0.3164, train acc 83.47%, f1 0.8350, precision 0.8338, recall 0.8361, auc 0.8347
epoch 901, loss 0.2808, train acc 83.45%, f1 0.8347, precision 0.8338, recall 0.8357, auc 0.8345
epoch 1001, loss 0.2786, train acc 83.42%, f1 0.8345, precision 0.8332, recall 0.8357, auc 0.8342
epoch 1101, loss 0.3576, train acc 83.41%, f1 0.8342, precision 0.8333, recall 0.8351, auc 0.8341
epoch 1201, loss 0.2534, train acc 83.45%, f1 0.8346, precision 0.8341, recall 0.8352, auc 0.8345
epoch 1301, loss 0.2357, train acc 83.45%, f1 0.8346, precision 0.8338, recall 0.8355, auc 0.8345
epoch 1401, loss 0.3691, train acc 83.48%, f1 0.8350, precision 0.8344, recall 0.8356, auc 0.8348
epoch 1501, loss 0.4524, train acc 83.45%, f1 0.8346, precision 0.8342, recall 0.8350, auc 0.8345
epoch 1601, loss 0.3819, train acc 83.42%, f1 0.8343, precision 0.8339, recall 0.8347, auc 0.8342
epoch 1701, loss 0.3841, train acc 83.44%, f1 0.8345, precision 0.8340, recall 0.8351, auc 0.8344
epoch 1801, loss 0.5065, train acc 83.54%, f1 0.8355, precision 0.8349, recall 0.8362, auc 0.8354
epoch 1901, loss 0.3799, train acc 83.54%, f1 0.8355, precision 0.8350, recall 0.8360, auc 0.8354
epoch 2001, loss 0.3514, train acc 83.58%, f1 0.8359, precision 0.8354, recall 0.8365, auc 0.8358
epoch 2101, loss 0.3546, train acc 83.54%, f1 0.8355, precision 0.8350, recall 0.8360, auc 0.8354
epoch 2201, loss 0.4430, train acc 83.61%, f1 0.8361, precision 0.8360, recall 0.8363, auc 0.8361
epoch 2301, loss 0.3215, train acc 83.70%, f1 0.8370, precision 0.8369, recall 0.8371, auc 0.8370
epoch 2401, loss 0.3152, train acc 83.70%, f1 0.8370, precision 0.8369, recall 0.8371, auc 0.8370
epoch 2501, loss 0.2699, train acc 83.83%, f1 0.8384, precision 0.8379, recall 0.8389, auc 0.8383
epoch 2601, loss 0.3490, train acc 83.92%, f1 0.8393, precision 0.8387, recall 0.8399, auc 0.8392
epoch 2701, loss 0.3040, train acc 83.93%, f1 0.8393, precision 0.8394, recall 0.8393, auc 0.8393
epoch 2801, loss 0.4767, train acc 84.03%, f1 0.8403, precision 0.8403, recall 0.8404, auc 0.8403
epoch 2901, loss 0.3428, train acc 84.21%, f1 0.8421, precision 0.8421, recall 0.8422, auc 0.8421
epoch 3001, loss 0.3537, train acc 84.31%, f1 0.8432, precision 0.8426, recall 0.8439, auc 0.8431
epoch 3101, loss 0.2537, train acc 84.44%, f1 0.8444, precision 0.8443, recall 0.8445, auc 0.8444
epoch 3201, loss 0.3912, train acc 84.60%, f1 0.8459, precision 0.8464, recall 0.8454, auc 0.8460
epoch 3301, loss 0.3394, train acc 84.63%, f1 0.8463, precision 0.8464, recall 0.8462, auc 0.8463
epoch 3401, loss 0.3662, train acc 84.79%, f1 0.8480, precision 0.8478, recall 0.8482, auc 0.8479
epoch 3501, loss 0.3715, train acc 84.92%, f1 0.8493, precision 0.8491, recall 0.8495, auc 0.8492
epoch 3601, loss 0.3335, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8505
epoch 3701, loss 0.4560, train acc 85.11%, f1 0.8511, precision 0.8510, recall 0.8512, auc 0.8511
epoch 3801, loss 0.3659, train acc 85.18%, f1 0.8519, precision 0.8516, recall 0.8522, auc 0.8518
epoch 3901, loss 0.3130, train acc 85.24%, f1 0.8524, precision 0.8526, recall 0.8521, auc 0.8524
epoch 4001, loss 0.3089, train acc 85.30%, f1 0.8530, precision 0.8532, recall 0.8527, auc 0.8530
epoch 4101, loss 0.3823, train acc 85.39%, f1 0.8539, precision 0.8540, recall 0.8539, auc 0.8539
epoch 4201, loss 0.3256, train acc 85.51%, f1 0.8550, precision 0.8556, recall 0.8544, auc 0.8551
epoch 4301, loss 0.3180, train acc 85.57%, f1 0.8557, precision 0.8559, recall 0.8556, auc 0.8557
epoch 4401, loss 0.3982, train acc 85.60%, f1 0.8561, precision 0.8554, recall 0.8568, auc 0.8560
epoch 4501, loss 0.4581, train acc 85.78%, f1 0.8577, precision 0.8579, recall 0.8575, auc 0.8578
epoch 4601, loss 0.2382, train acc 85.87%, f1 0.8587, precision 0.8585, recall 0.8589, auc 0.8587
epoch 4701, loss 0.4651, train acc 85.95%, f1 0.8595, precision 0.8593, recall 0.8597, auc 0.8595
epoch 4801, loss 0.3255, train acc 85.99%, f1 0.8598, precision 0.8603, recall 0.8592, auc 0.8599
epoch 4901, loss 0.2231, train acc 86.09%, f1 0.8608, precision 0.8617, recall 0.8599, auc 0.8609
epoch 5001, loss 0.2656, train acc 86.18%, f1 0.8618, precision 0.8619, recall 0.8618, auc 0.8618
epoch 5101, loss 0.3107, train acc 86.24%, f1 0.8625, precision 0.8616, recall 0.8634, auc 0.8624
epoch 5201, loss 0.2849, train acc 86.36%, f1 0.8636, precision 0.8638, recall 0.8634, auc 0.8636
epoch 5301, loss 0.3700, train acc 86.30%, f1 0.8631, precision 0.8625, recall 0.8638, auc 0.8630
epoch 5401, loss 0.2633, train acc 86.39%, f1 0.8640, precision 0.8633, recall 0.8648, auc 0.8639
epoch 5501, loss 0.3417, train acc 86.43%, f1 0.8644, precision 0.8640, recall 0.8648, auc 0.8643
epoch 5601, loss 0.3055, train acc 86.52%, f1 0.8651, precision 0.8652, recall 0.8651, auc 0.8652
epoch 5701, loss 0.1969, train acc 86.54%, f1 0.8656, precision 0.8648, recall 0.8663, auc 0.8654
epoch 5801, loss 0.3176, train acc 86.67%, f1 0.8668, precision 0.8661, recall 0.8675, auc 0.8667
epoch 5901, loss 0.2880, train acc 86.73%, f1 0.8674, precision 0.8667, recall 0.8680, auc 0.8673
epoch 6001, loss 0.3504, train acc 86.75%, f1 0.8677, precision 0.8667, recall 0.8686, auc 0.8675
epoch 6101, loss 0.3165, train acc 86.81%, f1 0.8683, precision 0.8672, recall 0.8693, auc 0.8681
epoch 6201, loss 0.4030, train acc 86.85%, f1 0.8686, precision 0.8681, recall 0.8691, auc 0.8685
epoch 6301, loss 0.2927, train acc 86.91%, f1 0.8692, precision 0.8685, recall 0.8699, auc 0.8691
epoch 6401, loss 0.3247, train acc 86.96%, f1 0.8695, precision 0.8696, recall 0.8695, auc 0.8696
epoch 6501, loss 0.2945, train acc 86.95%, f1 0.8697, precision 0.8684, recall 0.8710, auc 0.8695
epoch 6601, loss 0.3405, train acc 87.06%, f1 0.8709, precision 0.8690, recall 0.8728, auc 0.8706
epoch 6701, loss 0.2749, train acc 87.03%, f1 0.8704, precision 0.8698, recall 0.8709, auc 0.8703
epoch 6801, loss 0.2285, train acc 87.12%, f1 0.8711, precision 0.8716, recall 0.8706, auc 0.8712
epoch 6901, loss 0.3304, train acc 87.15%, f1 0.8716, precision 0.8708, recall 0.8723, auc 0.8715
epoch 7001, loss 0.2483, train acc 87.15%, f1 0.8716, precision 0.8710, recall 0.8722, auc 0.8715
epoch 7101, loss 0.3473, train acc 87.24%, f1 0.8724, precision 0.8720, recall 0.8729, auc 0.8724
epoch 7201, loss 0.2492, train acc 87.19%, f1 0.8719, precision 0.8717, recall 0.8722, auc 0.8719
epoch 7301, loss 0.2990, train acc 87.32%, f1 0.8734, precision 0.8717, recall 0.8751, auc 0.8732
epoch 7401, loss 0.2589, train acc 87.35%, f1 0.8736, precision 0.8732, recall 0.8739, auc 0.8735
epoch 7501, loss 0.3817, train acc 87.41%, f1 0.8744, precision 0.8725, recall 0.8762, auc 0.8741
epoch 7601, loss 0.1975, train acc 87.48%, f1 0.8748, precision 0.8743, recall 0.8754, auc 0.8748
epoch 7701, loss 0.2013, train acc 87.54%, f1 0.8755, precision 0.8746, recall 0.8765, auc 0.8754
epoch 7801, loss 0.2089, train acc 87.48%, f1 0.8751, precision 0.8730, recall 0.8772, auc 0.8748
epoch 7901, loss 0.2600, train acc 87.60%, f1 0.8763, precision 0.8744, recall 0.8782, auc 0.8760
epoch 8001, loss 0.3081, train acc 87.69%, f1 0.8771, precision 0.8757, recall 0.8786, auc 0.8769
epoch 8101, loss 0.3203, train acc 87.76%, f1 0.8776, precision 0.8775, recall 0.8776, auc 0.8776
epoch 8201, loss 0.2153, train acc 87.81%, f1 0.8781, precision 0.8777, recall 0.8785, auc 0.8781
epoch 8301, loss 0.3178, train acc 87.84%, f1 0.8784, precision 0.8784, recall 0.8784, auc 0.8784
epoch 8401, loss 0.2935, train acc 87.92%, f1 0.8794, precision 0.8782, recall 0.8805, auc 0.8792
epoch 8501, loss 0.2131, train acc 87.97%, f1 0.8798, precision 0.8787, recall 0.8809, auc 0.8797
epoch 8601, loss 0.3493, train acc 88.05%, f1 0.8806, precision 0.8797, recall 0.8816, auc 0.8805
epoch 8701, loss 0.1943, train acc 88.08%, f1 0.8811, precision 0.8787, recall 0.8835, auc 0.8808
epoch 8801, loss 0.2637, train acc 88.23%, f1 0.8825, precision 0.8812, recall 0.8838, auc 0.8823
epoch 8901, loss 0.2574, train acc 88.23%, f1 0.8823, precision 0.8821, recall 0.8826, auc 0.8823
epoch 9001, loss 0.3982, train acc 88.32%, f1 0.8835, precision 0.8818, recall 0.8851, auc 0.8832
epoch 9101, loss 0.3205, train acc 88.39%, f1 0.8838, precision 0.8842, recall 0.8835, auc 0.8839
epoch 9201, loss 0.2235, train acc 88.44%, f1 0.8845, precision 0.8842, recall 0.8847, auc 0.8844
epoch 9301, loss 0.2989, train acc 88.51%, f1 0.8852, precision 0.8839, recall 0.8865, auc 0.8851
epoch 9401, loss 0.1530, train acc 88.55%, f1 0.8858, precision 0.8839, recall 0.8876, auc 0.8855
epoch 9501, loss 0.2143, train acc 88.64%, f1 0.8866, precision 0.8855, recall 0.8877, auc 0.8864
epoch 9601, loss 0.2352, train acc 88.66%, f1 0.8865, precision 0.8869, recall 0.8862, auc 0.8866
epoch 9701, loss 0.3093, train acc 88.74%, f1 0.8875, precision 0.8866, recall 0.8884, auc 0.8874
epoch 9801, loss 0.2827, train acc 88.78%, f1 0.8880, precision 0.8864, recall 0.8896, auc 0.8878
epoch 9901, loss 0.2754, train acc 88.90%, f1 0.8891, precision 0.8881, recall 0.8901, auc 0.8890
epoch 10001, loss 0.2075, train acc 88.92%, f1 0.8894, precision 0.8871, recall 0.8918, auc 0.8892
epoch 10101, loss 0.3488, train acc 88.99%, f1 0.8900, precision 0.8891, recall 0.8909, auc 0.8899
epoch 10201, loss 0.2818, train acc 89.05%, f1 0.8908, precision 0.8887, recall 0.8928, auc 0.8905
epoch 10301, loss 0.2374, train acc 89.14%, f1 0.8916, precision 0.8905, recall 0.8927, auc 0.8914
epoch 10401, loss 0.3447, train acc 89.22%, f1 0.8923, precision 0.8909, recall 0.8938, auc 0.8922
epoch 10501, loss 0.2439, train acc 89.27%, f1 0.8929, precision 0.8916, recall 0.8942, auc 0.8927
epoch 10601, loss 0.2547, train acc 89.32%, f1 0.8934, precision 0.8916, recall 0.8952, auc 0.8932
epoch 10701, loss 0.1829, train acc 89.37%, f1 0.8938, precision 0.8929, recall 0.8947, auc 0.8937
epoch 10801, loss 0.2089, train acc 89.42%, f1 0.8944, precision 0.8926, recall 0.8962, auc 0.8942
epoch 10901, loss 0.2635, train acc 89.48%, f1 0.8951, precision 0.8926, recall 0.8977, auc 0.8948
epoch 11001, loss 0.1776, train acc 89.54%, f1 0.8956, precision 0.8935, recall 0.8978, auc 0.8954
epoch 11101, loss 0.2296, train acc 89.58%, f1 0.8959, precision 0.8949, recall 0.8970, auc 0.8958
epoch 11201, loss 0.3413, train acc 89.70%, f1 0.8973, precision 0.8947, recall 0.9000, auc 0.8970
epoch 11301, loss 0.2466, train acc 89.76%, f1 0.8976, precision 0.8972, recall 0.8981, auc 0.8976
epoch 11401, loss 0.2064, train acc 89.79%, f1 0.8981, precision 0.8964, recall 0.8997, auc 0.8979
epoch 11501, loss 0.2276, train acc 89.84%, f1 0.8984, precision 0.8976, recall 0.8993, auc 0.8984
epoch 11601, loss 0.1655, train acc 89.91%, f1 0.8993, precision 0.8973, recall 0.9013, auc 0.8991
epoch 11701, loss 0.1804, train acc 89.88%, f1 0.8991, precision 0.8962, recall 0.9020, auc 0.8988
epoch 11801, loss 0.2380, train acc 89.99%, f1 0.9000, precision 0.8991, recall 0.9009, auc 0.8999
epoch 11901, loss 0.1510, train acc 90.03%, f1 0.9005, precision 0.8982, recall 0.9029, auc 0.9003
epoch 12001, loss 0.2896, train acc 90.11%, f1 0.9014, precision 0.8986, recall 0.9042, auc 0.9011
epoch 12101, loss 0.2179, train acc 90.09%, f1 0.9012, precision 0.8984, recall 0.9040, auc 0.9009
epoch 12201, loss 0.2915, train acc 90.21%, f1 0.9022, precision 0.9014, recall 0.9029, auc 0.9021
epoch 12301, loss 0.2446, train acc 90.22%, f1 0.9023, precision 0.9014, recall 0.9033, auc 0.9022
epoch 12401, loss 0.2138, train acc 90.36%, f1 0.9039, precision 0.9012, recall 0.9066, auc 0.9036
epoch 12501, loss 0.1709, train acc 90.43%, f1 0.9046, precision 0.9024, recall 0.9067, auc 0.9043
epoch 12601, loss 0.2523, train acc 90.52%, f1 0.9055, precision 0.9033, recall 0.9076, auc 0.9052
epoch 12701, loss 0.2538, train acc 90.47%, f1 0.9050, precision 0.9025, recall 0.9074, auc 0.9047
epoch 12801, loss 0.2024, train acc 90.47%, f1 0.9048, precision 0.9039, recall 0.9057, auc 0.9047
epoch 12901, loss 0.2106, train acc 90.59%, f1 0.9061, precision 0.9041, recall 0.9081, auc 0.9059
epoch 13001, loss 0.2189, train acc 90.74%, f1 0.9078, precision 0.9041, recall 0.9114, auc 0.9074
epoch 13101, loss 0.1436, train acc 90.67%, f1 0.9069, precision 0.9049, recall 0.9089, auc 0.9067
epoch 13201, loss 0.2387, train acc 90.80%, f1 0.9083, precision 0.9058, recall 0.9107, auc 0.9080
epoch 13301, loss 0.1935, train acc 90.85%, f1 0.9088, precision 0.9063, recall 0.9112, auc 0.9085
epoch 13401, loss 0.2193, train acc 90.87%, f1 0.9090, precision 0.9065, recall 0.9114, auc 0.9087
epoch 13501, loss 0.1724, train acc 90.96%, f1 0.9098, precision 0.9082, recall 0.9115, auc 0.9096
epoch 13601, loss 0.2113, train acc 90.91%, f1 0.9093, precision 0.9077, recall 0.9109, auc 0.9091
epoch 13701, loss 0.3448, train acc 91.04%, f1 0.9106, precision 0.9080, recall 0.9132, auc 0.9104
epoch 13801, loss 0.2694, train acc 91.15%, f1 0.9118, precision 0.9084, recall 0.9152, auc 0.9115
epoch 13901, loss 0.2046, train acc 91.16%, f1 0.9118, precision 0.9102, recall 0.9133, auc 0.9116
epoch 14001, loss 0.1748, train acc 91.18%, f1 0.9120, precision 0.9097, recall 0.9143, auc 0.9118
epoch 14101, loss 0.2610, train acc 91.20%, f1 0.9123, precision 0.9089, recall 0.9158, auc 0.9120
epoch 14201, loss 0.2325, train acc 91.30%, f1 0.9132, precision 0.9116, recall 0.9148, auc 0.9130
epoch 14301, loss 0.2840, train acc 91.41%, f1 0.9141, precision 0.9140, recall 0.9142, auc 0.9141
epoch 14401, loss 0.1942, train acc 91.44%, f1 0.9144, precision 0.9143, recall 0.9146, auc 0.9144
epoch 14501, loss 0.1948, train acc 91.47%, f1 0.9148, precision 0.9143, recall 0.9152, auc 0.9147
epoch 14601, loss 0.2562, train acc 91.45%, f1 0.9144, precision 0.9152, recall 0.9136, auc 0.9145
epoch 14701, loss 0.2073, train acc 91.60%, f1 0.9163, precision 0.9135, recall 0.9190, auc 0.9160
epoch 14801, loss 0.2006, train acc 91.66%, f1 0.9167, precision 0.9157, recall 0.9176, auc 0.9166
epoch 14901, loss 0.2880, train acc 91.75%, f1 0.9175, precision 0.9171, recall 0.9179, auc 0.9175
epoch 15001, loss 0.1849, train acc 91.79%, f1 0.9181, precision 0.9164, recall 0.9197, auc 0.9179
epoch 15101, loss 0.2623, train acc 91.81%, f1 0.9183, precision 0.9166, recall 0.9200, auc 0.9181
epoch 15201, loss 0.1757, train acc 91.83%, f1 0.9185, precision 0.9157, recall 0.9213, auc 0.9183
epoch 15301, loss 0.1370, train acc 91.91%, f1 0.9194, precision 0.9161, recall 0.9227, auc 0.9191
epoch 15401, loss 0.2064, train acc 91.95%, f1 0.9198, precision 0.9163, recall 0.9232, auc 0.9195
epoch 15501, loss 0.1554, train acc 91.97%, f1 0.9199, precision 0.9175, recall 0.9224, auc 0.9197
epoch 15601, loss 0.1775, train acc 92.02%, f1 0.9204, precision 0.9182, recall 0.9227, auc 0.9202
epoch 15701, loss 0.2056, train acc 92.04%, f1 0.9206, precision 0.9191, recall 0.9221, auc 0.9204
epoch 15801, loss 0.2050, train acc 92.08%, f1 0.9208, precision 0.9202, recall 0.9214, auc 0.9208
epoch 15901, loss 0.1952, train acc 92.09%, f1 0.9211, precision 0.9183, recall 0.9239, auc 0.9209
epoch 16001, loss 0.2011, train acc 92.14%, f1 0.9216, precision 0.9194, recall 0.9239, auc 0.9214
epoch 16101, loss 0.1949, train acc 92.24%, f1 0.9225, precision 0.9215, recall 0.9236, auc 0.9224
epoch 16201, loss 0.2196, train acc 92.24%, f1 0.9225, precision 0.9213, recall 0.9237, auc 0.9224
epoch 16301, loss 0.2374, train acc 92.29%, f1 0.9230, precision 0.9217, recall 0.9244, auc 0.9229
epoch 16401, loss 0.1331, train acc 92.31%, f1 0.9232, precision 0.9217, recall 0.9247, auc 0.9231
epoch 16501, loss 0.2291, train acc 92.33%, f1 0.9234, precision 0.9222, recall 0.9247, auc 0.9233/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.2181, train acc 92.37%, f1 0.9239, precision 0.9217, recall 0.9261, auc 0.9237
epoch 16701, loss 0.3003, train acc 92.44%, f1 0.9245, precision 0.9239, recall 0.9251, auc 0.9244
epoch 16801, loss 0.2380, train acc 92.46%, f1 0.9248, precision 0.9228, recall 0.9268, auc 0.9246
epoch 16901, loss 0.1653, train acc 92.51%, f1 0.9251, precision 0.9248, recall 0.9255, auc 0.9251
epoch 17001, loss 0.1559, train acc 92.53%, f1 0.9255, precision 0.9230, recall 0.9280, auc 0.9253
epoch 17101, loss 0.2096, train acc 92.55%, f1 0.9257, precision 0.9230, recall 0.9284, auc 0.9255
epoch 17201, loss 0.1835, train acc 92.61%, f1 0.9263, precision 0.9239, recall 0.9287, auc 0.9261
epoch 17301, loss 0.1616, train acc 92.64%, f1 0.9265, precision 0.9252, recall 0.9278, auc 0.9264
epoch 17401, loss 0.1821, train acc 92.60%, f1 0.9259, precision 0.9278, recall 0.9239, auc 0.9260
epoch 17501, loss 0.1700, train acc 92.67%, f1 0.9270, precision 0.9235, recall 0.9304, auc 0.9267
epoch 17601, loss 0.1863, train acc 92.75%, f1 0.9274, precision 0.9285, recall 0.9264, auc 0.9275
epoch 17701, loss 0.1360, train acc 92.73%, f1 0.9274, precision 0.9254, recall 0.9295, auc 0.9273
epoch 17801, loss 0.1815, train acc 92.80%, f1 0.9281, precision 0.9261, recall 0.9302, auc 0.9280
epoch 17901, loss 0.2009, train acc 92.82%, f1 0.9282, precision 0.9282, recall 0.9281, auc 0.9282
epoch 18001, loss 0.1553, train acc 92.83%, f1 0.9285, precision 0.9260, recall 0.9310, auc 0.9283
epoch 18101, loss 0.2130, train acc 92.88%, f1 0.9290, precision 0.9267, recall 0.9312, auc 0.9288
epoch 18201, loss 0.1643, train acc 92.90%, f1 0.9291, precision 0.9277, recall 0.9305, auc 0.9290
epoch 18301, loss 0.1839, train acc 92.94%, f1 0.9296, precision 0.9275, recall 0.9317, auc 0.9294
epoch 18401, loss 0.1326, train acc 92.98%, f1 0.9300, precision 0.9272, recall 0.9329, auc 0.9298
epoch 18501, loss 0.2556, train acc 92.98%, f1 0.9299, precision 0.9290, recall 0.9308, auc 0.9298
epoch 18601, loss 0.1629, train acc 93.02%, f1 0.9303, precision 0.9291, recall 0.9314, auc 0.9302
epoch 18701, loss 0.1310, train acc 93.01%, f1 0.9303, precision 0.9282, recall 0.9324, auc 0.9301
epoch 18801, loss 0.2007, train acc 93.06%, f1 0.9307, precision 0.9290, recall 0.9325, auc 0.9306
epoch 18901, loss 0.2756, train acc 93.09%, f1 0.9310, precision 0.9302, recall 0.9318, auc 0.9309
epoch 19001, loss 0.2122, train acc 93.15%, f1 0.9316, precision 0.9305, recall 0.9327, auc 0.9315
epoch 19101, loss 0.2460, train acc 93.15%, f1 0.9315, precision 0.9320, recall 0.9310, auc 0.9315
epoch 19201, loss 0.2411, train acc 93.16%, f1 0.9318, precision 0.9293, recall 0.9343, auc 0.9316
epoch 19301, loss 0.1579, train acc 93.20%, f1 0.9320, precision 0.9321, recall 0.9318, auc 0.9320
epoch 19401, loss 0.1855, train acc 93.21%, f1 0.9322, precision 0.9313, recall 0.9331, auc 0.9321
epoch 19501, loss 0.2129, train acc 93.30%, f1 0.9329, precision 0.9345, recall 0.9313, auc 0.9330
epoch 19601, loss 0.1680, train acc 93.35%, f1 0.9334, precision 0.9343, recall 0.9326, auc 0.9335
epoch 19701, loss 0.1925, train acc 93.36%, f1 0.9336, precision 0.9333, recall 0.9339, auc 0.9336
epoch 19801, loss 0.1126, train acc 93.41%, f1 0.9341, precision 0.9340, recall 0.9342, auc 0.9341
epoch 19901, loss 0.1184, train acc 93.37%, f1 0.9337, precision 0.9334, recall 0.9339, auc 0.9337
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_1
./test_pima/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.7194444444444444

the Fscore is 0.6530612244897959

the precision is 0.5161290322580645

the recall is 0.8888888888888888

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_1
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5830, train acc 78.51%, f1 0.7847, precision 0.7863, recall 0.7831, auc 0.7851
epoch 201, loss 0.3754, train acc 81.41%, f1 0.8141, precision 0.8142, recall 0.8141, auc 0.8141
epoch 301, loss 0.4137, train acc 82.82%, f1 0.8283, precision 0.8280, recall 0.8285, auc 0.8282
epoch 401, loss 0.4462, train acc 83.23%, f1 0.8323, precision 0.8321, recall 0.8325, auc 0.8323
epoch 501, loss 0.3688, train acc 83.37%, f1 0.8338, precision 0.8331, recall 0.8344, auc 0.8337
epoch 601, loss 0.2223, train acc 83.36%, f1 0.8336, precision 0.8334, recall 0.8338, auc 0.8336
epoch 701, loss 0.5129, train acc 83.36%, f1 0.8337, precision 0.8332, recall 0.8341, auc 0.8336
epoch 801, loss 0.3696, train acc 83.45%, f1 0.8345, precision 0.8343, recall 0.8347, auc 0.8345
epoch 901, loss 0.4481, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8349, auc 0.8347
epoch 1001, loss 0.5028, train acc 83.45%, f1 0.8346, precision 0.8343, recall 0.8349, auc 0.8345
epoch 1101, loss 0.4491, train acc 83.43%, f1 0.8344, precision 0.8340, recall 0.8348, auc 0.8343
epoch 1201, loss 0.2760, train acc 83.46%, f1 0.8347, precision 0.8344, recall 0.8351, auc 0.8346
epoch 1301, loss 0.2657, train acc 83.45%, f1 0.8346, precision 0.8344, recall 0.8348, auc 0.8345
epoch 1401, loss 0.4666, train acc 83.39%, f1 0.8340, precision 0.8336, recall 0.8343, auc 0.8339
epoch 1501, loss 0.4298, train acc 83.42%, f1 0.8343, precision 0.8339, recall 0.8347, auc 0.8342
epoch 1601, loss 0.4312, train acc 83.50%, f1 0.8351, precision 0.8345, recall 0.8357, auc 0.8350
epoch 1701, loss 0.3017, train acc 83.51%, f1 0.8352, precision 0.8345, recall 0.8359, auc 0.8351
epoch 1801, loss 0.3548, train acc 83.49%, f1 0.8350, precision 0.8344, recall 0.8356, auc 0.8349
epoch 1901, loss 0.5312, train acc 83.46%, f1 0.8347, precision 0.8340, recall 0.8354, auc 0.8346
epoch 2001, loss 0.3488, train acc 83.50%, f1 0.8352, precision 0.8344, recall 0.8359, auc 0.8350
epoch 2101, loss 0.2741, train acc 83.60%, f1 0.8361, precision 0.8356, recall 0.8366, auc 0.8360
epoch 2201, loss 0.3956, train acc 83.61%, f1 0.8363, precision 0.8354, recall 0.8371, auc 0.8361
epoch 2301, loss 0.3643, train acc 83.71%, f1 0.8372, precision 0.8368, recall 0.8377, auc 0.8371
epoch 2401, loss 0.3266, train acc 83.73%, f1 0.8373, precision 0.8370, recall 0.8377, auc 0.8373
epoch 2501, loss 0.3765, train acc 83.75%, f1 0.8377, precision 0.8370, recall 0.8384, auc 0.8375
epoch 2601, loss 0.2499, train acc 83.85%, f1 0.8386, precision 0.8382, recall 0.8391, auc 0.8385
epoch 2701, loss 0.3700, train acc 83.99%, f1 0.8401, precision 0.8393, recall 0.8409, auc 0.8399
epoch 2801, loss 0.3578, train acc 84.05%, f1 0.8406, precision 0.8402, recall 0.8409, auc 0.8405
epoch 2901, loss 0.3336, train acc 84.18%, f1 0.8418, precision 0.8414, recall 0.8422, auc 0.8418
epoch 3001, loss 0.3084, train acc 84.28%, f1 0.8429, precision 0.8425, recall 0.8433, auc 0.8428
epoch 3101, loss 0.4832, train acc 84.36%, f1 0.8437, precision 0.8430, recall 0.8444, auc 0.8436
epoch 3201, loss 0.3649, train acc 84.52%, f1 0.8453, precision 0.8448, recall 0.8458, auc 0.8452
epoch 3301, loss 0.3041, train acc 84.58%, f1 0.8459, precision 0.8454, recall 0.8465, auc 0.8458
epoch 3401, loss 0.3753, train acc 84.71%, f1 0.8472, precision 0.8468, recall 0.8477, auc 0.8471
epoch 3501, loss 0.3208, train acc 84.86%, f1 0.8487, precision 0.8480, recall 0.8493, auc 0.8486
epoch 3601, loss 0.4441, train acc 84.89%, f1 0.8491, precision 0.8482, recall 0.8499, auc 0.8489
epoch 3701, loss 0.2628, train acc 84.99%, f1 0.8498, precision 0.8500, recall 0.8496, auc 0.8499
epoch 3801, loss 0.3378, train acc 85.10%, f1 0.8511, precision 0.8503, recall 0.8519, auc 0.8510
epoch 3901, loss 0.3506, train acc 85.14%, f1 0.8514, precision 0.8511, recall 0.8518, auc 0.8514
epoch 4001, loss 0.3920, train acc 85.23%, f1 0.8525, precision 0.8515, recall 0.8534, auc 0.8523
epoch 4101, loss 0.2776, train acc 85.33%, f1 0.8534, precision 0.8527, recall 0.8540, auc 0.8533
epoch 4201, loss 0.3388, train acc 85.41%, f1 0.8541, precision 0.8544, recall 0.8537, auc 0.8541
epoch 4301, loss 0.2736, train acc 85.51%, f1 0.8551, precision 0.8552, recall 0.8550, auc 0.8551
epoch 4401, loss 0.3563, train acc 85.53%, f1 0.8554, precision 0.8548, recall 0.8561, auc 0.8553
epoch 4501, loss 0.2917, train acc 85.64%, f1 0.8564, precision 0.8562, recall 0.8566, auc 0.8564
epoch 4601, loss 0.2701, train acc 85.75%, f1 0.8575, precision 0.8577, recall 0.8572, auc 0.8575
epoch 4701, loss 0.3585, train acc 85.75%, f1 0.8575, precision 0.8575, recall 0.8575, auc 0.8575
epoch 4801, loss 0.2540, train acc 85.86%, f1 0.8588, precision 0.8577, recall 0.8599, auc 0.8586
epoch 4901, loss 0.3577, train acc 85.91%, f1 0.8591, precision 0.8590, recall 0.8592, auc 0.8591
epoch 5001, loss 0.4301, train acc 85.99%, f1 0.8600, precision 0.8595, recall 0.8606, auc 0.8599
epoch 5101, loss 0.2307, train acc 86.09%, f1 0.8609, precision 0.8610, recall 0.8608, auc 0.8609
epoch 5201, loss 0.3365, train acc 86.09%, f1 0.8610, precision 0.8605, recall 0.8615, auc 0.8609
epoch 5301, loss 0.3770, train acc 86.17%, f1 0.8617, precision 0.8617, recall 0.8618, auc 0.8617
epoch 5401, loss 0.3107, train acc 86.21%, f1 0.8622, precision 0.8615, recall 0.8628, auc 0.8621
epoch 5501, loss 0.2953, train acc 86.27%, f1 0.8628, precision 0.8623, recall 0.8632, auc 0.8627
epoch 5601, loss 0.3829, train acc 86.35%, f1 0.8636, precision 0.8627, recall 0.8645, auc 0.8635
epoch 5701, loss 0.2947, train acc 86.39%, f1 0.8639, precision 0.8638, recall 0.8641, auc 0.8639
epoch 5801, loss 0.2499, train acc 86.43%, f1 0.8643, precision 0.8642, recall 0.8643, auc 0.8643
epoch 5901, loss 0.3302, train acc 86.52%, f1 0.8653, precision 0.8645, recall 0.8661, auc 0.8652
epoch 6001, loss 0.2824, train acc 86.54%, f1 0.8654, precision 0.8656, recall 0.8652, auc 0.8654
epoch 6101, loss 0.3070, train acc 86.59%, f1 0.8661, precision 0.8652, recall 0.8670, auc 0.8659
epoch 6201, loss 0.2808, train acc 86.64%, f1 0.8665, precision 0.8659, recall 0.8671, auc 0.8664
epoch 6301, loss 0.2854, train acc 86.66%, f1 0.8665, precision 0.8667, recall 0.8664, auc 0.8666
epoch 6401, loss 0.2266, train acc 86.67%, f1 0.8667, precision 0.8668, recall 0.8667, auc 0.8667
epoch 6501, loss 0.2392, train acc 86.74%, f1 0.8674, precision 0.8673, recall 0.8676, auc 0.8674
epoch 6601, loss 0.2831, train acc 86.70%, f1 0.8670, precision 0.8670, recall 0.8670, auc 0.8670
epoch 6701, loss 0.2869, train acc 86.84%, f1 0.8686, precision 0.8675, recall 0.8696, auc 0.8684
epoch 6801, loss 0.2887, train acc 86.86%, f1 0.8686, precision 0.8686, recall 0.8686, auc 0.8686
epoch 6901, loss 0.3100, train acc 86.79%, f1 0.8679, precision 0.8680, recall 0.8678, auc 0.8679
epoch 7001, loss 0.2329, train acc 86.88%, f1 0.8690, precision 0.8679, recall 0.8700, auc 0.8688
epoch 7101, loss 0.2525, train acc 87.00%, f1 0.8702, precision 0.8693, recall 0.8710, auc 0.8700
epoch 7201, loss 0.1512, train acc 87.04%, f1 0.8706, precision 0.8692, recall 0.8719, auc 0.8704
epoch 7301, loss 0.2522, train acc 87.10%, f1 0.8712, precision 0.8703, recall 0.8721, auc 0.8710
epoch 7401, loss 0.3015, train acc 87.13%, f1 0.8715, precision 0.8706, recall 0.8723, auc 0.8713
epoch 7501, loss 0.3636, train acc 87.16%, f1 0.8717, precision 0.8711, recall 0.8722, auc 0.8716
epoch 7601, loss 0.2924, train acc 87.26%, f1 0.8726, precision 0.8725, recall 0.8727, auc 0.8726
epoch 7701, loss 0.3234, train acc 87.28%, f1 0.8730, precision 0.8720, recall 0.8739, auc 0.8728
epoch 7801, loss 0.3450, train acc 87.36%, f1 0.8736, precision 0.8731, recall 0.8742, auc 0.8736
epoch 7901, loss 0.2487, train acc 87.36%, f1 0.8737, precision 0.8732, recall 0.8741, auc 0.8736
epoch 8001, loss 0.2613, train acc 87.45%, f1 0.8746, precision 0.8739, recall 0.8753, auc 0.8745
epoch 8101, loss 0.2937, train acc 87.46%, f1 0.8749, precision 0.8731, recall 0.8767, auc 0.8746
epoch 8201, loss 0.2393, train acc 87.55%, f1 0.8758, precision 0.8738, recall 0.8779, auc 0.8755/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2123, train acc 87.57%, f1 0.8758, precision 0.8757, recall 0.8758, auc 0.8757
epoch 8401, loss 0.2411, train acc 87.67%, f1 0.8768, precision 0.8764, recall 0.8771, auc 0.8767
epoch 8501, loss 0.3183, train acc 87.71%, f1 0.8772, precision 0.8766, recall 0.8779, auc 0.8771
epoch 8601, loss 0.2674, train acc 87.83%, f1 0.8782, precision 0.8785, recall 0.8780, auc 0.8783
epoch 8701, loss 0.3024, train acc 87.81%, f1 0.8783, precision 0.8774, recall 0.8791, auc 0.8781
epoch 8801, loss 0.4494, train acc 87.89%, f1 0.8791, precision 0.8777, recall 0.8805, auc 0.8789
epoch 8901, loss 0.2005, train acc 87.92%, f1 0.8794, precision 0.8780, recall 0.8808, auc 0.8792
epoch 9001, loss 0.3114, train acc 88.01%, f1 0.8802, precision 0.8793, recall 0.8812, auc 0.8801
epoch 9101, loss 0.2655, train acc 88.08%, f1 0.8810, precision 0.8789, recall 0.8832, auc 0.8808
epoch 9201, loss 0.3325, train acc 88.15%, f1 0.8817, precision 0.8800, recall 0.8834, auc 0.8815
epoch 9301, loss 0.2431, train acc 88.21%, f1 0.8824, precision 0.8801, recall 0.8847, auc 0.8821
epoch 9401, loss 0.2748, train acc 88.24%, f1 0.8826, precision 0.8812, recall 0.8840, auc 0.8824
epoch 9501, loss 0.3948, train acc 88.32%, f1 0.8833, precision 0.8823, recall 0.8844, auc 0.8832
epoch 9601, loss 0.2940, train acc 88.38%, f1 0.8838, precision 0.8834, recall 0.8843, auc 0.8838
epoch 9701, loss 0.2723, train acc 88.44%, f1 0.8844, precision 0.8839, recall 0.8850, auc 0.8844
epoch 9801, loss 0.1626, train acc 88.56%, f1 0.8855, precision 0.8861, recall 0.8850, auc 0.8856
epoch 9901, loss 0.2698, train acc 88.64%, f1 0.8865, precision 0.8858, recall 0.8871, auc 0.8864
epoch 10001, loss 0.1824, train acc 88.63%, f1 0.8862, precision 0.8867, recall 0.8857, auc 0.8863
epoch 10101, loss 0.3414, train acc 88.77%, f1 0.8879, precision 0.8865, recall 0.8893, auc 0.8877
epoch 10201, loss 0.2888, train acc 88.80%, f1 0.8880, precision 0.8877, recall 0.8884, auc 0.8880
epoch 10301, loss 0.3101, train acc 88.88%, f1 0.8890, precision 0.8873, recall 0.8906, auc 0.8888
epoch 10401, loss 0.2490, train acc 88.87%, f1 0.8886, precision 0.8890, recall 0.8882, auc 0.8887
epoch 10501, loss 0.3284, train acc 88.92%, f1 0.8893, precision 0.8885, recall 0.8901, auc 0.8892
epoch 10601, loss 0.2713, train acc 89.06%, f1 0.8907, precision 0.8902, recall 0.8912, auc 0.8906
epoch 10701, loss 0.2899, train acc 89.15%, f1 0.8916, precision 0.8907, recall 0.8926, auc 0.8915
epoch 10801, loss 0.2161, train acc 89.17%, f1 0.8917, precision 0.8912, recall 0.8923, auc 0.8917
epoch 10901, loss 0.3330, train acc 89.30%, f1 0.8934, precision 0.8901, recall 0.8967, auc 0.8930
epoch 11001, loss 0.2360, train acc 89.38%, f1 0.8940, precision 0.8920, recall 0.8961, auc 0.8938
epoch 11101, loss 0.1971, train acc 89.42%, f1 0.8945, precision 0.8922, recall 0.8967, auc 0.8942
epoch 11201, loss 0.2482, train acc 89.48%, f1 0.8949, precision 0.8936, recall 0.8963, auc 0.8948
epoch 11301, loss 0.2283, train acc 89.43%, f1 0.8943, precision 0.8944, recall 0.8943, auc 0.8943
epoch 11401, loss 0.3026, train acc 89.62%, f1 0.8964, precision 0.8951, recall 0.8976, auc 0.8962
epoch 11501, loss 0.2403, train acc 89.62%, f1 0.8963, precision 0.8954, recall 0.8973, auc 0.8962
epoch 11601, loss 0.3547, train acc 89.73%, f1 0.8973, precision 0.8976, recall 0.8969, auc 0.8973
epoch 11701, loss 0.2483, train acc 89.76%, f1 0.8981, precision 0.8936, recall 0.9027, auc 0.8976
epoch 11801, loss 0.2806, train acc 89.80%, f1 0.8983, precision 0.8961, recall 0.9005, auc 0.8980
epoch 11901, loss 0.2051, train acc 89.83%, f1 0.8985, precision 0.8967, recall 0.9004, auc 0.8983
epoch 12001, loss 0.2146, train acc 89.91%, f1 0.8994, precision 0.8968, recall 0.9020, auc 0.8991
epoch 12101, loss 0.2036, train acc 90.03%, f1 0.9006, precision 0.8981, recall 0.9031, auc 0.9003
epoch 12201, loss 0.1570, train acc 90.14%, f1 0.9016, precision 0.8995, recall 0.9038, auc 0.9014
epoch 12301, loss 0.2738, train acc 90.23%, f1 0.9025, precision 0.9009, recall 0.9041, auc 0.9023
epoch 12401, loss 0.2636, train acc 90.32%, f1 0.9034, precision 0.9016, recall 0.9052, auc 0.9032
epoch 12501, loss 0.1569, train acc 90.32%, f1 0.9034, precision 0.9014, recall 0.9054, auc 0.9032
epoch 12601, loss 0.2417, train acc 90.40%, f1 0.9040, precision 0.9042, recall 0.9037, auc 0.9040
epoch 12701, loss 0.1989, train acc 90.44%, f1 0.9045, precision 0.9038, recall 0.9051, auc 0.9044
epoch 12801, loss 0.0899, train acc 90.38%, f1 0.9038, precision 0.9035, recall 0.9041, auc 0.9037
epoch 12901, loss 0.2424, train acc 90.53%, f1 0.9054, precision 0.9045, recall 0.9062, auc 0.9053
epoch 13001, loss 0.2070, train acc 90.63%, f1 0.9063, precision 0.9064, recall 0.9062, auc 0.9063
epoch 13101, loss 0.1931, train acc 90.63%, f1 0.9063, precision 0.9062, recall 0.9064, auc 0.9063
epoch 13201, loss 0.2351, train acc 90.76%, f1 0.9077, precision 0.9063, recall 0.9092, auc 0.9076
epoch 13301, loss 0.2711, train acc 90.80%, f1 0.9082, precision 0.9064, recall 0.9100, auc 0.9080
epoch 13401, loss 0.1496, train acc 90.90%, f1 0.9091, precision 0.9084, recall 0.9097, auc 0.9090
epoch 13501, loss 0.2422, train acc 90.90%, f1 0.9092, precision 0.9068, recall 0.9116, auc 0.9090
epoch 13601, loss 0.2227, train acc 90.97%, f1 0.9098, precision 0.9092, recall 0.9104, auc 0.9097
epoch 13701, loss 0.2381, train acc 91.02%, f1 0.9101, precision 0.9109, recall 0.9092, auc 0.9102
epoch 13801, loss 0.2070, train acc 91.11%, f1 0.9112, precision 0.9099, recall 0.9126, auc 0.9111
epoch 13901, loss 0.2456, train acc 91.12%, f1 0.9117, precision 0.9065, recall 0.9168, auc 0.9112
epoch 14001, loss 0.2471, train acc 91.18%, f1 0.9119, precision 0.9114, recall 0.9123, auc 0.9118
epoch 14101, loss 0.1821, train acc 91.12%, f1 0.9113, precision 0.9103, recall 0.9123, auc 0.9112
epoch 14201, loss 0.1656, train acc 91.31%, f1 0.9132, precision 0.9126, recall 0.9137, auc 0.9131
epoch 14301, loss 0.1820, train acc 91.29%, f1 0.9132, precision 0.9106, recall 0.9158, auc 0.9129
epoch 14401, loss 0.2012, train acc 91.38%, f1 0.9138, precision 0.9144, recall 0.9132, auc 0.9138
epoch 14501, loss 0.2056, train acc 91.39%, f1 0.9140, precision 0.9136, recall 0.9144, auc 0.9139
epoch 14601, loss 0.2203, train acc 91.46%, f1 0.9147, precision 0.9134, recall 0.9161, auc 0.9146
epoch 14701, loss 0.2426, train acc 91.49%, f1 0.9150, precision 0.9143, recall 0.9157, auc 0.9149
epoch 14801, loss 0.1966, train acc 91.54%, f1 0.9157, precision 0.9128, recall 0.9187, auc 0.9154
epoch 14901, loss 0.2115, train acc 91.65%, f1 0.9165, precision 0.9164, recall 0.9166, auc 0.9165
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_1
./test_pima/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.722962962962963

the Fscore is 0.6578947368421053

the precision is 0.5102040816326531

the recall is 0.9259259259259259

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_1
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6045, train acc 79.04%, f1 0.7879, precision 0.7972, recall 0.7789, auc 0.7904
epoch 201, loss 0.3010, train acc 81.61%, f1 0.8164, precision 0.8153, recall 0.8174, auc 0.8161
epoch 301, loss 0.3016, train acc 82.83%, f1 0.8287, precision 0.8268, recall 0.8305, auc 0.8283
epoch 401, loss 0.3990, train acc 83.20%, f1 0.8326, precision 0.8297, recall 0.8355, auc 0.8320
epoch 501, loss 0.3933, train acc 83.42%, f1 0.8346, precision 0.8327, recall 0.8364, auc 0.8342
epoch 601, loss 0.2895, train acc 83.48%, f1 0.8352, precision 0.8334, recall 0.8369, auc 0.8348
epoch 701, loss 0.2842, train acc 83.40%, f1 0.8344, precision 0.8325, recall 0.8363, auc 0.8340
epoch 801, loss 0.3637, train acc 83.44%, f1 0.8347, precision 0.8333, recall 0.8361, auc 0.8344
epoch 901, loss 0.4243, train acc 83.39%, f1 0.8342, precision 0.8325, recall 0.8359, auc 0.8339
epoch 1001, loss 0.3913, train acc 83.45%, f1 0.8348, precision 0.8334, recall 0.8362, auc 0.8345
epoch 1101, loss 0.3637, train acc 83.46%, f1 0.8347, precision 0.8342, recall 0.8351, auc 0.8346
epoch 1201, loss 0.3325, train acc 83.47%, f1 0.8349, precision 0.8340, recall 0.8359, auc 0.8347
epoch 1301, loss 0.3084, train acc 83.49%, f1 0.8350, precision 0.8346, recall 0.8354, auc 0.8349
epoch 1401, loss 0.5287, train acc 83.49%, f1 0.8349, precision 0.8346, recall 0.8353, auc 0.8349
epoch 1501, loss 0.3612, train acc 83.50%, f1 0.8351, precision 0.8349, recall 0.8353, auc 0.8350
epoch 1601, loss 0.3902, train acc 83.47%, f1 0.8348, precision 0.8343, recall 0.8354, auc 0.8347
epoch 1701, loss 0.4332, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8349, auc 0.8347
epoch 1801, loss 0.3012, train acc 83.48%, f1 0.8348, precision 0.8347, recall 0.8349, auc 0.8348
epoch 1901, loss 0.2616, train acc 83.52%, f1 0.8353, precision 0.8348, recall 0.8358, auc 0.8352
epoch 2001, loss 0.4000, train acc 83.51%, f1 0.8351, precision 0.8347, recall 0.8356, auc 0.8351
epoch 2101, loss 0.3470, train acc 83.49%, f1 0.8349, precision 0.8351, recall 0.8347, auc 0.8349
epoch 2201, loss 0.3512, train acc 83.55%, f1 0.8355, precision 0.8353, recall 0.8358, auc 0.8355
epoch 2301, loss 0.3272, train acc 83.61%, f1 0.8360, precision 0.8364, recall 0.8356, auc 0.8361
epoch 2401, loss 0.3835, train acc 83.71%, f1 0.8372, precision 0.8370, recall 0.8373, auc 0.8371
epoch 2501, loss 0.4063, train acc 83.75%, f1 0.8374, precision 0.8375, recall 0.8374, auc 0.8375
epoch 2601, loss 0.4118, train acc 83.84%, f1 0.8386, precision 0.8375, recall 0.8397, auc 0.8384
epoch 2701, loss 0.4569, train acc 83.86%, f1 0.8385, precision 0.8387, recall 0.8384, auc 0.8386
epoch 2801, loss 0.3837, train acc 83.93%, f1 0.8392, precision 0.8396, recall 0.8388, auc 0.8393
epoch 2901, loss 0.4274, train acc 84.00%, f1 0.8398, precision 0.8405, recall 0.8392, auc 0.8400
epoch 3001, loss 0.3452, train acc 84.16%, f1 0.8415, precision 0.8417, recall 0.8413, auc 0.8416
epoch 3101, loss 0.3728, train acc 84.29%, f1 0.8429, precision 0.8427, recall 0.8432, auc 0.8429
epoch 3201, loss 0.3417, train acc 84.39%, f1 0.8439, precision 0.8439, recall 0.8439, auc 0.8439
epoch 3301, loss 0.2769, train acc 84.49%, f1 0.8448, precision 0.8457, recall 0.8438, auc 0.8449
epoch 3401, loss 0.4021, train acc 84.55%, f1 0.8456, precision 0.8450, recall 0.8462, auc 0.8455
epoch 3501, loss 0.3502, train acc 84.63%, f1 0.8465, precision 0.8456, recall 0.8474, auc 0.8463
epoch 3601, loss 0.2833, train acc 84.77%, f1 0.8477, precision 0.8479, recall 0.8474, auc 0.8477
epoch 3701, loss 0.2585, train acc 84.86%, f1 0.8486, precision 0.8483, recall 0.8490, auc 0.8486
epoch 3801, loss 0.3929, train acc 84.99%, f1 0.8498, precision 0.8501, recall 0.8496, auc 0.8499
epoch 3901, loss 0.2804, train acc 85.09%, f1 0.8507, precision 0.8518, recall 0.8497, auc 0.8509
epoch 4001, loss 0.3023, train acc 85.12%, f1 0.8512, precision 0.8513, recall 0.8511, auc 0.8512
epoch 4101, loss 0.4303, train acc 85.18%, f1 0.8519, precision 0.8512, recall 0.8527, auc 0.8518
epoch 4201, loss 0.3336, train acc 85.21%, f1 0.8521, precision 0.8519, recall 0.8524, auc 0.8521
epoch 4301, loss 0.2778, train acc 85.34%, f1 0.8535, precision 0.8527, recall 0.8543, auc 0.8534
epoch 4401, loss 0.2935, train acc 85.42%, f1 0.8543, precision 0.8534, recall 0.8553, auc 0.8542
epoch 4501, loss 0.4050, train acc 85.55%, f1 0.8556, precision 0.8551, recall 0.8561, auc 0.8555
epoch 4601, loss 0.2869, train acc 85.62%, f1 0.8564, precision 0.8553, recall 0.8574, auc 0.8562
epoch 4701, loss 0.3099, train acc 85.73%, f1 0.8574, precision 0.8571, recall 0.8577, auc 0.8573
epoch 4801, loss 0.2427, train acc 85.79%, f1 0.8579, precision 0.8581, recall 0.8577, auc 0.8579
epoch 4901, loss 0.2854, train acc 85.87%, f1 0.8586, precision 0.8589, recall 0.8583, auc 0.8587
epoch 5001, loss 0.3889, train acc 85.94%, f1 0.8594, precision 0.8592, recall 0.8596, auc 0.8594
epoch 5101, loss 0.3631, train acc 86.00%, f1 0.8600, precision 0.8598, recall 0.8603, auc 0.8600
epoch 5201, loss 0.2277, train acc 86.08%, f1 0.8608, precision 0.8607, recall 0.8609, auc 0.8608
epoch 5301, loss 0.3698, train acc 86.16%, f1 0.8616, precision 0.8618, recall 0.8613, auc 0.8616
epoch 5401, loss 0.3116, train acc 86.25%, f1 0.8625, precision 0.8624, recall 0.8627, auc 0.8625
epoch 5501, loss 0.2476, train acc 86.30%, f1 0.8630, precision 0.8631, recall 0.8628, auc 0.8630
epoch 5601, loss 0.3147, train acc 86.33%, f1 0.8633, precision 0.8629, recall 0.8638, auc 0.8633
epoch 5701, loss 0.2828, train acc 86.38%, f1 0.8639, precision 0.8630, recall 0.8648, auc 0.8638
epoch 5801, loss 0.4422, train acc 86.45%, f1 0.8645, precision 0.8644, recall 0.8646, auc 0.8645
epoch 5901, loss 0.2916, train acc 86.48%, f1 0.8648, precision 0.8647, recall 0.8649, auc 0.8648
epoch 6001, loss 0.3716, train acc 86.55%, f1 0.8655, precision 0.8658, recall 0.8652, auc 0.8655
epoch 6101, loss 0.3049, train acc 86.62%, f1 0.8661, precision 0.8669, recall 0.8652, auc 0.8662
epoch 6201, loss 0.3585, train acc 86.64%, f1 0.8663, precision 0.8667, recall 0.8660, auc 0.8664
epoch 6301, loss 0.2823, train acc 86.70%, f1 0.8670, precision 0.8673, recall 0.8666, auc 0.8670
epoch 6401, loss 0.3242, train acc 86.72%, f1 0.8672, precision 0.8673, recall 0.8671, auc 0.8672
epoch 6501, loss 0.2579, train acc 86.80%, f1 0.8680, precision 0.8681, recall 0.8679, auc 0.8680
epoch 6601, loss 0.2861, train acc 86.85%, f1 0.8684, precision 0.8690, recall 0.8677, auc 0.8685
epoch 6701, loss 0.2841, train acc 86.86%, f1 0.8685, precision 0.8692, recall 0.8678, auc 0.8686
epoch 6801, loss 0.2593, train acc 86.96%, f1 0.8695, precision 0.8701, recall 0.8689, auc 0.8696
epoch 6901, loss 0.2896, train acc 87.02%, f1 0.8703, precision 0.8695, recall 0.8711, auc 0.8702
epoch 7001, loss 0.2672, train acc 87.06%, f1 0.8705, precision 0.8711, recall 0.8698, auc 0.8706
epoch 7101, loss 0.2594, train acc 87.09%, f1 0.8709, precision 0.8709, recall 0.8709, auc 0.8709
epoch 7201, loss 0.3429, train acc 87.18%, f1 0.8717, precision 0.8725, recall 0.8709, auc 0.8718
epoch 7301, loss 0.3418, train acc 87.19%, f1 0.8719, precision 0.8720, recall 0.8718, auc 0.8719
epoch 7401, loss 0.3073, train acc 87.28%, f1 0.8727, precision 0.8734, recall 0.8720, auc 0.8728
epoch 7501, loss 0.4013, train acc 87.33%, f1 0.8732, precision 0.8736, recall 0.8729, auc 0.8733
epoch 7601, loss 0.2082, train acc 87.41%, f1 0.8740, precision 0.8749, recall 0.8730, auc 0.8741
epoch 7701, loss 0.1913, train acc 87.44%, f1 0.8744, precision 0.8744, recall 0.8745, auc 0.8744
epoch 7801, loss 0.2908, train acc 87.51%, f1 0.8751, precision 0.8749, recall 0.8753, auc 0.8751
epoch 7901, loss 0.2600, train acc 87.53%, f1 0.8753, precision 0.8757, recall 0.8748, auc 0.8753
epoch 8001, loss 0.2976, train acc 87.55%, f1 0.8756, precision 0.8754, recall 0.8757, auc 0.8755
epoch 8101, loss 0.3148, train acc 87.68%, f1 0.8768, precision 0.8770, recall 0.8767, auc 0.8768
epoch 8201, loss 0.1903, train acc 87.70%, f1 0.8769, precision 0.8772, recall 0.8767, auc 0.8770/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2021, train acc 87.80%, f1 0.8781, precision 0.8776, recall 0.8785, auc 0.8780
epoch 8401, loss 0.2550, train acc 87.84%, f1 0.8782, precision 0.8799, recall 0.8765, auc 0.8784
epoch 8501, loss 0.3634, train acc 87.91%, f1 0.8791, precision 0.8785, recall 0.8798, auc 0.8791
epoch 8601, loss 0.2457, train acc 88.01%, f1 0.8802, precision 0.8796, recall 0.8807, auc 0.8801
epoch 8701, loss 0.2854, train acc 88.11%, f1 0.8810, precision 0.8815, recall 0.8805, auc 0.8811
epoch 8801, loss 0.2233, train acc 88.14%, f1 0.8813, precision 0.8821, recall 0.8804, auc 0.8814
epoch 8901, loss 0.2541, train acc 88.23%, f1 0.8825, precision 0.8813, recall 0.8836, auc 0.8823
epoch 9001, loss 0.2757, train acc 88.31%, f1 0.8831, precision 0.8830, recall 0.8832, auc 0.8831
epoch 9101, loss 0.2162, train acc 88.36%, f1 0.8836, precision 0.8835, recall 0.8838, auc 0.8836
epoch 9201, loss 0.3002, train acc 88.43%, f1 0.8841, precision 0.8851, recall 0.8831, auc 0.8843
epoch 9301, loss 0.2437, train acc 88.42%, f1 0.8843, precision 0.8836, recall 0.8850, auc 0.8842
epoch 9401, loss 0.2417, train acc 88.60%, f1 0.8858, precision 0.8872, recall 0.8844, auc 0.8860
epoch 9501, loss 0.2931, train acc 88.68%, f1 0.8869, precision 0.8858, recall 0.8881, auc 0.8868
epoch 9601, loss 0.1702, train acc 88.70%, f1 0.8869, precision 0.8871, recall 0.8868, auc 0.8870
epoch 9701, loss 0.1496, train acc 88.75%, f1 0.8874, precision 0.8886, recall 0.8861, auc 0.8875
epoch 9801, loss 0.2004, train acc 88.85%, f1 0.8884, precision 0.8896, recall 0.8872, auc 0.8885
epoch 9901, loss 0.2345, train acc 88.98%, f1 0.8897, precision 0.8906, recall 0.8888, auc 0.8898
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_1
./test_pima/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.712962962962963

the Fscore is 0.6493506493506493

the precision is 0.5

the recall is 0.9259259259259259

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_1
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5374, train acc 79.07%, f1 0.7899, precision 0.7930, recall 0.7868, auc 0.7907
epoch 201, loss 0.3910, train acc 81.46%, f1 0.8145, precision 0.8148, recall 0.8142, auc 0.8146
epoch 301, loss 0.4582, train acc 82.88%, f1 0.8289, precision 0.8282, recall 0.8297, auc 0.8288
epoch 401, loss 0.4112, train acc 83.22%, f1 0.8323, precision 0.8319, recall 0.8326, auc 0.8322
epoch 501, loss 0.3559, train acc 83.33%, f1 0.8335, precision 0.8325, recall 0.8344, auc 0.8333
epoch 601, loss 0.3668, train acc 83.43%, f1 0.8345, precision 0.8338, recall 0.8351, auc 0.8343
epoch 701, loss 0.4557, train acc 83.49%, f1 0.8350, precision 0.8346, recall 0.8353, auc 0.8349
epoch 801, loss 0.3054, train acc 83.46%, f1 0.8347, precision 0.8343, recall 0.8350, auc 0.8346
epoch 901, loss 0.3586, train acc 83.47%, f1 0.8348, precision 0.8344, recall 0.8352, auc 0.8347
epoch 1001, loss 0.4364, train acc 83.45%, f1 0.8345, precision 0.8344, recall 0.8347, auc 0.8345
epoch 1101, loss 0.3074, train acc 83.48%, f1 0.8348, precision 0.8346, recall 0.8350, auc 0.8348
epoch 1201, loss 0.3594, train acc 83.50%, f1 0.8350, precision 0.8351, recall 0.8350, auc 0.8350
epoch 1301, loss 0.3411, train acc 83.48%, f1 0.8348, precision 0.8346, recall 0.8350, auc 0.8348
epoch 1401, loss 0.3271, train acc 83.41%, f1 0.8341, precision 0.8341, recall 0.8340, auc 0.8341
epoch 1501, loss 0.3426, train acc 83.42%, f1 0.8342, precision 0.8341, recall 0.8342, auc 0.8342
epoch 1601, loss 0.4568, train acc 83.39%, f1 0.8339, precision 0.8337, recall 0.8342, auc 0.8339
epoch 1701, loss 0.3553, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8347, auc 0.8347
epoch 1801, loss 0.3436, train acc 83.47%, f1 0.8348, precision 0.8345, recall 0.8350, auc 0.8347
epoch 1901, loss 0.4609, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8349, auc 0.8349
epoch 2001, loss 0.3909, train acc 83.45%, f1 0.8345, precision 0.8342, recall 0.8348, auc 0.8345
epoch 2101, loss 0.3869, train acc 83.53%, f1 0.8354, precision 0.8352, recall 0.8355, auc 0.8353
epoch 2201, loss 0.4652, train acc 83.54%, f1 0.8355, precision 0.8350, recall 0.8360, auc 0.8354
epoch 2301, loss 0.3321, train acc 83.56%, f1 0.8356, precision 0.8356, recall 0.8357, auc 0.8356
epoch 2401, loss 0.3497, train acc 83.64%, f1 0.8364, precision 0.8361, recall 0.8367, auc 0.8364
epoch 2501, loss 0.5052, train acc 83.65%, f1 0.8364, precision 0.8367, recall 0.8362, auc 0.8365
epoch 2601, loss 0.3978, train acc 83.77%, f1 0.8377, precision 0.8377, recall 0.8377, auc 0.8377
epoch 2701, loss 0.4379, train acc 83.85%, f1 0.8385, precision 0.8383, recall 0.8388, auc 0.8385
epoch 2801, loss 0.3440, train acc 83.96%, f1 0.8397, precision 0.8395, recall 0.8398, auc 0.8396
epoch 2901, loss 0.3711, train acc 83.95%, f1 0.8396, precision 0.8390, recall 0.8403, auc 0.8395
epoch 3001, loss 0.4087, train acc 84.17%, f1 0.8417, precision 0.8414, recall 0.8420, auc 0.8417
epoch 3101, loss 0.2317, train acc 84.27%, f1 0.8427, precision 0.8427, recall 0.8428, auc 0.8427
epoch 3201, loss 0.3993, train acc 84.41%, f1 0.8442, precision 0.8441, recall 0.8442, auc 0.8441
epoch 3301, loss 0.3121, train acc 84.52%, f1 0.8452, precision 0.8450, recall 0.8455, auc 0.8452
epoch 3401, loss 0.3335, train acc 84.64%, f1 0.8464, precision 0.8465, recall 0.8463, auc 0.8464
epoch 3501, loss 0.3054, train acc 84.75%, f1 0.8473, precision 0.8483, recall 0.8464, auc 0.8475
epoch 3601, loss 0.4612, train acc 84.87%, f1 0.8488, precision 0.8484, recall 0.8491, auc 0.8487
epoch 3701, loss 0.2902, train acc 84.92%, f1 0.8493, precision 0.8490, recall 0.8495, auc 0.8492
epoch 3801, loss 0.4969, train acc 85.04%, f1 0.8504, precision 0.8504, recall 0.8504, auc 0.8504
epoch 3901, loss 0.4329, train acc 85.11%, f1 0.8511, precision 0.8513, recall 0.8509, auc 0.8511
epoch 4001, loss 0.3978, train acc 85.26%, f1 0.8525, precision 0.8529, recall 0.8521, auc 0.8526
epoch 4101, loss 0.2989, train acc 85.29%, f1 0.8529, precision 0.8532, recall 0.8525, auc 0.8529
epoch 4201, loss 0.3463, train acc 85.38%, f1 0.8537, precision 0.8541, recall 0.8534, auc 0.8538
epoch 4301, loss 0.3697, train acc 85.50%, f1 0.8550, precision 0.8553, recall 0.8546, auc 0.8550
epoch 4401, loss 0.3532, train acc 85.59%, f1 0.8559, precision 0.8561, recall 0.8557, auc 0.8559
epoch 4501, loss 0.2827, train acc 85.67%, f1 0.8569, precision 0.8555, recall 0.8582, auc 0.8567
epoch 4601, loss 0.2578, train acc 85.69%, f1 0.8570, precision 0.8566, recall 0.8574, auc 0.8569
epoch 4701, loss 0.3284, train acc 85.81%, f1 0.8583, precision 0.8571, recall 0.8594, auc 0.8581
epoch 4801, loss 0.4169, train acc 85.85%, f1 0.8585, precision 0.8585, recall 0.8585, auc 0.8585
epoch 4901, loss 0.3785, train acc 85.95%, f1 0.8596, precision 0.8591, recall 0.8602, auc 0.8595
epoch 5001, loss 0.2952, train acc 86.00%, f1 0.8600, precision 0.8601, recall 0.8599, auc 0.8600
epoch 5101, loss 0.4049, train acc 86.06%, f1 0.8606, precision 0.8605, recall 0.8608, auc 0.8606
epoch 5201, loss 0.2185, train acc 86.18%, f1 0.8619, precision 0.8615, recall 0.8623, auc 0.8618
epoch 5301, loss 0.3548, train acc 86.22%, f1 0.8621, precision 0.8628, recall 0.8615, auc 0.8622
epoch 5401, loss 0.3519, train acc 86.21%, f1 0.8621, precision 0.8623, recall 0.8619, auc 0.8621
epoch 5501, loss 0.3803, train acc 86.32%, f1 0.8632, precision 0.8632, recall 0.8631, auc 0.8632
epoch 5601, loss 0.1994, train acc 86.34%, f1 0.8636, precision 0.8626, recall 0.8646, auc 0.8634
epoch 5701, loss 0.4004, train acc 86.40%, f1 0.8640, precision 0.8637, recall 0.8643, auc 0.8640
epoch 5801, loss 0.3042, train acc 86.45%, f1 0.8646, precision 0.8640, recall 0.8653, auc 0.8645
epoch 5901, loss 0.2200, train acc 86.50%, f1 0.8649, precision 0.8653, recall 0.8646, auc 0.8650
epoch 6001, loss 0.3504, train acc 86.51%, f1 0.8650, precision 0.8652, recall 0.8649, auc 0.8651
epoch 6101, loss 0.2576, train acc 86.54%, f1 0.8653, precision 0.8658, recall 0.8649, auc 0.8654
epoch 6201, loss 0.2129, train acc 86.63%, f1 0.8663, precision 0.8661, recall 0.8666, auc 0.8663
epoch 6301, loss 0.2230, train acc 86.65%, f1 0.8665, precision 0.8666, recall 0.8665, auc 0.8665
epoch 6401, loss 0.2506, train acc 86.69%, f1 0.8669, precision 0.8665, recall 0.8673, auc 0.8669
epoch 6501, loss 0.2868, train acc 86.72%, f1 0.8673, precision 0.8667, recall 0.8679, auc 0.8672
epoch 6601, loss 0.2726, train acc 86.81%, f1 0.8681, precision 0.8683, recall 0.8678, auc 0.8681
epoch 6701, loss 0.3395, train acc 86.81%, f1 0.8681, precision 0.8682, recall 0.8679, auc 0.8681
epoch 6801, loss 0.2614, train acc 86.87%, f1 0.8687, precision 0.8689, recall 0.8684, auc 0.8687
epoch 6901, loss 0.3251, train acc 86.91%, f1 0.8691, precision 0.8687, recall 0.8695, auc 0.8691
epoch 7001, loss 0.2423, train acc 86.98%, f1 0.8698, precision 0.8698, recall 0.8698, auc 0.8698
epoch 7101, loss 0.2514, train acc 87.01%, f1 0.8701, precision 0.8700, recall 0.8701, auc 0.8701
epoch 7201, loss 0.2533, train acc 87.09%, f1 0.8710, precision 0.8703, recall 0.8718, auc 0.8709
epoch 7301, loss 0.2624, train acc 87.08%, f1 0.8708, precision 0.8708, recall 0.8709, auc 0.8708
epoch 7401, loss 0.1966, train acc 87.14%, f1 0.8715, precision 0.8711, recall 0.8719, auc 0.8714
epoch 7501, loss 0.2758, train acc 87.14%, f1 0.8713, precision 0.8720, recall 0.8706, auc 0.8714
epoch 7601, loss 0.2163, train acc 87.26%, f1 0.8727, precision 0.8721, recall 0.8733, auc 0.8726
epoch 7701, loss 0.3705, train acc 87.29%, f1 0.8730, precision 0.8723, recall 0.8737, auc 0.8729
epoch 7801, loss 0.2476, train acc 87.34%, f1 0.8733, precision 0.8739, recall 0.8727, auc 0.8734
epoch 7901, loss 0.2621, train acc 87.34%, f1 0.8734, precision 0.8729, recall 0.8739, auc 0.8734
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_1
./test_pima/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6722222222222223

the Fscore is 0.6181818181818182

the precision is 0.4594594594594595

the recall is 0.9444444444444444

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5521, train acc 79.01%, f1 0.7878, precision 0.7966, recall 0.7793, auc 0.7901
epoch 201, loss 0.4149, train acc 81.43%, f1 0.8147, precision 0.8133, recall 0.8160, auc 0.8143
epoch 301, loss 0.3555, train acc 82.77%, f1 0.8282, precision 0.8257, recall 0.8308, auc 0.8277
epoch 401, loss 0.3636, train acc 83.24%, f1 0.8329, precision 0.8306, recall 0.8352, auc 0.8324
epoch 501, loss 0.4201, train acc 83.48%, f1 0.8353, precision 0.8332, recall 0.8373, auc 0.8348
epoch 601, loss 0.5421, train acc 83.43%, f1 0.8348, precision 0.8319, recall 0.8378, auc 0.8343
epoch 701, loss 0.2886, train acc 83.40%, f1 0.8344, precision 0.8322, recall 0.8366, auc 0.8340
epoch 801, loss 0.2935, train acc 83.40%, f1 0.8343, precision 0.8328, recall 0.8357, auc 0.8340
epoch 901, loss 0.2989, train acc 83.41%, f1 0.8343, precision 0.8334, recall 0.8352, auc 0.8341
epoch 1001, loss 0.4720, train acc 83.38%, f1 0.8341, precision 0.8329, recall 0.8352, auc 0.8338
epoch 1101, loss 0.3991, train acc 83.47%, f1 0.8349, precision 0.8337, recall 0.8362, auc 0.8347
epoch 1201, loss 0.3603, train acc 83.41%, f1 0.8343, precision 0.8334, recall 0.8352, auc 0.8341
epoch 1301, loss 0.5491, train acc 83.34%, f1 0.8335, precision 0.8330, recall 0.8341, auc 0.8334
epoch 1401, loss 0.3985, train acc 83.40%, f1 0.8342, precision 0.8334, recall 0.8350, auc 0.8340
epoch 1501, loss 0.3727, train acc 83.40%, f1 0.8341, precision 0.8338, recall 0.8344, auc 0.8340
epoch 1601, loss 0.3246, train acc 83.38%, f1 0.8338, precision 0.8334, recall 0.8343, auc 0.8337
epoch 1701, loss 0.2890, train acc 83.40%, f1 0.8340, precision 0.8338, recall 0.8343, auc 0.8340
epoch 1801, loss 0.3416, train acc 83.42%, f1 0.8343, precision 0.8338, recall 0.8347, auc 0.8342
epoch 1901, loss 0.2375, train acc 83.50%, f1 0.8350, precision 0.8349, recall 0.8351, auc 0.8350
epoch 2001, loss 0.3448, train acc 83.50%, f1 0.8350, precision 0.8349, recall 0.8351, auc 0.8350
epoch 2101, loss 0.3868, train acc 83.47%, f1 0.8348, precision 0.8346, recall 0.8350, auc 0.8347
epoch 2201, loss 0.3034, train acc 83.60%, f1 0.8360, precision 0.8358, recall 0.8362, auc 0.8360
epoch 2301, loss 0.3429, train acc 83.60%, f1 0.8360, precision 0.8363, recall 0.8356, auc 0.8360
epoch 2401, loss 0.3125, train acc 83.60%, f1 0.8361, precision 0.8358, recall 0.8363, auc 0.8360
epoch 2501, loss 0.2404, train acc 83.72%, f1 0.8372, precision 0.8369, recall 0.8376, auc 0.8372
epoch 2601, loss 0.4124, train acc 83.73%, f1 0.8373, precision 0.8371, recall 0.8375, auc 0.8373
epoch 2701, loss 0.5102, train acc 83.81%, f1 0.8381, precision 0.8379, recall 0.8384, auc 0.8381
epoch 2801, loss 0.4299, train acc 83.89%, f1 0.8389, precision 0.8392, recall 0.8386, auc 0.8389
epoch 2901, loss 0.3462, train acc 84.07%, f1 0.8406, precision 0.8409, recall 0.8404, auc 0.8407
epoch 3001, loss 0.3258, train acc 84.18%, f1 0.8418, precision 0.8419, recall 0.8416, auc 0.8418
epoch 3101, loss 0.3693, train acc 84.31%, f1 0.8432, precision 0.8427, recall 0.8437, auc 0.8431
epoch 3201, loss 0.3954, train acc 84.41%, f1 0.8442, precision 0.8435, recall 0.8448, auc 0.8441
epoch 3301, loss 0.3914, train acc 84.55%, f1 0.8454, precision 0.8461, recall 0.8446, auc 0.8455
epoch 3401, loss 0.2619, train acc 84.69%, f1 0.8469, precision 0.8469, recall 0.8470, auc 0.8469
epoch 3501, loss 0.3665, train acc 84.74%, f1 0.8473, precision 0.8479, recall 0.8466, auc 0.8474
epoch 3601, loss 0.3761, train acc 84.89%, f1 0.8488, precision 0.8494, recall 0.8483, auc 0.8489
epoch 3701, loss 0.3957, train acc 84.96%, f1 0.8495, precision 0.8497, recall 0.8493, auc 0.8496
epoch 3801, loss 0.4201, train acc 85.01%, f1 0.8501, precision 0.8502, recall 0.8499, auc 0.8501
epoch 3901, loss 0.3914, train acc 85.12%, f1 0.8511, precision 0.8518, recall 0.8504, auc 0.8512
epoch 4001, loss 0.2932, train acc 85.13%, f1 0.8512, precision 0.8516, recall 0.8508, auc 0.8513
epoch 4101, loss 0.3345, train acc 85.23%, f1 0.8524, precision 0.8521, recall 0.8527, auc 0.8523
epoch 4201, loss 0.2694, train acc 85.35%, f1 0.8535, precision 0.8534, recall 0.8536, auc 0.8535
epoch 4301, loss 0.4211, train acc 85.45%, f1 0.8546, precision 0.8543, recall 0.8549, auc 0.8545
epoch 4401, loss 0.2857, train acc 85.58%, f1 0.8559, precision 0.8553, recall 0.8564, auc 0.8558
epoch 4501, loss 0.3924, train acc 85.62%, f1 0.8562, precision 0.8564, recall 0.8560, auc 0.8562
epoch 4601, loss 0.2879, train acc 85.71%, f1 0.8572, precision 0.8565, recall 0.8579, auc 0.8571
epoch 4701, loss 0.2639, train acc 85.74%, f1 0.8574, precision 0.8577, recall 0.8570, auc 0.8574
epoch 4801, loss 0.2878, train acc 85.87%, f1 0.8586, precision 0.8593, recall 0.8578, auc 0.8587
epoch 4901, loss 0.3236, train acc 85.90%, f1 0.8591, precision 0.8584, recall 0.8597, auc 0.8590
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_1
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6772222222222222

the Fscore is 0.6219512195121951

the precision is 0.4636363636363636

the recall is 0.9444444444444444

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_1
----------------------



epoch 1, loss 0.6931, train acc 55.76%, f1 0.3190, precision 0.6922, recall 0.2073, auc 0.5576
epoch 101, loss 0.5460, train acc 78.29%, f1 0.7828, precision 0.7831, recall 0.7825, auc 0.7829
epoch 201, loss 0.4752, train acc 81.70%, f1 0.8170, precision 0.8168, recall 0.8172, auc 0.8170
epoch 301, loss 0.4759, train acc 82.87%, f1 0.8287, precision 0.8287, recall 0.8287, auc 0.8287
epoch 401, loss 0.2915, train acc 83.20%, f1 0.8320, precision 0.8321, recall 0.8319, auc 0.8320
epoch 501, loss 0.4648, train acc 83.45%, f1 0.8345, precision 0.8345, recall 0.8345, auc 0.8345
epoch 601, loss 0.4331, train acc 83.39%, f1 0.8339, precision 0.8339, recall 0.8339, auc 0.8339
epoch 701, loss 0.3761, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 801, loss 0.2873, train acc 83.43%, f1 0.8343, precision 0.8342, recall 0.8344, auc 0.8343
epoch 901, loss 0.3426, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 1001, loss 0.4001, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8348, auc 0.8347
epoch 1101, loss 0.3985, train acc 83.39%, f1 0.8339, precision 0.8338, recall 0.8340, auc 0.8339
epoch 1201, loss 0.3010, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8348, auc 0.8347
epoch 1301, loss 0.3492, train acc 83.45%, f1 0.8345, precision 0.8344, recall 0.8346, auc 0.8345
epoch 1401, loss 0.4036, train acc 83.48%, f1 0.8348, precision 0.8347, recall 0.8349, auc 0.8348
epoch 1501, loss 0.3678, train acc 83.40%, f1 0.8341, precision 0.8339, recall 0.8343, auc 0.8340
epoch 1601, loss 0.3541, train acc 83.38%, f1 0.8339, precision 0.8337, recall 0.8341, auc 0.8338
epoch 1701, loss 0.3762, train acc 83.43%, f1 0.8343, precision 0.8341, recall 0.8345, auc 0.8343
epoch 1801, loss 0.3444, train acc 83.45%, f1 0.8346, precision 0.8343, recall 0.8348, auc 0.8345
epoch 1901, loss 0.3076, train acc 83.47%, f1 0.8348, precision 0.8344, recall 0.8352, auc 0.8347
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_1
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6157407407407407

the Fscore is 0.5824175824175825

the precision is 0.4140625

the recall is 0.9814814814814815

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_1
----------------------



epoch 1, loss 0.6929, train acc 50.08%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5615, train acc 78.54%, f1 0.7804, precision 0.7978, recall 0.7637, auc 0.7854
epoch 201, loss 0.3944, train acc 81.57%, f1 0.8156, precision 0.8148, recall 0.8164, auc 0.8157
epoch 301, loss 0.3995, train acc 82.73%, f1 0.8279, precision 0.8240, recall 0.8318, auc 0.8274
epoch 401, loss 0.3818, train acc 83.27%, f1 0.8329, precision 0.8304, recall 0.8354, auc 0.8327
epoch 501, loss 0.4240, train acc 83.41%, f1 0.8338, precision 0.8337, recall 0.8339, auc 0.8341
epoch 601, loss 0.3087, train acc 83.41%, f1 0.8349, precision 0.8294, recall 0.8405, auc 0.8341
epoch 701, loss 0.5167, train acc 83.44%, f1 0.8346, precision 0.8322, recall 0.8371, auc 0.8344
epoch 801, loss 0.2773, train acc 83.46%, f1 0.8351, precision 0.8314, recall 0.8388, auc 0.8346
epoch 901, loss 0.3931, train acc 83.46%, f1 0.8351, precision 0.8313, recall 0.8389, auc 0.8346
epoch 1001, loss 0.3932, train acc 83.41%, f1 0.8348, precision 0.8301, recall 0.8395, auc 0.8341
epoch 1101, loss 0.3503, train acc 83.46%, f1 0.8342, precision 0.8346, recall 0.8338, auc 0.8346
epoch 1201, loss 0.3492, train acc 83.47%, f1 0.8347, precision 0.8332, recall 0.8362, auc 0.8347
epoch 1301, loss 0.3244, train acc 83.48%, f1 0.8342, precision 0.8360, recall 0.8324, auc 0.8348
epoch 1401, loss 0.3473, train acc 83.48%, f1 0.8352, precision 0.8319, recall 0.8385, auc 0.8348
epoch 1501, loss 0.3536, train acc 83.44%, f1 0.8348, precision 0.8315, recall 0.8381, auc 0.8344
epoch 1601, loss 0.3269, train acc 83.45%, f1 0.8343, precision 0.8342, recall 0.8343, auc 0.8345
epoch 1701, loss 0.3740, train acc 83.44%, f1 0.8347, precision 0.8321, recall 0.8373, auc 0.8344
epoch 1801, loss 0.4548, train acc 83.40%, f1 0.8343, precision 0.8316, recall 0.8370, auc 0.8340
epoch 1901, loss 0.3844, train acc 83.41%, f1 0.8340, precision 0.8332, recall 0.8349, auc 0.8341
epoch 2001, loss 0.3728, train acc 83.43%, f1 0.8347, precision 0.8313, recall 0.8381, auc 0.8343
epoch 2101, loss 0.4393, train acc 83.44%, f1 0.8346, precision 0.8324, recall 0.8369, auc 0.8344
epoch 2201, loss 0.4366, train acc 83.46%, f1 0.8352, precision 0.8311, recall 0.8393, auc 0.8347
epoch 2301, loss 0.3300, train acc 83.49%, f1 0.8345, precision 0.8349, recall 0.8341, auc 0.8349
epoch 2401, loss 0.3616, train acc 83.55%, f1 0.8360, precision 0.8322, recall 0.8399, auc 0.8356
epoch 2501, loss 0.3641, train acc 83.51%, f1 0.8350, precision 0.8345, recall 0.8355, auc 0.8351
epoch 2601, loss 0.3032, train acc 83.46%, f1 0.8342, precision 0.8349, recall 0.8336, auc 0.8346
epoch 2701, loss 0.3163, train acc 83.44%, f1 0.8349, precision 0.8310, recall 0.8389, auc 0.8344
epoch 2801, loss 0.3781, train acc 83.59%, f1 0.8361, precision 0.8339, recall 0.8382, auc 0.8359
epoch 2901, loss 0.4003, train acc 83.71%, f1 0.8373, precision 0.8352, recall 0.8393, auc 0.8371
epoch 3001, loss 0.3428, train acc 83.80%, f1 0.8381, precision 0.8361, recall 0.8402, auc 0.8380
epoch 3101, loss 0.4612, train acc 83.84%, f1 0.8381, precision 0.8384, recall 0.8378, auc 0.8384
epoch 3201, loss 0.3636, train acc 83.91%, f1 0.8394, precision 0.8366, recall 0.8423, auc 0.8391
epoch 3301, loss 0.3678, train acc 83.96%, f1 0.8394, precision 0.8392, recall 0.8396, auc 0.8396
epoch 3401, loss 0.3665, train acc 84.06%, f1 0.8406, precision 0.8393, recall 0.8418, auc 0.8406
epoch 3501, loss 0.2736, train acc 84.21%, f1 0.8424, precision 0.8397, recall 0.8450, auc 0.8421
epoch 3601, loss 0.3257, train acc 84.34%, f1 0.8434, precision 0.8422, recall 0.8447, auc 0.8434
epoch 3701, loss 0.2643, train acc 84.41%, f1 0.8437, precision 0.8448, recall 0.8426, auc 0.8441
epoch 3801, loss 0.4468, train acc 84.54%, f1 0.8452, precision 0.8450, recall 0.8453, auc 0.8454
epoch 3901, loss 0.3793, train acc 84.69%, f1 0.8466, precision 0.8470, recall 0.8462, auc 0.8469
epoch 4001, loss 0.2789, train acc 84.84%, f1 0.8483, precision 0.8471, recall 0.8495, auc 0.8484
epoch 4101, loss 0.3200, train acc 84.88%, f1 0.8487, precision 0.8478, recall 0.8496, auc 0.8488
epoch 4201, loss 0.5323, train acc 84.97%, f1 0.8499, precision 0.8472, recall 0.8526, auc 0.8497
epoch 4301, loss 0.2799, train acc 85.10%, f1 0.8507, precision 0.8511, recall 0.8503, auc 0.8510
epoch 4401, loss 0.3332, train acc 85.23%, f1 0.8523, precision 0.8510, recall 0.8535, auc 0.8523
epoch 4501, loss 0.3197, train acc 85.30%, f1 0.8528, precision 0.8526, recall 0.8531, auc 0.8530
epoch 4601, loss 0.4394, train acc 85.43%, f1 0.8542, precision 0.8535, recall 0.8549, auc 0.8543
epoch 4701, loss 0.2971, train acc 85.49%, f1 0.8547, precision 0.8546, recall 0.8548, auc 0.8549
epoch 4801, loss 0.3103, train acc 85.59%, f1 0.8559, precision 0.8543, recall 0.8576, auc 0.8559
epoch 4901, loss 0.3625, train acc 85.69%, f1 0.8564, precision 0.8579, recall 0.8550, auc 0.8569
epoch 5001, loss 0.2840, train acc 85.72%, f1 0.8567, precision 0.8584, recall 0.8551, auc 0.8572
epoch 5101, loss 0.3918, train acc 85.81%, f1 0.8579, precision 0.8578, recall 0.8580, auc 0.8581
epoch 5201, loss 0.2684, train acc 85.89%, f1 0.8586, precision 0.8587, recall 0.8586, auc 0.8589
epoch 5301, loss 0.2590, train acc 85.94%, f1 0.8590, precision 0.8599, recall 0.8582, auc 0.8594
epoch 5401, loss 0.2585, train acc 86.01%, f1 0.8597, precision 0.8611, recall 0.8583, auc 0.8601
epoch 5501, loss 0.3335, train acc 86.09%, f1 0.8605, precision 0.8614, recall 0.8597, auc 0.8609
epoch 5601, loss 0.3233, train acc 86.14%, f1 0.8612, precision 0.8612, recall 0.8612, auc 0.8614
epoch 5701, loss 0.3290, train acc 86.08%, f1 0.8609, precision 0.8589, recall 0.8630, auc 0.8608
epoch 5801, loss 0.3313, train acc 86.30%, f1 0.8631, precision 0.8611, recall 0.8651, auc 0.8630
epoch 5901, loss 0.3474, train acc 86.26%, f1 0.8620, precision 0.8642, recall 0.8598, auc 0.8626
epoch 6001, loss 0.2854, train acc 86.34%, f1 0.8631, precision 0.8634, recall 0.8628, auc 0.8634
epoch 6101, loss 0.3271, train acc 86.43%, f1 0.8640, precision 0.8643, recall 0.8637, auc 0.8643
epoch 6201, loss 0.3764, train acc 86.40%, f1 0.8636, precision 0.8651, recall 0.8620, auc 0.8640
epoch 6301, loss 0.2274, train acc 86.45%, f1 0.8640, precision 0.8658, recall 0.8621, auc 0.8645
epoch 6401, loss 0.2473, train acc 86.51%, f1 0.8652, precision 0.8633, recall 0.8671, auc 0.8651
epoch 6501, loss 0.2762, train acc 86.54%, f1 0.8654, precision 0.8643, recall 0.8664, auc 0.8654
epoch 6601, loss 0.2504, train acc 86.65%, f1 0.8658, precision 0.8684, recall 0.8633, auc 0.8665
epoch 6701, loss 0.2261, train acc 86.68%, f1 0.8662, precision 0.8683, recall 0.8642, auc 0.8668
epoch 6801, loss 0.2714, train acc 86.62%, f1 0.8662, precision 0.8651, recall 0.8672, auc 0.8662
epoch 6901, loss 0.3250, train acc 86.75%, f1 0.8669, precision 0.8693, recall 0.8645, auc 0.8675
epoch 7001, loss 0.2593, train acc 86.70%, f1 0.8666, precision 0.8683, recall 0.8648, auc 0.8670
epoch 7101, loss 0.2968, train acc 86.71%, f1 0.8674, precision 0.8639, recall 0.8709, auc 0.8671
epoch 7201, loss 0.2543, train acc 86.79%, f1 0.8674, precision 0.8693, recall 0.8655, auc 0.8679
epoch 7301, loss 0.2256, train acc 86.89%, f1 0.8682, precision 0.8718, recall 0.8645, auc 0.8689
epoch 7401, loss 0.3725, train acc 86.87%, f1 0.8683, precision 0.8692, recall 0.8675, auc 0.8687
epoch 7501, loss 0.2572, train acc 86.90%, f1 0.8692, precision 0.8666, recall 0.8718, auc 0.8690
epoch 7601, loss 0.4480, train acc 87.00%, f1 0.8695, precision 0.8718, recall 0.8671, auc 0.8700
epoch 7701, loss 0.3009, train acc 86.96%, f1 0.8696, precision 0.8681, recall 0.8711, auc 0.8696
epoch 7801, loss 0.2318, train acc 87.06%, f1 0.8705, precision 0.8694, recall 0.8716, auc 0.8706
epoch 7901, loss 0.2174, train acc 87.12%, f1 0.8715, precision 0.8680, recall 0.8751, auc 0.8712
epoch 8001, loss 0.2864, train acc 87.11%, f1 0.8711, precision 0.8700, recall 0.8721, auc 0.8711
epoch 8101, loss 0.2526, train acc 87.18%, f1 0.8713, precision 0.8734, recall 0.8692, auc 0.8718
epoch 8201, loss 0.1703, train acc 87.16%, f1 0.8716, precision 0.8707, recall 0.8725, auc 0.8716
epoch 8301, loss 0.3648, train acc 87.21%, f1 0.8721, precision 0.8707, recall 0.8734, auc 0.8721
epoch 8401, loss 0.1895, train acc 87.31%, f1 0.8734, precision 0.8700, recall 0.8768, auc 0.8731
epoch 8501, loss 0.2764, train acc 87.35%, f1 0.8735, precision 0.8720, recall 0.8750, auc 0.8735
epoch 8601, loss 0.2579, train acc 87.29%, f1 0.8728, precision 0.8720, recall 0.8735, auc 0.8729
epoch 8701, loss 0.3062, train acc 87.39%, f1 0.8732, precision 0.8764, recall 0.8700, auc 0.8739
epoch 8801, loss 0.2055, train acc 87.44%, f1 0.8741, precision 0.8752, recall 0.8729, auc 0.8744
epoch 8901, loss 0.3025, train acc 87.52%, f1 0.8746, precision 0.8775, recall 0.8717, auc 0.8752
epoch 9001, loss 0.2918, train acc 87.59%, f1 0.8756, precision 0.8768, recall 0.8744, auc 0.8759
epoch 9101, loss 0.2720, train acc 87.62%, f1 0.8757, precision 0.8781, recall 0.8733, auc 0.8762
epoch 9201, loss 0.3378, train acc 87.58%, f1 0.8756, precision 0.8759, recall 0.8752, auc 0.8758
epoch 9301, loss 0.2610, train acc 87.68%, f1 0.8764, precision 0.8773, recall 0.8755, auc 0.8768
epoch 9401, loss 0.2504, train acc 87.75%, f1 0.8772, precision 0.8776, recall 0.8768, auc 0.8775
epoch 9501, loss 0.2251, train acc 87.73%, f1 0.8774, precision 0.8754, recall 0.8793, auc 0.8773
epoch 9601, loss 0.3475, train acc 87.77%, f1 0.8773, precision 0.8792, recall 0.8754, auc 0.8777
epoch 9701, loss 0.2764, train acc 87.86%, f1 0.8785, precision 0.8779, recall 0.8792, auc 0.8786
epoch 9801, loss 0.2631, train acc 87.86%, f1 0.8782, precision 0.8793, recall 0.8772, auc 0.8786
epoch 9901, loss 0.3008, train acc 87.94%, f1 0.8789, precision 0.8812, recall 0.8766, auc 0.8794
epoch 10001, loss 0.1936, train acc 88.02%, f1 0.8800, precision 0.8806, recall 0.8793, auc 0.8802
epoch 10101, loss 0.1322, train acc 88.07%, f1 0.8805, precision 0.8806, recall 0.8804, auc 0.8807
epoch 10201, loss 0.3344, train acc 88.12%, f1 0.8808, precision 0.8824, recall 0.8792, auc 0.8812
epoch 10301, loss 0.2417, train acc 88.12%, f1 0.8808, precision 0.8821, recall 0.8795, auc 0.8812
epoch 10401, loss 0.3480, train acc 88.20%, f1 0.8821, precision 0.8805, recall 0.8837, auc 0.8820
epoch 10501, loss 0.2358, train acc 88.32%, f1 0.8830, precision 0.8826, recall 0.8835, auc 0.8832
epoch 10601, loss 0.2802, train acc 88.39%, f1 0.8838, precision 0.8831, recall 0.8845, auc 0.8839
epoch 10701, loss 0.2930, train acc 88.45%, f1 0.8843, precision 0.8839, recall 0.8848, auc 0.8845
epoch 10801, loss 0.3831, train acc 88.47%, f1 0.8843, precision 0.8856, recall 0.8830, auc 0.8847
epoch 10901, loss 0.2855, train acc 88.50%, f1 0.8849, precision 0.8843, recall 0.8855, auc 0.8850
epoch 11001, loss 0.2456, train acc 88.53%, f1 0.8850, precision 0.8861, recall 0.8839, auc 0.8853
epoch 11101, loss 0.2755, train acc 88.64%, f1 0.8864, precision 0.8854, recall 0.8873, auc 0.8864
epoch 11201, loss 0.2660, train acc 88.59%, f1 0.8859, precision 0.8842, recall 0.8877, auc 0.8859
epoch 11301, loss 0.2089, train acc 88.69%, f1 0.8869, precision 0.8858, recall 0.8880, auc 0.8869
epoch 11401, loss 0.2734, train acc 88.75%, f1 0.8871, precision 0.8893, recall 0.8849, auc 0.8875
epoch 11501, loss 0.3059, train acc 88.84%, f1 0.8885, precision 0.8865, recall 0.8904, auc 0.8884
epoch 11601, loss 0.2500, train acc 88.89%, f1 0.8892, precision 0.8849, recall 0.8936, auc 0.8889
epoch 11701, loss 0.2182, train acc 89.00%, f1 0.8896, precision 0.8911, recall 0.8882, auc 0.8900
epoch 11801, loss 0.1923, train acc 89.09%, f1 0.8910, precision 0.8891, recall 0.8928, auc 0.8909
epoch 11901, loss 0.3854, train acc 89.08%, f1 0.8912, precision 0.8869, recall 0.8955, auc 0.8908
epoch 12001, loss 0.3535, train acc 89.20%, f1 0.8919, precision 0.8915, recall 0.8923, auc 0.8920
epoch 12101, loss 0.2363, train acc 89.21%, f1 0.8923, precision 0.8898, recall 0.8948, auc 0.8921
epoch 12201, loss 0.1929, train acc 89.31%, f1 0.8932, precision 0.8908, recall 0.8957, auc 0.8931
epoch 12301, loss 0.2471, train acc 89.34%, f1 0.8934, precision 0.8921, recall 0.8947, auc 0.8934
epoch 12401, loss 0.2582, train acc 89.38%, f1 0.8936, precision 0.8940, recall 0.8932, auc 0.8938
epoch 12501, loss 0.1770, train acc 89.44%, f1 0.8940, precision 0.8965, recall 0.8915, auc 0.8944
epoch 12601, loss 0.2501, train acc 89.53%, f1 0.8955, precision 0.8922, recall 0.8988, auc 0.8953
epoch 12701, loss 0.2077, train acc 89.54%, f1 0.8953, precision 0.8944, recall 0.8963, auc 0.8954
epoch 12801, loss 0.2792, train acc 89.60%, f1 0.8959, precision 0.8955, recall 0.8963, auc 0.8960
epoch 12901, loss 0.2844, train acc 89.61%, f1 0.8962, precision 0.8939, recall 0.8985, auc 0.8961
epoch 13001, loss 0.1793, train acc 89.75%, f1 0.8976, precision 0.8955, recall 0.8998, auc 0.8976
epoch 13101, loss 0.1794, train acc 89.78%, f1 0.8978, precision 0.8969, recall 0.8986, auc 0.8978
epoch 13201, loss 0.3009, train acc 89.81%, f1 0.8982, precision 0.8955, recall 0.9011, auc 0.8981
epoch 13301, loss 0.2544, train acc 89.85%, f1 0.8983, precision 0.8989, recall 0.8977, auc 0.8985
epoch 13401, loss 0.1860, train acc 89.91%, f1 0.8989, precision 0.8988, recall 0.8991, auc 0.8991
epoch 13501, loss 0.2146, train acc 89.97%, f1 0.8994, precision 0.9007, recall 0.8981, auc 0.8997
epoch 13601, loss 0.3233, train acc 90.05%, f1 0.9005, precision 0.8988, recall 0.9022, auc 0.9005
epoch 13701, loss 0.1549, train acc 90.11%, f1 0.9015, precision 0.8972, recall 0.9058, auc 0.9012
epoch 13801, loss 0.1427, train acc 90.15%, f1 0.9014, precision 0.9012, recall 0.9015, auc 0.9015
epoch 13901, loss 0.2634, train acc 90.16%, f1 0.9016, precision 0.9000, recall 0.9032, auc 0.9016
epoch 14001, loss 0.2396, train acc 90.22%, f1 0.9025, precision 0.8987, recall 0.9064, auc 0.9022
epoch 14101, loss 0.2809, train acc 90.33%, f1 0.9033, precision 0.9017, recall 0.9050, auc 0.9033
epoch 14201, loss 0.1940, train acc 90.35%, f1 0.9034, precision 0.9031, recall 0.9037, auc 0.9035
epoch 14301, loss 0.2302, train acc 90.34%, f1 0.9032, precision 0.9035, recall 0.9028, auc 0.9034
epoch 14401, loss 0.2397, train acc 90.50%, f1 0.9050, precision 0.9038, recall 0.9061, auc 0.9050
epoch 14501, loss 0.2617, train acc 90.51%, f1 0.9052, precision 0.9031, recall 0.9072, auc 0.9051
epoch 14601, loss 0.1812, train acc 90.53%, f1 0.9055, precision 0.9027, recall 0.9083, auc 0.9053
epoch 14701, loss 0.2412, train acc 90.59%, f1 0.9060, precision 0.9040, recall 0.9080, auc 0.9059
epoch 14801, loss 0.3152, train acc 90.63%, f1 0.9064, precision 0.9040, recall 0.9088, auc 0.9063
epoch 14901, loss 0.2364, train acc 90.69%, f1 0.9072, precision 0.9035, recall 0.9109, auc 0.9069
epoch 15001, loss 0.2572, train acc 90.74%, f1 0.9074, precision 0.9057, recall 0.9092, auc 0.9074
epoch 15101, loss 0.2022, train acc 90.81%, f1 0.9080, precision 0.9075, recall 0.9084, auc 0.9081
epoch 15201, loss 0.3058, train acc 90.78%, f1 0.9077, precision 0.9072, recall 0.9083, auc 0.9078
epoch 15301, loss 0.1638, train acc 90.81%, f1 0.9083, precision 0.9049, recall 0.9118, auc 0.9081
epoch 15401, loss 0.1675, train acc 90.90%, f1 0.9087, precision 0.9097, recall 0.9077, auc 0.9090
epoch 15501, loss 0.2716, train acc 90.98%, f1 0.9097, precision 0.9085, recall 0.9109, auc 0.9098
epoch 15601, loss 0.1386, train acc 91.03%, f1 0.9102, precision 0.9097, recall 0.9106, auc 0.9103
epoch 15701, loss 0.1892, train acc 91.06%, f1 0.9105, precision 0.9101, recall 0.9108, auc 0.9106
epoch 15801, loss 0.1886, train acc 91.09%, f1 0.9108, precision 0.9111, recall 0.9105, auc 0.9109
epoch 15901, loss 0.1623, train acc 91.14%, f1 0.9113, precision 0.9109, recall 0.9116, auc 0.9114
epoch 16001, loss 0.1514, train acc 91.27%, f1 0.9127, precision 0.9115, recall 0.9140, auc 0.9127
epoch 16101, loss 0.2698, train acc 91.20%, f1 0.9120, precision 0.9105, recall 0.9135, auc 0.9120
epoch 16201, loss 0.1510, train acc 91.32%, f1 0.9130, precision 0.9131, recall 0.9130, auc 0.9132
epoch 16301, loss 0.1699, train acc 91.33%, f1 0.9133, precision 0.9117, recall 0.9149, auc 0.9133
epoch 16401, loss 0.1955, train acc 91.35%, f1 0.9133, precision 0.9141, recall 0.9125, auc 0.9135
epoch 16501, loss 0.1265, train acc 91.48%, f1 0.9147, precision 0.9137, recall 0.9158, auc 0.9148/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.2129, train acc 91.50%, f1 0.9149, precision 0.9144, recall 0.9154, auc 0.9150
epoch 16701, loss 0.2109, train acc 91.45%, f1 0.9147, precision 0.9108, recall 0.9187, auc 0.9145
epoch 16801, loss 0.1888, train acc 91.54%, f1 0.9151, precision 0.9166, recall 0.9137, auc 0.9154
epoch 16901, loss 0.2680, train acc 91.60%, f1 0.9161, precision 0.9142, recall 0.9179, auc 0.9160
epoch 17001, loss 0.1685, train acc 91.62%, f1 0.9162, precision 0.9148, recall 0.9177, auc 0.9162
epoch 17101, loss 0.1823, train acc 91.68%, f1 0.9167, precision 0.9161, recall 0.9173, auc 0.9168
epoch 17201, loss 0.1606, train acc 91.69%, f1 0.9168, precision 0.9167, recall 0.9168, auc 0.9169
epoch 17301, loss 0.1611, train acc 91.81%, f1 0.9180, precision 0.9171, recall 0.9190, auc 0.9181
epoch 17401, loss 0.2240, train acc 91.85%, f1 0.9187, precision 0.9155, recall 0.9219, auc 0.9185
epoch 17501, loss 0.2164, train acc 91.88%, f1 0.9186, precision 0.9191, recall 0.9181, auc 0.9188
epoch 17601, loss 0.1755, train acc 91.87%, f1 0.9185, precision 0.9195, recall 0.9176, auc 0.9187
epoch 17701, loss 0.2336, train acc 91.90%, f1 0.9188, precision 0.9195, recall 0.9180, auc 0.9190
epoch 17801, loss 0.2212, train acc 91.98%, f1 0.9199, precision 0.9170, recall 0.9229, auc 0.9198
epoch 17901, loss 0.2919, train acc 92.04%, f1 0.9204, precision 0.9183, recall 0.9225, auc 0.9204
epoch 18001, loss 0.1466, train acc 92.05%, f1 0.9206, precision 0.9184, recall 0.9228, auc 0.9205
epoch 18101, loss 0.2100, train acc 92.06%, f1 0.9206, precision 0.9189, recall 0.9223, auc 0.9206
epoch 18201, loss 0.1920, train acc 92.12%, f1 0.9211, precision 0.9212, recall 0.9209, auc 0.9212
epoch 18301, loss 0.1373, train acc 92.16%, f1 0.9215, precision 0.9222, recall 0.9208, auc 0.9216
epoch 18401, loss 0.1722, train acc 92.18%, f1 0.9218, precision 0.9203, recall 0.9233, auc 0.9218
epoch 18501, loss 0.2273, train acc 92.15%, f1 0.9215, precision 0.9195, recall 0.9236, auc 0.9215
epoch 18601, loss 0.2073, train acc 92.27%, f1 0.9227, precision 0.9223, recall 0.9231, auc 0.9227
epoch 18701, loss 0.1250, train acc 92.28%, f1 0.9226, precision 0.9230, recall 0.9222, auc 0.9228
epoch 18801, loss 0.2275, train acc 92.30%, f1 0.9231, precision 0.9207, recall 0.9255, auc 0.9230
epoch 18901, loss 0.2450, train acc 92.37%, f1 0.9239, precision 0.9206, recall 0.9271, auc 0.9237
epoch 19001, loss 0.1438, train acc 92.39%, f1 0.9241, precision 0.9208, recall 0.9273, auc 0.9239
epoch 19101, loss 0.1395, train acc 92.38%, f1 0.9237, precision 0.9228, recall 0.9246, auc 0.9238
epoch 19201, loss 0.1846, train acc 92.49%, f1 0.9249, precision 0.9231, recall 0.9267, auc 0.9249
epoch 19301, loss 0.1135, train acc 92.46%, f1 0.9245, precision 0.9236, recall 0.9255, auc 0.9246
epoch 19401, loss 0.1376, train acc 92.56%, f1 0.9255, precision 0.9259, recall 0.9250, auc 0.9256
epoch 19501, loss 0.1231, train acc 92.50%, f1 0.9250, precision 0.9241, recall 0.9259, auc 0.9250
epoch 19601, loss 0.1656, train acc 92.63%, f1 0.9263, precision 0.9252, recall 0.9273, auc 0.9263
epoch 19701, loss 0.1811, train acc 92.63%, f1 0.9262, precision 0.9264, recall 0.9260, auc 0.9263
epoch 19801, loss 0.1363, train acc 92.66%, f1 0.9263, precision 0.9279, recall 0.9247, auc 0.9266
epoch 19901, loss 0.1741, train acc 92.71%, f1 0.9271, precision 0.9250, recall 0.9293, auc 0.9271
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_1
./test_pima/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.720925925925926

the Fscore is 0.6524822695035462

the precision is 0.5287356321839081

the recall is 0.8518518518518519

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_1
----------------------



epoch 1, loss 0.6934, train acc 50.20%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5523, train acc 78.36%, f1 0.7892, precision 0.7664, recall 0.8134, auc 0.7837
epoch 201, loss 0.4668, train acc 81.49%, f1 0.8146, precision 0.8128, recall 0.8165, auc 0.8149
epoch 301, loss 0.4606, train acc 82.83%, f1 0.8277, precision 0.8271, recall 0.8283, auc 0.8283
epoch 401, loss 0.4666, train acc 83.25%, f1 0.8312, precision 0.8342, recall 0.8283, auc 0.8325
epoch 501, loss 0.3129, train acc 83.42%, f1 0.8330, precision 0.8357, recall 0.8303, auc 0.8342
epoch 601, loss 0.3983, train acc 83.40%, f1 0.8329, precision 0.8350, recall 0.8309, auc 0.8340
epoch 701, loss 0.2566, train acc 83.39%, f1 0.8325, precision 0.8361, recall 0.8290, auc 0.8338
epoch 801, loss 0.4994, train acc 83.41%, f1 0.8329, precision 0.8357, recall 0.8301, auc 0.8340
epoch 901, loss 0.4138, train acc 83.48%, f1 0.8336, precision 0.8364, recall 0.8307, auc 0.8348
epoch 1001, loss 0.4487, train acc 83.42%, f1 0.8329, precision 0.8362, recall 0.8295, auc 0.8342
epoch 1101, loss 0.4040, train acc 83.39%, f1 0.8323, precision 0.8369, recall 0.8278, auc 0.8339
epoch 1201, loss 0.2274, train acc 83.45%, f1 0.8335, precision 0.8355, recall 0.8314, auc 0.8345
epoch 1301, loss 0.2557, train acc 83.33%, f1 0.8322, precision 0.8346, recall 0.8299, auc 0.8333
epoch 1401, loss 0.2863, train acc 83.47%, f1 0.8338, precision 0.8349, recall 0.8328, auc 0.8347
epoch 1501, loss 0.4254, train acc 83.46%, f1 0.8334, precision 0.8363, recall 0.8305, auc 0.8346
epoch 1601, loss 0.3160, train acc 83.48%, f1 0.8334, precision 0.8369, recall 0.8299, auc 0.8347
epoch 1701, loss 0.4018, train acc 83.44%, f1 0.8331, precision 0.8366, recall 0.8296, auc 0.8344
epoch 1801, loss 0.3390, train acc 83.47%, f1 0.8332, precision 0.8372, recall 0.8293, auc 0.8347
epoch 1901, loss 0.2565, train acc 83.39%, f1 0.8328, precision 0.8351, recall 0.8306, auc 0.8339
epoch 2001, loss 0.3923, train acc 83.49%, f1 0.8336, precision 0.8370, recall 0.8302, auc 0.8349
epoch 2101, loss 0.3442, train acc 83.45%, f1 0.8338, precision 0.8343, recall 0.8332, auc 0.8345
epoch 2201, loss 0.2982, train acc 83.49%, f1 0.8337, precision 0.8366, recall 0.8308, auc 0.8349
epoch 2301, loss 0.4234, train acc 83.50%, f1 0.8338, precision 0.8363, recall 0.8313, auc 0.8350
epoch 2401, loss 0.4766, train acc 83.49%, f1 0.8335, precision 0.8370, recall 0.8301, auc 0.8348
epoch 2501, loss 0.3580, train acc 83.51%, f1 0.8343, precision 0.8350, recall 0.8336, auc 0.8351
epoch 2601, loss 0.2426, train acc 83.63%, f1 0.8356, precision 0.8359, recall 0.8353, auc 0.8363
epoch 2701, loss 0.4636, train acc 83.62%, f1 0.8352, precision 0.8370, recall 0.8335, auc 0.8362
epoch 2801, loss 0.3859, train acc 83.65%, f1 0.8363, precision 0.8341, recall 0.8386, auc 0.8365
epoch 2901, loss 0.3244, train acc 83.62%, f1 0.8352, precision 0.8370, recall 0.8333, auc 0.8362
epoch 3001, loss 0.3248, train acc 83.69%, f1 0.8360, precision 0.8373, recall 0.8347, auc 0.8369
epoch 3101, loss 0.3243, train acc 83.79%, f1 0.8367, precision 0.8396, recall 0.8338, auc 0.8378
epoch 3201, loss 0.3790, train acc 83.89%, f1 0.8380, precision 0.8396, recall 0.8363, auc 0.8389
epoch 3301, loss 0.3339, train acc 83.95%, f1 0.8398, precision 0.8353, recall 0.8443, auc 0.8396
epoch 3401, loss 0.3171, train acc 84.06%, f1 0.8399, precision 0.8404, recall 0.8393, auc 0.8406
epoch 3501, loss 0.2734, train acc 84.20%, f1 0.8408, precision 0.8441, recall 0.8375, auc 0.8420
epoch 3601, loss 0.3707, train acc 84.30%, f1 0.8423, precision 0.8431, recall 0.8414, auc 0.8430
epoch 3701, loss 0.3550, train acc 84.42%, f1 0.8435, precision 0.8440, recall 0.8430, auc 0.8442
epoch 3801, loss 0.3418, train acc 84.60%, f1 0.8453, precision 0.8454, recall 0.8453, auc 0.8460
epoch 3901, loss 0.2383, train acc 84.64%, f1 0.8461, precision 0.8445, recall 0.8478, auc 0.8464
epoch 4001, loss 0.2943, train acc 84.82%, f1 0.8478, precision 0.8470, recall 0.8486, auc 0.8482
epoch 4101, loss 0.3016, train acc 84.97%, f1 0.8494, precision 0.8477, recall 0.8512, auc 0.8497
epoch 4201, loss 0.2179, train acc 85.02%, f1 0.8497, precision 0.8496, recall 0.8497, auc 0.8502
epoch 4301, loss 0.3221, train acc 85.10%, f1 0.8504, precision 0.8507, recall 0.8500, auc 0.8510
epoch 4401, loss 0.3532, train acc 85.18%, f1 0.8507, precision 0.8540, recall 0.8474, auc 0.8518
epoch 4501, loss 0.2861, train acc 85.27%, f1 0.8521, precision 0.8524, recall 0.8519, auc 0.8527
epoch 4601, loss 0.3715, train acc 85.29%, f1 0.8524, precision 0.8520, recall 0.8528, auc 0.8529
epoch 4701, loss 0.2795, train acc 85.46%, f1 0.8540, precision 0.8540, recall 0.8540, auc 0.8546
epoch 4801, loss 0.2353, train acc 85.50%, f1 0.8549, precision 0.8522, recall 0.8575, auc 0.8550
epoch 4901, loss 0.4399, train acc 85.60%, f1 0.8556, precision 0.8545, recall 0.8568, auc 0.8560
epoch 5001, loss 0.3838, train acc 85.65%, f1 0.8566, precision 0.8524, recall 0.8609, auc 0.8565
epoch 5101, loss 0.3350, train acc 85.74%, f1 0.8566, precision 0.8577, recall 0.8556, auc 0.8574
epoch 5201, loss 0.2998, train acc 85.84%, f1 0.8578, precision 0.8583, recall 0.8574, auc 0.8584
epoch 5301, loss 0.2473, train acc 85.91%, f1 0.8587, precision 0.8573, recall 0.8602, auc 0.8591
epoch 5401, loss 0.2749, train acc 85.98%, f1 0.8594, precision 0.8585, recall 0.8604, auc 0.8598
epoch 5501, loss 0.3594, train acc 85.95%, f1 0.8589, precision 0.8588, recall 0.8591, auc 0.8595
epoch 5601, loss 0.2124, train acc 86.14%, f1 0.8609, precision 0.8607, recall 0.8611, auc 0.8614
epoch 5701, loss 0.2920, train acc 86.17%, f1 0.8612, precision 0.8609, recall 0.8614, auc 0.8617
epoch 5801, loss 0.3593, train acc 86.20%, f1 0.8610, precision 0.8639, recall 0.8582, auc 0.8620
epoch 5901, loss 0.3300, train acc 86.27%, f1 0.8625, precision 0.8605, recall 0.8645, auc 0.8627
epoch 6001, loss 0.2921, train acc 86.32%, f1 0.8630, precision 0.8611, recall 0.8649, auc 0.8632
epoch 6101, loss 0.3165, train acc 86.42%, f1 0.8638, precision 0.8633, recall 0.8643, auc 0.8642
epoch 6201, loss 0.3183, train acc 86.46%, f1 0.8639, precision 0.8652, recall 0.8627, auc 0.8646
epoch 6301, loss 0.3765, train acc 86.52%, f1 0.8646, precision 0.8652, recall 0.8639, auc 0.8652
epoch 6401, loss 0.4025, train acc 86.45%, f1 0.8643, precision 0.8625, recall 0.8661, auc 0.8645
epoch 6501, loss 0.3138, train acc 86.60%, f1 0.8653, precision 0.8665, recall 0.8641, auc 0.8660
epoch 6601, loss 0.2661, train acc 86.65%, f1 0.8661, precision 0.8655, recall 0.8667, auc 0.8665
epoch 6701, loss 0.2690, train acc 86.72%, f1 0.8669, precision 0.8659, recall 0.8678, auc 0.8672
epoch 6801, loss 0.4704, train acc 86.77%, f1 0.8676, precision 0.8647, recall 0.8705, auc 0.8677
epoch 6901, loss 0.2927, train acc 86.78%, f1 0.8671, precision 0.8683, recall 0.8658, auc 0.8678
epoch 7001, loss 0.2748, train acc 86.81%, f1 0.8672, precision 0.8695, recall 0.8650, auc 0.8681
epoch 7101, loss 0.3513, train acc 86.85%, f1 0.8682, precision 0.8667, recall 0.8697, auc 0.8685
epoch 7201, loss 0.3374, train acc 86.93%, f1 0.8692, precision 0.8663, recall 0.8721, auc 0.8693
epoch 7301, loss 0.2387, train acc 86.81%, f1 0.8675, precision 0.8681, recall 0.8669, auc 0.8681
epoch 7401, loss 0.2131, train acc 86.94%, f1 0.8691, precision 0.8681, recall 0.8701, auc 0.8694
epoch 7501, loss 0.3804, train acc 87.01%, f1 0.8695, precision 0.8699, recall 0.8691, auc 0.8701
epoch 7601, loss 0.2843, train acc 87.06%, f1 0.8708, precision 0.8660, recall 0.8757, auc 0.8706
epoch 7701, loss 0.3679, train acc 87.09%, f1 0.8706, precision 0.8691, recall 0.8721, auc 0.8709
epoch 7801, loss 0.1953, train acc 87.06%, f1 0.8703, precision 0.8690, recall 0.8715, auc 0.8706
epoch 7901, loss 0.3929, train acc 87.12%, f1 0.8705, precision 0.8713, recall 0.8697, auc 0.8712
epoch 8001, loss 0.2019, train acc 87.18%, f1 0.8715, precision 0.8698, recall 0.8732, auc 0.8718
epoch 8101, loss 0.2965, train acc 87.15%, f1 0.8706, precision 0.8732, recall 0.8681, auc 0.8715
epoch 8201, loss 0.2953, train acc 87.22%, f1 0.8717, precision 0.8720, recall 0.8714, auc 0.8722/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.3024, train acc 87.27%, f1 0.8724, precision 0.8714, recall 0.8734, auc 0.8727
epoch 8401, loss 0.2197, train acc 87.36%, f1 0.8731, precision 0.8736, recall 0.8725, auc 0.8736
epoch 8501, loss 0.3289, train acc 87.33%, f1 0.8721, precision 0.8769, recall 0.8674, auc 0.8732
epoch 8601, loss 0.2816, train acc 87.41%, f1 0.8734, precision 0.8749, recall 0.8719, auc 0.8741
epoch 8701, loss 0.3372, train acc 87.45%, f1 0.8740, precision 0.8746, recall 0.8734, auc 0.8745
epoch 8801, loss 0.2862, train acc 87.48%, f1 0.8743, precision 0.8742, recall 0.8744, auc 0.8748
epoch 8901, loss 0.3596, train acc 87.47%, f1 0.8744, precision 0.8734, recall 0.8753, auc 0.8747
epoch 9001, loss 0.3009, train acc 87.54%, f1 0.8749, precision 0.8754, recall 0.8743, auc 0.8754
epoch 9101, loss 0.2390, train acc 87.58%, f1 0.8749, precision 0.8782, recall 0.8716, auc 0.8758
epoch 9201, loss 0.3094, train acc 87.62%, f1 0.8761, precision 0.8739, recall 0.8782, auc 0.8763
epoch 9301, loss 0.2125, train acc 87.65%, f1 0.8761, precision 0.8751, recall 0.8772, auc 0.8765
epoch 9401, loss 0.2817, train acc 87.73%, f1 0.8768, precision 0.8769, recall 0.8767, auc 0.8773
epoch 9501, loss 0.2237, train acc 87.80%, f1 0.8777, precision 0.8771, recall 0.8782, auc 0.8780
epoch 9601, loss 0.3326, train acc 87.81%, f1 0.8777, precision 0.8769, recall 0.8784, auc 0.8781
epoch 9701, loss 0.4019, train acc 87.86%, f1 0.8776, precision 0.8816, recall 0.8736, auc 0.8786
epoch 9801, loss 0.3224, train acc 87.91%, f1 0.8785, precision 0.8789, recall 0.8782, auc 0.8791
epoch 9901, loss 0.2644, train acc 87.90%, f1 0.8787, precision 0.8776, recall 0.8798, auc 0.8790
epoch 10001, loss 0.2435, train acc 87.98%, f1 0.8792, precision 0.8807, recall 0.8776, auc 0.8798
epoch 10101, loss 0.2303, train acc 88.05%, f1 0.8797, precision 0.8822, recall 0.8773, auc 0.8805
epoch 10201, loss 0.2038, train acc 88.05%, f1 0.8797, precision 0.8822, recall 0.8772, auc 0.8805
epoch 10301, loss 0.2961, train acc 88.12%, f1 0.8807, precision 0.8808, recall 0.8805, auc 0.8812
epoch 10401, loss 0.2769, train acc 88.23%, f1 0.8820, precision 0.8809, recall 0.8832, auc 0.8823
epoch 10501, loss 0.2952, train acc 88.23%, f1 0.8815, precision 0.8842, recall 0.8788, auc 0.8823
epoch 10601, loss 0.2716, train acc 88.29%, f1 0.8826, precision 0.8817, recall 0.8835, auc 0.8829
epoch 10701, loss 0.2211, train acc 88.37%, f1 0.8831, precision 0.8841, recall 0.8822, auc 0.8837
epoch 10801, loss 0.3363, train acc 88.40%, f1 0.8836, precision 0.8837, recall 0.8834, auc 0.8840
epoch 10901, loss 0.2772, train acc 88.56%, f1 0.8847, precision 0.8883, recall 0.8811, auc 0.8856
epoch 11001, loss 0.2090, train acc 88.57%, f1 0.8852, precision 0.8856, recall 0.8847, auc 0.8857
epoch 11101, loss 0.2677, train acc 88.61%, f1 0.8855, precision 0.8862, recall 0.8849, auc 0.8861
epoch 11201, loss 0.3476, train acc 88.67%, f1 0.8866, precision 0.8843, recall 0.8888, auc 0.8867
epoch 11301, loss 0.3506, train acc 88.69%, f1 0.8863, precision 0.8875, recall 0.8851, auc 0.8869
epoch 11401, loss 0.2975, train acc 88.80%, f1 0.8870, precision 0.8920, recall 0.8820, auc 0.8880
epoch 11501, loss 0.3217, train acc 88.86%, f1 0.8882, precision 0.8882, recall 0.8882, auc 0.8886
epoch 11601, loss 0.2695, train acc 88.97%, f1 0.8892, precision 0.8903, recall 0.8880, auc 0.8897
epoch 11701, loss 0.2908, train acc 89.05%, f1 0.8901, precision 0.8893, recall 0.8910, auc 0.8905
epoch 11801, loss 0.2490, train acc 89.14%, f1 0.8906, precision 0.8939, recall 0.8872, auc 0.8914
epoch 11901, loss 0.2445, train acc 89.13%, f1 0.8907, precision 0.8920, recall 0.8894, auc 0.8913
epoch 12001, loss 0.1670, train acc 89.25%, f1 0.8916, precision 0.8954, recall 0.8879, auc 0.8925
epoch 12101, loss 0.1843, train acc 89.29%, f1 0.8928, precision 0.8905, recall 0.8951, auc 0.8929
epoch 12201, loss 0.3194, train acc 89.35%, f1 0.8929, precision 0.8942, recall 0.8917, auc 0.8935
epoch 12301, loss 0.1692, train acc 89.38%, f1 0.8932, precision 0.8943, recall 0.8921, auc 0.8938
epoch 12401, loss 0.2154, train acc 89.46%, f1 0.8941, precision 0.8946, recall 0.8936, auc 0.8946
epoch 12501, loss 0.2054, train acc 89.49%, f1 0.8943, precision 0.8962, recall 0.8924, auc 0.8949
epoch 12601, loss 0.2208, train acc 89.53%, f1 0.8950, precision 0.8934, recall 0.8967, auc 0.8953
epoch 12701, loss 0.2282, train acc 89.56%, f1 0.8950, precision 0.8966, recall 0.8935, auc 0.8956
epoch 12801, loss 0.2257, train acc 89.67%, f1 0.8964, precision 0.8956, recall 0.8972, auc 0.8967
epoch 12901, loss 0.2927, train acc 89.68%, f1 0.8962, precision 0.8977, recall 0.8947, auc 0.8968
epoch 13001, loss 0.2431, train acc 89.68%, f1 0.8968, precision 0.8936, recall 0.9000, auc 0.8969
epoch 13101, loss 0.2865, train acc 89.83%, f1 0.8977, precision 0.8992, recall 0.8963, auc 0.8983
epoch 13201, loss 0.2834, train acc 89.88%, f1 0.8985, precision 0.8977, recall 0.8993, auc 0.8988
epoch 13301, loss 0.3558, train acc 89.85%, f1 0.8980, precision 0.8991, recall 0.8969, auc 0.8985
epoch 13401, loss 0.3382, train acc 89.92%, f1 0.8988, precision 0.8988, recall 0.8989, auc 0.8992
epoch 13501, loss 0.1852, train acc 90.00%, f1 0.8996, precision 0.8998, recall 0.8994, auc 0.9000
epoch 13601, loss 0.2642, train acc 90.05%, f1 0.8997, precision 0.9031, recall 0.8963, auc 0.9005
epoch 13701, loss 0.2607, train acc 90.06%, f1 0.9000, precision 0.9014, recall 0.8986, auc 0.9006
epoch 13801, loss 0.2368, train acc 90.12%, f1 0.9007, precision 0.9020, recall 0.8994, auc 0.9012
epoch 13901, loss 0.2143, train acc 90.17%, f1 0.9009, precision 0.9042, recall 0.8977, auc 0.9017
epoch 14001, loss 0.1966, train acc 90.17%, f1 0.9012, precision 0.9022, recall 0.9002, auc 0.9017
epoch 14101, loss 0.2154, train acc 90.25%, f1 0.9020, precision 0.9028, recall 0.9011, auc 0.9024
epoch 14201, loss 0.2354, train acc 90.30%, f1 0.9028, precision 0.9015, recall 0.9040, auc 0.9030
epoch 14301, loss 0.1980, train acc 90.34%, f1 0.9031, precision 0.9030, recall 0.9031, auc 0.9034
epoch 14401, loss 0.2396, train acc 90.36%, f1 0.9033, precision 0.9029, recall 0.9038, auc 0.9036
epoch 14501, loss 0.2276, train acc 90.38%, f1 0.9033, precision 0.9046, recall 0.9019, auc 0.9038
epoch 14601, loss 0.2172, train acc 90.48%, f1 0.9044, precision 0.9049, recall 0.9039, auc 0.9048
epoch 14701, loss 0.2683, train acc 90.50%, f1 0.9046, precision 0.9052, recall 0.9040, auc 0.9050
epoch 14801, loss 0.1647, train acc 90.59%, f1 0.9056, precision 0.9051, recall 0.9061, auc 0.9059
epoch 14901, loss 0.3061, train acc 90.51%, f1 0.9044, precision 0.9075, recall 0.9013, auc 0.9051
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_1
./test_pima/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.7044444444444444

the Fscore is 0.64

the precision is 0.5

the recall is 0.8888888888888888

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_1
----------------------



epoch 1, loss 0.6933, train acc 49.86%, f1 0.6654, precision 0.4986, recall 1.0000, auc 0.5000
epoch 101, loss 0.5438, train acc 77.88%, f1 0.7762, precision 0.7833, recall 0.7693, auc 0.7788
epoch 201, loss 0.3964, train acc 81.66%, f1 0.8157, precision 0.8172, recall 0.8142, auc 0.8166
epoch 301, loss 0.4384, train acc 82.79%, f1 0.8274, precision 0.8275, recall 0.8272, auc 0.8279
epoch 401, loss 0.3946, train acc 83.24%, f1 0.8320, precision 0.8317, recall 0.8323, auc 0.8324
epoch 501, loss 0.2573, train acc 83.40%, f1 0.8336, precision 0.8331, recall 0.8342, auc 0.8340
epoch 601, loss 0.4137, train acc 83.47%, f1 0.8350, precision 0.8311, recall 0.8389, auc 0.8347
epoch 701, loss 0.3869, train acc 83.43%, f1 0.8336, precision 0.8351, recall 0.8321, auc 0.8343
epoch 801, loss 0.4726, train acc 83.46%, f1 0.8345, precision 0.8326, recall 0.8365, auc 0.8346
epoch 901, loss 0.3742, train acc 83.43%, f1 0.8338, precision 0.8339, recall 0.8338, auc 0.8343
epoch 1001, loss 0.3819, train acc 83.43%, f1 0.8338, precision 0.8338, recall 0.8339, auc 0.8343
epoch 1101, loss 0.4069, train acc 83.39%, f1 0.8334, precision 0.8339, recall 0.8328, auc 0.8339
epoch 1201, loss 0.3229, train acc 83.47%, f1 0.8346, precision 0.8330, recall 0.8362, auc 0.8347
epoch 1301, loss 0.3111, train acc 83.43%, f1 0.8349, precision 0.8296, recall 0.8402, auc 0.8343
epoch 1401, loss 0.4874, train acc 83.46%, f1 0.8340, precision 0.8350, recall 0.8330, auc 0.8346
epoch 1501, loss 0.3246, train acc 83.46%, f1 0.8338, precision 0.8358, recall 0.8317, auc 0.8346
epoch 1601, loss 0.4561, train acc 83.46%, f1 0.8345, precision 0.8324, recall 0.8366, auc 0.8346
epoch 1701, loss 0.3260, train acc 83.46%, f1 0.8346, precision 0.8322, recall 0.8370, auc 0.8346
epoch 1801, loss 0.4424, train acc 83.46%, f1 0.8344, precision 0.8328, recall 0.8360, auc 0.8346
epoch 1901, loss 0.3169, train acc 83.42%, f1 0.8343, precision 0.8314, recall 0.8373, auc 0.8342
epoch 2001, loss 0.5133, train acc 83.40%, f1 0.8334, precision 0.8342, recall 0.8326, auc 0.8340
epoch 2101, loss 0.2987, train acc 83.33%, f1 0.8334, precision 0.8307, recall 0.8361, auc 0.8333
epoch 2201, loss 0.3764, train acc 83.44%, f1 0.8340, precision 0.8337, recall 0.8343, auc 0.8344
epoch 2301, loss 0.3645, train acc 83.44%, f1 0.8345, precision 0.8316, recall 0.8374, auc 0.8344
epoch 2401, loss 0.4074, train acc 83.35%, f1 0.8333, precision 0.8319, recall 0.8348, auc 0.8335
epoch 2501, loss 0.4841, train acc 83.45%, f1 0.8337, precision 0.8355, recall 0.8319, auc 0.8345
epoch 2601, loss 0.5389, train acc 83.50%, f1 0.8343, precision 0.8357, recall 0.8329, auc 0.8350
epoch 2701, loss 0.4119, train acc 83.62%, f1 0.8355, precision 0.8368, recall 0.8342, auc 0.8362
epoch 2801, loss 0.3298, train acc 83.59%, f1 0.8354, precision 0.8352, recall 0.8357, auc 0.8359
epoch 2901, loss 0.4682, train acc 83.67%, f1 0.8358, precision 0.8378, recall 0.8338, auc 0.8367
epoch 3001, loss 0.3497, train acc 83.73%, f1 0.8371, precision 0.8354, recall 0.8388, auc 0.8373
epoch 3101, loss 0.2784, train acc 83.90%, f1 0.8389, precision 0.8370, recall 0.8409, auc 0.8390
epoch 3201, loss 0.3638, train acc 83.90%, f1 0.8383, precision 0.8397, recall 0.8369, auc 0.8390
epoch 3301, loss 0.2577, train acc 83.95%, f1 0.8390, precision 0.8393, recall 0.8388, auc 0.8395
epoch 3401, loss 0.3653, train acc 84.05%, f1 0.8401, precision 0.8399, recall 0.8403, auc 0.8405
epoch 3501, loss 0.3750, train acc 84.18%, f1 0.8410, precision 0.8428, recall 0.8392, auc 0.8418
epoch 3601, loss 0.5143, train acc 84.27%, f1 0.8422, precision 0.8424, recall 0.8419, auc 0.8426
epoch 3701, loss 0.3831, train acc 84.43%, f1 0.8435, precision 0.8451, recall 0.8420, auc 0.8443
epoch 3801, loss 0.4185, train acc 84.56%, f1 0.8451, precision 0.8455, recall 0.8447, auc 0.8456
epoch 3901, loss 0.3942, train acc 84.72%, f1 0.8463, precision 0.8488, recall 0.8438, auc 0.8472
epoch 4001, loss 0.2924, train acc 84.80%, f1 0.8476, precision 0.8473, recall 0.8479, auc 0.8480
epoch 4101, loss 0.3217, train acc 84.88%, f1 0.8482, precision 0.8492, recall 0.8472, auc 0.8488
epoch 4201, loss 0.4707, train acc 84.99%, f1 0.8497, precision 0.8488, recall 0.8506, auc 0.8499
epoch 4301, loss 0.3487, train acc 85.09%, f1 0.8507, precision 0.8492, recall 0.8521, auc 0.8509
epoch 4401, loss 0.2662, train acc 85.17%, f1 0.8519, precision 0.8484, recall 0.8554, auc 0.8517
epoch 4501, loss 0.2740, train acc 85.17%, f1 0.8511, precision 0.8521, recall 0.8500, auc 0.8517
epoch 4601, loss 0.4620, train acc 85.34%, f1 0.8529, precision 0.8534, recall 0.8524, auc 0.8534
epoch 4701, loss 0.2259, train acc 85.30%, f1 0.8536, precision 0.8480, recall 0.8592, auc 0.8530
epoch 4801, loss 0.2438, train acc 85.38%, f1 0.8539, precision 0.8506, recall 0.8573, auc 0.8538
epoch 4901, loss 0.3360, train acc 85.47%, f1 0.8547, precision 0.8523, recall 0.8571, auc 0.8547
epoch 5001, loss 0.4213, train acc 85.53%, f1 0.8552, precision 0.8532, recall 0.8573, auc 0.8553
epoch 5101, loss 0.3108, train acc 85.59%, f1 0.8561, precision 0.8525, recall 0.8598, auc 0.8559
epoch 5201, loss 0.3633, train acc 85.65%, f1 0.8561, precision 0.8560, recall 0.8562, auc 0.8565
epoch 5301, loss 0.2904, train acc 85.71%, f1 0.8568, precision 0.8565, recall 0.8570, auc 0.8571
epoch 5401, loss 0.3555, train acc 85.74%, f1 0.8567, precision 0.8582, recall 0.8552, auc 0.8574
epoch 5501, loss 0.2433, train acc 85.84%, f1 0.8581, precision 0.8574, recall 0.8589, auc 0.8584
epoch 5601, loss 0.3257, train acc 85.83%, f1 0.8580, precision 0.8574, recall 0.8586, auc 0.8583
epoch 5701, loss 0.2529, train acc 85.89%, f1 0.8589, precision 0.8564, recall 0.8614, auc 0.8589
epoch 5801, loss 0.4203, train acc 85.92%, f1 0.8592, precision 0.8568, recall 0.8616, auc 0.8592
epoch 5901, loss 0.3889, train acc 85.98%, f1 0.8598, precision 0.8575, recall 0.8622, auc 0.8599
epoch 6001, loss 0.3105, train acc 86.08%, f1 0.8607, precision 0.8589, recall 0.8625, auc 0.8608
epoch 6101, loss 0.2556, train acc 86.10%, f1 0.8601, precision 0.8629, recall 0.8574, auc 0.8610
epoch 6201, loss 0.3903, train acc 86.14%, f1 0.8610, precision 0.8613, recall 0.8607, auc 0.8614
epoch 6301, loss 0.2925, train acc 86.21%, f1 0.8623, precision 0.8591, recall 0.8655, auc 0.8622
epoch 6401, loss 0.2915, train acc 86.29%, f1 0.8627, precision 0.8615, recall 0.8639, auc 0.8629
epoch 6501, loss 0.2234, train acc 86.33%, f1 0.8627, precision 0.8636, recall 0.8618, auc 0.8633
epoch 6601, loss 0.3295, train acc 86.37%, f1 0.8630, precision 0.8655, recall 0.8604, auc 0.8637
epoch 6701, loss 0.2286, train acc 86.40%, f1 0.8635, precision 0.8646, recall 0.8623, auc 0.8640
epoch 6801, loss 0.2891, train acc 86.39%, f1 0.8635, precision 0.8636, recall 0.8633, auc 0.8639
epoch 6901, loss 0.4110, train acc 86.46%, f1 0.8648, precision 0.8609, recall 0.8688, auc 0.8646
epoch 7001, loss 0.2964, train acc 86.55%, f1 0.8654, precision 0.8634, recall 0.8675, auc 0.8655
epoch 7101, loss 0.3338, train acc 86.61%, f1 0.8660, precision 0.8642, recall 0.8678, auc 0.8661
epoch 7201, loss 0.2880, train acc 86.65%, f1 0.8661, precision 0.8666, recall 0.8655, auc 0.8665
epoch 7301, loss 0.3390, train acc 86.68%, f1 0.8663, precision 0.8671, recall 0.8656, auc 0.8668
epoch 7401, loss 0.2620, train acc 86.71%, f1 0.8669, precision 0.8660, recall 0.8678, auc 0.8671
epoch 7501, loss 0.3398, train acc 86.81%, f1 0.8679, precision 0.8668, recall 0.8690, auc 0.8681
epoch 7601, loss 0.1867, train acc 86.76%, f1 0.8672, precision 0.8675, recall 0.8668, auc 0.8676
epoch 7701, loss 0.4411, train acc 86.85%, f1 0.8686, precision 0.8655, recall 0.8716, auc 0.8685
epoch 7801, loss 0.2769, train acc 86.95%, f1 0.8691, precision 0.8695, recall 0.8687, auc 0.8695
epoch 7901, loss 0.3049, train acc 86.96%, f1 0.8696, precision 0.8674, recall 0.8718, auc 0.8696
epoch 8001, loss 0.3522, train acc 86.97%, f1 0.8698, precision 0.8669, recall 0.8727, auc 0.8697
epoch 8101, loss 0.2606, train acc 87.02%, f1 0.8705, precision 0.8665, recall 0.8745, auc 0.8702
epoch 8201, loss 0.3222, train acc 87.10%, f1 0.8703, precision 0.8722, recall 0.8685, auc 0.8710/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.3116, train acc 87.08%, f1 0.8703, precision 0.8707, recall 0.8700, auc 0.8708
epoch 8401, loss 0.3262, train acc 87.20%, f1 0.8715, precision 0.8721, recall 0.8709, auc 0.8720
epoch 8501, loss 0.2863, train acc 87.26%, f1 0.8723, precision 0.8720, recall 0.8727, auc 0.8726
epoch 8601, loss 0.2297, train acc 87.23%, f1 0.8718, precision 0.8729, recall 0.8708, auc 0.8723
epoch 8701, loss 0.1560, train acc 87.38%, f1 0.8738, precision 0.8713, recall 0.8763, auc 0.8738
epoch 8801, loss 0.3213, train acc 87.32%, f1 0.8727, precision 0.8734, recall 0.8720, auc 0.8732
epoch 8901, loss 0.3255, train acc 87.44%, f1 0.8741, precision 0.8735, recall 0.8746, auc 0.8744
epoch 9001, loss 0.2357, train acc 87.48%, f1 0.8744, precision 0.8749, recall 0.8740, auc 0.8748
epoch 9101, loss 0.2088, train acc 87.59%, f1 0.8759, precision 0.8730, recall 0.8789, auc 0.8759
epoch 9201, loss 0.3202, train acc 87.57%, f1 0.8756, precision 0.8738, recall 0.8775, auc 0.8757
epoch 9301, loss 0.3142, train acc 87.61%, f1 0.8762, precision 0.8729, recall 0.8795, auc 0.8761
epoch 9401, loss 0.2010, train acc 87.73%, f1 0.8770, precision 0.8763, recall 0.8777, auc 0.8773
epoch 9501, loss 0.1908, train acc 87.80%, f1 0.8781, precision 0.8751, recall 0.8811, auc 0.8780
epoch 9601, loss 0.1950, train acc 87.78%, f1 0.8772, precision 0.8791, recall 0.8753, auc 0.8778
epoch 9701, loss 0.2287, train acc 87.89%, f1 0.8785, precision 0.8790, recall 0.8779, auc 0.8789
epoch 9801, loss 0.3097, train acc 87.98%, f1 0.8796, precision 0.8789, recall 0.8803, auc 0.8798
epoch 9901, loss 0.2247, train acc 88.01%, f1 0.8797, precision 0.8805, recall 0.8789, auc 0.8801
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_1
./test_pima/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.7079629629629629

the Fscore is 0.6451612903225805

the precision is 0.49504950495049505

the recall is 0.9259259259259259

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_1
----------------------



epoch 1, loss 0.6932, train acc 50.03%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5886, train acc 78.47%, f1 0.7895, precision 0.7720, recall 0.8077, auc 0.7848
epoch 201, loss 0.4502, train acc 81.65%, f1 0.8171, precision 0.8139, recall 0.8204, auc 0.8165
epoch 301, loss 0.3689, train acc 82.70%, f1 0.8266, precision 0.8282, recall 0.8250, auc 0.8270
epoch 401, loss 0.3899, train acc 83.20%, f1 0.8325, precision 0.8295, recall 0.8356, auc 0.8320
epoch 501, loss 0.5062, train acc 83.32%, f1 0.8327, precision 0.8345, recall 0.8309, auc 0.8332
epoch 601, loss 0.2488, train acc 83.42%, f1 0.8346, precision 0.8317, recall 0.8376, auc 0.8342
epoch 701, loss 0.3800, train acc 83.45%, f1 0.8346, precision 0.8337, recall 0.8354, auc 0.8345
epoch 801, loss 0.3077, train acc 83.48%, f1 0.8351, precision 0.8329, recall 0.8374, auc 0.8348
epoch 901, loss 0.3813, train acc 83.49%, f1 0.8351, precision 0.8334, recall 0.8369, auc 0.8349
epoch 1001, loss 0.3811, train acc 83.43%, f1 0.8341, precision 0.8342, recall 0.8340, auc 0.8343
epoch 1101, loss 0.4849, train acc 83.43%, f1 0.8341, precision 0.8343, recall 0.8340, auc 0.8343
epoch 1201, loss 0.3188, train acc 83.41%, f1 0.8343, precision 0.8329, recall 0.8357, auc 0.8341
epoch 1301, loss 0.4867, train acc 83.42%, f1 0.8340, precision 0.8347, recall 0.8333, auc 0.8342
epoch 1401, loss 0.4838, train acc 83.45%, f1 0.8357, precision 0.8295, recall 0.8419, auc 0.8345
epoch 1501, loss 0.3324, train acc 83.41%, f1 0.8344, precision 0.8324, recall 0.8365, auc 0.8341
epoch 1601, loss 0.3787, train acc 83.40%, f1 0.8336, precision 0.8348, recall 0.8324, auc 0.8340
epoch 1701, loss 0.3473, train acc 83.44%, f1 0.8346, precision 0.8330, recall 0.8362, auc 0.8344
epoch 1801, loss 0.4032, train acc 83.44%, f1 0.8341, precision 0.8350, recall 0.8332, auc 0.8344
epoch 1901, loss 0.3991, train acc 83.49%, f1 0.8350, precision 0.8338, recall 0.8363, auc 0.8349
epoch 2001, loss 0.2651, train acc 83.47%, f1 0.8350, precision 0.8331, recall 0.8369, auc 0.8347
epoch 2101, loss 0.2470, train acc 83.48%, f1 0.8346, precision 0.8350, recall 0.8342, auc 0.8348
epoch 2201, loss 0.3820, train acc 83.50%, f1 0.8347, precision 0.8355, recall 0.8340, auc 0.8350
epoch 2301, loss 0.3748, train acc 83.49%, f1 0.8345, precision 0.8360, recall 0.8330, auc 0.8349
epoch 2401, loss 0.4273, train acc 83.56%, f1 0.8360, precision 0.8333, recall 0.8388, auc 0.8356
epoch 2501, loss 0.3075, train acc 83.45%, f1 0.8335, precision 0.8379, recall 0.8291, auc 0.8345
epoch 2601, loss 0.4896, train acc 83.52%, f1 0.8353, precision 0.8341, recall 0.8365, auc 0.8352
epoch 2701, loss 0.3729, train acc 83.52%, f1 0.8347, precision 0.8367, recall 0.8326, auc 0.8352
epoch 2801, loss 0.3152, train acc 83.58%, f1 0.8353, precision 0.8371, recall 0.8336, auc 0.8358
epoch 2901, loss 0.3589, train acc 83.65%, f1 0.8366, precision 0.8353, recall 0.8380, auc 0.8365
epoch 3001, loss 0.3995, train acc 83.73%, f1 0.8374, precision 0.8359, recall 0.8390, auc 0.8373
epoch 3101, loss 0.4899, train acc 83.76%, f1 0.8377, precision 0.8363, recall 0.8392, auc 0.8376
epoch 3201, loss 0.2910, train acc 83.77%, f1 0.8387, precision 0.8330, recall 0.8445, auc 0.8377
epoch 3301, loss 0.3632, train acc 83.93%, f1 0.8394, precision 0.8386, recall 0.8402, auc 0.8393
epoch 3401, loss 0.3431, train acc 84.07%, f1 0.8406, precision 0.8407, recall 0.8405, auc 0.8407
epoch 3501, loss 0.2424, train acc 84.11%, f1 0.8411, precision 0.8404, recall 0.8418, auc 0.8411
epoch 3601, loss 0.3539, train acc 84.14%, f1 0.8407, precision 0.8435, recall 0.8379, auc 0.8414
epoch 3701, loss 0.4122, train acc 84.35%, f1 0.8439, precision 0.8410, recall 0.8469, auc 0.8435
epoch 3801, loss 0.3193, train acc 84.46%, f1 0.8453, precision 0.8410, recall 0.8497, auc 0.8447
epoch 3901, loss 0.2969, train acc 84.53%, f1 0.8451, precision 0.8453, recall 0.8449, auc 0.8453
epoch 4001, loss 0.4076, train acc 84.68%, f1 0.8466, precision 0.8467, recall 0.8466, auc 0.8468
epoch 4101, loss 0.4213, train acc 84.78%, f1 0.8486, precision 0.8435, recall 0.8538, auc 0.8478
epoch 4201, loss 0.2459, train acc 84.91%, f1 0.8493, precision 0.8476, recall 0.8510, auc 0.8491
epoch 4301, loss 0.3211, train acc 84.96%, f1 0.8498, precision 0.8482, recall 0.8514, auc 0.8496
epoch 4401, loss 0.3379, train acc 85.12%, f1 0.8510, precision 0.8519, recall 0.8501, auc 0.8512
epoch 4501, loss 0.2990, train acc 85.31%, f1 0.8528, precision 0.8539, recall 0.8517, auc 0.8531
epoch 4601, loss 0.3751, train acc 85.37%, f1 0.8543, precision 0.8503, recall 0.8583, auc 0.8537
epoch 4701, loss 0.4380, train acc 85.45%, f1 0.8545, precision 0.8540, recall 0.8550, auc 0.8545
epoch 4801, loss 0.3445, train acc 85.54%, f1 0.8559, precision 0.8523, recall 0.8594, auc 0.8554
epoch 4901, loss 0.2242, train acc 85.63%, f1 0.8562, precision 0.8560, recall 0.8565, auc 0.8563
epoch 5001, loss 0.3600, train acc 85.68%, f1 0.8564, precision 0.8578, recall 0.8551, auc 0.8568
epoch 5101, loss 0.2774, train acc 85.67%, f1 0.8567, precision 0.8564, recall 0.8569, auc 0.8567
epoch 5201, loss 0.2060, train acc 85.84%, f1 0.8586, precision 0.8566, recall 0.8605, auc 0.8584
epoch 5301, loss 0.3456, train acc 85.92%, f1 0.8592, precision 0.8584, recall 0.8600, auc 0.8592
epoch 5401, loss 0.2521, train acc 86.00%, f1 0.8603, precision 0.8577, recall 0.8630, auc 0.8600
epoch 5501, loss 0.3147, train acc 86.02%, f1 0.8605, precision 0.8584, recall 0.8625, auc 0.8602
epoch 5601, loss 0.3219, train acc 86.10%, f1 0.8608, precision 0.8618, recall 0.8598, auc 0.8610
epoch 5701, loss 0.3154, train acc 86.16%, f1 0.8619, precision 0.8598, recall 0.8640, auc 0.8616
epoch 5801, loss 0.3064, train acc 86.21%, f1 0.8627, precision 0.8582, recall 0.8673, auc 0.8621
epoch 5901, loss 0.3849, train acc 86.27%, f1 0.8627, precision 0.8621, recall 0.8632, auc 0.8627
epoch 6001, loss 0.3377, train acc 86.35%, f1 0.8632, precision 0.8647, recall 0.8616, auc 0.8635
epoch 6101, loss 0.1753, train acc 86.35%, f1 0.8635, precision 0.8628, recall 0.8643, auc 0.8635
epoch 6201, loss 0.3244, train acc 86.42%, f1 0.8638, precision 0.8657, recall 0.8619, auc 0.8642
epoch 6301, loss 0.3356, train acc 86.50%, f1 0.8649, precision 0.8646, recall 0.8653, auc 0.8650
epoch 6401, loss 0.2776, train acc 86.46%, f1 0.8647, precision 0.8633, recall 0.8661, auc 0.8646
epoch 6501, loss 0.1955, train acc 86.58%, f1 0.8659, precision 0.8650, recall 0.8667, auc 0.8658
epoch 6601, loss 0.3261, train acc 86.53%, f1 0.8653, precision 0.8648, recall 0.8658, auc 0.8653
epoch 6701, loss 0.2431, train acc 86.73%, f1 0.8675, precision 0.8659, recall 0.8690, auc 0.8673
epoch 6801, loss 0.3479, train acc 86.75%, f1 0.8676, precision 0.8668, recall 0.8683, auc 0.8675
epoch 6901, loss 0.2417, train acc 86.67%, f1 0.8667, precision 0.8661, recall 0.8673, auc 0.8667
epoch 7001, loss 0.2286, train acc 86.76%, f1 0.8677, precision 0.8663, recall 0.8691, auc 0.8676
epoch 7101, loss 0.3510, train acc 86.81%, f1 0.8681, precision 0.8675, recall 0.8687, auc 0.8681
epoch 7201, loss 0.2779, train acc 86.84%, f1 0.8685, precision 0.8668, recall 0.8703, auc 0.8684
epoch 7301, loss 0.2091, train acc 86.84%, f1 0.8684, precision 0.8675, recall 0.8694, auc 0.8684
epoch 7401, loss 0.2218, train acc 86.86%, f1 0.8683, precision 0.8699, recall 0.8667, auc 0.8686
epoch 7501, loss 0.2357, train acc 86.97%, f1 0.8700, precision 0.8672, recall 0.8729, auc 0.8697
epoch 7601, loss 0.2841, train acc 87.03%, f1 0.8705, precision 0.8685, recall 0.8725, auc 0.8703
epoch 7701, loss 0.3191, train acc 87.05%, f1 0.8708, precision 0.8679, recall 0.8737, auc 0.8705
epoch 7801, loss 0.2658, train acc 87.07%, f1 0.8708, precision 0.8694, recall 0.8722, auc 0.8707
epoch 7901, loss 0.2887, train acc 87.11%, f1 0.8710, precision 0.8712, recall 0.8708, auc 0.8711
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_1
./test_pima/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.7022222222222222

the Fscore is 0.6415094339622641

the precision is 0.4857142857142857

the recall is 0.9444444444444444

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_1
----------------------



epoch 1, loss 0.6930, train acc 50.12%, f1 0.6678, precision 0.5012, recall 1.0000, auc 0.5000
epoch 101, loss 0.5661, train acc 78.16%, f1 0.7846, precision 0.7757, recall 0.7937, auc 0.7816
epoch 201, loss 0.3599, train acc 81.42%, f1 0.8149, precision 0.8135, recall 0.8163, auc 0.8142
epoch 301, loss 0.4112, train acc 82.67%, f1 0.8267, precision 0.8286, recall 0.8247, auc 0.8267
epoch 401, loss 0.4473, train acc 83.17%, f1 0.8314, precision 0.8349, recall 0.8280, auc 0.8317
epoch 501, loss 0.4401, train acc 83.29%, f1 0.8324, precision 0.8372, recall 0.8276, auc 0.8329
epoch 601, loss 0.3418, train acc 83.43%, f1 0.8341, precision 0.8372, recall 0.8310, auc 0.8343
epoch 701, loss 0.3202, train acc 83.40%, f1 0.8336, precision 0.8378, recall 0.8294, auc 0.8340
epoch 801, loss 0.3546, train acc 83.41%, f1 0.8339, precision 0.8371, recall 0.8308, auc 0.8342
epoch 901, loss 0.2794, train acc 83.43%, f1 0.8338, precision 0.8387, recall 0.8289, auc 0.8344
epoch 1001, loss 0.5051, train acc 83.38%, f1 0.8333, precision 0.8381, recall 0.8285, auc 0.8338
epoch 1101, loss 0.3759, train acc 83.46%, f1 0.8338, precision 0.8398, recall 0.8280, auc 0.8346
epoch 1201, loss 0.3295, train acc 83.45%, f1 0.8342, precision 0.8375, recall 0.8310, auc 0.8345
epoch 1301, loss 0.3758, train acc 83.42%, f1 0.8339, precision 0.8376, recall 0.8302, auc 0.8342
epoch 1401, loss 0.3819, train acc 83.42%, f1 0.8338, precision 0.8382, recall 0.8294, auc 0.8343
epoch 1501, loss 0.4655, train acc 83.35%, f1 0.8338, precision 0.8347, recall 0.8328, auc 0.8335
epoch 1601, loss 0.3400, train acc 83.45%, f1 0.8343, precision 0.8374, recall 0.8313, auc 0.8346
epoch 1701, loss 0.4493, train acc 83.47%, f1 0.8341, precision 0.8394, recall 0.8289, auc 0.8347
epoch 1801, loss 0.5810, train acc 83.49%, f1 0.8349, precision 0.8371, recall 0.8327, auc 0.8349
epoch 1901, loss 0.3676, train acc 83.43%, f1 0.8347, precision 0.8350, recall 0.8343, auc 0.8343
epoch 2001, loss 0.4864, train acc 83.48%, f1 0.8350, precision 0.8361, recall 0.8339, auc 0.8348
epoch 2101, loss 0.3303, train acc 83.48%, f1 0.8346, precision 0.8378, recall 0.8314, auc 0.8348
epoch 2201, loss 0.4304, train acc 83.52%, f1 0.8351, precision 0.8378, recall 0.8325, auc 0.8353
epoch 2301, loss 0.3471, train acc 83.49%, f1 0.8340, precision 0.8406, recall 0.8275, auc 0.8349
epoch 2401, loss 0.4257, train acc 83.55%, f1 0.8354, precision 0.8380, recall 0.8329, auc 0.8355
epoch 2501, loss 0.2587, train acc 83.56%, f1 0.8360, precision 0.8362, recall 0.8358, auc 0.8356
epoch 2601, loss 0.4136, train acc 83.53%, f1 0.8353, precision 0.8370, recall 0.8337, auc 0.8353
epoch 2701, loss 0.4681, train acc 83.58%, f1 0.8355, precision 0.8394, recall 0.8316, auc 0.8359
epoch 2801, loss 0.3933, train acc 83.72%, f1 0.8373, precision 0.8387, recall 0.8359, auc 0.8372
epoch 2901, loss 0.3439, train acc 83.70%, f1 0.8369, precision 0.8397, recall 0.8341, auc 0.8370
epoch 3001, loss 0.4122, train acc 83.82%, f1 0.8379, precision 0.8417, recall 0.8342, auc 0.8383
epoch 3101, loss 0.3728, train acc 83.89%, f1 0.8386, precision 0.8420, recall 0.8352, auc 0.8389
epoch 3201, loss 0.3819, train acc 83.93%, f1 0.8388, precision 0.8433, recall 0.8343, auc 0.8393
epoch 3301, loss 0.3766, train acc 84.02%, f1 0.8407, precision 0.8402, recall 0.8412, auc 0.8402
epoch 3401, loss 0.3356, train acc 84.16%, f1 0.8411, precision 0.8460, recall 0.8362, auc 0.8416
epoch 3501, loss 0.3883, train acc 84.27%, f1 0.8426, precision 0.8451, recall 0.8401, auc 0.8427
epoch 3601, loss 0.3337, train acc 84.38%, f1 0.8434, precision 0.8478, recall 0.8390, auc 0.8438
epoch 3701, loss 0.3851, train acc 84.48%, f1 0.8441, precision 0.8500, recall 0.8384, auc 0.8448
epoch 3801, loss 0.3692, train acc 84.62%, f1 0.8458, precision 0.8500, recall 0.8416, auc 0.8462
epoch 3901, loss 0.3576, train acc 84.75%, f1 0.8475, precision 0.8496, recall 0.8454, auc 0.8475
epoch 4001, loss 0.2874, train acc 84.89%, f1 0.8482, precision 0.8540, recall 0.8426, auc 0.8489
epoch 4101, loss 0.4389, train acc 84.98%, f1 0.8494, precision 0.8535, recall 0.8454, auc 0.8498
epoch 4201, loss 0.3667, train acc 84.98%, f1 0.8498, precision 0.8515, recall 0.8482, auc 0.8498
epoch 4301, loss 0.3213, train acc 85.13%, f1 0.8513, precision 0.8535, recall 0.8490, auc 0.8513
epoch 4401, loss 0.2904, train acc 85.21%, f1 0.8521, precision 0.8543, recall 0.8500, auc 0.8522
epoch 4501, loss 0.2805, train acc 85.33%, f1 0.8531, precision 0.8564, recall 0.8499, auc 0.8533
epoch 4601, loss 0.3240, train acc 85.46%, f1 0.8544, precision 0.8579, recall 0.8509, auc 0.8546
epoch 4701, loss 0.3294, train acc 85.54%, f1 0.8552, precision 0.8586, recall 0.8518, auc 0.8554
epoch 4801, loss 0.2848, train acc 85.58%, f1 0.8560, precision 0.8571, recall 0.8548, auc 0.8558
epoch 4901, loss 0.3443, train acc 85.73%, f1 0.8570, precision 0.8611, recall 0.8530, auc 0.8573
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_1
./test_pima/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6772222222222222

the Fscore is 0.6219512195121951

the precision is 0.4636363636363636

the recall is 0.9444444444444444

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_1
----------------------



epoch 1, loss 0.6934, train acc 50.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5837, train acc 77.90%, f1 0.7781, precision 0.7771, recall 0.7792, auc 0.7790
epoch 201, loss 0.3895, train acc 81.20%, f1 0.8118, precision 0.8083, recall 0.8154, auc 0.8120
epoch 301, loss 0.3384, train acc 82.65%, f1 0.8263, precision 0.8233, recall 0.8293, auc 0.8265
epoch 401, loss 0.4335, train acc 83.15%, f1 0.8311, precision 0.8286, recall 0.8337, auc 0.8315
epoch 501, loss 0.2622, train acc 83.38%, f1 0.8333, precision 0.8317, recall 0.8349, auc 0.8338
epoch 601, loss 0.3918, train acc 83.47%, f1 0.8341, precision 0.8329, recall 0.8352, auc 0.8347
epoch 701, loss 0.4017, train acc 83.49%, f1 0.8338, precision 0.8349, recall 0.8328, auc 0.8349
epoch 801, loss 0.4019, train acc 83.42%, f1 0.8338, precision 0.8318, recall 0.8358, auc 0.8342
epoch 901, loss 0.2378, train acc 83.44%, f1 0.8339, precision 0.8322, recall 0.8356, auc 0.8344
epoch 1001, loss 0.2938, train acc 83.44%, f1 0.8337, precision 0.8329, recall 0.8345, auc 0.8344
epoch 1101, loss 0.4627, train acc 83.48%, f1 0.8337, precision 0.8351, recall 0.8323, auc 0.8348
epoch 1201, loss 0.3766, train acc 83.45%, f1 0.8346, precision 0.8302, recall 0.8391, auc 0.8346
epoch 1301, loss 0.4095, train acc 83.37%, f1 0.8338, precision 0.8291, recall 0.8385, auc 0.8337
epoch 1401, loss 0.4252, train acc 83.46%, f1 0.8337, precision 0.8341, recall 0.8333, auc 0.8346
epoch 1501, loss 0.4196, train acc 83.46%, f1 0.8335, precision 0.8349, recall 0.8320, auc 0.8346
epoch 1601, loss 0.4931, train acc 83.47%, f1 0.8340, precision 0.8334, recall 0.8345, auc 0.8347
epoch 1701, loss 0.4174, train acc 83.50%, f1 0.8340, precision 0.8350, recall 0.8329, auc 0.8350
epoch 1801, loss 0.4021, train acc 83.48%, f1 0.8342, precision 0.8332, recall 0.8352, auc 0.8348
epoch 1901, loss 0.3550, train acc 83.46%, f1 0.8340, precision 0.8330, recall 0.8351, auc 0.8346
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_1
./test_pima/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.5907407407407407

the Fscore is 0.5668449197860963

the precision is 0.39849624060150374

the recall is 0.9814814814814815

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_1
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6318, train acc 78.22%, f1 0.7788, precision 0.7910, recall 0.7669, auc 0.7822
epoch 201, loss 0.4698, train acc 80.66%, f1 0.8064, precision 0.8071, recall 0.8058, auc 0.8066
epoch 301, loss 0.3736, train acc 81.89%, f1 0.8190, precision 0.8187, recall 0.8193, auc 0.8189
epoch 401, loss 0.3971, train acc 82.79%, f1 0.8280, precision 0.8273, recall 0.8287, auc 0.8279
epoch 501, loss 0.3665, train acc 83.19%, f1 0.8319, precision 0.8315, recall 0.8323, auc 0.8319
epoch 601, loss 0.4502, train acc 83.41%, f1 0.8342, precision 0.8337, recall 0.8348, auc 0.8341
epoch 701, loss 0.2850, train acc 83.48%, f1 0.8350, precision 0.8343, recall 0.8356, auc 0.8348
epoch 801, loss 0.3639, train acc 83.52%, f1 0.8353, precision 0.8348, recall 0.8357, auc 0.8352
epoch 901, loss 0.4333, train acc 83.46%, f1 0.8347, precision 0.8341, recall 0.8352, auc 0.8346
epoch 1001, loss 0.3050, train acc 83.47%, f1 0.8348, precision 0.8346, recall 0.8350, auc 0.8347
epoch 1101, loss 0.4308, train acc 83.44%, f1 0.8345, precision 0.8343, recall 0.8347, auc 0.8344
epoch 1201, loss 0.4238, train acc 83.45%, f1 0.8346, precision 0.8344, recall 0.8348, auc 0.8345
epoch 1301, loss 0.3822, train acc 83.50%, f1 0.8350, precision 0.8350, recall 0.8349, auc 0.8350
epoch 1401, loss 0.3767, train acc 83.49%, f1 0.8349, precision 0.8348, recall 0.8350, auc 0.8349
epoch 1501, loss 0.3966, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8349, auc 0.8349
epoch 1601, loss 0.2847, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 1701, loss 0.4548, train acc 83.52%, f1 0.8353, precision 0.8352, recall 0.8353, auc 0.8352
epoch 1801, loss 0.3969, train acc 83.50%, f1 0.8350, precision 0.8350, recall 0.8350, auc 0.8350
epoch 1901, loss 0.3345, train acc 83.50%, f1 0.8350, precision 0.8351, recall 0.8349, auc 0.8350
epoch 2001, loss 0.4399, train acc 83.45%, f1 0.8345, precision 0.8345, recall 0.8345, auc 0.8345
epoch 2101, loss 0.3984, train acc 83.44%, f1 0.8344, precision 0.8344, recall 0.8344, auc 0.8344
epoch 2201, loss 0.4272, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8351, auc 0.8351
epoch 2301, loss 0.3811, train acc 83.52%, f1 0.8352, precision 0.8351, recall 0.8353, auc 0.8352
epoch 2401, loss 0.2988, train acc 83.50%, f1 0.8350, precision 0.8351, recall 0.8350, auc 0.8350
epoch 2501, loss 0.3971, train acc 83.45%, f1 0.8344, precision 0.8346, recall 0.8342, auc 0.8345
epoch 2601, loss 0.3709, train acc 83.49%, f1 0.8349, precision 0.8350, recall 0.8348, auc 0.8349
epoch 2701, loss 0.5268, train acc 83.46%, f1 0.8345, precision 0.8349, recall 0.8341, auc 0.8346
epoch 2801, loss 0.3876, train acc 83.47%, f1 0.8346, precision 0.8349, recall 0.8344, auc 0.8347
epoch 2901, loss 0.4171, train acc 83.48%, f1 0.8348, precision 0.8350, recall 0.8346, auc 0.8348
epoch 3001, loss 0.3239, train acc 83.48%, f1 0.8348, precision 0.8350, recall 0.8346, auc 0.8348
epoch 3101, loss 0.3212, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8348, auc 0.8349
epoch 3201, loss 0.2743, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 3301, loss 0.3711, train acc 83.50%, f1 0.8349, precision 0.8354, recall 0.8344, auc 0.8350
epoch 3401, loss 0.5158, train acc 83.49%, f1 0.8348, precision 0.8351, recall 0.8346, auc 0.8349
epoch 3501, loss 0.3493, train acc 83.45%, f1 0.8344, precision 0.8348, recall 0.8340, auc 0.8345
epoch 3601, loss 0.4331, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8350, auc 0.8351
epoch 3701, loss 0.3254, train acc 83.52%, f1 0.8352, precision 0.8353, recall 0.8350, auc 0.8352
epoch 3801, loss 0.3150, train acc 83.53%, f1 0.8352, precision 0.8357, recall 0.8348, auc 0.8353
epoch 3901, loss 0.3384, train acc 83.50%, f1 0.8350, precision 0.8353, recall 0.8346, auc 0.8350
epoch 4001, loss 0.4034, train acc 83.57%, f1 0.8356, precision 0.8362, recall 0.8349, auc 0.8357
epoch 4101, loss 0.3869, train acc 83.57%, f1 0.8357, precision 0.8358, recall 0.8355, auc 0.8357
epoch 4201, loss 0.3757, train acc 83.57%, f1 0.8357, precision 0.8360, recall 0.8353, auc 0.8357
epoch 4301, loss 0.2875, train acc 83.58%, f1 0.8357, precision 0.8361, recall 0.8354, auc 0.8358
epoch 4401, loss 0.3414, train acc 83.62%, f1 0.8361, precision 0.8366, recall 0.8356, auc 0.8362
epoch 4501, loss 0.4509, train acc 83.61%, f1 0.8360, precision 0.8365, recall 0.8356, auc 0.8361
epoch 4601, loss 0.3520, train acc 83.59%, f1 0.8358, precision 0.8366, recall 0.8350, auc 0.8359
epoch 4701, loss 0.3569, train acc 83.64%, f1 0.8364, precision 0.8366, recall 0.8362, auc 0.8364
epoch 4801, loss 0.2721, train acc 83.60%, f1 0.8360, precision 0.8361, recall 0.8358, auc 0.8360
epoch 4901, loss 0.3448, train acc 83.66%, f1 0.8366, precision 0.8366, recall 0.8365, auc 0.8366
epoch 5001, loss 0.3554, train acc 83.73%, f1 0.8373, precision 0.8374, recall 0.8372, auc 0.8373
epoch 5101, loss 0.3291, train acc 83.70%, f1 0.8370, precision 0.8371, recall 0.8369, auc 0.8370
epoch 5201, loss 0.4205, train acc 83.72%, f1 0.8372, precision 0.8373, recall 0.8371, auc 0.8372
epoch 5301, loss 0.4740, train acc 83.76%, f1 0.8374, precision 0.8382, recall 0.8367, auc 0.8376
epoch 5401, loss 0.3402, train acc 83.83%, f1 0.8382, precision 0.8385, recall 0.8379, auc 0.8383
epoch 5501, loss 0.3880, train acc 83.80%, f1 0.8380, precision 0.8381, recall 0.8379, auc 0.8380
epoch 5601, loss 0.2607, train acc 83.85%, f1 0.8385, precision 0.8384, recall 0.8387, auc 0.8385
epoch 5701, loss 0.3364, train acc 83.86%, f1 0.8385, precision 0.8388, recall 0.8382, auc 0.8386
epoch 5801, loss 0.3646, train acc 83.88%, f1 0.8388, precision 0.8391, recall 0.8385, auc 0.8388
epoch 5901, loss 0.3848, train acc 83.91%, f1 0.8390, precision 0.8392, recall 0.8388, auc 0.8391
epoch 6001, loss 0.2534, train acc 83.97%, f1 0.8396, precision 0.8400, recall 0.8393, auc 0.8397
epoch 6101, loss 0.2803, train acc 83.97%, f1 0.8397, precision 0.8399, recall 0.8395, auc 0.8397
epoch 6201, loss 0.3676, train acc 84.05%, f1 0.8405, precision 0.8406, recall 0.8404, auc 0.8405
epoch 6301, loss 0.3943, train acc 84.05%, f1 0.8405, precision 0.8405, recall 0.8406, auc 0.8405
epoch 6401, loss 0.3535, train acc 84.09%, f1 0.8409, precision 0.8407, recall 0.8411, auc 0.8409
epoch 6501, loss 0.3599, train acc 84.09%, f1 0.8408, precision 0.8411, recall 0.8405, auc 0.8409
epoch 6601, loss 0.3466, train acc 84.00%, f1 0.8400, precision 0.8398, recall 0.8401, auc 0.8400
epoch 6701, loss 0.3503, train acc 84.14%, f1 0.8415, precision 0.8410, recall 0.8421, auc 0.8414
epoch 6801, loss 0.3788, train acc 84.13%, f1 0.8414, precision 0.8412, recall 0.8416, auc 0.8413
epoch 6901, loss 0.3630, train acc 84.19%, f1 0.8419, precision 0.8416, recall 0.8422, auc 0.8419
epoch 7001, loss 0.3938, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8423, auc 0.8423
epoch 7101, loss 0.3693, train acc 84.23%, f1 0.8425, precision 0.8415, recall 0.8435, auc 0.8423
epoch 7201, loss 0.3418, train acc 84.30%, f1 0.8431, precision 0.8428, recall 0.8433, auc 0.8430
epoch 7301, loss 0.3116, train acc 84.29%, f1 0.8429, precision 0.8431, recall 0.8427, auc 0.8429
epoch 7401, loss 0.4165, train acc 84.30%, f1 0.8430, precision 0.8433, recall 0.8426, auc 0.8430
epoch 7501, loss 0.4156, train acc 84.29%, f1 0.8429, precision 0.8430, recall 0.8427, auc 0.8429
epoch 7601, loss 0.4670, train acc 84.38%, f1 0.8438, precision 0.8438, recall 0.8438, auc 0.8438
epoch 7701, loss 0.2715, train acc 84.39%, f1 0.8439, precision 0.8438, recall 0.8440, auc 0.8439
epoch 7801, loss 0.3485, train acc 84.33%, f1 0.8433, precision 0.8434, recall 0.8433, auc 0.8433
epoch 7901, loss 0.3922, train acc 84.42%, f1 0.8441, precision 0.8444, recall 0.8438, auc 0.8442
epoch 8001, loss 0.3861, train acc 84.43%, f1 0.8443, precision 0.8443, recall 0.8443, auc 0.8443
epoch 8101, loss 0.4142, train acc 84.49%, f1 0.8448, precision 0.8450, recall 0.8447, auc 0.8449
epoch 8201, loss 0.3548, train acc 84.51%, f1 0.8451, precision 0.8451, recall 0.8451, auc 0.8451
epoch 8301, loss 0.4771, train acc 84.45%, f1 0.8445, precision 0.8442, recall 0.8448, auc 0.8445
epoch 8401, loss 0.3751, train acc 84.55%, f1 0.8455, precision 0.8457, recall 0.8454, auc 0.8455
epoch 8501, loss 0.3313, train acc 84.53%, f1 0.8453, precision 0.8457, recall 0.8448, auc 0.8453
epoch 8601, loss 0.3201, train acc 84.51%, f1 0.8451, precision 0.8450, recall 0.8451, auc 0.8451
epoch 8701, loss 0.2681, train acc 84.56%, f1 0.8456, precision 0.8456, recall 0.8457, auc 0.8456
epoch 8801, loss 0.4679, train acc 84.58%, f1 0.8458, precision 0.8459, recall 0.8457, auc 0.8458
epoch 8901, loss 0.4363, train acc 84.60%, f1 0.8461, precision 0.8458, recall 0.8464, auc 0.8460
epoch 9001, loss 0.3218, train acc 84.63%, f1 0.8462, precision 0.8468, recall 0.8456, auc 0.8463
epoch 9101, loss 0.2911, train acc 84.66%, f1 0.8465, precision 0.8468, recall 0.8463, auc 0.8466
epoch 9201, loss 0.4568, train acc 84.65%, f1 0.8464, precision 0.8467, recall 0.8461, auc 0.8465
epoch 9301, loss 0.2795, train acc 84.72%, f1 0.8472, precision 0.8474, recall 0.8470, auc 0.8472
epoch 9401, loss 0.3410, train acc 84.70%, f1 0.8470, precision 0.8470, recall 0.8471, auc 0.8470
epoch 9501, loss 0.3389, train acc 84.72%, f1 0.8471, precision 0.8473, recall 0.8469, auc 0.8472
epoch 9601, loss 0.3403, train acc 84.71%, f1 0.8472, precision 0.8469, recall 0.8474, auc 0.8471
epoch 9701, loss 0.2737, train acc 84.74%, f1 0.8474, precision 0.8474, recall 0.8475, auc 0.8474
epoch 9801, loss 0.3903, train acc 84.73%, f1 0.8472, precision 0.8473, recall 0.8472, auc 0.8473
epoch 9901, loss 0.3200, train acc 84.72%, f1 0.8472, precision 0.8471, recall 0.8473, auc 0.8472
epoch 10001, loss 0.3169, train acc 84.81%, f1 0.8481, precision 0.8479, recall 0.8483, auc 0.8481
epoch 10101, loss 0.2729, train acc 84.79%, f1 0.8479, precision 0.8483, recall 0.8475, auc 0.8479
epoch 10201, loss 0.4467, train acc 84.78%, f1 0.8479, precision 0.8476, recall 0.8482, auc 0.8478
epoch 10301, loss 0.2778, train acc 84.77%, f1 0.8476, precision 0.8482, recall 0.8471, auc 0.8477
epoch 10401, loss 0.3181, train acc 84.79%, f1 0.8479, precision 0.8479, recall 0.8478, auc 0.8479
epoch 10501, loss 0.2460, train acc 84.80%, f1 0.8480, precision 0.8480, recall 0.8479, auc 0.8480
epoch 10601, loss 0.4448, train acc 84.80%, f1 0.8480, precision 0.8479, recall 0.8481, auc 0.8480
epoch 10701, loss 0.4202, train acc 84.84%, f1 0.8485, precision 0.8483, recall 0.8486, auc 0.8484
epoch 10801, loss 0.3492, train acc 84.85%, f1 0.8485, precision 0.8488, recall 0.8481, auc 0.8485
epoch 10901, loss 0.3904, train acc 84.80%, f1 0.8481, precision 0.8480, recall 0.8481, auc 0.8480
epoch 11001, loss 0.3243, train acc 84.88%, f1 0.8489, precision 0.8485, recall 0.8493, auc 0.8488
epoch 11101, loss 0.4330, train acc 84.85%, f1 0.8485, precision 0.8484, recall 0.8487, auc 0.8485
epoch 11201, loss 0.3252, train acc 84.83%, f1 0.8483, precision 0.8484, recall 0.8482, auc 0.8483
epoch 11301, loss 0.2988, train acc 84.79%, f1 0.8479, precision 0.8478, recall 0.8479, auc 0.8479
epoch 11401, loss 0.3994, train acc 84.84%, f1 0.8484, precision 0.8483, recall 0.8485, auc 0.8484
epoch 11501, loss 0.3671, train acc 84.94%, f1 0.8494, precision 0.8492, recall 0.8496, auc 0.8494
epoch 11601, loss 0.3642, train acc 84.92%, f1 0.8492, precision 0.8493, recall 0.8491, auc 0.8492
epoch 11701, loss 0.3307, train acc 84.96%, f1 0.8496, precision 0.8494, recall 0.8498, auc 0.8496
epoch 11801, loss 0.3487, train acc 84.94%, f1 0.8493, precision 0.8495, recall 0.8492, auc 0.8494
epoch 11901, loss 0.3210, train acc 84.94%, f1 0.8493, precision 0.8496, recall 0.8491, auc 0.8494
epoch 12001, loss 0.3809, train acc 84.92%, f1 0.8493, precision 0.8488, recall 0.8499, auc 0.8492
epoch 12101, loss 0.3446, train acc 84.98%, f1 0.8498, precision 0.8501, recall 0.8495, auc 0.8498
epoch 12201, loss 0.3190, train acc 85.02%, f1 0.8502, precision 0.8500, recall 0.8504, auc 0.8502
epoch 12301, loss 0.3269, train acc 84.99%, f1 0.8499, precision 0.8499, recall 0.8500, auc 0.8499
epoch 12401, loss 0.3677, train acc 84.96%, f1 0.8496, precision 0.8494, recall 0.8498, auc 0.8496
epoch 12501, loss 0.3164, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8505, auc 0.8505
epoch 12601, loss 0.3184, train acc 85.04%, f1 0.8504, precision 0.8503, recall 0.8505, auc 0.8504
epoch 12701, loss 0.4137, train acc 85.01%, f1 0.8502, precision 0.8501, recall 0.8503, auc 0.8501
epoch 12801, loss 0.4620, train acc 85.09%, f1 0.8508, precision 0.8512, recall 0.8505, auc 0.8509
epoch 12901, loss 0.4075, train acc 85.05%, f1 0.8506, precision 0.8504, recall 0.8508, auc 0.8505
epoch 13001, loss 0.3285, train acc 85.05%, f1 0.8505, precision 0.8504, recall 0.8505, auc 0.8505
epoch 13101, loss 0.3726, train acc 85.05%, f1 0.8505, precision 0.8504, recall 0.8506, auc 0.8505
epoch 13201, loss 0.3045, train acc 85.16%, f1 0.8516, precision 0.8516, recall 0.8516, auc 0.8516
epoch 13301, loss 0.3229, train acc 85.06%, f1 0.8506, precision 0.8506, recall 0.8506, auc 0.8506
epoch 13401, loss 0.2335, train acc 85.09%, f1 0.8509, precision 0.8508, recall 0.8511, auc 0.8509
epoch 13501, loss 0.2713, train acc 85.11%, f1 0.8511, precision 0.8510, recall 0.8512, auc 0.8511
epoch 13601, loss 0.3192, train acc 85.12%, f1 0.8511, precision 0.8514, recall 0.8509, auc 0.8512
epoch 13701, loss 0.3758, train acc 85.04%, f1 0.8504, precision 0.8502, recall 0.8507, auc 0.8504
epoch 13801, loss 0.3667, train acc 85.17%, f1 0.8517, precision 0.8519, recall 0.8515, auc 0.8517
epoch 13901, loss 0.2875, train acc 85.17%, f1 0.8517, precision 0.8515, recall 0.8518, auc 0.8517
epoch 14001, loss 0.2370, train acc 85.12%, f1 0.8512, precision 0.8510, recall 0.8514, auc 0.8512
epoch 14101, loss 0.3948, train acc 85.22%, f1 0.8522, precision 0.8521, recall 0.8523, auc 0.8522
epoch 14201, loss 0.3691, train acc 85.17%, f1 0.8517, precision 0.8516, recall 0.8518, auc 0.8517
epoch 14301, loss 0.2285, train acc 85.10%, f1 0.8510, precision 0.8510, recall 0.8511, auc 0.8510
epoch 14401, loss 0.3812, train acc 85.14%, f1 0.8514, precision 0.8514, recall 0.8513, auc 0.8514
epoch 14501, loss 0.2917, train acc 85.19%, f1 0.8518, precision 0.8519, recall 0.8517, auc 0.8519
epoch 14601, loss 0.3449, train acc 85.17%, f1 0.8518, precision 0.8517, recall 0.8518, auc 0.8517
epoch 14701, loss 0.3086, train acc 85.25%, f1 0.8525, precision 0.8526, recall 0.8523, auc 0.8525
epoch 14801, loss 0.1850, train acc 85.19%, f1 0.8519, precision 0.8520, recall 0.8518, auc 0.8519
epoch 14901, loss 0.3331, train acc 85.21%, f1 0.8521, precision 0.8520, recall 0.8522, auc 0.8521
epoch 15001, loss 0.2910, train acc 85.24%, f1 0.8524, precision 0.8523, recall 0.8525, auc 0.8524
epoch 15101, loss 0.3278, train acc 85.21%, f1 0.8521, precision 0.8518, recall 0.8524, auc 0.8521
epoch 15201, loss 0.2694, train acc 85.21%, f1 0.8521, precision 0.8521, recall 0.8521, auc 0.8521
epoch 15301, loss 0.3599, train acc 85.24%, f1 0.8524, precision 0.8524, recall 0.8524, auc 0.8524
epoch 15401, loss 0.4079, train acc 85.23%, f1 0.8523, precision 0.8520, recall 0.8527, auc 0.8523
epoch 15501, loss 0.2523, train acc 85.20%, f1 0.8520, precision 0.8522, recall 0.8518, auc 0.8520
epoch 15601, loss 0.2748, train acc 85.27%, f1 0.8527, precision 0.8527, recall 0.8527, auc 0.8527
epoch 15701, loss 0.3401, train acc 85.24%, f1 0.8524, precision 0.8524, recall 0.8523, auc 0.8524
epoch 15801, loss 0.2943, train acc 85.26%, f1 0.8526, precision 0.8525, recall 0.8526, auc 0.8526
epoch 15901, loss 0.3866, train acc 85.20%, f1 0.8520, precision 0.8522, recall 0.8517, auc 0.8520
epoch 16001, loss 0.2964, train acc 85.28%, f1 0.8528, precision 0.8529, recall 0.8528, auc 0.8528
epoch 16101, loss 0.3541, train acc 85.29%, f1 0.8529, precision 0.8526, recall 0.8532, auc 0.8529
epoch 16201, loss 0.4191, train acc 85.33%, f1 0.8533, precision 0.8534, recall 0.8533, auc 0.8533
epoch 16301, loss 0.3372, train acc 85.33%, f1 0.8534, precision 0.8531, recall 0.8536, auc 0.8533
epoch 16401, loss 0.2430, train acc 85.26%, f1 0.8525, precision 0.8527, recall 0.8523, auc 0.8526
epoch 16501, loss 0.3165, train acc 85.32%, f1 0.8532, precision 0.8533, recall 0.8530, auc 0.8532/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.3160, train acc 85.31%, f1 0.8531, precision 0.8531, recall 0.8531, auc 0.8531
epoch 16701, loss 0.3740, train acc 85.32%, f1 0.8532, precision 0.8534, recall 0.8530, auc 0.8532
epoch 16801, loss 0.3725, train acc 85.30%, f1 0.8530, precision 0.8532, recall 0.8527, auc 0.8530
epoch 16901, loss 0.3340, train acc 85.25%, f1 0.8526, precision 0.8524, recall 0.8528, auc 0.8525
epoch 17001, loss 0.2642, train acc 85.24%, f1 0.8524, precision 0.8523, recall 0.8526, auc 0.8524
epoch 17101, loss 0.2445, train acc 85.27%, f1 0.8528, precision 0.8526, recall 0.8529, auc 0.8527
epoch 17201, loss 0.4116, train acc 85.32%, f1 0.8533, precision 0.8528, recall 0.8539, auc 0.8532
epoch 17301, loss 0.3016, train acc 85.27%, f1 0.8527, precision 0.8528, recall 0.8526, auc 0.8527
epoch 17401, loss 0.3349, train acc 85.33%, f1 0.8534, precision 0.8532, recall 0.8535, auc 0.8533
epoch 17501, loss 0.2794, train acc 85.33%, f1 0.8533, precision 0.8534, recall 0.8532, auc 0.8533
epoch 17601, loss 0.2685, train acc 85.37%, f1 0.8537, precision 0.8537, recall 0.8537, auc 0.8537
epoch 17701, loss 0.4058, train acc 85.37%, f1 0.8537, precision 0.8536, recall 0.8539, auc 0.8537
epoch 17801, loss 0.3611, train acc 85.37%, f1 0.8537, precision 0.8539, recall 0.8535, auc 0.8537
epoch 17901, loss 0.4340, train acc 85.39%, f1 0.8539, precision 0.8540, recall 0.8537, auc 0.8539
epoch 18001, loss 0.3096, train acc 85.37%, f1 0.8537, precision 0.8536, recall 0.8538, auc 0.8537
epoch 18101, loss 0.3270, train acc 85.40%, f1 0.8541, precision 0.8540, recall 0.8541, auc 0.8540
epoch 18201, loss 0.2806, train acc 85.39%, f1 0.8539, precision 0.8539, recall 0.8539, auc 0.8539
epoch 18301, loss 0.3529, train acc 85.40%, f1 0.8541, precision 0.8539, recall 0.8543, auc 0.8540
epoch 18401, loss 0.3840, train acc 85.36%, f1 0.8536, precision 0.8537, recall 0.8534, auc 0.8536
epoch 18501, loss 0.3529, train acc 85.36%, f1 0.8536, precision 0.8536, recall 0.8535, auc 0.8536
epoch 18601, loss 0.2497, train acc 85.34%, f1 0.8534, precision 0.8533, recall 0.8535, auc 0.8534
epoch 18701, loss 0.3449, train acc 85.36%, f1 0.8536, precision 0.8534, recall 0.8538, auc 0.8536
epoch 18801, loss 0.3980, train acc 85.40%, f1 0.8540, precision 0.8540, recall 0.8539, auc 0.8540
epoch 18901, loss 0.2357, train acc 85.38%, f1 0.8538, precision 0.8538, recall 0.8538, auc 0.8538
epoch 19001, loss 0.3035, train acc 85.35%, f1 0.8535, precision 0.8535, recall 0.8534, auc 0.8535
epoch 19101, loss 0.2847, train acc 85.40%, f1 0.8540, precision 0.8539, recall 0.8541, auc 0.8540
epoch 19201, loss 0.2545, train acc 85.43%, f1 0.8543, precision 0.8542, recall 0.8544, auc 0.8543
epoch 19301, loss 0.4404, train acc 85.41%, f1 0.8541, precision 0.8541, recall 0.8542, auc 0.8541
epoch 19401, loss 0.2379, train acc 85.43%, f1 0.8543, precision 0.8543, recall 0.8543, auc 0.8543
epoch 19501, loss 0.3855, train acc 85.37%, f1 0.8538, precision 0.8536, recall 0.8539, auc 0.8537
epoch 19601, loss 0.3637, train acc 85.43%, f1 0.8544, precision 0.8538, recall 0.8549, auc 0.8543
epoch 19701, loss 0.2500, train acc 85.44%, f1 0.8544, precision 0.8544, recall 0.8544, auc 0.8544
epoch 19801, loss 0.2873, train acc 85.43%, f1 0.8543, precision 0.8544, recall 0.8543, auc 0.8543
epoch 19901, loss 0.3285, train acc 85.46%, f1 0.8546, precision 0.8547, recall 0.8545, auc 0.8546
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_1
./test_pima/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6422222222222222

the Fscore is 0.5964912280701755

the precision is 0.4358974358974359

the recall is 0.9444444444444444

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6211, train acc 77.67%, f1 0.7676, precision 0.8004, recall 0.7373, auc 0.7767
epoch 201, loss 0.4590, train acc 80.23%, f1 0.8012, precision 0.8059, recall 0.7965, auc 0.8023
epoch 301, loss 0.4749, train acc 81.95%, f1 0.8196, precision 0.8190, recall 0.8201, auc 0.8195
epoch 401, loss 0.3941, train acc 82.68%, f1 0.8273, precision 0.8251, recall 0.8294, auc 0.8268
epoch 501, loss 0.3287, train acc 83.09%, f1 0.8315, precision 0.8284, recall 0.8347, auc 0.8309
epoch 601, loss 0.4844, train acc 83.39%, f1 0.8344, precision 0.8319, recall 0.8368, auc 0.8339
epoch 701, loss 0.4373, train acc 83.44%, f1 0.8348, precision 0.8328, recall 0.8369, auc 0.8344
epoch 801, loss 0.4387, train acc 83.46%, f1 0.8353, precision 0.8315, recall 0.8391, auc 0.8346
epoch 901, loss 0.3551, train acc 83.45%, f1 0.8350, precision 0.8326, recall 0.8374, auc 0.8345
epoch 1001, loss 0.4153, train acc 83.42%, f1 0.8347, precision 0.8323, recall 0.8371, auc 0.8342
epoch 1101, loss 0.4341, train acc 83.44%, f1 0.8347, precision 0.8332, recall 0.8362, auc 0.8344
epoch 1201, loss 0.3576, train acc 83.46%, f1 0.8349, precision 0.8336, recall 0.8362, auc 0.8346
epoch 1301, loss 0.5344, train acc 83.49%, f1 0.8352, precision 0.8339, recall 0.8365, auc 0.8349
epoch 1401, loss 0.4535, train acc 83.53%, f1 0.8356, precision 0.8341, recall 0.8371, auc 0.8353
epoch 1501, loss 0.3639, train acc 83.51%, f1 0.8353, precision 0.8342, recall 0.8364, auc 0.8351
epoch 1601, loss 0.3874, train acc 83.51%, f1 0.8352, precision 0.8345, recall 0.8358, auc 0.8351
epoch 1701, loss 0.3304, train acc 83.52%, f1 0.8353, precision 0.8349, recall 0.8357, auc 0.8352
epoch 1801, loss 0.4006, train acc 83.49%, f1 0.8349, precision 0.8347, recall 0.8351, auc 0.8349
epoch 1901, loss 0.3774, train acc 83.45%, f1 0.8346, precision 0.8343, recall 0.8349, auc 0.8345
epoch 2001, loss 0.3534, train acc 83.42%, f1 0.8341, precision 0.8343, recall 0.8340, auc 0.8342
epoch 2101, loss 0.3062, train acc 83.46%, f1 0.8346, precision 0.8344, recall 0.8348, auc 0.8346
epoch 2201, loss 0.3876, train acc 83.50%, f1 0.8350, precision 0.8348, recall 0.8353, auc 0.8350
epoch 2301, loss 0.4551, train acc 83.48%, f1 0.8349, precision 0.8345, recall 0.8352, auc 0.8348
epoch 2401, loss 0.3253, train acc 83.43%, f1 0.8343, precision 0.8343, recall 0.8344, auc 0.8343
epoch 2501, loss 0.3951, train acc 83.46%, f1 0.8346, precision 0.8347, recall 0.8344, auc 0.8346
epoch 2601, loss 0.3115, train acc 83.45%, f1 0.8346, precision 0.8340, recall 0.8352, auc 0.8345
epoch 2701, loss 0.4368, train acc 83.41%, f1 0.8342, precision 0.8340, recall 0.8343, auc 0.8341
epoch 2801, loss 0.3590, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8347, auc 0.8347
epoch 2901, loss 0.4314, train acc 83.46%, f1 0.8346, precision 0.8345, recall 0.8346, auc 0.8346
epoch 3001, loss 0.2457, train acc 83.50%, f1 0.8350, precision 0.8347, recall 0.8354, auc 0.8350
epoch 3101, loss 0.3294, train acc 83.50%, f1 0.8350, precision 0.8349, recall 0.8351, auc 0.8350
epoch 3201, loss 0.3193, train acc 83.50%, f1 0.8351, precision 0.8347, recall 0.8355, auc 0.8350
epoch 3301, loss 0.3905, train acc 83.54%, f1 0.8354, precision 0.8352, recall 0.8357, auc 0.8354
epoch 3401, loss 0.4180, train acc 83.50%, f1 0.8351, precision 0.8344, recall 0.8359, auc 0.8350
epoch 3501, loss 0.3398, train acc 83.52%, f1 0.8352, precision 0.8350, recall 0.8355, auc 0.8352
epoch 3601, loss 0.5028, train acc 83.56%, f1 0.8358, precision 0.8351, recall 0.8365, auc 0.8356
epoch 3701, loss 0.4278, train acc 83.51%, f1 0.8352, precision 0.8349, recall 0.8355, auc 0.8351
epoch 3801, loss 0.4845, train acc 83.51%, f1 0.8352, precision 0.8346, recall 0.8358, auc 0.8351
epoch 3901, loss 0.3392, train acc 83.51%, f1 0.8353, precision 0.8347, recall 0.8358, auc 0.8351
epoch 4001, loss 0.3300, train acc 83.56%, f1 0.8356, precision 0.8354, recall 0.8358, auc 0.8356
epoch 4101, loss 0.3030, train acc 83.55%, f1 0.8356, precision 0.8354, recall 0.8357, auc 0.8355
epoch 4201, loss 0.3076, train acc 83.55%, f1 0.8357, precision 0.8350, recall 0.8363, auc 0.8355
epoch 4301, loss 0.2610, train acc 83.59%, f1 0.8360, precision 0.8358, recall 0.8362, auc 0.8359
epoch 4401, loss 0.3228, train acc 83.62%, f1 0.8362, precision 0.8365, recall 0.8359, auc 0.8362
epoch 4501, loss 0.4035, train acc 83.70%, f1 0.8369, precision 0.8373, recall 0.8364, auc 0.8370
epoch 4601, loss 0.3553, train acc 83.67%, f1 0.8367, precision 0.8370, recall 0.8364, auc 0.8367
epoch 4701, loss 0.4071, train acc 83.70%, f1 0.8371, precision 0.8366, recall 0.8376, auc 0.8370
epoch 4801, loss 0.3867, train acc 83.70%, f1 0.8370, precision 0.8371, recall 0.8370, auc 0.8370
epoch 4901, loss 0.3698, train acc 83.71%, f1 0.8372, precision 0.8367, recall 0.8376, auc 0.8371
epoch 5001, loss 0.3242, train acc 83.77%, f1 0.8378, precision 0.8375, recall 0.8380, auc 0.8377
epoch 5101, loss 0.2568, train acc 83.76%, f1 0.8377, precision 0.8372, recall 0.8382, auc 0.8376
epoch 5201, loss 0.3760, train acc 83.77%, f1 0.8377, precision 0.8377, recall 0.8377, auc 0.8377
epoch 5301, loss 0.5254, train acc 83.85%, f1 0.8385, precision 0.8382, recall 0.8389, auc 0.8385
epoch 5401, loss 0.3885, train acc 83.90%, f1 0.8391, precision 0.8386, recall 0.8395, auc 0.8390
epoch 5501, loss 0.4538, train acc 83.89%, f1 0.8389, precision 0.8388, recall 0.8390, auc 0.8389
epoch 5601, loss 0.3460, train acc 83.93%, f1 0.8393, precision 0.8394, recall 0.8391, auc 0.8393
epoch 5701, loss 0.4565, train acc 83.97%, f1 0.8397, precision 0.8396, recall 0.8398, auc 0.8397
epoch 5801, loss 0.3212, train acc 84.00%, f1 0.8400, precision 0.8401, recall 0.8399, auc 0.8400
epoch 5901, loss 0.3278, train acc 84.04%, f1 0.8403, precision 0.8407, recall 0.8400, auc 0.8404
epoch 6001, loss 0.2569, train acc 84.05%, f1 0.8406, precision 0.8402, recall 0.8411, auc 0.8405
epoch 6101, loss 0.2830, train acc 84.07%, f1 0.8407, precision 0.8407, recall 0.8408, auc 0.8407
epoch 6201, loss 0.3635, train acc 84.13%, f1 0.8413, precision 0.8411, recall 0.8416, auc 0.8413
epoch 6301, loss 0.4755, train acc 84.09%, f1 0.8409, precision 0.8408, recall 0.8411, auc 0.8409
epoch 6401, loss 0.4426, train acc 84.13%, f1 0.8413, precision 0.8416, recall 0.8409, auc 0.8413
epoch 6501, loss 0.3523, train acc 84.21%, f1 0.8421, precision 0.8423, recall 0.8419, auc 0.8421
epoch 6601, loss 0.4123, train acc 84.24%, f1 0.8424, precision 0.8425, recall 0.8424, auc 0.8424
epoch 6701, loss 0.3261, train acc 84.25%, f1 0.8426, precision 0.8422, recall 0.8429, auc 0.8425
epoch 6801, loss 0.2829, train acc 84.25%, f1 0.8426, precision 0.8422, recall 0.8429, auc 0.8425
epoch 6901, loss 0.3344, train acc 84.21%, f1 0.8421, precision 0.8419, recall 0.8423, auc 0.8421
epoch 7001, loss 0.4045, train acc 84.29%, f1 0.8429, precision 0.8428, recall 0.8430, auc 0.8429
epoch 7101, loss 0.3654, train acc 84.32%, f1 0.8432, precision 0.8434, recall 0.8430, auc 0.8432
epoch 7201, loss 0.4142, train acc 84.32%, f1 0.8432, precision 0.8433, recall 0.8431, auc 0.8432
epoch 7301, loss 0.3206, train acc 84.39%, f1 0.8439, precision 0.8441, recall 0.8437, auc 0.8439
epoch 7401, loss 0.2797, train acc 84.34%, f1 0.8435, precision 0.8429, recall 0.8441, auc 0.8434
epoch 7501, loss 0.4313, train acc 84.40%, f1 0.8439, precision 0.8441, recall 0.8437, auc 0.8440
epoch 7601, loss 0.3699, train acc 84.46%, f1 0.8446, precision 0.8446, recall 0.8446, auc 0.8446
epoch 7701, loss 0.3304, train acc 84.46%, f1 0.8445, precision 0.8446, recall 0.8445, auc 0.8446
epoch 7801, loss 0.4087, train acc 84.45%, f1 0.8445, precision 0.8444, recall 0.8447, auc 0.8445
epoch 7901, loss 0.3928, train acc 84.53%, f1 0.8452, precision 0.8454, recall 0.8450, auc 0.8453
epoch 8001, loss 0.3115, train acc 84.49%, f1 0.8448, precision 0.8451, recall 0.8445, auc 0.8449
epoch 8101, loss 0.4323, train acc 84.51%, f1 0.8450, precision 0.8453, recall 0.8447, auc 0.8451
epoch 8201, loss 0.3323, train acc 84.56%, f1 0.8456, precision 0.8458, recall 0.8455, auc 0.8456/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.4218, train acc 84.61%, f1 0.8461, precision 0.8464, recall 0.8457, auc 0.8461
epoch 8401, loss 0.3898, train acc 84.57%, f1 0.8458, precision 0.8455, recall 0.8461, auc 0.8457
epoch 8501, loss 0.3892, train acc 84.55%, f1 0.8455, precision 0.8456, recall 0.8454, auc 0.8455
epoch 8601, loss 0.3117, train acc 84.54%, f1 0.8454, precision 0.8456, recall 0.8452, auc 0.8454
epoch 8701, loss 0.5377, train acc 84.59%, f1 0.8460, precision 0.8458, recall 0.8462, auc 0.8459
epoch 8801, loss 0.3276, train acc 84.62%, f1 0.8462, precision 0.8462, recall 0.8462, auc 0.8462
epoch 8901, loss 0.3295, train acc 84.62%, f1 0.8462, precision 0.8461, recall 0.8463, auc 0.8462
epoch 9001, loss 0.3349, train acc 84.64%, f1 0.8464, precision 0.8463, recall 0.8465, auc 0.8464
epoch 9101, loss 0.2984, train acc 84.71%, f1 0.8471, precision 0.8467, recall 0.8475, auc 0.8471
epoch 9201, loss 0.3777, train acc 84.66%, f1 0.8467, precision 0.8465, recall 0.8469, auc 0.8466
epoch 9301, loss 0.2498, train acc 84.70%, f1 0.8470, precision 0.8470, recall 0.8471, auc 0.8470
epoch 9401, loss 0.2950, train acc 84.70%, f1 0.8470, precision 0.8467, recall 0.8474, auc 0.8470
epoch 9501, loss 0.3441, train acc 84.67%, f1 0.8467, precision 0.8467, recall 0.8468, auc 0.8467
epoch 9601, loss 0.4166, train acc 84.65%, f1 0.8465, precision 0.8466, recall 0.8463, auc 0.8465
epoch 9701, loss 0.2638, train acc 84.71%, f1 0.8471, precision 0.8469, recall 0.8473, auc 0.8471
epoch 9801, loss 0.3506, train acc 84.78%, f1 0.8478, precision 0.8477, recall 0.8479, auc 0.8478
epoch 9901, loss 0.3679, train acc 84.72%, f1 0.8473, precision 0.8469, recall 0.8477, auc 0.8472
epoch 10001, loss 0.4555, train acc 84.72%, f1 0.8472, precision 0.8472, recall 0.8472, auc 0.8472
epoch 10101, loss 0.2456, train acc 84.74%, f1 0.8475, precision 0.8471, recall 0.8479, auc 0.8474
epoch 10201, loss 0.2756, train acc 84.79%, f1 0.8479, precision 0.8476, recall 0.8482, auc 0.8479
epoch 10301, loss 0.3394, train acc 84.81%, f1 0.8480, precision 0.8483, recall 0.8478, auc 0.8481
epoch 10401, loss 0.3275, train acc 84.81%, f1 0.8481, precision 0.8482, recall 0.8480, auc 0.8481
epoch 10501, loss 0.4020, train acc 84.80%, f1 0.8480, precision 0.8480, recall 0.8481, auc 0.8480
epoch 10601, loss 0.3873, train acc 84.83%, f1 0.8482, precision 0.8484, recall 0.8481, auc 0.8483
epoch 10701, loss 0.2649, train acc 84.85%, f1 0.8485, precision 0.8486, recall 0.8484, auc 0.8485
epoch 10801, loss 0.3281, train acc 84.82%, f1 0.8483, precision 0.8480, recall 0.8486, auc 0.8482
epoch 10901, loss 0.2852, train acc 84.84%, f1 0.8483, precision 0.8487, recall 0.8480, auc 0.8484
epoch 11001, loss 0.4289, train acc 84.84%, f1 0.8484, precision 0.8483, recall 0.8485, auc 0.8484
epoch 11101, loss 0.3688, train acc 84.85%, f1 0.8485, precision 0.8484, recall 0.8485, auc 0.8485
epoch 11201, loss 0.4182, train acc 84.93%, f1 0.8493, precision 0.8493, recall 0.8492, auc 0.8493
epoch 11301, loss 0.3023, train acc 84.93%, f1 0.8493, precision 0.8493, recall 0.8492, auc 0.8493
epoch 11401, loss 0.3048, train acc 84.91%, f1 0.8491, precision 0.8490, recall 0.8493, auc 0.8491
epoch 11501, loss 0.3159, train acc 84.92%, f1 0.8492, precision 0.8492, recall 0.8492, auc 0.8492
epoch 11601, loss 0.4315, train acc 84.98%, f1 0.8498, precision 0.8497, recall 0.8498, auc 0.8498
epoch 11701, loss 0.2820, train acc 84.93%, f1 0.8493, precision 0.8493, recall 0.8492, auc 0.8493
epoch 11801, loss 0.2904, train acc 84.94%, f1 0.8494, precision 0.8494, recall 0.8494, auc 0.8494
epoch 11901, loss 0.3562, train acc 84.99%, f1 0.8499, precision 0.8498, recall 0.8499, auc 0.8499
epoch 12001, loss 0.3557, train acc 84.95%, f1 0.8494, precision 0.8495, recall 0.8493, auc 0.8495
epoch 12101, loss 0.2503, train acc 85.00%, f1 0.8500, precision 0.8500, recall 0.8500, auc 0.8500
epoch 12201, loss 0.3126, train acc 85.03%, f1 0.8503, precision 0.8500, recall 0.8506, auc 0.8503
epoch 12301, loss 0.2143, train acc 85.04%, f1 0.8504, precision 0.8505, recall 0.8503, auc 0.8504
epoch 12401, loss 0.3093, train acc 85.01%, f1 0.8501, precision 0.8502, recall 0.8501, auc 0.8501
epoch 12501, loss 0.3709, train acc 85.03%, f1 0.8502, precision 0.8504, recall 0.8501, auc 0.8503
epoch 12601, loss 0.3543, train acc 85.03%, f1 0.8503, precision 0.8503, recall 0.8503, auc 0.8503
epoch 12701, loss 0.2577, train acc 85.01%, f1 0.8501, precision 0.8501, recall 0.8501, auc 0.8501
epoch 12801, loss 0.2426, train acc 85.10%, f1 0.8510, precision 0.8509, recall 0.8511, auc 0.8510
epoch 12901, loss 0.3105, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8508, auc 0.8509
epoch 13001, loss 0.3401, train acc 85.07%, f1 0.8507, precision 0.8505, recall 0.8509, auc 0.8507
epoch 13101, loss 0.4026, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8508, auc 0.8507
epoch 13201, loss 0.3332, train acc 85.10%, f1 0.8511, precision 0.8508, recall 0.8513, auc 0.8510
epoch 13301, loss 0.3025, train acc 85.10%, f1 0.8510, precision 0.8509, recall 0.8511, auc 0.8510
epoch 13401, loss 0.3331, train acc 85.15%, f1 0.8515, precision 0.8516, recall 0.8513, auc 0.8515
epoch 13501, loss 0.2720, train acc 85.07%, f1 0.8507, precision 0.8508, recall 0.8506, auc 0.8507
epoch 13601, loss 0.3155, train acc 85.14%, f1 0.8514, precision 0.8516, recall 0.8512, auc 0.8514
epoch 13701, loss 0.3317, train acc 85.15%, f1 0.8515, precision 0.8514, recall 0.8516, auc 0.8515
epoch 13801, loss 0.3302, train acc 85.16%, f1 0.8516, precision 0.8518, recall 0.8513, auc 0.8516
epoch 13901, loss 0.3493, train acc 85.13%, f1 0.8513, precision 0.8513, recall 0.8513, auc 0.8513
epoch 14001, loss 0.2633, train acc 85.19%, f1 0.8518, precision 0.8519, recall 0.8518, auc 0.8519
epoch 14101, loss 0.3206, train acc 85.17%, f1 0.8517, precision 0.8518, recall 0.8515, auc 0.8517
epoch 14201, loss 0.4073, train acc 85.14%, f1 0.8514, precision 0.8516, recall 0.8512, auc 0.8514
epoch 14301, loss 0.3539, train acc 85.20%, f1 0.8520, precision 0.8518, recall 0.8523, auc 0.8520
epoch 14401, loss 0.3169, train acc 85.21%, f1 0.8521, precision 0.8519, recall 0.8524, auc 0.8521
epoch 14501, loss 0.2678, train acc 85.22%, f1 0.8522, precision 0.8523, recall 0.8521, auc 0.8522
epoch 14601, loss 0.2717, train acc 85.17%, f1 0.8517, precision 0.8516, recall 0.8519, auc 0.8517
epoch 14701, loss 0.3045, train acc 85.20%, f1 0.8520, precision 0.8521, recall 0.8519, auc 0.8520
epoch 14801, loss 0.3039, train acc 85.23%, f1 0.8523, precision 0.8523, recall 0.8524, auc 0.8523
epoch 14901, loss 0.3382, train acc 85.22%, f1 0.8523, precision 0.8519, recall 0.8527, auc 0.8522
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_1
./test_pima/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6614814814814813

the Fscore is 0.6117647058823529

the precision is 0.4482758620689655

the recall is 0.9629629629629629

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_1
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6162, train acc 77.33%, f1 0.7597, precision 0.8084, recall 0.7165, auc 0.7733
epoch 201, loss 0.4565, train acc 80.30%, f1 0.8014, precision 0.8079, recall 0.7951, auc 0.8030
epoch 301, loss 0.4061, train acc 81.86%, f1 0.8189, precision 0.8177, recall 0.8201, auc 0.8186
epoch 401, loss 0.3672, train acc 82.68%, f1 0.8276, precision 0.8236, recall 0.8317, auc 0.8268
epoch 501, loss 0.3330, train acc 83.12%, f1 0.8320, precision 0.8280, recall 0.8361, auc 0.8312
epoch 601, loss 0.3605, train acc 83.36%, f1 0.8346, precision 0.8300, recall 0.8392, auc 0.8336
epoch 701, loss 0.3861, train acc 83.41%, f1 0.8350, precision 0.8307, recall 0.8394, auc 0.8341
epoch 801, loss 0.4358, train acc 83.46%, f1 0.8352, precision 0.8322, recall 0.8382, auc 0.8346
epoch 901, loss 0.3726, train acc 83.46%, f1 0.8350, precision 0.8326, recall 0.8375, auc 0.8346
epoch 1001, loss 0.3557, train acc 83.44%, f1 0.8349, precision 0.8324, recall 0.8374, auc 0.8344
epoch 1101, loss 0.4622, train acc 83.46%, f1 0.8350, precision 0.8329, recall 0.8371, auc 0.8346
epoch 1201, loss 0.3519, train acc 83.48%, f1 0.8351, precision 0.8337, recall 0.8365, auc 0.8348
epoch 1301, loss 0.4556, train acc 83.51%, f1 0.8355, precision 0.8333, recall 0.8377, auc 0.8351
epoch 1401, loss 0.3954, train acc 83.46%, f1 0.8350, precision 0.8327, recall 0.8373, auc 0.8346
epoch 1501, loss 0.4125, train acc 83.49%, f1 0.8351, precision 0.8342, recall 0.8360, auc 0.8349
epoch 1601, loss 0.3496, train acc 83.48%, f1 0.8350, precision 0.8340, recall 0.8360, auc 0.8348
epoch 1701, loss 0.3140, train acc 83.47%, f1 0.8348, precision 0.8341, recall 0.8354, auc 0.8347
epoch 1801, loss 0.4605, train acc 83.48%, f1 0.8350, precision 0.8341, recall 0.8359, auc 0.8348
epoch 1901, loss 0.4536, train acc 83.43%, f1 0.8343, precision 0.8340, recall 0.8346, auc 0.8343
epoch 2001, loss 0.3374, train acc 83.52%, f1 0.8352, precision 0.8350, recall 0.8354, auc 0.8352
epoch 2101, loss 0.3602, train acc 83.48%, f1 0.8349, precision 0.8343, recall 0.8355, auc 0.8348
epoch 2201, loss 0.3590, train acc 83.50%, f1 0.8351, precision 0.8349, recall 0.8352, auc 0.8350
epoch 2301, loss 0.3717, train acc 83.51%, f1 0.8352, precision 0.8347, recall 0.8356, auc 0.8351
epoch 2401, loss 0.2989, train acc 83.52%, f1 0.8353, precision 0.8350, recall 0.8356, auc 0.8352
epoch 2501, loss 0.3142, train acc 83.51%, f1 0.8352, precision 0.8348, recall 0.8356, auc 0.8351
epoch 2601, loss 0.4025, train acc 83.47%, f1 0.8348, precision 0.8345, recall 0.8350, auc 0.8347
epoch 2701, loss 0.3379, train acc 83.46%, f1 0.8346, precision 0.8349, recall 0.8343, auc 0.8346
epoch 2801, loss 0.3176, train acc 83.50%, f1 0.8352, precision 0.8340, recall 0.8365, auc 0.8350
epoch 2901, loss 0.3355, train acc 83.46%, f1 0.8347, precision 0.8343, recall 0.8351, auc 0.8346
epoch 3001, loss 0.3693, train acc 83.47%, f1 0.8347, precision 0.8345, recall 0.8349, auc 0.8347
epoch 3101, loss 0.3924, train acc 83.50%, f1 0.8350, precision 0.8351, recall 0.8349, auc 0.8350
epoch 3201, loss 0.4492, train acc 83.48%, f1 0.8349, precision 0.8343, recall 0.8354, auc 0.8348
epoch 3301, loss 0.2991, train acc 83.50%, f1 0.8350, precision 0.8348, recall 0.8352, auc 0.8350
epoch 3401, loss 0.3368, train acc 83.49%, f1 0.8350, precision 0.8348, recall 0.8351, auc 0.8349
epoch 3501, loss 0.3951, train acc 83.50%, f1 0.8350, precision 0.8347, recall 0.8354, auc 0.8350
epoch 3601, loss 0.2942, train acc 83.52%, f1 0.8352, precision 0.8351, recall 0.8353, auc 0.8352
epoch 3701, loss 0.2667, train acc 83.54%, f1 0.8353, precision 0.8357, recall 0.8350, auc 0.8354
epoch 3801, loss 0.5053, train acc 83.56%, f1 0.8357, precision 0.8356, recall 0.8357, auc 0.8356
epoch 3901, loss 0.3664, train acc 83.53%, f1 0.8353, precision 0.8353, recall 0.8352, auc 0.8353
epoch 4001, loss 0.3272, train acc 83.52%, f1 0.8352, precision 0.8352, recall 0.8353, auc 0.8352
epoch 4101, loss 0.3214, train acc 83.58%, f1 0.8357, precision 0.8362, recall 0.8351, auc 0.8358
epoch 4201, loss 0.2878, train acc 83.58%, f1 0.8358, precision 0.8359, recall 0.8357, auc 0.8358
epoch 4301, loss 0.4459, train acc 83.54%, f1 0.8353, precision 0.8357, recall 0.8348, auc 0.8354
epoch 4401, loss 0.4620, train acc 83.56%, f1 0.8355, precision 0.8357, recall 0.8353, auc 0.8356
epoch 4501, loss 0.3364, train acc 83.58%, f1 0.8358, precision 0.8358, recall 0.8358, auc 0.8358
epoch 4601, loss 0.2903, train acc 83.60%, f1 0.8361, precision 0.8357, recall 0.8364, auc 0.8360
epoch 4701, loss 0.3294, train acc 83.61%, f1 0.8362, precision 0.8358, recall 0.8366, auc 0.8361
epoch 4801, loss 0.3961, train acc 83.63%, f1 0.8363, precision 0.8362, recall 0.8363, auc 0.8363
epoch 4901, loss 0.3960, train acc 83.65%, f1 0.8365, precision 0.8363, recall 0.8368, auc 0.8365
epoch 5001, loss 0.3982, train acc 83.70%, f1 0.8371, precision 0.8368, recall 0.8374, auc 0.8370
epoch 5101, loss 0.3101, train acc 83.70%, f1 0.8369, precision 0.8371, recall 0.8368, auc 0.8370
epoch 5201, loss 0.5057, train acc 83.77%, f1 0.8378, precision 0.8374, recall 0.8381, auc 0.8377
epoch 5301, loss 0.3407, train acc 83.72%, f1 0.8372, precision 0.8375, recall 0.8369, auc 0.8372
epoch 5401, loss 0.3972, train acc 83.74%, f1 0.8374, precision 0.8374, recall 0.8374, auc 0.8374
epoch 5501, loss 0.3654, train acc 83.73%, f1 0.8373, precision 0.8372, recall 0.8374, auc 0.8373
epoch 5601, loss 0.4278, train acc 83.81%, f1 0.8381, precision 0.8382, recall 0.8380, auc 0.8381
epoch 5701, loss 0.4156, train acc 83.82%, f1 0.8382, precision 0.8382, recall 0.8383, auc 0.8382
epoch 5801, loss 0.3427, train acc 83.86%, f1 0.8386, precision 0.8384, recall 0.8388, auc 0.8386
epoch 5901, loss 0.2598, train acc 83.87%, f1 0.8387, precision 0.8384, recall 0.8391, auc 0.8387
epoch 6001, loss 0.2471, train acc 83.85%, f1 0.8385, precision 0.8387, recall 0.8382, auc 0.8385
epoch 6101, loss 0.4552, train acc 83.91%, f1 0.8390, precision 0.8393, recall 0.8387, auc 0.8391
epoch 6201, loss 0.3735, train acc 83.89%, f1 0.8389, precision 0.8389, recall 0.8390, auc 0.8389
epoch 6301, loss 0.3477, train acc 83.92%, f1 0.8392, precision 0.8391, recall 0.8394, auc 0.8392
epoch 6401, loss 0.2982, train acc 83.94%, f1 0.8394, precision 0.8393, recall 0.8396, auc 0.8394
epoch 6501, loss 0.3632, train acc 83.97%, f1 0.8397, precision 0.8398, recall 0.8396, auc 0.8397
epoch 6601, loss 0.3257, train acc 83.97%, f1 0.8398, precision 0.8394, recall 0.8402, auc 0.8397
epoch 6701, loss 0.4126, train acc 83.90%, f1 0.8390, precision 0.8392, recall 0.8388, auc 0.8390
epoch 6801, loss 0.2899, train acc 84.05%, f1 0.8405, precision 0.8402, recall 0.8408, auc 0.8405
epoch 6901, loss 0.4921, train acc 84.05%, f1 0.8405, precision 0.8405, recall 0.8405, auc 0.8405
epoch 7001, loss 0.3927, train acc 84.02%, f1 0.8403, precision 0.8398, recall 0.8407, auc 0.8402
epoch 7101, loss 0.3721, train acc 84.09%, f1 0.8409, precision 0.8406, recall 0.8413, auc 0.8409
epoch 7201, loss 0.3389, train acc 84.08%, f1 0.8408, precision 0.8411, recall 0.8405, auc 0.8408
epoch 7301, loss 0.3960, train acc 84.11%, f1 0.8411, precision 0.8411, recall 0.8412, auc 0.8411
epoch 7401, loss 0.3057, train acc 84.11%, f1 0.8411, precision 0.8410, recall 0.8413, auc 0.8411
epoch 7501, loss 0.4000, train acc 84.14%, f1 0.8414, precision 0.8413, recall 0.8414, auc 0.8414
epoch 7601, loss 0.4917, train acc 84.15%, f1 0.8414, precision 0.8416, recall 0.8413, auc 0.8415
epoch 7701, loss 0.3844, train acc 84.09%, f1 0.8409, precision 0.8408, recall 0.8411, auc 0.8409
epoch 7801, loss 0.3160, train acc 84.18%, f1 0.8418, precision 0.8419, recall 0.8416, auc 0.8418
epoch 7901, loss 0.4194, train acc 84.16%, f1 0.8416, precision 0.8417, recall 0.8416, auc 0.8416
epoch 8001, loss 0.4827, train acc 84.18%, f1 0.8418, precision 0.8417, recall 0.8420, auc 0.8418
epoch 8101, loss 0.3573, train acc 84.26%, f1 0.8427, precision 0.8425, recall 0.8428, auc 0.8426
epoch 8201, loss 0.3490, train acc 84.23%, f1 0.8423, precision 0.8422, recall 0.8424, auc 0.8423/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2656, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8422, auc 0.8422
epoch 8401, loss 0.3928, train acc 84.26%, f1 0.8425, precision 0.8427, recall 0.8424, auc 0.8426
epoch 8501, loss 0.3797, train acc 84.30%, f1 0.8430, precision 0.8428, recall 0.8432, auc 0.8430
epoch 8601, loss 0.3325, train acc 84.19%, f1 0.8419, precision 0.8420, recall 0.8418, auc 0.8419
epoch 8701, loss 0.4372, train acc 84.28%, f1 0.8428, precision 0.8429, recall 0.8428, auc 0.8428
epoch 8801, loss 0.3223, train acc 84.31%, f1 0.8432, precision 0.8431, recall 0.8432, auc 0.8431
epoch 8901, loss 0.4246, train acc 84.32%, f1 0.8433, precision 0.8432, recall 0.8433, auc 0.8432
epoch 9001, loss 0.4273, train acc 84.36%, f1 0.8436, precision 0.8439, recall 0.8432, auc 0.8436
epoch 9101, loss 0.3177, train acc 84.36%, f1 0.8435, precision 0.8438, recall 0.8433, auc 0.8436
epoch 9201, loss 0.3624, train acc 84.38%, f1 0.8438, precision 0.8438, recall 0.8439, auc 0.8438
epoch 9301, loss 0.2731, train acc 84.39%, f1 0.8439, precision 0.8440, recall 0.8439, auc 0.8439
epoch 9401, loss 0.2809, train acc 84.42%, f1 0.8442, precision 0.8441, recall 0.8443, auc 0.8442
epoch 9501, loss 0.3559, train acc 84.41%, f1 0.8441, precision 0.8442, recall 0.8440, auc 0.8441
epoch 9601, loss 0.3885, train acc 84.41%, f1 0.8441, precision 0.8441, recall 0.8440, auc 0.8441
epoch 9701, loss 0.3075, train acc 84.47%, f1 0.8448, precision 0.8446, recall 0.8449, auc 0.8447
epoch 9801, loss 0.3562, train acc 84.41%, f1 0.8441, precision 0.8443, recall 0.8439, auc 0.8441
epoch 9901, loss 0.3953, train acc 84.46%, f1 0.8446, precision 0.8446, recall 0.8445, auc 0.8446
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_1
./test_pima/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6114814814814815

the Fscore is 0.5777777777777777

the precision is 0.4126984126984127

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_1
----------------------



epoch 1, loss 0.6931, train acc 72.73%, f1 0.7569, precision 0.6828, recall 0.8491, auc 0.7273
epoch 101, loss 0.6009, train acc 77.46%, f1 0.7738, precision 0.7768, recall 0.7707, auc 0.7746
epoch 201, loss 0.5024, train acc 80.37%, f1 0.8036, precision 0.8043, recall 0.8028, auc 0.8037
epoch 301, loss 0.5445, train acc 81.76%, f1 0.8177, precision 0.8175, recall 0.8179, auc 0.8176
epoch 401, loss 0.4640, train acc 82.72%, f1 0.8273, precision 0.8269, recall 0.8277, auc 0.8272
epoch 501, loss 0.3650, train acc 83.13%, f1 0.8314, precision 0.8312, recall 0.8316, auc 0.8313
epoch 601, loss 0.3855, train acc 83.38%, f1 0.8338, precision 0.8337, recall 0.8340, auc 0.8338
epoch 701, loss 0.5035, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8348, auc 0.8347
epoch 801, loss 0.4128, train acc 83.46%, f1 0.8346, precision 0.8345, recall 0.8347, auc 0.8346
epoch 901, loss 0.5233, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8347, auc 0.8347
epoch 1001, loss 0.4560, train acc 83.53%, f1 0.8353, precision 0.8351, recall 0.8355, auc 0.8353
epoch 1101, loss 0.3367, train acc 83.52%, f1 0.8352, precision 0.8352, recall 0.8353, auc 0.8352
epoch 1201, loss 0.3851, train acc 83.54%, f1 0.8354, precision 0.8354, recall 0.8354, auc 0.8354
epoch 1301, loss 0.3042, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8350, auc 0.8349
epoch 1401, loss 0.4648, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8349, auc 0.8349
epoch 1501, loss 0.3810, train acc 83.50%, f1 0.8350, precision 0.8351, recall 0.8350, auc 0.8350
epoch 1601, loss 0.4932, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 1701, loss 0.4829, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8347, auc 0.8348
epoch 1801, loss 0.4297, train acc 83.49%, f1 0.8348, precision 0.8349, recall 0.8347, auc 0.8349
epoch 1901, loss 0.4126, train acc 83.51%, f1 0.8351, precision 0.8350, recall 0.8352, auc 0.8351
epoch 2001, loss 0.3316, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 2101, loss 0.3455, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 2201, loss 0.4512, train acc 83.43%, f1 0.8343, precision 0.8343, recall 0.8344, auc 0.8343
epoch 2301, loss 0.4295, train acc 83.43%, f1 0.8343, precision 0.8343, recall 0.8344, auc 0.8343
epoch 2401, loss 0.3938, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 2501, loss 0.4125, train acc 83.44%, f1 0.8344, precision 0.8344, recall 0.8344, auc 0.8344
epoch 2601, loss 0.4069, train acc 83.48%, f1 0.8348, precision 0.8349, recall 0.8347, auc 0.8348
epoch 2701, loss 0.4196, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8351, auc 0.8351
epoch 2801, loss 0.4607, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8348, auc 0.8349
epoch 2901, loss 0.4270, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8347, auc 0.8347
epoch 3001, loss 0.3579, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8352, auc 0.8351
epoch 3101, loss 0.2818, train acc 83.51%, f1 0.8351, precision 0.8350, recall 0.8352, auc 0.8351
epoch 3201, loss 0.4230, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8347, auc 0.8347
epoch 3301, loss 0.3084, train acc 83.50%, f1 0.8350, precision 0.8351, recall 0.8350, auc 0.8350
epoch 3401, loss 0.4389, train acc 83.44%, f1 0.8344, precision 0.8344, recall 0.8344, auc 0.8344
epoch 3501, loss 0.2820, train acc 83.50%, f1 0.8350, precision 0.8349, recall 0.8351, auc 0.8350
epoch 3601, loss 0.4058, train acc 83.52%, f1 0.8352, precision 0.8352, recall 0.8351, auc 0.8352
epoch 3701, loss 0.4314, train acc 83.54%, f1 0.8354, precision 0.8354, recall 0.8355, auc 0.8354
epoch 3801, loss 0.3794, train acc 83.50%, f1 0.8350, precision 0.8350, recall 0.8351, auc 0.8350
epoch 3901, loss 0.4397, train acc 83.53%, f1 0.8353, precision 0.8352, recall 0.8354, auc 0.8353
epoch 4001, loss 0.3555, train acc 83.54%, f1 0.8354, precision 0.8354, recall 0.8354, auc 0.8354
epoch 4101, loss 0.3129, train acc 83.53%, f1 0.8353, precision 0.8354, recall 0.8351, auc 0.8353
epoch 4201, loss 0.3295, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 4301, loss 0.3825, train acc 83.55%, f1 0.8355, precision 0.8356, recall 0.8355, auc 0.8355
epoch 4401, loss 0.3797, train acc 83.56%, f1 0.8356, precision 0.8356, recall 0.8357, auc 0.8356
epoch 4501, loss 0.3595, train acc 83.56%, f1 0.8356, precision 0.8356, recall 0.8356, auc 0.8356
epoch 4601, loss 0.3464, train acc 83.59%, f1 0.8359, precision 0.8359, recall 0.8359, auc 0.8359
epoch 4701, loss 0.4404, train acc 83.60%, f1 0.8360, precision 0.8360, recall 0.8360, auc 0.8360
epoch 4801, loss 0.3157, train acc 83.57%, f1 0.8357, precision 0.8357, recall 0.8357, auc 0.8357
epoch 4901, loss 0.2805, train acc 83.56%, f1 0.8356, precision 0.8356, recall 0.8356, auc 0.8356
epoch 5001, loss 0.4326, train acc 83.62%, f1 0.8362, precision 0.8363, recall 0.8362, auc 0.8362
epoch 5101, loss 0.3102, train acc 83.64%, f1 0.8364, precision 0.8363, recall 0.8364, auc 0.8364
epoch 5201, loss 0.3891, train acc 83.60%, f1 0.8360, precision 0.8360, recall 0.8360, auc 0.8360
epoch 5301, loss 0.3459, train acc 83.64%, f1 0.8363, precision 0.8364, recall 0.8363, auc 0.8364
epoch 5401, loss 0.3147, train acc 83.66%, f1 0.8366, precision 0.8366, recall 0.8366, auc 0.8366
epoch 5501, loss 0.4169, train acc 83.71%, f1 0.8371, precision 0.8372, recall 0.8370, auc 0.8371
epoch 5601, loss 0.3806, train acc 83.67%, f1 0.8367, precision 0.8366, recall 0.8368, auc 0.8367
epoch 5701, loss 0.3624, train acc 83.62%, f1 0.8362, precision 0.8362, recall 0.8361, auc 0.8362
epoch 5801, loss 0.4258, train acc 83.78%, f1 0.8378, precision 0.8377, recall 0.8378, auc 0.8378
epoch 5901, loss 0.3270, train acc 83.73%, f1 0.8373, precision 0.8374, recall 0.8372, auc 0.8373
epoch 6001, loss 0.3100, train acc 83.77%, f1 0.8377, precision 0.8377, recall 0.8377, auc 0.8377
epoch 6101, loss 0.4029, train acc 83.85%, f1 0.8385, precision 0.8384, recall 0.8386, auc 0.8385
epoch 6201, loss 0.2938, train acc 83.87%, f1 0.8387, precision 0.8388, recall 0.8385, auc 0.8387
epoch 6301, loss 0.3429, train acc 83.87%, f1 0.8387, precision 0.8388, recall 0.8386, auc 0.8387
epoch 6401, loss 0.4158, train acc 83.90%, f1 0.8390, precision 0.8389, recall 0.8391, auc 0.8390
epoch 6501, loss 0.4010, train acc 83.92%, f1 0.8392, precision 0.8392, recall 0.8392, auc 0.8392
epoch 6601, loss 0.4054, train acc 83.92%, f1 0.8392, precision 0.8391, recall 0.8393, auc 0.8392
epoch 6701, loss 0.4010, train acc 84.00%, f1 0.8400, precision 0.8400, recall 0.8400, auc 0.8400
epoch 6801, loss 0.3275, train acc 83.99%, f1 0.8399, precision 0.8399, recall 0.8398, auc 0.8399
epoch 6901, loss 0.2519, train acc 83.99%, f1 0.8399, precision 0.8400, recall 0.8399, auc 0.8399
epoch 7001, loss 0.4419, train acc 84.01%, f1 0.8401, precision 0.8400, recall 0.8401, auc 0.8401
epoch 7101, loss 0.4387, train acc 84.07%, f1 0.8407, precision 0.8407, recall 0.8408, auc 0.8407
epoch 7201, loss 0.4211, train acc 84.10%, f1 0.8410, precision 0.8410, recall 0.8409, auc 0.8410
epoch 7301, loss 0.3687, train acc 84.09%, f1 0.8408, precision 0.8409, recall 0.8408, auc 0.8409
epoch 7401, loss 0.3188, train acc 84.12%, f1 0.8412, precision 0.8412, recall 0.8412, auc 0.8412
epoch 7501, loss 0.3242, train acc 84.14%, f1 0.8414, precision 0.8414, recall 0.8414, auc 0.8414
epoch 7601, loss 0.4153, train acc 84.15%, f1 0.8415, precision 0.8415, recall 0.8415, auc 0.8415
epoch 7701, loss 0.4350, train acc 84.16%, f1 0.8416, precision 0.8417, recall 0.8415, auc 0.8416
epoch 7801, loss 0.2595, train acc 84.18%, f1 0.8418, precision 0.8419, recall 0.8417, auc 0.8418
epoch 7901, loss 0.3449, train acc 84.19%, f1 0.8419, precision 0.8418, recall 0.8420, auc 0.8419
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_1
./test_pima/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6457407407407407

the Fscore is 0.6022727272727272

the precision is 0.4344262295081967

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_1
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6314, train acc 77.91%, f1 0.7755, precision 0.7884, recall 0.7629, auc 0.7791
epoch 201, loss 0.5521, train acc 80.44%, f1 0.8042, precision 0.8051, recall 0.8034, auc 0.8044
epoch 301, loss 0.4419, train acc 82.00%, f1 0.8200, precision 0.8199, recall 0.8201, auc 0.8200
epoch 401, loss 0.4406, train acc 82.80%, f1 0.8282, precision 0.8275, recall 0.8289, auc 0.8280
epoch 501, loss 0.4351, train acc 83.14%, f1 0.8315, precision 0.8307, recall 0.8324, auc 0.8314
epoch 601, loss 0.4873, train acc 83.29%, f1 0.8330, precision 0.8325, recall 0.8336, auc 0.8329
epoch 701, loss 0.3785, train acc 83.37%, f1 0.8339, precision 0.8329, recall 0.8348, auc 0.8337
epoch 801, loss 0.3484, train acc 83.44%, f1 0.8345, precision 0.8341, recall 0.8349, auc 0.8344
epoch 901, loss 0.3115, train acc 83.43%, f1 0.8344, precision 0.8338, recall 0.8350, auc 0.8343
epoch 1001, loss 0.3600, train acc 83.46%, f1 0.8346, precision 0.8344, recall 0.8349, auc 0.8346
epoch 1101, loss 0.3409, train acc 83.47%, f1 0.8347, precision 0.8344, recall 0.8350, auc 0.8347
epoch 1201, loss 0.2777, train acc 83.48%, f1 0.8348, precision 0.8345, recall 0.8351, auc 0.8348
epoch 1301, loss 0.4758, train acc 83.51%, f1 0.8351, precision 0.8350, recall 0.8352, auc 0.8351
epoch 1401, loss 0.3698, train acc 83.50%, f1 0.8350, precision 0.8349, recall 0.8352, auc 0.8350
epoch 1501, loss 0.3425, train acc 83.53%, f1 0.8353, precision 0.8353, recall 0.8353, auc 0.8353
epoch 1601, loss 0.4601, train acc 83.52%, f1 0.8353, precision 0.8351, recall 0.8354, auc 0.8352
epoch 1701, loss 0.2849, train acc 83.52%, f1 0.8352, precision 0.8352, recall 0.8353, auc 0.8352
epoch 1801, loss 0.3605, train acc 83.52%, f1 0.8352, precision 0.8352, recall 0.8352, auc 0.8352
epoch 1901, loss 0.2390, train acc 83.46%, f1 0.8346, precision 0.8347, recall 0.8344, auc 0.8346
epoch 2001, loss 0.4589, train acc 83.46%, f1 0.8346, precision 0.8347, recall 0.8346, auc 0.8346
epoch 2101, loss 0.3455, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8347, auc 0.8346
epoch 2201, loss 0.3469, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 2301, loss 0.3375, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8348, auc 0.8347
epoch 2401, loss 0.4191, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8350, auc 0.8351
epoch 2501, loss 0.2989, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8348, auc 0.8347
epoch 2601, loss 0.4035, train acc 83.45%, f1 0.8345, precision 0.8344, recall 0.8346, auc 0.8345
epoch 2701, loss 0.3161, train acc 83.49%, f1 0.8349, precision 0.8348, recall 0.8349, auc 0.8349
epoch 2801, loss 0.4220, train acc 83.48%, f1 0.8349, precision 0.8348, recall 0.8350, auc 0.8348
epoch 2901, loss 0.4574, train acc 83.50%, f1 0.8351, precision 0.8347, recall 0.8354, auc 0.8350
epoch 3001, loss 0.4034, train acc 83.47%, f1 0.8348, precision 0.8347, recall 0.8348, auc 0.8347
epoch 3101, loss 0.4858, train acc 83.48%, f1 0.8348, precision 0.8346, recall 0.8351, auc 0.8348
epoch 3201, loss 0.3097, train acc 83.50%, f1 0.8350, precision 0.8347, recall 0.8353, auc 0.8350
epoch 3301, loss 0.3485, train acc 83.51%, f1 0.8351, precision 0.8349, recall 0.8354, auc 0.8351
epoch 3401, loss 0.3815, train acc 83.52%, f1 0.8353, precision 0.8350, recall 0.8355, auc 0.8352
epoch 3501, loss 0.3179, train acc 83.51%, f1 0.8352, precision 0.8350, recall 0.8354, auc 0.8351
epoch 3601, loss 0.4318, train acc 83.51%, f1 0.8352, precision 0.8350, recall 0.8353, auc 0.8351
epoch 3701, loss 0.2627, train acc 83.53%, f1 0.8353, precision 0.8352, recall 0.8355, auc 0.8353
epoch 3801, loss 0.3385, train acc 83.54%, f1 0.8354, precision 0.8353, recall 0.8355, auc 0.8354
epoch 3901, loss 0.3338, train acc 83.57%, f1 0.8358, precision 0.8356, recall 0.8360, auc 0.8357
epoch 4001, loss 0.3097, train acc 83.58%, f1 0.8358, precision 0.8357, recall 0.8358, auc 0.8358
epoch 4101, loss 0.4148, train acc 83.58%, f1 0.8358, precision 0.8357, recall 0.8359, auc 0.8358
epoch 4201, loss 0.4396, train acc 83.60%, f1 0.8360, precision 0.8360, recall 0.8361, auc 0.8360
epoch 4301, loss 0.3359, train acc 83.64%, f1 0.8364, precision 0.8363, recall 0.8365, auc 0.8364
epoch 4401, loss 0.3687, train acc 83.65%, f1 0.8365, precision 0.8365, recall 0.8365, auc 0.8365
epoch 4501, loss 0.3297, train acc 83.64%, f1 0.8364, precision 0.8363, recall 0.8365, auc 0.8364
epoch 4601, loss 0.2595, train acc 83.66%, f1 0.8365, precision 0.8366, recall 0.8365, auc 0.8366
epoch 4701, loss 0.3462, train acc 83.69%, f1 0.8369, precision 0.8370, recall 0.8368, auc 0.8369
epoch 4801, loss 0.4343, train acc 83.68%, f1 0.8368, precision 0.8368, recall 0.8367, auc 0.8368
epoch 4901, loss 0.4055, train acc 83.73%, f1 0.8373, precision 0.8374, recall 0.8372, auc 0.8373
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_1
./test_pima/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.5914814814814814

the Fscore is 0.5652173913043479

the precision is 0.4

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6108, train acc 78.47%, f1 0.7862, precision 0.7808, recall 0.7918, auc 0.7847
epoch 201, loss 0.4863, train acc 80.41%, f1 0.8042, precision 0.8040, recall 0.8044, auc 0.8041
epoch 301, loss 0.4554, train acc 81.90%, f1 0.8190, precision 0.8191, recall 0.8188, auc 0.8190
epoch 401, loss 0.3801, train acc 82.82%, f1 0.8282, precision 0.8284, recall 0.8279, auc 0.8282
epoch 501, loss 0.4039, train acc 83.08%, f1 0.8308, precision 0.8311, recall 0.8304, auc 0.8308
epoch 601, loss 0.4131, train acc 83.30%, f1 0.8330, precision 0.8331, recall 0.8328, auc 0.8330
epoch 701, loss 0.4510, train acc 83.44%, f1 0.8344, precision 0.8345, recall 0.8343, auc 0.8344
epoch 801, loss 0.3259, train acc 83.41%, f1 0.8340, precision 0.8341, recall 0.8339, auc 0.8341
epoch 901, loss 0.4156, train acc 83.44%, f1 0.8344, precision 0.8345, recall 0.8343, auc 0.8344
epoch 1001, loss 0.3404, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8347, auc 0.8347
epoch 1101, loss 0.4599, train acc 83.47%, f1 0.8347, precision 0.8348, recall 0.8346, auc 0.8347
epoch 1201, loss 0.4343, train acc 83.51%, f1 0.8351, precision 0.8352, recall 0.8351, auc 0.8351
epoch 1301, loss 0.5005, train acc 83.45%, f1 0.8345, precision 0.8346, recall 0.8343, auc 0.8345
epoch 1401, loss 0.3687, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 1501, loss 0.3956, train acc 83.45%, f1 0.8345, precision 0.8345, recall 0.8345, auc 0.8345
epoch 1601, loss 0.4406, train acc 83.45%, f1 0.8345, precision 0.8345, recall 0.8345, auc 0.8345
epoch 1701, loss 0.2954, train acc 83.44%, f1 0.8344, precision 0.8344, recall 0.8343, auc 0.8344
epoch 1801, loss 0.3634, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 1901, loss 0.4251, train acc 83.42%, f1 0.8341, precision 0.8342, recall 0.8340, auc 0.8342
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_1
./test_pima/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.61

the Fscore is 0.5806451612903226

the precision is 0.4090909090909091

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_1
----------------------



epoch 1, loss 0.6933, train acc 49.85%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6110, train acc 77.68%, f1 0.7849, precision 0.7595, recall 0.8122, auc 0.7767
epoch 201, loss 0.4729, train acc 80.40%, f1 0.8055, precision 0.8016, recall 0.8095, auc 0.8040
epoch 301, loss 0.3865, train acc 81.87%, f1 0.8185, precision 0.8220, recall 0.8150, auc 0.8187
epoch 401, loss 0.4208, train acc 82.78%, f1 0.8275, precision 0.8317, recall 0.8233, auc 0.8279
epoch 501, loss 0.3756, train acc 83.15%, f1 0.8318, precision 0.8326, recall 0.8310, auc 0.8315
epoch 601, loss 0.4489, train acc 83.35%, f1 0.8334, precision 0.8366, recall 0.8302, auc 0.8335
epoch 701, loss 0.3183, train acc 83.43%, f1 0.8346, precision 0.8358, recall 0.8334, auc 0.8343
epoch 801, loss 0.5107, train acc 83.46%, f1 0.8348, precision 0.8362, recall 0.8333, auc 0.8346
epoch 901, loss 0.2555, train acc 83.49%, f1 0.8351, precision 0.8365, recall 0.8337, auc 0.8349
epoch 1001, loss 0.3450, train acc 83.45%, f1 0.8342, precision 0.8380, recall 0.8305, auc 0.8345
epoch 1101, loss 0.4669, train acc 83.46%, f1 0.8348, precision 0.8365, recall 0.8331, auc 0.8346
epoch 1201, loss 0.3205, train acc 83.50%, f1 0.8350, precision 0.8377, recall 0.8322, auc 0.8350
epoch 1301, loss 0.3121, train acc 83.52%, f1 0.8354, precision 0.8367, recall 0.8340, auc 0.8352
epoch 1401, loss 0.2792, train acc 83.47%, f1 0.8349, precision 0.8363, recall 0.8335, auc 0.8347
epoch 1501, loss 0.4004, train acc 83.48%, f1 0.8351, precision 0.8359, recall 0.8343, auc 0.8348
epoch 1601, loss 0.3394, train acc 83.48%, f1 0.8353, precision 0.8355, recall 0.8351, auc 0.8348
epoch 1701, loss 0.4005, train acc 83.52%, f1 0.8350, precision 0.8384, recall 0.8317, auc 0.8352
epoch 1801, loss 0.3910, train acc 83.52%, f1 0.8356, precision 0.8362, recall 0.8350, auc 0.8352
epoch 1901, loss 0.3443, train acc 83.52%, f1 0.8355, precision 0.8367, recall 0.8344, auc 0.8352
epoch 2001, loss 0.4550, train acc 83.53%, f1 0.8354, precision 0.8373, recall 0.8336, auc 0.8353
epoch 2101, loss 0.4559, train acc 83.47%, f1 0.8354, precision 0.8347, recall 0.8361, auc 0.8347
epoch 2201, loss 0.3045, train acc 83.47%, f1 0.8352, precision 0.8354, recall 0.8350, auc 0.8347
epoch 2301, loss 0.4072, train acc 83.48%, f1 0.8348, precision 0.8371, recall 0.8325, auc 0.8348
epoch 2401, loss 0.5271, train acc 83.50%, f1 0.8359, precision 0.8337, recall 0.8381, auc 0.8350
epoch 2501, loss 0.3416, train acc 83.44%, f1 0.8349, precision 0.8349, recall 0.8350, auc 0.8344
epoch 2601, loss 0.4372, train acc 83.51%, f1 0.8351, precision 0.8375, recall 0.8327, auc 0.8351
epoch 2701, loss 0.3101, train acc 83.51%, f1 0.8356, precision 0.8356, recall 0.8356, auc 0.8351
epoch 2801, loss 0.4162, train acc 83.53%, f1 0.8360, precision 0.8352, recall 0.8367, auc 0.8353
epoch 2901, loss 0.5220, train acc 83.47%, f1 0.8352, precision 0.8355, recall 0.8349, auc 0.8347
epoch 3001, loss 0.3628, train acc 83.44%, f1 0.8346, precision 0.8361, recall 0.8330, auc 0.8344
epoch 3101, loss 0.3686, train acc 83.51%, f1 0.8353, precision 0.8367, recall 0.8338, auc 0.8351
epoch 3201, loss 0.2836, train acc 83.51%, f1 0.8357, precision 0.8352, recall 0.8362, auc 0.8351
epoch 3301, loss 0.5022, train acc 83.52%, f1 0.8359, precision 0.8350, recall 0.8368, auc 0.8352
epoch 3401, loss 0.3653, train acc 83.48%, f1 0.8349, precision 0.8372, recall 0.8326, auc 0.8349
epoch 3501, loss 0.4229, train acc 83.52%, f1 0.8362, precision 0.8335, recall 0.8389, auc 0.8352
epoch 3601, loss 0.4642, train acc 83.52%, f1 0.8357, precision 0.8354, recall 0.8360, auc 0.8352
epoch 3701, loss 0.5124, train acc 83.52%, f1 0.8356, precision 0.8363, recall 0.8349, auc 0.8352
epoch 3801, loss 0.3734, train acc 83.55%, f1 0.8365, precision 0.8336, recall 0.8395, auc 0.8355
epoch 3901, loss 0.4606, train acc 83.49%, f1 0.8351, precision 0.8365, recall 0.8337, auc 0.8349
epoch 4001, loss 0.3200, train acc 83.48%, f1 0.8352, precision 0.8356, recall 0.8348, auc 0.8348
epoch 4101, loss 0.3663, train acc 83.58%, f1 0.8359, precision 0.8381, recall 0.8337, auc 0.8358
epoch 4201, loss 0.2527, train acc 83.59%, f1 0.8366, precision 0.8356, recall 0.8376, auc 0.8359
epoch 4301, loss 0.4294, train acc 83.61%, f1 0.8361, precision 0.8384, recall 0.8338, auc 0.8361
epoch 4401, loss 0.3012, train acc 83.64%, f1 0.8366, precision 0.8378, recall 0.8354, auc 0.8364
epoch 4501, loss 0.2972, train acc 83.66%, f1 0.8367, precision 0.8389, recall 0.8344, auc 0.8366
epoch 4601, loss 0.3996, train acc 83.67%, f1 0.8371, precision 0.8372, recall 0.8371, auc 0.8367
epoch 4701, loss 0.3910, train acc 83.67%, f1 0.8373, precision 0.8368, recall 0.8377, auc 0.8367
epoch 4801, loss 0.3571, train acc 83.71%, f1 0.8376, precision 0.8374, recall 0.8377, auc 0.8371
epoch 4901, loss 0.3461, train acc 83.71%, f1 0.8372, precision 0.8390, recall 0.8354, auc 0.8371
epoch 5001, loss 0.4406, train acc 83.69%, f1 0.8377, precision 0.8363, recall 0.8390, auc 0.8369
epoch 5101, loss 0.4798, train acc 83.73%, f1 0.8377, precision 0.8380, recall 0.8374, auc 0.8373
epoch 5201, loss 0.5474, train acc 83.76%, f1 0.8380, precision 0.8385, recall 0.8375, auc 0.8376
epoch 5301, loss 0.3302, train acc 83.79%, f1 0.8384, precision 0.8386, recall 0.8382, auc 0.8379
epoch 5401, loss 0.3241, train acc 83.79%, f1 0.8385, precision 0.8377, recall 0.8393, auc 0.8379
epoch 5501, loss 0.4102, train acc 83.77%, f1 0.8378, precision 0.8397, recall 0.8358, auc 0.8377
epoch 5601, loss 0.3253, train acc 83.85%, f1 0.8388, precision 0.8395, recall 0.8381, auc 0.8385
epoch 5701, loss 0.2466, train acc 83.87%, f1 0.8393, precision 0.8384, recall 0.8402, auc 0.8387
epoch 5801, loss 0.3971, train acc 83.91%, f1 0.8388, precision 0.8425, recall 0.8352, auc 0.8391
epoch 5901, loss 0.3069, train acc 83.91%, f1 0.8400, precision 0.8376, recall 0.8424, auc 0.8390
epoch 6001, loss 0.3181, train acc 83.95%, f1 0.8398, precision 0.8408, recall 0.8388, auc 0.8395
epoch 6101, loss 0.2199, train acc 83.99%, f1 0.8400, precision 0.8420, recall 0.8380, auc 0.8399
epoch 6201, loss 0.2482, train acc 84.02%, f1 0.8405, precision 0.8413, recall 0.8398, auc 0.8402
epoch 6301, loss 0.4959, train acc 84.02%, f1 0.8403, precision 0.8424, recall 0.8383, auc 0.8402
epoch 6401, loss 0.4108, train acc 84.01%, f1 0.8404, precision 0.8414, recall 0.8393, auc 0.8401
epoch 6501, loss 0.3589, train acc 84.05%, f1 0.8410, precision 0.8406, recall 0.8414, auc 0.8405
epoch 6601, loss 0.3173, train acc 84.09%, f1 0.8413, precision 0.8418, recall 0.8407, auc 0.8409
epoch 6701, loss 0.4542, train acc 84.10%, f1 0.8409, precision 0.8436, recall 0.8383, auc 0.8410
epoch 6801, loss 0.3267, train acc 84.12%, f1 0.8412, precision 0.8436, recall 0.8388, auc 0.8412
epoch 6901, loss 0.4667, train acc 84.13%, f1 0.8414, precision 0.8433, recall 0.8396, auc 0.8413
epoch 7001, loss 0.4727, train acc 84.14%, f1 0.8421, precision 0.8411, recall 0.8432, auc 0.8414
epoch 7101, loss 0.3711, train acc 84.18%, f1 0.8420, precision 0.8435, recall 0.8406, auc 0.8418
epoch 7201, loss 0.3534, train acc 84.24%, f1 0.8427, precision 0.8437, recall 0.8416, auc 0.8424
epoch 7301, loss 0.2913, train acc 84.24%, f1 0.8431, precision 0.8423, recall 0.8439, auc 0.8424
epoch 7401, loss 0.4618, train acc 84.23%, f1 0.8428, precision 0.8422, recall 0.8435, auc 0.8423
epoch 7501, loss 0.5293, train acc 84.26%, f1 0.8433, precision 0.8422, recall 0.8444, auc 0.8426
epoch 7601, loss 0.3414, train acc 84.23%, f1 0.8428, precision 0.8430, recall 0.8425, auc 0.8423
epoch 7701, loss 0.3308, train acc 84.27%, f1 0.8436, precision 0.8415, recall 0.8456, auc 0.8427
epoch 7801, loss 0.2475, train acc 84.29%, f1 0.8434, precision 0.8431, recall 0.8436, auc 0.8428
epoch 7901, loss 0.2975, train acc 84.27%, f1 0.8435, precision 0.8413, recall 0.8458, auc 0.8426
epoch 8001, loss 0.2812, train acc 84.30%, f1 0.8432, precision 0.8445, recall 0.8419, auc 0.8430
epoch 8101, loss 0.3692, train acc 84.34%, f1 0.8435, precision 0.8453, recall 0.8417, auc 0.8434
epoch 8201, loss 0.3441, train acc 84.35%, f1 0.8437, precision 0.8450, recall 0.8425, auc 0.8435
epoch 8301, loss 0.3700, train acc 84.35%, f1 0.8441, precision 0.8435, recall 0.8447, auc 0.8435
epoch 8401, loss 0.4730, train acc 84.35%, f1 0.8442, precision 0.8432, recall 0.8451, auc 0.8435
epoch 8501, loss 0.3368, train acc 84.37%, f1 0.8443, precision 0.8435, recall 0.8451, auc 0.8437
epoch 8601, loss 0.3175, train acc 84.38%, f1 0.8444, precision 0.8438, recall 0.8449, auc 0.8438
epoch 8701, loss 0.2719, train acc 84.42%, f1 0.8450, precision 0.8429, recall 0.8471, auc 0.8442
epoch 8801, loss 0.3069, train acc 84.44%, f1 0.8450, precision 0.8446, recall 0.8454, auc 0.8444
epoch 8901, loss 0.3874, train acc 84.49%, f1 0.8453, precision 0.8459, recall 0.8447, auc 0.8449
epoch 9001, loss 0.2267, train acc 84.45%, f1 0.8447, precision 0.8462, recall 0.8432, auc 0.8445
epoch 9101, loss 0.3163, train acc 84.49%, f1 0.8453, precision 0.8456, recall 0.8449, auc 0.8449
epoch 9201, loss 0.3004, train acc 84.46%, f1 0.8452, precision 0.8444, recall 0.8461, auc 0.8446
epoch 9301, loss 0.3620, train acc 84.51%, f1 0.8456, precision 0.8456, recall 0.8456, auc 0.8451
epoch 9401, loss 0.3344, train acc 84.54%, f1 0.8456, precision 0.8468, recall 0.8445, auc 0.8454
epoch 9501, loss 0.3993, train acc 84.58%, f1 0.8465, precision 0.8451, recall 0.8479, auc 0.8458
epoch 9601, loss 0.2921, train acc 84.53%, f1 0.8456, precision 0.8465, recall 0.8448, auc 0.8453
epoch 9701, loss 0.3632, train acc 84.54%, f1 0.8458, precision 0.8460, recall 0.8457, auc 0.8454
epoch 9801, loss 0.4233, train acc 84.56%, f1 0.8459, precision 0.8469, recall 0.8449, auc 0.8456
epoch 9901, loss 0.3354, train acc 84.52%, f1 0.8453, precision 0.8472, recall 0.8433, auc 0.8452
epoch 10001, loss 0.3062, train acc 84.58%, f1 0.8463, precision 0.8458, recall 0.8469, auc 0.8458
epoch 10101, loss 0.2779, train acc 84.62%, f1 0.8467, precision 0.8468, recall 0.8466, auc 0.8462
epoch 10201, loss 0.2835, train acc 84.62%, f1 0.8468, precision 0.8462, recall 0.8474, auc 0.8462
epoch 10301, loss 0.3324, train acc 84.61%, f1 0.8470, precision 0.8449, recall 0.8490, auc 0.8461
epoch 10401, loss 0.3364, train acc 84.61%, f1 0.8468, precision 0.8457, recall 0.8479, auc 0.8461
epoch 10501, loss 0.2697, train acc 84.67%, f1 0.8472, precision 0.8467, recall 0.8478, auc 0.8467
epoch 10601, loss 0.2942, train acc 84.65%, f1 0.8471, precision 0.8464, recall 0.8478, auc 0.8465
epoch 10701, loss 0.3375, train acc 84.63%, f1 0.8466, precision 0.8475, recall 0.8456, auc 0.8463
epoch 10801, loss 0.2795, train acc 84.68%, f1 0.8474, precision 0.8468, recall 0.8480, auc 0.8468
epoch 10901, loss 0.3402, train acc 84.66%, f1 0.8468, precision 0.8480, recall 0.8456, auc 0.8466
epoch 11001, loss 0.2873, train acc 84.69%, f1 0.8475, precision 0.8463, recall 0.8488, auc 0.8469
epoch 11101, loss 0.2649, train acc 84.68%, f1 0.8477, precision 0.8456, recall 0.8498, auc 0.8468
epoch 11201, loss 0.4488, train acc 84.65%, f1 0.8467, precision 0.8483, recall 0.8451, auc 0.8465
epoch 11301, loss 0.2579, train acc 84.70%, f1 0.8479, precision 0.8452, recall 0.8505, auc 0.8469
epoch 11401, loss 0.3189, train acc 84.69%, f1 0.8475, precision 0.8467, recall 0.8482, auc 0.8469
epoch 11501, loss 0.2500, train acc 84.66%, f1 0.8471, precision 0.8472, recall 0.8470, auc 0.8466
epoch 11601, loss 0.2585, train acc 84.73%, f1 0.8479, precision 0.8472, recall 0.8485, auc 0.8473
epoch 11701, loss 0.4945, train acc 84.65%, f1 0.8470, precision 0.8466, recall 0.8474, auc 0.8465
epoch 11801, loss 0.3827, train acc 84.78%, f1 0.8480, precision 0.8494, recall 0.8466, auc 0.8478
epoch 11901, loss 0.3962, train acc 84.73%, f1 0.8478, precision 0.8472, recall 0.8485, auc 0.8473
epoch 12001, loss 0.2883, train acc 84.79%, f1 0.8485, precision 0.8476, recall 0.8494, auc 0.8479
epoch 12101, loss 0.3357, train acc 84.80%, f1 0.8482, precision 0.8497, recall 0.8467, auc 0.8480
epoch 12201, loss 0.3313, train acc 84.79%, f1 0.8484, precision 0.8485, recall 0.8482, auc 0.8479
epoch 12301, loss 0.3484, train acc 84.79%, f1 0.8479, precision 0.8506, recall 0.8451, auc 0.8479
epoch 12401, loss 0.3096, train acc 84.73%, f1 0.8478, precision 0.8478, recall 0.8477, auc 0.8473
epoch 12501, loss 0.2537, train acc 84.78%, f1 0.8483, precision 0.8479, recall 0.8488, auc 0.8478
epoch 12601, loss 0.2770, train acc 84.82%, f1 0.8487, precision 0.8481, recall 0.8493, auc 0.8482
epoch 12701, loss 0.3627, train acc 84.83%, f1 0.8487, precision 0.8489, recall 0.8485, auc 0.8483
epoch 12801, loss 0.3441, train acc 84.84%, f1 0.8487, precision 0.8491, recall 0.8483, auc 0.8484
epoch 12901, loss 0.3051, train acc 84.81%, f1 0.8486, precision 0.8487, recall 0.8484, auc 0.8481
epoch 13001, loss 0.3054, train acc 84.82%, f1 0.8485, precision 0.8495, recall 0.8475, auc 0.8482
epoch 13101, loss 0.3262, train acc 84.81%, f1 0.8486, precision 0.8486, recall 0.8485, auc 0.8481
epoch 13201, loss 0.3701, train acc 84.88%, f1 0.8488, precision 0.8512, recall 0.8465, auc 0.8488
epoch 13301, loss 0.4390, train acc 84.92%, f1 0.8494, precision 0.8506, recall 0.8483, auc 0.8492
epoch 13401, loss 0.3429, train acc 84.93%, f1 0.8498, precision 0.8495, recall 0.8502, auc 0.8493
epoch 13501, loss 0.3094, train acc 84.92%, f1 0.8498, precision 0.8490, recall 0.8507, auc 0.8492
epoch 13601, loss 0.3753, train acc 84.91%, f1 0.8497, precision 0.8487, recall 0.8507, auc 0.8491
epoch 13701, loss 0.4404, train acc 84.91%, f1 0.8497, precision 0.8487, recall 0.8508, auc 0.8491
epoch 13801, loss 0.3959, train acc 84.92%, f1 0.8498, precision 0.8487, recall 0.8510, auc 0.8492
epoch 13901, loss 0.3142, train acc 84.94%, f1 0.8495, precision 0.8516, recall 0.8475, auc 0.8494
epoch 14001, loss 0.4712, train acc 84.91%, f1 0.8495, precision 0.8497, recall 0.8492, auc 0.8491
epoch 14101, loss 0.3838, train acc 84.97%, f1 0.8506, precision 0.8481, recall 0.8531, auc 0.8497
epoch 14201, loss 0.3632, train acc 84.97%, f1 0.8494, precision 0.8539, recall 0.8449, auc 0.8497
epoch 14301, loss 0.2902, train acc 84.92%, f1 0.8496, precision 0.8502, recall 0.8489, auc 0.8492
epoch 14401, loss 0.2704, train acc 84.94%, f1 0.8498, precision 0.8500, recall 0.8497, auc 0.8494
epoch 14501, loss 0.4083, train acc 84.96%, f1 0.8499, precision 0.8509, recall 0.8489, auc 0.8496
epoch 14601, loss 0.2987, train acc 84.98%, f1 0.8504, precision 0.8497, recall 0.8510, auc 0.8498
epoch 14701, loss 0.2586, train acc 85.01%, f1 0.8504, precision 0.8511, recall 0.8497, auc 0.8501
epoch 14801, loss 0.2620, train acc 85.02%, f1 0.8505, precision 0.8510, recall 0.8501, auc 0.8502
epoch 14901, loss 0.5044, train acc 85.02%, f1 0.8508, precision 0.8503, recall 0.8512, auc 0.8502
epoch 15001, loss 0.2521, train acc 85.06%, f1 0.8507, precision 0.8526, recall 0.8488, auc 0.8506
epoch 15101, loss 0.3575, train acc 85.03%, f1 0.8503, precision 0.8526, recall 0.8480, auc 0.8503
epoch 15201, loss 0.3072, train acc 85.04%, f1 0.8509, precision 0.8506, recall 0.8512, auc 0.8504
epoch 15301, loss 0.3379, train acc 84.99%, f1 0.8501, precision 0.8514, recall 0.8489, auc 0.8499
epoch 15401, loss 0.3275, train acc 85.02%, f1 0.8503, precision 0.8521, recall 0.8486, auc 0.8502
epoch 15501, loss 0.3279, train acc 85.04%, f1 0.8505, precision 0.8526, recall 0.8484, auc 0.8504
epoch 15601, loss 0.3391, train acc 85.04%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8504
epoch 15701, loss 0.4326, train acc 85.04%, f1 0.8511, precision 0.8495, recall 0.8527, auc 0.8504
epoch 15801, loss 0.3097, train acc 85.07%, f1 0.8511, precision 0.8513, recall 0.8510, auc 0.8507
epoch 15901, loss 0.3538, train acc 85.07%, f1 0.8508, precision 0.8527, recall 0.8489, auc 0.8507
epoch 16001, loss 0.4449, train acc 85.04%, f1 0.8505, precision 0.8526, recall 0.8485, auc 0.8504
epoch 16101, loss 0.4012, train acc 85.08%, f1 0.8513, precision 0.8511, recall 0.8514, auc 0.8508
epoch 16201, loss 0.4406, train acc 85.11%, f1 0.8515, precision 0.8521, recall 0.8509, auc 0.8511
epoch 16301, loss 0.3951, train acc 85.08%, f1 0.8516, precision 0.8495, recall 0.8537, auc 0.8508
epoch 16401, loss 0.3205, train acc 85.09%, f1 0.8512, precision 0.8521, recall 0.8504, auc 0.8509
epoch 16501, loss 0.3301, train acc 85.09%, f1 0.8519, precision 0.8485, recall 0.8554, auc 0.8509/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.2737, train acc 85.12%, f1 0.8513, precision 0.8530, recall 0.8496, auc 0.8512
epoch 16701, loss 0.3582, train acc 85.11%, f1 0.8515, precision 0.8520, recall 0.8509, auc 0.8511
epoch 16801, loss 0.2954, train acc 85.10%, f1 0.8518, precision 0.8496, recall 0.8541, auc 0.8510
epoch 16901, loss 0.3401, train acc 85.09%, f1 0.8512, precision 0.8520, recall 0.8505, auc 0.8509
epoch 17001, loss 0.3716, train acc 85.13%, f1 0.8514, precision 0.8534, recall 0.8495, auc 0.8513
epoch 17101, loss 0.3108, train acc 85.12%, f1 0.8515, precision 0.8521, recall 0.8509, auc 0.8512
epoch 17201, loss 0.3025, train acc 85.15%, f1 0.8521, precision 0.8515, recall 0.8527, auc 0.8515
epoch 17301, loss 0.3690, train acc 85.17%, f1 0.8520, precision 0.8532, recall 0.8507, auc 0.8517
epoch 17401, loss 0.3892, train acc 85.15%, f1 0.8521, precision 0.8514, recall 0.8528, auc 0.8515
epoch 17501, loss 0.3804, train acc 85.13%, f1 0.8513, precision 0.8540, recall 0.8487, auc 0.8514
epoch 17601, loss 0.2942, train acc 85.16%, f1 0.8519, precision 0.8525, recall 0.8512, auc 0.8516
epoch 17701, loss 0.4793, train acc 85.16%, f1 0.8524, precision 0.8507, recall 0.8540, auc 0.8516
epoch 17801, loss 0.2845, train acc 85.17%, f1 0.8520, precision 0.8528, recall 0.8512, auc 0.8517
epoch 17901, loss 0.3450, train acc 85.17%, f1 0.8519, precision 0.8531, recall 0.8507, auc 0.8517
epoch 18001, loss 0.2959, train acc 85.16%, f1 0.8519, precision 0.8525, recall 0.8514, auc 0.8516
epoch 18101, loss 0.2546, train acc 85.16%, f1 0.8523, precision 0.8507, recall 0.8539, auc 0.8516
epoch 18201, loss 0.2660, train acc 85.17%, f1 0.8519, precision 0.8534, recall 0.8504, auc 0.8517
epoch 18301, loss 0.3801, train acc 85.15%, f1 0.8519, precision 0.8525, recall 0.8513, auc 0.8515
epoch 18401, loss 0.3069, train acc 85.17%, f1 0.8520, precision 0.8525, recall 0.8516, auc 0.8517
epoch 18501, loss 0.2543, train acc 85.20%, f1 0.8523, precision 0.8532, recall 0.8514, auc 0.8520
epoch 18601, loss 0.2893, train acc 85.22%, f1 0.8529, precision 0.8513, recall 0.8546, auc 0.8522
epoch 18701, loss 0.2390, train acc 85.26%, f1 0.8524, precision 0.8557, recall 0.8492, auc 0.8526
epoch 18801, loss 0.3871, train acc 85.18%, f1 0.8521, precision 0.8530, recall 0.8513, auc 0.8518
epoch 18901, loss 0.1818, train acc 85.24%, f1 0.8526, precision 0.8539, recall 0.8513, auc 0.8524
epoch 19001, loss 0.3526, train acc 85.20%, f1 0.8521, precision 0.8539, recall 0.8502, auc 0.8520
epoch 19101, loss 0.3061, train acc 85.22%, f1 0.8527, precision 0.8525, recall 0.8529, auc 0.8522
epoch 19201, loss 0.3900, train acc 85.27%, f1 0.8531, precision 0.8528, recall 0.8534, auc 0.8526
epoch 19301, loss 0.3234, train acc 85.27%, f1 0.8531, precision 0.8534, recall 0.8529, auc 0.8527
epoch 19401, loss 0.3236, train acc 85.26%, f1 0.8532, precision 0.8521, recall 0.8544, auc 0.8526
epoch 19501, loss 0.3921, train acc 85.28%, f1 0.8527, precision 0.8557, recall 0.8497, auc 0.8528
epoch 19601, loss 0.3201, train acc 85.30%, f1 0.8536, precision 0.8528, recall 0.8544, auc 0.8530
epoch 19701, loss 0.3031, train acc 85.28%, f1 0.8532, precision 0.8536, recall 0.8527, auc 0.8528
epoch 19801, loss 0.3997, train acc 85.26%, f1 0.8529, precision 0.8537, recall 0.8522, auc 0.8526
epoch 19901, loss 0.2459, train acc 85.27%, f1 0.8528, precision 0.8545, recall 0.8512, auc 0.8527
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_1
./test_pima/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6279629629629631

the Fscore is 0.5847953216374269

the precision is 0.42735042735042733

the recall is 0.9259259259259259

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_1
----------------------



epoch 1, loss 0.6933, train acc 49.91%, f1 0.6659, precision 0.4991, recall 1.0000, auc 0.5000
epoch 101, loss 0.6174, train acc 77.81%, f1 0.7882, precision 0.7525, recall 0.8275, auc 0.7781
epoch 201, loss 0.4609, train acc 80.23%, f1 0.8021, precision 0.8016, recall 0.8027, auc 0.8023
epoch 301, loss 0.4279, train acc 81.80%, f1 0.8173, precision 0.8188, recall 0.8159, auc 0.8180
epoch 401, loss 0.3867, train acc 82.74%, f1 0.8263, precision 0.8302, recall 0.8224, auc 0.8274
epoch 501, loss 0.3870, train acc 83.18%, f1 0.8305, precision 0.8352, recall 0.8259, auc 0.8318
epoch 601, loss 0.2855, train acc 83.33%, f1 0.8324, precision 0.8356, recall 0.8292, auc 0.8333
epoch 701, loss 0.3310, train acc 83.44%, f1 0.8337, precision 0.8355, recall 0.8319, auc 0.8344
epoch 801, loss 0.4771, train acc 83.45%, f1 0.8334, precision 0.8374, recall 0.8294, auc 0.8345
epoch 901, loss 0.2666, train acc 83.43%, f1 0.8335, precision 0.8361, recall 0.8308, auc 0.8343
epoch 1001, loss 0.4191, train acc 83.45%, f1 0.8337, precision 0.8363, recall 0.8311, auc 0.8345
epoch 1101, loss 0.5118, train acc 83.50%, f1 0.8343, precision 0.8362, recall 0.8325, auc 0.8350
epoch 1201, loss 0.4471, train acc 83.51%, f1 0.8345, precision 0.8358, recall 0.8332, auc 0.8351
epoch 1301, loss 0.3501, train acc 83.47%, f1 0.8339, precision 0.8366, recall 0.8312, auc 0.8347
epoch 1401, loss 0.3391, train acc 83.42%, f1 0.8338, precision 0.8343, recall 0.8332, auc 0.8342
epoch 1501, loss 0.4325, train acc 83.51%, f1 0.8344, precision 0.8365, recall 0.8324, auc 0.8351
epoch 1601, loss 0.3141, train acc 83.52%, f1 0.8348, precision 0.8356, recall 0.8339, auc 0.8352
epoch 1701, loss 0.5108, train acc 83.47%, f1 0.8336, precision 0.8379, recall 0.8293, auc 0.8347
epoch 1801, loss 0.3214, train acc 83.48%, f1 0.8344, precision 0.8348, recall 0.8340, auc 0.8348
epoch 1901, loss 0.3626, train acc 83.48%, f1 0.8342, precision 0.8362, recall 0.8322, auc 0.8348
epoch 2001, loss 0.4127, train acc 83.45%, f1 0.8338, precision 0.8356, recall 0.8320, auc 0.8345
epoch 2101, loss 0.4387, train acc 83.42%, f1 0.8336, precision 0.8352, recall 0.8321, auc 0.8342
epoch 2201, loss 0.2986, train acc 83.49%, f1 0.8347, precision 0.8343, recall 0.8350, auc 0.8349
epoch 2301, loss 0.3187, train acc 83.50%, f1 0.8341, precision 0.8369, recall 0.8314, auc 0.8350
epoch 2401, loss 0.4495, train acc 83.50%, f1 0.8346, precision 0.8354, recall 0.8338, auc 0.8350
epoch 2501, loss 0.3941, train acc 83.43%, f1 0.8334, precision 0.8362, recall 0.8307, auc 0.8343
epoch 2601, loss 0.3530, train acc 83.47%, f1 0.8336, precision 0.8378, recall 0.8294, auc 0.8347
epoch 2701, loss 0.3999, train acc 83.47%, f1 0.8346, precision 0.8337, recall 0.8354, auc 0.8347
epoch 2801, loss 0.3900, train acc 83.43%, f1 0.8336, precision 0.8355, recall 0.8317, auc 0.8343
epoch 2901, loss 0.3344, train acc 83.48%, f1 0.8340, precision 0.8366, recall 0.8315, auc 0.8348
epoch 3001, loss 0.3092, train acc 83.52%, f1 0.8345, precision 0.8369, recall 0.8320, auc 0.8352
epoch 3101, loss 0.4845, train acc 83.51%, f1 0.8345, precision 0.8360, recall 0.8330, auc 0.8351
epoch 3201, loss 0.3484, train acc 83.50%, f1 0.8342, precision 0.8369, recall 0.8315, auc 0.8350
epoch 3301, loss 0.4159, train acc 83.48%, f1 0.8341, precision 0.8365, recall 0.8317, auc 0.8348
epoch 3401, loss 0.4375, train acc 83.48%, f1 0.8338, precision 0.8375, recall 0.8302, auc 0.8348
epoch 3501, loss 0.3370, train acc 83.49%, f1 0.8341, precision 0.8367, recall 0.8316, auc 0.8349
epoch 3601, loss 0.2375, train acc 83.53%, f1 0.8349, precision 0.8354, recall 0.8343, auc 0.8353
epoch 3701, loss 0.3914, train acc 83.49%, f1 0.8339, precision 0.8378, recall 0.8300, auc 0.8349
epoch 3801, loss 0.5209, train acc 83.46%, f1 0.8341, precision 0.8350, recall 0.8332, auc 0.8346
epoch 3901, loss 0.4850, train acc 83.55%, f1 0.8351, precision 0.8357, recall 0.8344, auc 0.8355
epoch 4001, loss 0.3854, train acc 83.55%, f1 0.8349, precision 0.8365, recall 0.8334, auc 0.8355
epoch 4101, loss 0.2903, train acc 83.53%, f1 0.8345, precision 0.8367, recall 0.8323, auc 0.8353
epoch 4201, loss 0.3422, train acc 83.53%, f1 0.8346, precision 0.8370, recall 0.8322, auc 0.8353
epoch 4301, loss 0.3281, train acc 83.56%, f1 0.8354, precision 0.8353, recall 0.8354, auc 0.8356
epoch 4401, loss 0.3938, train acc 83.57%, f1 0.8348, precision 0.8379, recall 0.8318, auc 0.8357
epoch 4501, loss 0.3353, train acc 83.59%, f1 0.8354, precision 0.8367, recall 0.8341, auc 0.8359
epoch 4601, loss 0.3674, train acc 83.52%, f1 0.8348, precision 0.8357, recall 0.8338, auc 0.8352
epoch 4701, loss 0.2792, train acc 83.59%, f1 0.8355, precision 0.8362, recall 0.8348, auc 0.8359
epoch 4801, loss 0.3679, train acc 83.62%, f1 0.8356, precision 0.8369, recall 0.8343, auc 0.8362
epoch 4901, loss 0.3328, train acc 83.61%, f1 0.8354, precision 0.8374, recall 0.8334, auc 0.8361
epoch 5001, loss 0.3746, train acc 83.63%, f1 0.8359, precision 0.8365, recall 0.8353, auc 0.8363
epoch 5101, loss 0.3725, train acc 83.66%, f1 0.8360, precision 0.8378, recall 0.8342, auc 0.8366
epoch 5201, loss 0.4159, train acc 83.58%, f1 0.8349, precision 0.8380, recall 0.8318, auc 0.8358
epoch 5301, loss 0.3592, train acc 83.67%, f1 0.8365, precision 0.8361, recall 0.8368, auc 0.8367
epoch 5401, loss 0.3051, train acc 83.66%, f1 0.8362, precision 0.8369, recall 0.8355, auc 0.8366
epoch 5501, loss 0.2809, train acc 83.75%, f1 0.8365, precision 0.8401, recall 0.8330, auc 0.8375
epoch 5601, loss 0.2593, train acc 83.73%, f1 0.8367, precision 0.8382, recall 0.8352, auc 0.8373
epoch 5701, loss 0.2775, train acc 83.78%, f1 0.8369, precision 0.8404, recall 0.8334, auc 0.8378
epoch 5801, loss 0.3463, train acc 83.76%, f1 0.8369, precision 0.8392, recall 0.8346, auc 0.8376
epoch 5901, loss 0.3044, train acc 83.73%, f1 0.8372, precision 0.8362, recall 0.8383, auc 0.8373
epoch 6001, loss 0.3369, train acc 83.83%, f1 0.8380, precision 0.8380, recall 0.8381, auc 0.8383
epoch 6101, loss 0.3465, train acc 83.76%, f1 0.8367, precision 0.8395, recall 0.8340, auc 0.8376
epoch 6201, loss 0.2684, train acc 83.89%, f1 0.8387, precision 0.8384, recall 0.8390, auc 0.8389
epoch 6301, loss 0.2927, train acc 83.94%, f1 0.8389, precision 0.8398, recall 0.8381, auc 0.8394
epoch 6401, loss 0.3046, train acc 83.98%, f1 0.8385, precision 0.8440, recall 0.8330, auc 0.8398
epoch 6501, loss 0.3267, train acc 83.98%, f1 0.8394, precision 0.8400, recall 0.8387, auc 0.8398
epoch 6601, loss 0.4490, train acc 83.98%, f1 0.8389, precision 0.8420, recall 0.8359, auc 0.8398
epoch 6701, loss 0.4463, train acc 84.03%, f1 0.8400, precision 0.8401, recall 0.8399, auc 0.8403
epoch 6801, loss 0.2772, train acc 84.09%, f1 0.8403, precision 0.8421, recall 0.8386, auc 0.8409
epoch 6901, loss 0.4044, train acc 84.16%, f1 0.8404, precision 0.8452, recall 0.8355, auc 0.8416
epoch 7001, loss 0.3422, train acc 84.13%, f1 0.8405, precision 0.8437, recall 0.8373, auc 0.8413
epoch 7101, loss 0.3223, train acc 84.20%, f1 0.8416, precision 0.8423, recall 0.8408, auc 0.8420
epoch 7201, loss 0.3530, train acc 84.18%, f1 0.8415, precision 0.8420, recall 0.8409, auc 0.8418
epoch 7301, loss 0.4011, train acc 84.26%, f1 0.8423, precision 0.8422, recall 0.8424, auc 0.8426
epoch 7401, loss 0.3306, train acc 84.23%, f1 0.8424, precision 0.8403, recall 0.8445, auc 0.8423
epoch 7501, loss 0.4274, train acc 84.25%, f1 0.8418, precision 0.8438, recall 0.8399, auc 0.8424
epoch 7601, loss 0.2937, train acc 84.25%, f1 0.8421, precision 0.8426, recall 0.8416, auc 0.8425
epoch 7701, loss 0.4941, train acc 84.23%, f1 0.8423, precision 0.8406, recall 0.8441, auc 0.8423
epoch 7801, loss 0.4220, train acc 84.33%, f1 0.8433, precision 0.8420, recall 0.8445, auc 0.8433
epoch 7901, loss 0.2890, train acc 84.37%, f1 0.8435, precision 0.8432, recall 0.8438, auc 0.8437
epoch 8001, loss 0.2985, train acc 84.38%, f1 0.8431, precision 0.8454, recall 0.8409, auc 0.8438
epoch 8101, loss 0.2826, train acc 84.46%, f1 0.8440, precision 0.8458, recall 0.8422, auc 0.8446
epoch 8201, loss 0.2544, train acc 84.41%, f1 0.8437, precision 0.8441, recall 0.8433, auc 0.8441/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.4516, train acc 84.41%, f1 0.8436, precision 0.8447, recall 0.8425, auc 0.8441
epoch 8401, loss 0.3602, train acc 84.39%, f1 0.8436, precision 0.8439, recall 0.8433, auc 0.8439
epoch 8501, loss 0.3697, train acc 84.50%, f1 0.8445, precision 0.8458, recall 0.8432, auc 0.8450
epoch 8601, loss 0.2911, train acc 84.52%, f1 0.8447, precision 0.8463, recall 0.8430, auc 0.8452
epoch 8701, loss 0.3651, train acc 84.58%, f1 0.8457, precision 0.8449, recall 0.8464, auc 0.8458
epoch 8801, loss 0.4159, train acc 84.57%, f1 0.8454, precision 0.8455, recall 0.8454, auc 0.8457
epoch 8901, loss 0.3417, train acc 84.56%, f1 0.8447, precision 0.8480, recall 0.8414, auc 0.8456
epoch 9001, loss 0.4036, train acc 84.59%, f1 0.8453, precision 0.8471, recall 0.8435, auc 0.8459
epoch 9101, loss 0.3945, train acc 84.59%, f1 0.8455, precision 0.8461, recall 0.8449, auc 0.8459
epoch 9201, loss 0.4122, train acc 84.52%, f1 0.8448, precision 0.8459, recall 0.8436, auc 0.8452
epoch 9301, loss 0.3910, train acc 84.61%, f1 0.8458, precision 0.8461, recall 0.8455, auc 0.8461
epoch 9401, loss 0.2830, train acc 84.62%, f1 0.8455, precision 0.8479, recall 0.8430, auc 0.8462
epoch 9501, loss 0.2709, train acc 84.60%, f1 0.8455, precision 0.8471, recall 0.8439, auc 0.8460
epoch 9601, loss 0.3166, train acc 84.60%, f1 0.8456, precision 0.8463, recall 0.8449, auc 0.8460
epoch 9701, loss 0.3856, train acc 84.58%, f1 0.8456, precision 0.8451, recall 0.8461, auc 0.8458
epoch 9801, loss 0.4257, train acc 84.67%, f1 0.8462, precision 0.8478, recall 0.8446, auc 0.8467
epoch 9901, loss 0.3639, train acc 84.66%, f1 0.8458, precision 0.8488, recall 0.8428, auc 0.8466
epoch 10001, loss 0.4547, train acc 84.69%, f1 0.8467, precision 0.8465, recall 0.8469, auc 0.8469
epoch 10101, loss 0.2657, train acc 84.59%, f1 0.8453, precision 0.8471, recall 0.8435, auc 0.8459
epoch 10201, loss 0.3601, train acc 84.64%, f1 0.8464, precision 0.8453, recall 0.8475, auc 0.8465
epoch 10301, loss 0.2303, train acc 84.73%, f1 0.8468, precision 0.8476, recall 0.8461, auc 0.8473
epoch 10401, loss 0.3412, train acc 84.75%, f1 0.8473, precision 0.8472, recall 0.8474, auc 0.8475
epoch 10501, loss 0.4351, train acc 84.75%, f1 0.8475, precision 0.8461, recall 0.8489, auc 0.8475
epoch 10601, loss 0.3263, train acc 84.76%, f1 0.8473, precision 0.8477, recall 0.8469, auc 0.8476
epoch 10701, loss 0.3489, train acc 84.80%, f1 0.8477, precision 0.8484, recall 0.8469, auc 0.8480
epoch 10801, loss 0.4688, train acc 84.85%, f1 0.8478, precision 0.8499, recall 0.8458, auc 0.8485
epoch 10901, loss 0.3539, train acc 84.79%, f1 0.8481, precision 0.8455, recall 0.8508, auc 0.8479
epoch 11001, loss 0.4682, train acc 84.86%, f1 0.8481, precision 0.8492, recall 0.8470, auc 0.8486
epoch 11101, loss 0.3646, train acc 84.81%, f1 0.8479, precision 0.8476, recall 0.8481, auc 0.8481
epoch 11201, loss 0.3374, train acc 84.80%, f1 0.8474, precision 0.8490, recall 0.8458, auc 0.8480
epoch 11301, loss 0.4922, train acc 84.85%, f1 0.8481, precision 0.8490, recall 0.8471, auc 0.8485
epoch 11401, loss 0.3592, train acc 84.85%, f1 0.8476, precision 0.8511, recall 0.8441, auc 0.8485
epoch 11501, loss 0.4031, train acc 84.84%, f1 0.8480, precision 0.8483, recall 0.8477, auc 0.8484
epoch 11601, loss 0.2816, train acc 84.84%, f1 0.8478, precision 0.8500, recall 0.8456, auc 0.8484
epoch 11701, loss 0.3007, train acc 84.92%, f1 0.8488, precision 0.8496, recall 0.8480, auc 0.8492
epoch 11801, loss 0.3709, train acc 84.89%, f1 0.8490, precision 0.8469, recall 0.8512, auc 0.8489
epoch 11901, loss 0.2887, train acc 84.84%, f1 0.8481, precision 0.8485, recall 0.8477, auc 0.8484
epoch 12001, loss 0.4079, train acc 84.91%, f1 0.8485, precision 0.8503, recall 0.8466, auc 0.8491
epoch 12101, loss 0.3508, train acc 84.87%, f1 0.8481, precision 0.8500, recall 0.8462, auc 0.8487
epoch 12201, loss 0.3107, train acc 84.86%, f1 0.8480, precision 0.8497, recall 0.8463, auc 0.8485
epoch 12301, loss 0.4553, train acc 84.96%, f1 0.8487, precision 0.8520, recall 0.8454, auc 0.8495
epoch 12401, loss 0.3309, train acc 84.96%, f1 0.8490, precision 0.8510, recall 0.8470, auc 0.8496
epoch 12501, loss 0.3231, train acc 85.00%, f1 0.8493, precision 0.8514, recall 0.8472, auc 0.8500
epoch 12601, loss 0.2514, train acc 85.00%, f1 0.8497, precision 0.8496, recall 0.8499, auc 0.8500
epoch 12701, loss 0.4864, train acc 85.03%, f1 0.8502, precision 0.8496, recall 0.8507, auc 0.8503
epoch 12801, loss 0.2791, train acc 85.01%, f1 0.8496, precision 0.8510, recall 0.8481, auc 0.8501
epoch 12901, loss 0.3379, train acc 85.00%, f1 0.8496, precision 0.8508, recall 0.8483, auc 0.8500
epoch 13001, loss 0.3795, train acc 85.07%, f1 0.8504, precision 0.8504, recall 0.8504, auc 0.8507
epoch 13101, loss 0.3371, train acc 85.06%, f1 0.8503, precision 0.8505, recall 0.8501, auc 0.8506
epoch 13201, loss 0.3387, train acc 85.04%, f1 0.8502, precision 0.8500, recall 0.8503, auc 0.8504
epoch 13301, loss 0.3653, train acc 85.07%, f1 0.8504, precision 0.8509, recall 0.8499, auc 0.8507
epoch 13401, loss 0.3971, train acc 85.13%, f1 0.8506, precision 0.8529, recall 0.8483, auc 0.8513
epoch 13501, loss 0.4081, train acc 85.14%, f1 0.8506, precision 0.8534, recall 0.8479, auc 0.8514
epoch 13601, loss 0.2631, train acc 85.11%, f1 0.8505, precision 0.8525, recall 0.8485, auc 0.8511
epoch 13701, loss 0.3870, train acc 85.15%, f1 0.8507, precision 0.8536, recall 0.8479, auc 0.8515
epoch 13801, loss 0.3897, train acc 85.14%, f1 0.8506, precision 0.8538, recall 0.8474, auc 0.8514
epoch 13901, loss 0.3317, train acc 85.14%, f1 0.8512, precision 0.8510, recall 0.8514, auc 0.8514
epoch 14001, loss 0.2850, train acc 85.17%, f1 0.8512, precision 0.8525, recall 0.8498, auc 0.8517
epoch 14101, loss 0.2883, train acc 85.13%, f1 0.8511, precision 0.8507, recall 0.8514, auc 0.8513
epoch 14201, loss 0.3079, train acc 85.15%, f1 0.8508, precision 0.8531, recall 0.8486, auc 0.8515
epoch 14301, loss 0.3257, train acc 85.12%, f1 0.8507, precision 0.8521, recall 0.8493, auc 0.8512
epoch 14401, loss 0.3595, train acc 85.14%, f1 0.8507, precision 0.8527, recall 0.8488, auc 0.8514
epoch 14501, loss 0.2760, train acc 85.19%, f1 0.8510, precision 0.8546, recall 0.8475, auc 0.8519
epoch 14601, loss 0.4167, train acc 85.14%, f1 0.8509, precision 0.8521, recall 0.8498, auc 0.8514
epoch 14701, loss 0.4134, train acc 85.19%, f1 0.8511, precision 0.8540, recall 0.8482, auc 0.8519
epoch 14801, loss 0.3086, train acc 85.22%, f1 0.8520, precision 0.8519, recall 0.8520, auc 0.8522
epoch 14901, loss 0.2522, train acc 85.22%, f1 0.8517, precision 0.8532, recall 0.8502, auc 0.8522
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_1
./test_pima/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.652962962962963

the Fscore is 0.6024096385542168

the precision is 0.44642857142857145

the recall is 0.9259259259259259

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_1
----------------------



epoch 1, loss 0.6933, train acc 49.90%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5920, train acc 77.67%, f1 0.7703, precision 0.7946, recall 0.7474, auc 0.7767
epoch 201, loss 0.4725, train acc 80.29%, f1 0.8014, precision 0.8090, recall 0.7939, auc 0.8029
epoch 301, loss 0.4717, train acc 81.83%, f1 0.8186, precision 0.8190, recall 0.8182, auc 0.8183
epoch 401, loss 0.2934, train acc 82.76%, f1 0.8280, precision 0.8277, recall 0.8283, auc 0.8276
epoch 501, loss 0.4246, train acc 83.13%, f1 0.8319, precision 0.8304, recall 0.8334, auc 0.8313
epoch 601, loss 0.4276, train acc 83.39%, f1 0.8349, precision 0.8319, recall 0.8379, auc 0.8339
epoch 701, loss 0.4931, train acc 83.41%, f1 0.8348, precision 0.8325, recall 0.8372, auc 0.8340
epoch 801, loss 0.3602, train acc 83.49%, f1 0.8361, precision 0.8317, recall 0.8404, auc 0.8349
epoch 901, loss 0.4919, train acc 83.50%, f1 0.8359, precision 0.8329, recall 0.8390, auc 0.8350
epoch 1001, loss 0.3834, train acc 83.49%, f1 0.8358, precision 0.8328, recall 0.8389, auc 0.8349
epoch 1101, loss 0.4538, train acc 83.48%, f1 0.8360, precision 0.8312, recall 0.8409, auc 0.8347
epoch 1201, loss 0.3961, train acc 83.49%, f1 0.8354, precision 0.8343, recall 0.8366, auc 0.8349
epoch 1301, loss 0.3259, train acc 83.49%, f1 0.8357, precision 0.8331, recall 0.8384, auc 0.8349
epoch 1401, loss 0.4211, train acc 83.51%, f1 0.8359, precision 0.8335, recall 0.8383, auc 0.8351
epoch 1501, loss 0.3821, train acc 83.45%, f1 0.8356, precision 0.8314, recall 0.8399, auc 0.8344
epoch 1601, loss 0.3458, train acc 83.48%, f1 0.8354, precision 0.8336, recall 0.8372, auc 0.8347
epoch 1701, loss 0.3874, train acc 83.49%, f1 0.8359, precision 0.8326, recall 0.8393, auc 0.8349
epoch 1801, loss 0.4550, train acc 83.52%, f1 0.8358, precision 0.8346, recall 0.8369, auc 0.8352
epoch 1901, loss 0.3410, train acc 83.46%, f1 0.8354, precision 0.8330, recall 0.8379, auc 0.8346
epoch 2001, loss 0.4423, train acc 83.44%, f1 0.8352, precision 0.8328, recall 0.8376, auc 0.8344
epoch 2101, loss 0.3582, train acc 83.52%, f1 0.8358, precision 0.8340, recall 0.8376, auc 0.8351
epoch 2201, loss 0.4190, train acc 83.48%, f1 0.8353, precision 0.8343, recall 0.8363, auc 0.8348
epoch 2301, loss 0.3417, train acc 83.44%, f1 0.8353, precision 0.8326, recall 0.8380, auc 0.8344
epoch 2401, loss 0.4451, train acc 83.46%, f1 0.8352, precision 0.8339, recall 0.8365, auc 0.8346
epoch 2501, loss 0.3972, train acc 83.49%, f1 0.8354, precision 0.8345, recall 0.8364, auc 0.8349
epoch 2601, loss 0.4298, train acc 83.48%, f1 0.8352, precision 0.8350, recall 0.8354, auc 0.8348
epoch 2701, loss 0.4763, train acc 83.50%, f1 0.8357, precision 0.8336, recall 0.8379, auc 0.8350
epoch 2801, loss 0.5551, train acc 83.50%, f1 0.8357, precision 0.8338, recall 0.8376, auc 0.8350
epoch 2901, loss 0.4472, train acc 83.47%, f1 0.8356, precision 0.8323, recall 0.8390, auc 0.8347
epoch 3001, loss 0.6151, train acc 83.49%, f1 0.8355, precision 0.8342, recall 0.8367, auc 0.8349
epoch 3101, loss 0.4365, train acc 83.46%, f1 0.8353, precision 0.8335, recall 0.8370, auc 0.8346
epoch 3201, loss 0.4057, train acc 83.49%, f1 0.8354, precision 0.8343, recall 0.8366, auc 0.8349
epoch 3301, loss 0.3989, train acc 83.48%, f1 0.8351, precision 0.8349, recall 0.8354, auc 0.8348
epoch 3401, loss 0.3775, train acc 83.45%, f1 0.8351, precision 0.8338, recall 0.8364, auc 0.8345
epoch 3501, loss 0.3172, train acc 83.52%, f1 0.8357, precision 0.8348, recall 0.8366, auc 0.8352
epoch 3601, loss 0.3286, train acc 83.52%, f1 0.8359, precision 0.8341, recall 0.8376, auc 0.8352
epoch 3701, loss 0.3729, train acc 83.56%, f1 0.8363, precision 0.8346, recall 0.8380, auc 0.8356
epoch 3801, loss 0.3940, train acc 83.54%, f1 0.8356, precision 0.8359, recall 0.8353, auc 0.8354
epoch 3901, loss 0.2861, train acc 83.58%, f1 0.8367, precision 0.8339, recall 0.8396, auc 0.8358
epoch 4001, loss 0.3964, train acc 83.55%, f1 0.8357, precision 0.8362, recall 0.8353, auc 0.8355
epoch 4101, loss 0.3234, train acc 83.58%, f1 0.8361, precision 0.8361, recall 0.8362, auc 0.8358
epoch 4201, loss 0.3153, train acc 83.54%, f1 0.8359, precision 0.8351, recall 0.8367, auc 0.8354
epoch 4301, loss 0.3330, train acc 83.63%, f1 0.8368, precision 0.8358, recall 0.8377, auc 0.8363
epoch 4401, loss 0.3652, train acc 83.63%, f1 0.8370, precision 0.8351, recall 0.8388, auc 0.8363
epoch 4501, loss 0.3231, train acc 83.67%, f1 0.8373, precision 0.8357, recall 0.8388, auc 0.8367
epoch 4601, loss 0.4275, train acc 83.68%, f1 0.8371, precision 0.8369, recall 0.8374, auc 0.8368
epoch 4701, loss 0.3648, train acc 83.65%, f1 0.8369, precision 0.8364, recall 0.8374, auc 0.8365
epoch 4801, loss 0.4065, train acc 83.61%, f1 0.8365, precision 0.8362, recall 0.8368, auc 0.8361
epoch 4901, loss 0.2692, train acc 83.74%, f1 0.8381, precision 0.8363, recall 0.8398, auc 0.8374
epoch 5001, loss 0.2888, train acc 83.71%, f1 0.8374, precision 0.8377, recall 0.8371, auc 0.8371
epoch 5101, loss 0.4524, train acc 83.80%, f1 0.8386, precision 0.8370, recall 0.8402, auc 0.8380
epoch 5201, loss 0.3715, train acc 83.84%, f1 0.8388, precision 0.8382, recall 0.8394, auc 0.8384
epoch 5301, loss 0.2904, train acc 83.85%, f1 0.8391, precision 0.8375, recall 0.8408, auc 0.8385
epoch 5401, loss 0.3171, train acc 83.86%, f1 0.8393, precision 0.8371, recall 0.8416, auc 0.8386
epoch 5501, loss 0.3120, train acc 83.84%, f1 0.8393, precision 0.8366, recall 0.8419, auc 0.8384
epoch 5601, loss 0.3183, train acc 83.87%, f1 0.8392, precision 0.8382, recall 0.8403, auc 0.8387
epoch 5701, loss 0.3667, train acc 83.82%, f1 0.8389, precision 0.8369, recall 0.8410, auc 0.8382
epoch 5801, loss 0.4647, train acc 83.91%, f1 0.8396, precision 0.8387, recall 0.8406, auc 0.8391
epoch 5901, loss 0.3398, train acc 83.90%, f1 0.8394, precision 0.8390, recall 0.8397, auc 0.8390
epoch 6001, loss 0.3847, train acc 83.95%, f1 0.8401, precision 0.8382, recall 0.8421, auc 0.8395
epoch 6101, loss 0.3537, train acc 83.95%, f1 0.8401, precision 0.8388, recall 0.8413, auc 0.8395
epoch 6201, loss 0.2915, train acc 83.99%, f1 0.8404, precision 0.8396, recall 0.8412, auc 0.8399
epoch 6301, loss 0.3478, train acc 84.06%, f1 0.8407, precision 0.8418, recall 0.8395, auc 0.8406
epoch 6401, loss 0.3150, train acc 84.04%, f1 0.8411, precision 0.8392, recall 0.8430, auc 0.8404
epoch 6501, loss 0.3601, train acc 84.08%, f1 0.8417, precision 0.8383, recall 0.8452, auc 0.8408
epoch 6601, loss 0.4690, train acc 84.01%, f1 0.8402, precision 0.8412, recall 0.8392, auc 0.8401
epoch 6701, loss 0.2746, train acc 84.02%, f1 0.8404, precision 0.8409, recall 0.8400, auc 0.8402
epoch 6801, loss 0.4290, train acc 84.12%, f1 0.8416, precision 0.8410, recall 0.8421, auc 0.8412
epoch 6901, loss 0.3209, train acc 84.20%, f1 0.8429, precision 0.8401, recall 0.8456, auc 0.8420
epoch 7001, loss 0.3217, train acc 84.21%, f1 0.8427, precision 0.8412, recall 0.8441, auc 0.8421
epoch 7101, loss 0.3565, train acc 84.25%, f1 0.8427, precision 0.8429, recall 0.8426, auc 0.8425
epoch 7201, loss 0.3853, train acc 84.18%, f1 0.8425, precision 0.8405, recall 0.8444, auc 0.8418
epoch 7301, loss 0.3643, train acc 84.19%, f1 0.8425, precision 0.8406, recall 0.8445, auc 0.8419
epoch 7401, loss 0.4618, train acc 84.29%, f1 0.8436, precision 0.8419, recall 0.8453, auc 0.8429
epoch 7501, loss 0.4205, train acc 84.27%, f1 0.8428, precision 0.8435, recall 0.8421, auc 0.8427
epoch 7601, loss 0.2573, train acc 84.33%, f1 0.8436, precision 0.8438, recall 0.8433, auc 0.8433
epoch 7701, loss 0.3134, train acc 84.32%, f1 0.8438, precision 0.8424, recall 0.8452, auc 0.8432
epoch 7801, loss 0.2469, train acc 84.35%, f1 0.8437, precision 0.8442, recall 0.8433, auc 0.8435
epoch 7901, loss 0.3862, train acc 84.26%, f1 0.8430, precision 0.8427, recall 0.8432, auc 0.8426
epoch 8001, loss 0.4541, train acc 84.36%, f1 0.8438, precision 0.8439, recall 0.8438, auc 0.8436
epoch 8101, loss 0.4129, train acc 84.42%, f1 0.8443, precision 0.8453, recall 0.8432, auc 0.8442
epoch 8201, loss 0.2472, train acc 84.41%, f1 0.8444, precision 0.8445, recall 0.8444, auc 0.8441/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.4007, train acc 84.46%, f1 0.8452, precision 0.8438, recall 0.8465, auc 0.8446
epoch 8401, loss 0.3538, train acc 84.46%, f1 0.8450, precision 0.8442, recall 0.8458, auc 0.8446
epoch 8501, loss 0.4796, train acc 84.45%, f1 0.8454, precision 0.8423, recall 0.8486, auc 0.8445
epoch 8601, loss 0.2998, train acc 84.50%, f1 0.8454, precision 0.8449, recall 0.8459, auc 0.8450
epoch 8701, loss 0.3238, train acc 84.52%, f1 0.8455, precision 0.8458, recall 0.8452, auc 0.8452
epoch 8801, loss 0.3648, train acc 84.57%, f1 0.8460, precision 0.8459, recall 0.8461, auc 0.8457
epoch 8901, loss 0.3332, train acc 84.53%, f1 0.8459, precision 0.8442, recall 0.8476, auc 0.8453
epoch 9001, loss 0.2995, train acc 84.58%, f1 0.8466, precision 0.8441, recall 0.8491, auc 0.8458
epoch 9101, loss 0.5361, train acc 84.56%, f1 0.8455, precision 0.8474, recall 0.8436, auc 0.8456
epoch 9201, loss 0.4330, train acc 84.60%, f1 0.8467, precision 0.8441, recall 0.8494, auc 0.8460
epoch 9301, loss 0.2974, train acc 84.62%, f1 0.8469, precision 0.8446, recall 0.8492, auc 0.8462
epoch 9401, loss 0.4608, train acc 84.64%, f1 0.8468, precision 0.8459, recall 0.8478, auc 0.8464
epoch 9501, loss 0.3380, train acc 84.62%, f1 0.8466, precision 0.8460, recall 0.8473, auc 0.8462
epoch 9601, loss 0.3480, train acc 84.70%, f1 0.8471, precision 0.8478, recall 0.8464, auc 0.8470
epoch 9701, loss 0.3772, train acc 84.61%, f1 0.8464, precision 0.8462, recall 0.8467, auc 0.8461
epoch 9801, loss 0.3949, train acc 84.69%, f1 0.8471, precision 0.8472, recall 0.8470, auc 0.8469
epoch 9901, loss 0.3210, train acc 84.72%, f1 0.8473, precision 0.8484, recall 0.8462, auc 0.8472
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_1
./test_pima/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6314814814814815

the Fscore is 0.5909090909090909

the precision is 0.4262295081967213

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_1
----------------------



epoch 1, loss 0.6929, train acc 49.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6095, train acc 77.82%, f1 0.7680, precision 0.8056, recall 0.7337, auc 0.7783
epoch 201, loss 0.4545, train acc 80.46%, f1 0.8032, precision 0.8094, recall 0.7972, auc 0.8047
epoch 301, loss 0.4162, train acc 82.06%, f1 0.8214, precision 0.8182, recall 0.8246, auc 0.8206
epoch 401, loss 0.3732, train acc 82.78%, f1 0.8284, precision 0.8258, recall 0.8309, auc 0.8278
epoch 501, loss 0.3664, train acc 83.16%, f1 0.8323, precision 0.8292, recall 0.8354, auc 0.8316
epoch 601, loss 0.3925, train acc 83.33%, f1 0.8345, precision 0.8290, recall 0.8400, auc 0.8333
epoch 701, loss 0.3147, train acc 83.39%, f1 0.8350, precision 0.8303, recall 0.8397, auc 0.8339
epoch 801, loss 0.3428, train acc 83.46%, f1 0.8358, precision 0.8300, recall 0.8417, auc 0.8346
epoch 901, loss 0.3965, train acc 83.50%, f1 0.8356, precision 0.8332, recall 0.8379, auc 0.8350
epoch 1001, loss 0.3593, train acc 83.48%, f1 0.8357, precision 0.8314, recall 0.8400, auc 0.8348
epoch 1101, loss 0.4164, train acc 83.47%, f1 0.8354, precision 0.8324, recall 0.8385, auc 0.8347
epoch 1201, loss 0.3175, train acc 83.45%, f1 0.8352, precision 0.8320, recall 0.8384, auc 0.8345
epoch 1301, loss 0.2955, train acc 83.48%, f1 0.8357, precision 0.8318, recall 0.8396, auc 0.8348
epoch 1401, loss 0.2599, train acc 83.50%, f1 0.8358, precision 0.8322, recall 0.8394, auc 0.8350
epoch 1501, loss 0.3443, train acc 83.50%, f1 0.8354, precision 0.8335, recall 0.8374, auc 0.8350
epoch 1601, loss 0.3447, train acc 83.51%, f1 0.8356, precision 0.8338, recall 0.8373, auc 0.8351
epoch 1701, loss 0.3512, train acc 83.48%, f1 0.8353, precision 0.8331, recall 0.8376, auc 0.8348
epoch 1801, loss 0.4437, train acc 83.44%, f1 0.8351, precision 0.8318, recall 0.8384, auc 0.8344
epoch 1901, loss 0.3271, train acc 83.46%, f1 0.8353, precision 0.8322, recall 0.8385, auc 0.8346
epoch 2001, loss 0.3234, train acc 83.48%, f1 0.8359, precision 0.8306, recall 0.8412, auc 0.8348
epoch 2101, loss 0.2992, train acc 83.46%, f1 0.8354, precision 0.8321, recall 0.8386, auc 0.8346
epoch 2201, loss 0.3803, train acc 83.42%, f1 0.8345, precision 0.8333, recall 0.8358, auc 0.8342
epoch 2301, loss 0.3831, train acc 83.44%, f1 0.8349, precision 0.8327, recall 0.8372, auc 0.8344
epoch 2401, loss 0.3409, train acc 83.52%, f1 0.8355, precision 0.8345, recall 0.8364, auc 0.8352
epoch 2501, loss 0.3437, train acc 83.52%, f1 0.8355, precision 0.8342, recall 0.8368, auc 0.8352
epoch 2601, loss 0.4839, train acc 83.50%, f1 0.8355, precision 0.8335, recall 0.8376, auc 0.8350
epoch 2701, loss 0.3287, train acc 83.50%, f1 0.8358, precision 0.8319, recall 0.8397, auc 0.8350
epoch 2801, loss 0.2977, train acc 83.50%, f1 0.8354, precision 0.8335, recall 0.8374, auc 0.8350
epoch 2901, loss 0.4860, train acc 83.50%, f1 0.8351, precision 0.8345, recall 0.8357, auc 0.8350
epoch 3001, loss 0.3225, train acc 83.51%, f1 0.8351, precision 0.8352, recall 0.8350, auc 0.8351
epoch 3101, loss 0.4981, train acc 83.53%, f1 0.8355, precision 0.8349, recall 0.8361, auc 0.8353
epoch 3201, loss 0.3882, train acc 83.50%, f1 0.8350, precision 0.8353, recall 0.8348, auc 0.8350
epoch 3301, loss 0.3309, train acc 83.50%, f1 0.8356, precision 0.8328, recall 0.8385, auc 0.8350
epoch 3401, loss 0.4920, train acc 83.55%, f1 0.8356, precision 0.8350, recall 0.8363, auc 0.8355
epoch 3501, loss 0.3397, train acc 83.51%, f1 0.8357, precision 0.8330, recall 0.8384, auc 0.8351
epoch 3601, loss 0.4184, train acc 83.53%, f1 0.8355, precision 0.8348, recall 0.8362, auc 0.8353
epoch 3701, loss 0.4148, train acc 83.54%, f1 0.8361, precision 0.8331, recall 0.8390, auc 0.8354
epoch 3801, loss 0.3495, train acc 83.57%, f1 0.8361, precision 0.8343, recall 0.8379, auc 0.8357
epoch 3901, loss 0.4052, train acc 83.54%, f1 0.8365, precision 0.8313, recall 0.8417, auc 0.8354
epoch 4001, loss 0.3762, train acc 83.55%, f1 0.8356, precision 0.8354, recall 0.8358, auc 0.8355
epoch 4101, loss 0.3816, train acc 83.44%, f1 0.8348, precision 0.8330, recall 0.8367, auc 0.8344
epoch 4201, loss 0.4001, train acc 83.58%, f1 0.8356, precision 0.8367, recall 0.8346, auc 0.8358
epoch 4301, loss 0.2469, train acc 83.62%, f1 0.8367, precision 0.8346, recall 0.8389, auc 0.8362
epoch 4401, loss 0.3169, train acc 83.62%, f1 0.8361, precision 0.8371, recall 0.8350, auc 0.8362
epoch 4501, loss 0.3428, train acc 83.61%, f1 0.8370, precision 0.8325, recall 0.8416, auc 0.8361
epoch 4601, loss 0.5284, train acc 83.62%, f1 0.8362, precision 0.8370, recall 0.8353, auc 0.8363
epoch 4701, loss 0.4272, train acc 83.68%, f1 0.8371, precision 0.8356, recall 0.8386, auc 0.8368
epoch 4801, loss 0.3365, train acc 83.70%, f1 0.8375, precision 0.8356, recall 0.8393, auc 0.8370
epoch 4901, loss 0.2809, train acc 83.74%, f1 0.8379, precision 0.8358, recall 0.8400, auc 0.8374
epoch 5001, loss 0.3597, train acc 83.70%, f1 0.8379, precision 0.8337, recall 0.8421, auc 0.8370
epoch 5101, loss 0.3214, train acc 83.75%, f1 0.8380, precision 0.8355, recall 0.8405, auc 0.8375
epoch 5201, loss 0.4799, train acc 83.79%, f1 0.8379, precision 0.8382, recall 0.8376, auc 0.8379
epoch 5301, loss 0.3533, train acc 83.79%, f1 0.8382, precision 0.8367, recall 0.8398, auc 0.8379
epoch 5401, loss 0.3605, train acc 83.84%, f1 0.8387, precision 0.8378, recall 0.8396, auc 0.8384
epoch 5501, loss 0.3068, train acc 83.80%, f1 0.8387, precision 0.8355, recall 0.8419, auc 0.8380
epoch 5601, loss 0.3931, train acc 83.91%, f1 0.8394, precision 0.8379, recall 0.8410, auc 0.8391
epoch 5701, loss 0.3555, train acc 83.93%, f1 0.8397, precision 0.8381, recall 0.8412, auc 0.8393
epoch 5801, loss 0.3031, train acc 83.91%, f1 0.8393, precision 0.8384, recall 0.8403, auc 0.8391
epoch 5901, loss 0.4270, train acc 83.91%, f1 0.8395, precision 0.8374, recall 0.8417, auc 0.8391
epoch 6001, loss 0.3963, train acc 83.95%, f1 0.8400, precision 0.8376, recall 0.8425, auc 0.8395
epoch 6101, loss 0.2652, train acc 83.98%, f1 0.8406, precision 0.8367, recall 0.8445, auc 0.8398
epoch 6201, loss 0.4643, train acc 83.98%, f1 0.8402, precision 0.8387, recall 0.8417, auc 0.8398
epoch 6301, loss 0.4475, train acc 84.01%, f1 0.8400, precision 0.8409, recall 0.8391, auc 0.8401
epoch 6401, loss 0.2838, train acc 84.03%, f1 0.8409, precision 0.8382, recall 0.8436, auc 0.8403
epoch 6501, loss 0.3911, train acc 84.04%, f1 0.8405, precision 0.8403, recall 0.8406, auc 0.8404
epoch 6601, loss 0.2587, train acc 84.09%, f1 0.8410, precision 0.8409, recall 0.8411, auc 0.8409
epoch 6701, loss 0.3213, train acc 84.09%, f1 0.8412, precision 0.8396, recall 0.8429, auc 0.8409
epoch 6801, loss 0.3489, train acc 84.11%, f1 0.8407, precision 0.8434, recall 0.8380, auc 0.8411
epoch 6901, loss 0.3860, train acc 84.13%, f1 0.8417, precision 0.8403, recall 0.8431, auc 0.8413
epoch 7001, loss 0.3692, train acc 84.15%, f1 0.8413, precision 0.8427, recall 0.8400, auc 0.8415
epoch 7101, loss 0.3835, train acc 84.19%, f1 0.8425, precision 0.8398, recall 0.8451, auc 0.8419
epoch 7201, loss 0.3855, train acc 84.23%, f1 0.8427, precision 0.8410, recall 0.8444, auc 0.8423
epoch 7301, loss 0.3409, train acc 84.23%, f1 0.8430, precision 0.8395, recall 0.8466, auc 0.8423
epoch 7401, loss 0.3444, train acc 84.24%, f1 0.8424, precision 0.8428, recall 0.8420, auc 0.8424
epoch 7501, loss 0.3092, train acc 84.28%, f1 0.8432, precision 0.8416, recall 0.8449, auc 0.8428
epoch 7601, loss 0.3360, train acc 84.29%, f1 0.8435, precision 0.8404, recall 0.8466, auc 0.8428
epoch 7701, loss 0.2886, train acc 84.29%, f1 0.8429, precision 0.8430, recall 0.8428, auc 0.8429
epoch 7801, loss 0.3767, train acc 84.28%, f1 0.8426, precision 0.8441, recall 0.8412, auc 0.8428
epoch 7901, loss 0.3519, train acc 84.30%, f1 0.8431, precision 0.8431, recall 0.8431, auc 0.8430
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_1
./test_pima/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6364814814814814

the Fscore is 0.5942857142857143

the precision is 0.4297520661157025

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_1
----------------------



epoch 1, loss 0.6932, train acc 50.31%, f1 0.6694, precision 0.5031, recall 1.0000, auc 0.5000
epoch 101, loss 0.6171, train acc 78.23%, f1 0.7852, precision 0.7797, recall 0.7907, auc 0.7823
epoch 201, loss 0.4563, train acc 80.58%, f1 0.8053, precision 0.8126, recall 0.7981, auc 0.8059
epoch 301, loss 0.5049, train acc 81.98%, f1 0.8205, precision 0.8224, recall 0.8185, auc 0.8198
epoch 401, loss 0.4092, train acc 82.80%, f1 0.8282, precision 0.8321, recall 0.8244, auc 0.8280
epoch 501, loss 0.4282, train acc 83.13%, f1 0.8316, precision 0.8354, recall 0.8279, auc 0.8314
epoch 601, loss 0.4886, train acc 83.39%, f1 0.8344, precision 0.8373, recall 0.8315, auc 0.8340
epoch 701, loss 0.4022, train acc 83.42%, f1 0.8344, precision 0.8384, recall 0.8305, auc 0.8342
epoch 801, loss 0.3714, train acc 83.46%, f1 0.8350, precision 0.8382, recall 0.8319, auc 0.8346
epoch 901, loss 0.3617, train acc 83.49%, f1 0.8359, precision 0.8361, recall 0.8356, auc 0.8349
epoch 1001, loss 0.4601, train acc 83.50%, f1 0.8357, precision 0.8376, recall 0.8339, auc 0.8351
epoch 1101, loss 0.2593, train acc 83.50%, f1 0.8352, precision 0.8391, recall 0.8315, auc 0.8350
epoch 1201, loss 0.3843, train acc 83.44%, f1 0.8352, precision 0.8365, recall 0.8339, auc 0.8344
epoch 1301, loss 0.4211, train acc 83.47%, f1 0.8350, precision 0.8386, recall 0.8315, auc 0.8347
epoch 1401, loss 0.3305, train acc 83.44%, f1 0.8350, precision 0.8375, recall 0.8325, auc 0.8344
epoch 1501, loss 0.3015, train acc 83.42%, f1 0.8346, precision 0.8374, recall 0.8319, auc 0.8342
epoch 1601, loss 0.3821, train acc 83.48%, f1 0.8354, precision 0.8374, recall 0.8335, auc 0.8348
epoch 1701, loss 0.3777, train acc 83.45%, f1 0.8354, precision 0.8361, recall 0.8347, auc 0.8345
epoch 1801, loss 0.5298, train acc 83.49%, f1 0.8354, precision 0.8379, recall 0.8330, auc 0.8349
epoch 1901, loss 0.3699, train acc 83.45%, f1 0.8349, precision 0.8380, recall 0.8318, auc 0.8345
epoch 2001, loss 0.4605, train acc 83.45%, f1 0.8350, precision 0.8373, recall 0.8328, auc 0.8345
epoch 2101, loss 0.3342, train acc 83.48%, f1 0.8354, precision 0.8377, recall 0.8331, auc 0.8348
epoch 2201, loss 0.4039, train acc 83.47%, f1 0.8353, precision 0.8373, recall 0.8333, auc 0.8347
epoch 2301, loss 0.2433, train acc 83.48%, f1 0.8349, precision 0.8393, recall 0.8306, auc 0.8348
epoch 2401, loss 0.4138, train acc 83.44%, f1 0.8349, precision 0.8380, recall 0.8317, auc 0.8345
epoch 2501, loss 0.3861, train acc 83.41%, f1 0.8348, precision 0.8366, recall 0.8330, auc 0.8341
epoch 2601, loss 0.3461, train acc 83.40%, f1 0.8349, precision 0.8358, recall 0.8340, auc 0.8340
epoch 2701, loss 0.3297, train acc 83.46%, f1 0.8354, precision 0.8367, recall 0.8341, auc 0.8347
epoch 2801, loss 0.3809, train acc 83.50%, f1 0.8353, precision 0.8389, recall 0.8318, auc 0.8350
epoch 2901, loss 0.3153, train acc 83.51%, f1 0.8357, precision 0.8378, recall 0.8336, auc 0.8351
epoch 3001, loss 0.3518, train acc 83.44%, f1 0.8349, precision 0.8378, recall 0.8319, auc 0.8344
epoch 3101, loss 0.3414, train acc 83.46%, f1 0.8351, precision 0.8380, recall 0.8322, auc 0.8347
epoch 3201, loss 0.3563, train acc 83.48%, f1 0.8357, precision 0.8365, recall 0.8350, auc 0.8348
epoch 3301, loss 0.3267, train acc 83.53%, f1 0.8361, precision 0.8372, recall 0.8350, auc 0.8353
epoch 3401, loss 0.3864, train acc 83.53%, f1 0.8360, precision 0.8377, recall 0.8343, auc 0.8353
epoch 3501, loss 0.3143, train acc 83.52%, f1 0.8356, precision 0.8389, recall 0.8323, auc 0.8353
epoch 3601, loss 0.3850, train acc 83.52%, f1 0.8358, precision 0.8378, recall 0.8338, auc 0.8352
epoch 3701, loss 0.2857, train acc 83.53%, f1 0.8361, precision 0.8376, recall 0.8345, auc 0.8353
epoch 3801, loss 0.3823, train acc 83.55%, f1 0.8360, precision 0.8386, recall 0.8335, auc 0.8355
epoch 3901, loss 0.4741, train acc 83.55%, f1 0.8361, precision 0.8383, recall 0.8339, auc 0.8355
epoch 4001, loss 0.3023, train acc 83.57%, f1 0.8364, precision 0.8383, recall 0.8344, auc 0.8357
epoch 4101, loss 0.3724, train acc 83.57%, f1 0.8363, precision 0.8386, recall 0.8341, auc 0.8357
epoch 4201, loss 0.2995, train acc 83.59%, f1 0.8363, precision 0.8396, recall 0.8331, auc 0.8360
epoch 4301, loss 0.3466, train acc 83.62%, f1 0.8363, precision 0.8407, recall 0.8320, auc 0.8362
epoch 4401, loss 0.3672, train acc 83.61%, f1 0.8366, precision 0.8397, recall 0.8334, auc 0.8362
epoch 4501, loss 0.3465, train acc 83.61%, f1 0.8366, precision 0.8391, recall 0.8341, auc 0.8361
epoch 4601, loss 0.3494, train acc 83.64%, f1 0.8368, precision 0.8399, recall 0.8337, auc 0.8364
epoch 4701, loss 0.3883, train acc 83.64%, f1 0.8376, precision 0.8368, recall 0.8383, auc 0.8364
epoch 4801, loss 0.3344, train acc 83.71%, f1 0.8376, precision 0.8400, recall 0.8352, auc 0.8371
epoch 4901, loss 0.3423, train acc 83.69%, f1 0.8378, precision 0.8384, recall 0.8372, auc 0.8369
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_1
./test_pima/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.5922222222222222

the Fscore is 0.56353591160221

the precision is 0.4015748031496063

the recall is 0.9444444444444444

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_1
----------------------



epoch 1, loss 0.6931, train acc 50.01%, f1 0.6668, precision 0.5001, recall 1.0000, auc 0.5000
epoch 101, loss 0.6073, train acc 70.99%, f1 0.7650, precision 0.6430, recall 0.9442, auc 0.7098
epoch 201, loss 0.4762, train acc 79.42%, f1 0.8082, precision 0.7570, recall 0.8668, auc 0.7942
epoch 301, loss 0.3828, train acc 81.72%, f1 0.8212, precision 0.8040, recall 0.8391, auc 0.8172
epoch 401, loss 0.4661, train acc 82.62%, f1 0.8269, precision 0.8238, recall 0.8299, auc 0.8262
epoch 501, loss 0.4189, train acc 83.07%, f1 0.8297, precision 0.8350, recall 0.8244, auc 0.8307
epoch 601, loss 0.4547, train acc 83.20%, f1 0.8302, precision 0.8393, recall 0.8213, auc 0.8320
epoch 701, loss 0.4312, train acc 83.34%, f1 0.8313, precision 0.8419, recall 0.8210, auc 0.8334
epoch 801, loss 0.4700, train acc 83.41%, f1 0.8321, precision 0.8426, recall 0.8219, auc 0.8341
epoch 901, loss 0.3436, train acc 83.45%, f1 0.8326, precision 0.8425, recall 0.8229, auc 0.8345
epoch 1001, loss 0.3575, train acc 83.45%, f1 0.8325, precision 0.8428, recall 0.8225, auc 0.8345
epoch 1101, loss 0.3324, train acc 83.46%, f1 0.8332, precision 0.8408, recall 0.8258, auc 0.8347
epoch 1201, loss 0.4210, train acc 83.44%, f1 0.8324, precision 0.8429, recall 0.8222, auc 0.8344
epoch 1301, loss 0.3000, train acc 83.47%, f1 0.8331, precision 0.8416, recall 0.8248, auc 0.8347
epoch 1401, loss 0.3242, train acc 83.46%, f1 0.8327, precision 0.8425, recall 0.8231, auc 0.8346
epoch 1501, loss 0.4971, train acc 83.46%, f1 0.8333, precision 0.8401, recall 0.8265, auc 0.8346
epoch 1601, loss 0.2747, train acc 83.50%, f1 0.8337, precision 0.8407, recall 0.8267, auc 0.8350
epoch 1701, loss 0.3520, train acc 83.50%, f1 0.8343, precision 0.8378, recall 0.8308, auc 0.8350
epoch 1801, loss 0.3716, train acc 83.49%, f1 0.8340, precision 0.8388, recall 0.8292, auc 0.8349
epoch 1901, loss 0.4471, train acc 83.49%, f1 0.8341, precision 0.8387, recall 0.8295, auc 0.8349
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_1
./test_pima/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.625

the Fscore is 0.5901639344262295

the precision is 0.4186046511627907

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_1
----------------------



epoch 1, loss 0.6924, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6221, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5951, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5699, train acc 65.15%, f1 0.0093, precision 0.5000, recall 0.0047, auc 0.5011
epoch 401, loss 0.5474, train acc 67.43%, f1 0.1525, precision 0.8182, recall 0.0841, auc 0.5371
epoch 501, loss 0.5293, train acc 71.50%, f1 0.3590, precision 0.8305, recall 0.2290, auc 0.6020
epoch 601, loss 0.5154, train acc 73.94%, f1 0.4904, precision 0.7700, recall 0.3598, auc 0.6512
epoch 701, loss 0.5049, train acc 75.08%, f1 0.5565, precision 0.7328, recall 0.4486, auc 0.6805
epoch 801, loss 0.4971, train acc 76.06%, f1 0.5995, precision 0.7190, recall 0.5140, auc 0.7033
epoch 901, loss 0.4915, train acc 76.87%, f1 0.6283, precision 0.7143, recall 0.5607, auc 0.7204
epoch 1001, loss 0.4873, train acc 76.55%, f1 0.6289, precision 0.7011, recall 0.5701, auc 0.7200
epoch 1101, loss 0.4843, train acc 76.71%, f1 0.6380, precision 0.6961, recall 0.5888, auc 0.7256
epoch 1201, loss 0.4820, train acc 76.71%, f1 0.6452, precision 0.6878, recall 0.6075, auc 0.7300
epoch 1301, loss 0.4797, train acc 77.04%, f1 0.6519, precision 0.6911, recall 0.6168, auc 0.7347
epoch 1401, loss 0.4768, train acc 77.36%, f1 0.6516, precision 0.7027, recall 0.6075, auc 0.7350
epoch 1501, loss 0.4735, train acc 77.20%, f1 0.6465, precision 0.7033, recall 0.5981, auc 0.7316
epoch 1601, loss 0.4702, train acc 77.04%, f1 0.6430, precision 0.7017, recall 0.5935, auc 0.7292
epoch 1701, loss 0.4672, train acc 77.36%, f1 0.6463, precision 0.7095, recall 0.5935, auc 0.7317
epoch 1801, loss 0.4644, train acc 77.20%, f1 0.6465, precision 0.7033, recall 0.5981, auc 0.7316
epoch 1901, loss 0.4617, train acc 77.52%, f1 0.6584, precision 0.7000, recall 0.6215, auc 0.7395
epoch 2001, loss 0.4591, train acc 77.36%, f1 0.6551, precision 0.6984, recall 0.6168, auc 0.7372
epoch 2101, loss 0.4565, train acc 77.85%, f1 0.6600, precision 0.7097, recall 0.6168, auc 0.7409
epoch 2201, loss 0.4536, train acc 78.18%, f1 0.6633, precision 0.7174, recall 0.6168, auc 0.7434
epoch 2301, loss 0.4513, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 2401, loss 0.4489, train acc 77.85%, f1 0.6600, precision 0.7097, recall 0.6168, auc 0.7409
epoch 2501, loss 0.4464, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 2601, loss 0.4438, train acc 78.18%, f1 0.6633, precision 0.7174, recall 0.6168, auc 0.7434
epoch 2701, loss 0.4412, train acc 78.34%, f1 0.6667, precision 0.7189, recall 0.6215, auc 0.7457
epoch 2801, loss 0.4388, train acc 78.34%, f1 0.6683, precision 0.7166, recall 0.6262, auc 0.7468
epoch 2901, loss 0.4366, train acc 78.50%, f1 0.6716, precision 0.7181, recall 0.6308, auc 0.7492
epoch 3001, loss 0.4344, train acc 78.50%, f1 0.6716, precision 0.7181, recall 0.6308, auc 0.7492
epoch 3101, loss 0.4322, train acc 78.66%, f1 0.6733, precision 0.7219, recall 0.6308, auc 0.7504
epoch 3201, loss 0.4301, train acc 78.66%, f1 0.6733, precision 0.7219, recall 0.6308, auc 0.7504
epoch 3301, loss 0.4280, train acc 79.32%, f1 0.6833, precision 0.7326, recall 0.6402, auc 0.7576
epoch 3401, loss 0.4258, train acc 79.48%, f1 0.6850, precision 0.7366, recall 0.6402, auc 0.7588
epoch 3501, loss 0.4234, train acc 79.48%, f1 0.6850, precision 0.7366, recall 0.6402, auc 0.7588
epoch 3601, loss 0.4205, train acc 79.64%, f1 0.6867, precision 0.7405, recall 0.6402, auc 0.7601
epoch 3701, loss 0.4176, train acc 80.29%, f1 0.6967, precision 0.7514, recall 0.6495, auc 0.7673
epoch 3801, loss 0.4145, train acc 80.46%, f1 0.7000, precision 0.7527, recall 0.6542, auc 0.7696
epoch 3901, loss 0.4112, train acc 80.29%, f1 0.6967, precision 0.7514, recall 0.6495, auc 0.7673
epoch 4001, loss 0.4077, train acc 80.29%, f1 0.6983, precision 0.7487, recall 0.6542, auc 0.7684
epoch 4101, loss 0.4039, train acc 80.29%, f1 0.6983, precision 0.7487, recall 0.6542, auc 0.7684
epoch 4201, loss 0.3995, train acc 80.62%, f1 0.7032, precision 0.7540, recall 0.6589, auc 0.7719
epoch 4301, loss 0.3947, train acc 80.94%, f1 0.7082, precision 0.7594, recall 0.6636, auc 0.7755
epoch 4401, loss 0.3893, train acc 81.43%, f1 0.7164, precision 0.7660, recall 0.6729, auc 0.7814
epoch 4501, loss 0.3830, train acc 81.92%, f1 0.7218, precision 0.7784, recall 0.6729, auc 0.7852
epoch 4601, loss 0.3773, train acc 82.08%, f1 0.7264, precision 0.7766, recall 0.6822, auc 0.7886
epoch 4701, loss 0.3721, train acc 83.55%, f1 0.7518, precision 0.7927, recall 0.7150, auc 0.8075
epoch 4801, loss 0.3670, train acc 83.39%, f1 0.7500, precision 0.7887, recall 0.7150, auc 0.8062
epoch 4901, loss 0.3621, train acc 83.88%, f1 0.7579, precision 0.7949, recall 0.7243, auc 0.8121
epoch 5001, loss 0.3574, train acc 84.36%, f1 0.7659, precision 0.8010, recall 0.7336, auc 0.8181
epoch 5101, loss 0.3532, train acc 85.02%, f1 0.7756, precision 0.8112, recall 0.7430, auc 0.8252
epoch 5201, loss 0.3494, train acc 85.02%, f1 0.7745, precision 0.8144, recall 0.7383, auc 0.8242
epoch 5301, loss 0.3459, train acc 85.02%, f1 0.7756, precision 0.8112, recall 0.7430, auc 0.8252
epoch 5401, loss 0.3427, train acc 84.85%, f1 0.7726, precision 0.8103, recall 0.7383, auc 0.8229
epoch 5501, loss 0.3396, train acc 84.85%, f1 0.7726, precision 0.8103, recall 0.7383, auc 0.8229
epoch 5601, loss 0.3368, train acc 85.02%, f1 0.7745, precision 0.8144, recall 0.7383, auc 0.8242
epoch 5701, loss 0.3339, train acc 85.67%, f1 0.7833, precision 0.8281, recall 0.7430, auc 0.8302
epoch 5801, loss 0.3312, train acc 85.83%, f1 0.7852, precision 0.8325, recall 0.7430, auc 0.8315
epoch 5901, loss 0.3285, train acc 85.99%, f1 0.7892, precision 0.8299, recall 0.7523, auc 0.8349
epoch 6001, loss 0.3261, train acc 86.32%, f1 0.7931, precision 0.8385, recall 0.7523, auc 0.8374
epoch 6101, loss 0.3238, train acc 86.64%, f1 0.7990, precision 0.8402, recall 0.7617, auc 0.8421
epoch 6201, loss 0.3212, train acc 86.81%, f1 0.8010, precision 0.8446, recall 0.7617, auc 0.8433
epoch 6301, loss 0.3179, train acc 86.81%, f1 0.8020, precision 0.8410, recall 0.7664, auc 0.8444
epoch 6401, loss 0.3138, train acc 87.13%, f1 0.8087, precision 0.8392, recall 0.7804, auc 0.8502
epoch 6501, loss 0.3104, train acc 87.30%, f1 0.8107, precision 0.8434, recall 0.7804, auc 0.8514
epoch 6601, loss 0.3073, train acc 87.30%, f1 0.8107, precision 0.8434, recall 0.7804, auc 0.8514
epoch 6701, loss 0.3042, train acc 87.46%, f1 0.8136, precision 0.8442, recall 0.7850, auc 0.8538
epoch 6801, loss 0.3012, train acc 87.46%, f1 0.8145, precision 0.8408, recall 0.7897, auc 0.8549
epoch 6901, loss 0.2976, train acc 87.13%, f1 0.8096, precision 0.8358, recall 0.7850, auc 0.8513
epoch 7001, loss 0.2940, train acc 87.62%, f1 0.8182, precision 0.8382, recall 0.7991, auc 0.8583
epoch 7101, loss 0.2904, train acc 87.95%, f1 0.8238, precision 0.8398, recall 0.8084, auc 0.8630
epoch 7201, loss 0.2868, train acc 88.60%, f1 0.8341, precision 0.8462, recall 0.8224, auc 0.8712
epoch 7301, loss 0.2834, train acc 88.93%, f1 0.8389, precision 0.8510, recall 0.8271, auc 0.8748
epoch 7401, loss 0.2803, train acc 89.58%, f1 0.8491, precision 0.8571, recall 0.8411, auc 0.8831
epoch 7501, loss 0.2775, train acc 89.58%, f1 0.8498, precision 0.8538, recall 0.8458, auc 0.8841
epoch 7601, loss 0.2748, train acc 90.07%, f1 0.8571, precision 0.8592, recall 0.8551, auc 0.8901
epoch 7701, loss 0.2721, train acc 90.39%, f1 0.8618, precision 0.8638, recall 0.8598, auc 0.8937
epoch 7801, loss 0.2686, train acc 89.90%, f1 0.8545, precision 0.8585, recall 0.8505, auc 0.8877
epoch 7901, loss 0.2655, train acc 90.07%, f1 0.8565, precision 0.8626, recall 0.8505, auc 0.8890
epoch 8001, loss 0.2631, train acc 90.23%, f1 0.8592, precision 0.8632, recall 0.8551, auc 0.8913
epoch 8101, loss 0.2608, train acc 90.23%, f1 0.8592, precision 0.8632, recall 0.8551, auc 0.8913
epoch 8201, loss 0.2587, train acc 90.23%, f1 0.8592, precision 0.8632, recall 0.8551, auc 0.8913
epoch 8301, loss 0.2565, train acc 90.23%, f1 0.8585, precision 0.8667, recall 0.8505, auc 0.8902
epoch 8401, loss 0.2544, train acc 90.23%, f1 0.8585, precision 0.8667, recall 0.8505, auc 0.8902
epoch 8501, loss 0.2522, train acc 90.39%, f1 0.8612, precision 0.8673, recall 0.8551, auc 0.8926
epoch 8601, loss 0.2502, train acc 90.39%, f1 0.8612, precision 0.8673, recall 0.8551, auc 0.8926
epoch 8701, loss 0.2482, train acc 90.55%, f1 0.8638, precision 0.8679, recall 0.8598, auc 0.8949
epoch 8801, loss 0.2462, train acc 90.72%, f1 0.8659, precision 0.8720, recall 0.8598, auc 0.8962
epoch 8901, loss 0.2439, train acc 90.88%, f1 0.8692, precision 0.8692, recall 0.8692, auc 0.8996
epoch 9001, loss 0.2419, train acc 91.04%, f1 0.8712, precision 0.8732, recall 0.8692, auc 0.9008
epoch 9101, loss 0.2400, train acc 91.21%, f1 0.8732, precision 0.8774, recall 0.8692, auc 0.9021
epoch 9201, loss 0.2382, train acc 91.53%, f1 0.8785, precision 0.8785, recall 0.8785, auc 0.9068
epoch 9301, loss 0.2366, train acc 91.69%, f1 0.8811, precision 0.8791, recall 0.8832, auc 0.9091
epoch 9401, loss 0.2350, train acc 91.86%, f1 0.8837, precision 0.8796, recall 0.8879, auc 0.9114
epoch 9501, loss 0.2336, train acc 92.18%, f1 0.8884, precision 0.8843, recall 0.8925, auc 0.9150
epoch 9601, loss 0.2322, train acc 92.02%, f1 0.8858, precision 0.8837, recall 0.8879, auc 0.9127
epoch 9701, loss 0.2309, train acc 92.02%, f1 0.8858, precision 0.8837, recall 0.8879, auc 0.9127
epoch 9801, loss 0.2296, train acc 92.02%, f1 0.8858, precision 0.8837, recall 0.8879, auc 0.9127
epoch 9901, loss 0.2283, train acc 92.18%, f1 0.8884, precision 0.8843, recall 0.8925, auc 0.9150
epoch 10001, loss 0.2270, train acc 92.51%, f1 0.8935, precision 0.8853, recall 0.9019, auc 0.9197
epoch 10101, loss 0.2257, train acc 92.51%, f1 0.8935, precision 0.8853, recall 0.9019, auc 0.9197
epoch 10201, loss 0.2245, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 10301, loss 0.2234, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 10401, loss 0.2223, train acc 92.67%, f1 0.8961, precision 0.8858, recall 0.9065, auc 0.9220
epoch 10501, loss 0.2212, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 10601, loss 0.2202, train acc 92.83%, f1 0.8981, precision 0.8899, recall 0.9065, auc 0.9233
epoch 10701, loss 0.2191, train acc 92.83%, f1 0.8986, precision 0.8864, recall 0.9112, auc 0.9244
epoch 10801, loss 0.2181, train acc 93.00%, f1 0.9007, precision 0.8904, recall 0.9112, auc 0.9256
epoch 10901, loss 0.2171, train acc 93.00%, f1 0.9002, precision 0.8940, recall 0.9065, auc 0.9245
epoch 11001, loss 0.2162, train acc 93.00%, f1 0.9002, precision 0.8940, recall 0.9065, auc 0.9245
epoch 11101, loss 0.2153, train acc 93.00%, f1 0.9002, precision 0.8940, recall 0.9065, auc 0.9245
epoch 11201, loss 0.2144, train acc 93.00%, f1 0.9002, precision 0.8940, recall 0.9065, auc 0.9245
epoch 11301, loss 0.2135, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 11401, loss 0.2126, train acc 93.16%, f1 0.9028, precision 0.8945, recall 0.9112, auc 0.9269
epoch 11501, loss 0.2118, train acc 93.32%, f1 0.9049, precision 0.8986, recall 0.9112, auc 0.9281
epoch 11601, loss 0.2110, train acc 93.32%, f1 0.9049, precision 0.8986, recall 0.9112, auc 0.9281
epoch 11701, loss 0.2102, train acc 93.32%, f1 0.9049, precision 0.8986, recall 0.9112, auc 0.9281
epoch 11801, loss 0.2092, train acc 93.65%, f1 0.9091, precision 0.9070, recall 0.9112, auc 0.9306
epoch 11901, loss 0.2083, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 12001, loss 0.2075, train acc 93.65%, f1 0.9091, precision 0.9070, recall 0.9112, auc 0.9306
epoch 12101, loss 0.2067, train acc 93.65%, f1 0.9091, precision 0.9070, recall 0.9112, auc 0.9306
epoch 12201, loss 0.2059, train acc 93.65%, f1 0.9091, precision 0.9070, recall 0.9112, auc 0.9306
epoch 12301, loss 0.2052, train acc 93.65%, f1 0.9091, precision 0.9070, recall 0.9112, auc 0.9306
epoch 12401, loss 0.2044, train acc 93.65%, f1 0.9091, precision 0.9070, recall 0.9112, auc 0.9306
epoch 12501, loss 0.2037, train acc 93.81%, f1 0.9112, precision 0.9112, recall 0.9112, auc 0.9319
epoch 12601, loss 0.2030, train acc 93.81%, f1 0.9112, precision 0.9112, recall 0.9112, auc 0.9319
epoch 12701, loss 0.2023, train acc 93.81%, f1 0.9112, precision 0.9112, recall 0.9112, auc 0.9319
epoch 12801, loss 0.2017, train acc 93.97%, f1 0.9138, precision 0.9116, recall 0.9159, auc 0.9342
epoch 12901, loss 0.2010, train acc 93.97%, f1 0.9138, precision 0.9116, recall 0.9159, auc 0.9342
epoch 13001, loss 0.2004, train acc 94.14%, f1 0.9163, precision 0.9120, recall 0.9206, auc 0.9365
epoch 13101, loss 0.1997, train acc 94.14%, f1 0.9163, precision 0.9120, recall 0.9206, auc 0.9365
epoch 13201, loss 0.1991, train acc 94.30%, f1 0.9184, precision 0.9163, recall 0.9206, auc 0.9378
epoch 13301, loss 0.1985, train acc 94.46%, f1 0.9206, precision 0.9206, recall 0.9206, auc 0.9390
epoch 13401, loss 0.1979, train acc 94.46%, f1 0.9206, precision 0.9206, recall 0.9206, auc 0.9390
epoch 13501, loss 0.1973, train acc 94.46%, f1 0.9206, precision 0.9206, recall 0.9206, auc 0.9390
epoch 13601, loss 0.1967, train acc 94.63%, f1 0.9231, precision 0.9209, recall 0.9252, auc 0.9414
epoch 13701, loss 0.1962, train acc 94.63%, f1 0.9231, precision 0.9209, recall 0.9252, auc 0.9414
epoch 13801, loss 0.1956, train acc 94.63%, f1 0.9231, precision 0.9209, recall 0.9252, auc 0.9414
epoch 13901, loss 0.1951, train acc 94.63%, f1 0.9231, precision 0.9209, recall 0.9252, auc 0.9414
epoch 14001, loss 0.1945, train acc 94.63%, f1 0.9231, precision 0.9209, recall 0.9252, auc 0.9414
epoch 14101, loss 0.1940, train acc 94.63%, f1 0.9231, precision 0.9209, recall 0.9252, auc 0.9414
epoch 14201, loss 0.1934, train acc 94.63%, f1 0.9231, precision 0.9209, recall 0.9252, auc 0.9414
epoch 14301, loss 0.1918, train acc 93.65%, f1 0.9091, precision 0.9070, recall 0.9112, auc 0.9306
epoch 14401, loss 0.1899, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 14501, loss 0.1889, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 14601, loss 0.1882, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 14701, loss 0.1876, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 14801, loss 0.1870, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 14901, loss 0.1864, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 15001, loss 0.1859, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 15101, loss 0.1853, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 15201, loss 0.1848, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 15301, loss 0.1843, train acc 93.65%, f1 0.9087, precision 0.9108, recall 0.9065, auc 0.9295
epoch 15401, loss 0.1838, train acc 93.65%, f1 0.9087, precision 0.9108, recall 0.9065, auc 0.9295
epoch 15501, loss 0.1833, train acc 93.81%, f1 0.9108, precision 0.9151, recall 0.9065, auc 0.9308
epoch 15601, loss 0.1828, train acc 93.81%, f1 0.9108, precision 0.9151, recall 0.9065, auc 0.9308
epoch 15701, loss 0.1823, train acc 93.81%, f1 0.9108, precision 0.9151, recall 0.9065, auc 0.9308
epoch 15801, loss 0.1819, train acc 93.97%, f1 0.9129, precision 0.9194, recall 0.9065, auc 0.9320
epoch 15901, loss 0.1814, train acc 93.97%, f1 0.9129, precision 0.9194, recall 0.9065, auc 0.9320
epoch 16001, loss 0.1810, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 16101, loss 0.1805, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 16201, loss 0.1801, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 16301, loss 0.1797, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 16401, loss 0.1792, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 16501, loss 0.1781, train acc 93.49%, f1 0.9061, precision 0.9104, recall 0.9019, auc 0.9272/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.1773, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 16701, loss 0.1768, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 16801, loss 0.1763, train acc 93.65%, f1 0.9082, precision 0.9147, recall 0.9019, auc 0.9284
epoch 16901, loss 0.1758, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17001, loss 0.1753, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17101, loss 0.1749, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17201, loss 0.1744, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17301, loss 0.1740, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17401, loss 0.1736, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17501, loss 0.1732, train acc 93.81%, f1 0.9104, precision 0.9190, recall 0.9019, auc 0.9297
epoch 17601, loss 0.1728, train acc 93.97%, f1 0.9129, precision 0.9194, recall 0.9065, auc 0.9320
epoch 17701, loss 0.1723, train acc 93.97%, f1 0.9129, precision 0.9194, recall 0.9065, auc 0.9320
epoch 17801, loss 0.1719, train acc 93.97%, f1 0.9129, precision 0.9194, recall 0.9065, auc 0.9320
epoch 17901, loss 0.1715, train acc 93.97%, f1 0.9129, precision 0.9194, recall 0.9065, auc 0.9320
epoch 18001, loss 0.1711, train acc 93.97%, f1 0.9129, precision 0.9194, recall 0.9065, auc 0.9320
epoch 18101, loss 0.1708, train acc 93.97%, f1 0.9129, precision 0.9194, recall 0.9065, auc 0.9320
epoch 18201, loss 0.1704, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18301, loss 0.1700, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18401, loss 0.1696, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18501, loss 0.1692, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18601, loss 0.1689, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18701, loss 0.1685, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18801, loss 0.1681, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 18901, loss 0.1678, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19001, loss 0.1674, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19101, loss 0.1671, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19201, loss 0.1667, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19301, loss 0.1664, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19401, loss 0.1661, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19501, loss 0.1657, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19601, loss 0.1654, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19701, loss 0.1650, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19801, loss 0.1647, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
epoch 19901, loss 0.1644, train acc 94.14%, f1 0.9155, precision 0.9198, recall 0.9112, auc 0.9344
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_normal_20000
normal
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_1
./test_pima/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.6642592592592593

the Fscore is 0.5544554455445543

the precision is 0.5957446808510638

the recall is 0.5185185185185185

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_1
----------------------



epoch 1, loss 0.7000, train acc 34.85%, f1 0.5169, precision 0.3485, recall 1.0000, auc 0.5000
epoch 101, loss 0.6251, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5973, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5725, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5500, train acc 66.78%, f1 0.1207, precision 0.7778, recall 0.0654, auc 0.5277
epoch 501, loss 0.5317, train acc 70.36%, f1 0.3106, precision 0.8200, recall 0.1916, auc 0.5845
epoch 601, loss 0.5175, train acc 73.62%, f1 0.4671, precision 0.7889, recall 0.3318, auc 0.6421
epoch 701, loss 0.5067, train acc 75.41%, f1 0.5572, precision 0.7480, recall 0.4439, auc 0.6820
epoch 801, loss 0.4987, train acc 75.90%, f1 0.5934, precision 0.7200, recall 0.5047, auc 0.6998
epoch 901, loss 0.4927, train acc 76.38%, f1 0.6154, precision 0.7117, recall 0.5421, auc 0.7123
epoch 1001, loss 0.4883, train acc 76.71%, f1 0.6324, precision 0.7029, recall 0.5748, auc 0.7224
epoch 1101, loss 0.4852, train acc 76.71%, f1 0.6380, precision 0.6961, recall 0.5888, auc 0.7256
epoch 1201, loss 0.4827, train acc 76.71%, f1 0.6434, precision 0.6898, recall 0.6028, auc 0.7289
epoch 1301, loss 0.4805, train acc 77.04%, f1 0.6519, precision 0.6911, recall 0.6168, auc 0.7347
epoch 1401, loss 0.4777, train acc 77.36%, f1 0.6534, precision 0.7005, recall 0.6121, auc 0.7361
epoch 1501, loss 0.4740, train acc 76.87%, f1 0.6414, precision 0.6978, recall 0.5935, auc 0.7280
epoch 1601, loss 0.4703, train acc 76.71%, f1 0.6380, precision 0.6961, recall 0.5888, auc 0.7256
epoch 1701, loss 0.4669, train acc 77.36%, f1 0.6463, precision 0.7095, recall 0.5935, auc 0.7317
epoch 1801, loss 0.4638, train acc 76.71%, f1 0.6398, precision 0.6940, recall 0.5935, auc 0.7267
epoch 1901, loss 0.4609, train acc 77.52%, f1 0.6567, precision 0.7021, recall 0.6168, auc 0.7384
epoch 2001, loss 0.4582, train acc 77.36%, f1 0.6516, precision 0.7027, recall 0.6075, auc 0.7350
epoch 2101, loss 0.4555, train acc 77.69%, f1 0.6549, precision 0.7104, recall 0.6075, auc 0.7375
epoch 2201, loss 0.4534, train acc 78.01%, f1 0.6599, precision 0.7158, recall 0.6121, auc 0.7411
epoch 2301, loss 0.4513, train acc 78.34%, f1 0.6667, precision 0.7189, recall 0.6215, auc 0.7457
epoch 2401, loss 0.4493, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 2501, loss 0.4470, train acc 77.85%, f1 0.6583, precision 0.7120, recall 0.6121, auc 0.7398
epoch 2601, loss 0.4443, train acc 77.85%, f1 0.6583, precision 0.7120, recall 0.6121, auc 0.7398
epoch 2701, loss 0.4418, train acc 78.34%, f1 0.6683, precision 0.7166, recall 0.6262, auc 0.7468
epoch 2801, loss 0.4394, train acc 78.18%, f1 0.6667, precision 0.7128, recall 0.6262, auc 0.7456
epoch 2901, loss 0.4371, train acc 78.18%, f1 0.6667, precision 0.7128, recall 0.6262, auc 0.7456
epoch 3001, loss 0.4349, train acc 78.34%, f1 0.6700, precision 0.7143, recall 0.6308, auc 0.7479
epoch 3101, loss 0.4328, train acc 78.18%, f1 0.6667, precision 0.7128, recall 0.6262, auc 0.7456
epoch 3201, loss 0.4309, train acc 78.99%, f1 0.6799, precision 0.7249, recall 0.6402, auc 0.7551
epoch 3301, loss 0.4290, train acc 79.15%, f1 0.6832, precision 0.7263, recall 0.6449, auc 0.7574
epoch 3401, loss 0.4272, train acc 79.15%, f1 0.6816, precision 0.7287, recall 0.6402, auc 0.7563
epoch 3501, loss 0.4254, train acc 79.48%, f1 0.6897, precision 0.7292, recall 0.6542, auc 0.7621
epoch 3601, loss 0.4233, train acc 79.80%, f1 0.6931, precision 0.7368, recall 0.6542, auc 0.7646
epoch 3701, loss 0.4204, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 3801, loss 0.4178, train acc 80.13%, f1 0.6950, precision 0.7473, recall 0.6495, auc 0.7660
epoch 3901, loss 0.4154, train acc 80.29%, f1 0.6983, precision 0.7487, recall 0.6542, auc 0.7684
epoch 4001, loss 0.4130, train acc 80.29%, f1 0.6983, precision 0.7487, recall 0.6542, auc 0.7684
epoch 4101, loss 0.4109, train acc 80.29%, f1 0.6983, precision 0.7487, recall 0.6542, auc 0.7684
epoch 4201, loss 0.4088, train acc 80.13%, f1 0.6950, precision 0.7473, recall 0.6495, auc 0.7660
epoch 4301, loss 0.4068, train acc 79.64%, f1 0.6867, precision 0.7405, recall 0.6402, auc 0.7601
epoch 4401, loss 0.4048, train acc 79.80%, f1 0.6900, precision 0.7419, recall 0.6449, auc 0.7624
epoch 4501, loss 0.4024, train acc 80.13%, f1 0.6950, precision 0.7473, recall 0.6495, auc 0.7660
epoch 4601, loss 0.3997, train acc 80.13%, f1 0.6980, precision 0.7421, recall 0.6589, auc 0.7682
epoch 4701, loss 0.3970, train acc 80.62%, f1 0.7032, precision 0.7540, recall 0.6589, auc 0.7719
epoch 4801, loss 0.3941, train acc 80.78%, f1 0.7065, precision 0.7553, recall 0.6636, auc 0.7743
epoch 4901, loss 0.3904, train acc 80.78%, f1 0.7050, precision 0.7581, recall 0.6589, auc 0.7732
epoch 5001, loss 0.3858, train acc 81.27%, f1 0.7132, precision 0.7647, recall 0.6682, auc 0.7791
epoch 5101, loss 0.3813, train acc 82.25%, f1 0.7309, precision 0.7749, recall 0.6916, auc 0.7920
epoch 5201, loss 0.3765, train acc 82.74%, f1 0.7402, precision 0.7784, recall 0.7056, auc 0.7991
epoch 5301, loss 0.3719, train acc 82.74%, f1 0.7427, precision 0.7727, recall 0.7150, auc 0.8012
epoch 5401, loss 0.3665, train acc 83.22%, f1 0.7506, precision 0.7789, recall 0.7243, auc 0.8071
epoch 5501, loss 0.3605, train acc 83.39%, f1 0.7536, precision 0.7800, recall 0.7290, auc 0.8095
epoch 5601, loss 0.3552, train acc 83.88%, f1 0.7626, precision 0.7833, recall 0.7430, auc 0.8165
epoch 5701, loss 0.3505, train acc 84.04%, f1 0.7633, precision 0.7900, recall 0.7383, auc 0.8167
epoch 5801, loss 0.3457, train acc 84.53%, f1 0.7689, precision 0.8020, recall 0.7383, auc 0.8204
epoch 5901, loss 0.3402, train acc 84.69%, f1 0.7718, precision 0.8030, recall 0.7430, auc 0.8227
epoch 6001, loss 0.3336, train acc 85.18%, f1 0.7807, precision 0.8060, recall 0.7570, auc 0.8298
epoch 6101, loss 0.3271, train acc 85.83%, f1 0.7904, precision 0.8159, recall 0.7664, auc 0.8369
epoch 6201, loss 0.3202, train acc 86.32%, f1 0.7981, precision 0.8218, recall 0.7757, auc 0.8429
epoch 6301, loss 0.3143, train acc 86.81%, f1 0.8048, precision 0.8308, recall 0.7804, auc 0.8477
epoch 6401, loss 0.3088, train acc 86.97%, f1 0.8077, precision 0.8317, recall 0.7850, auc 0.8500
epoch 6501, loss 0.3038, train acc 87.30%, f1 0.8134, precision 0.8333, recall 0.7944, auc 0.8547
epoch 6601, loss 0.2991, train acc 87.79%, f1 0.8210, precision 0.8390, recall 0.8037, auc 0.8606
epoch 6701, loss 0.2946, train acc 87.79%, f1 0.8210, precision 0.8390, recall 0.8037, auc 0.8606
epoch 6801, loss 0.2903, train acc 88.11%, f1 0.8266, precision 0.8406, recall 0.8131, auc 0.8653
epoch 6901, loss 0.2860, train acc 88.44%, f1 0.8305, precision 0.8488, recall 0.8131, auc 0.8678
epoch 7001, loss 0.2815, train acc 88.76%, f1 0.8353, precision 0.8537, recall 0.8178, auc 0.8714
epoch 7101, loss 0.2764, train acc 88.93%, f1 0.8373, precision 0.8578, recall 0.8178, auc 0.8726
epoch 7201, loss 0.2708, train acc 89.09%, f1 0.8393, precision 0.8621, recall 0.8178, auc 0.8739
epoch 7301, loss 0.2655, train acc 89.58%, f1 0.8462, precision 0.8713, recall 0.8224, auc 0.8787
epoch 7401, loss 0.2611, train acc 89.58%, f1 0.8462, precision 0.8713, recall 0.8224, auc 0.8787
epoch 7501, loss 0.2572, train acc 89.74%, f1 0.8489, precision 0.8719, recall 0.8271, auc 0.8811
epoch 7601, loss 0.2536, train acc 89.41%, f1 0.8434, precision 0.8706, recall 0.8178, auc 0.8764
epoch 7701, loss 0.2501, train acc 89.09%, f1 0.8401, precision 0.8585, recall 0.8224, auc 0.8750
epoch 7801, loss 0.2467, train acc 89.58%, f1 0.8483, precision 0.8606, recall 0.8364, auc 0.8820
epoch 7901, loss 0.2433, train acc 89.90%, f1 0.8538, precision 0.8619, recall 0.8458, auc 0.8866
epoch 8001, loss 0.2399, train acc 90.07%, f1 0.8551, precision 0.8696, recall 0.8411, auc 0.8868
epoch 8101, loss 0.2363, train acc 90.23%, f1 0.8585, precision 0.8667, recall 0.8505, auc 0.8902
epoch 8201, loss 0.2326, train acc 90.39%, f1 0.8599, precision 0.8744, recall 0.8458, auc 0.8904/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2290, train acc 90.55%, f1 0.8626, precision 0.8750, recall 0.8505, auc 0.8927
epoch 8401, loss 0.2258, train acc 90.72%, f1 0.8659, precision 0.8720, recall 0.8598, auc 0.8962
epoch 8501, loss 0.2229, train acc 90.72%, f1 0.8659, precision 0.8720, recall 0.8598, auc 0.8962
epoch 8601, loss 0.2202, train acc 91.04%, f1 0.8706, precision 0.8768, recall 0.8645, auc 0.8997
epoch 8701, loss 0.2174, train acc 91.53%, f1 0.8774, precision 0.8857, recall 0.8692, auc 0.9046
epoch 8801, loss 0.2148, train acc 91.53%, f1 0.8774, precision 0.8857, recall 0.8692, auc 0.9046
epoch 8901, loss 0.2124, train acc 91.53%, f1 0.8774, precision 0.8857, recall 0.8692, auc 0.9046
epoch 9001, loss 0.2100, train acc 91.53%, f1 0.8774, precision 0.8857, recall 0.8692, auc 0.9046
epoch 9101, loss 0.2064, train acc 91.69%, f1 0.8794, precision 0.8900, recall 0.8692, auc 0.9058
epoch 9201, loss 0.2032, train acc 92.18%, f1 0.8863, precision 0.8990, recall 0.8738, auc 0.9107
epoch 9301, loss 0.2007, train acc 92.35%, f1 0.8884, precision 0.9034, recall 0.8738, auc 0.9119
epoch 9401, loss 0.1984, train acc 92.18%, f1 0.8863, precision 0.8990, recall 0.8738, auc 0.9107
epoch 9501, loss 0.1962, train acc 92.18%, f1 0.8863, precision 0.8990, recall 0.8738, auc 0.9107
epoch 9601, loss 0.1941, train acc 92.18%, f1 0.8868, precision 0.8952, recall 0.8785, auc 0.9118
epoch 9701, loss 0.1920, train acc 92.35%, f1 0.8889, precision 0.8995, recall 0.8785, auc 0.9130
epoch 9801, loss 0.1901, train acc 92.35%, f1 0.8889, precision 0.8995, recall 0.8785, auc 0.9130
epoch 9901, loss 0.1882, train acc 92.18%, f1 0.8868, precision 0.8952, recall 0.8785, auc 0.9118
epoch 10001, loss 0.1864, train acc 92.51%, f1 0.8920, precision 0.8962, recall 0.8879, auc 0.9164
epoch 10101, loss 0.1846, train acc 92.83%, f1 0.8972, precision 0.8972, recall 0.8972, auc 0.9211
epoch 10201, loss 0.1829, train acc 93.16%, f1 0.9023, precision 0.8981, recall 0.9065, auc 0.9258
epoch 10301, loss 0.1812, train acc 93.32%, f1 0.9044, precision 0.9023, recall 0.9065, auc 0.9270
epoch 10401, loss 0.1795, train acc 93.32%, f1 0.9044, precision 0.9023, recall 0.9065, auc 0.9270
epoch 10501, loss 0.1779, train acc 93.32%, f1 0.9044, precision 0.9023, recall 0.9065, auc 0.9270
epoch 10601, loss 0.1763, train acc 93.32%, f1 0.9044, precision 0.9023, recall 0.9065, auc 0.9270
epoch 10701, loss 0.1748, train acc 93.32%, f1 0.9044, precision 0.9023, recall 0.9065, auc 0.9270
epoch 10801, loss 0.1733, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 10901, loss 0.1719, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 11001, loss 0.1705, train acc 93.49%, f1 0.9065, precision 0.9065, recall 0.9065, auc 0.9283
epoch 11101, loss 0.1691, train acc 93.81%, f1 0.9116, precision 0.9074, recall 0.9159, auc 0.9329
epoch 11201, loss 0.1678, train acc 93.81%, f1 0.9116, precision 0.9074, recall 0.9159, auc 0.9329
epoch 11301, loss 0.1665, train acc 94.14%, f1 0.9163, precision 0.9120, recall 0.9206, auc 0.9365
epoch 11401, loss 0.1652, train acc 94.14%, f1 0.9163, precision 0.9120, recall 0.9206, auc 0.9365
epoch 11501, loss 0.1639, train acc 94.14%, f1 0.9163, precision 0.9120, recall 0.9206, auc 0.9365
epoch 11601, loss 0.1627, train acc 94.14%, f1 0.9163, precision 0.9120, recall 0.9206, auc 0.9365
epoch 11701, loss 0.1615, train acc 94.30%, f1 0.9188, precision 0.9124, recall 0.9252, auc 0.9389
epoch 11801, loss 0.1604, train acc 94.30%, f1 0.9188, precision 0.9124, recall 0.9252, auc 0.9389
epoch 11901, loss 0.1592, train acc 94.46%, f1 0.9209, precision 0.9167, recall 0.9252, auc 0.9401
epoch 12001, loss 0.1581, train acc 94.46%, f1 0.9209, precision 0.9167, recall 0.9252, auc 0.9401
epoch 12101, loss 0.1570, train acc 94.46%, f1 0.9209, precision 0.9167, recall 0.9252, auc 0.9401
epoch 12201, loss 0.1560, train acc 94.46%, f1 0.9209, precision 0.9167, recall 0.9252, auc 0.9401
epoch 12301, loss 0.1549, train acc 94.63%, f1 0.9231, precision 0.9209, recall 0.9252, auc 0.9414
epoch 12401, loss 0.1539, train acc 94.79%, f1 0.9252, precision 0.9252, recall 0.9252, auc 0.9426
epoch 12501, loss 0.1529, train acc 94.79%, f1 0.9252, precision 0.9252, recall 0.9252, auc 0.9426
epoch 12601, loss 0.1519, train acc 94.79%, f1 0.9252, precision 0.9252, recall 0.9252, auc 0.9426
epoch 12701, loss 0.1510, train acc 94.79%, f1 0.9252, precision 0.9252, recall 0.9252, auc 0.9426
epoch 12801, loss 0.1500, train acc 95.11%, f1 0.9299, precision 0.9299, recall 0.9299, auc 0.9462
epoch 12901, loss 0.1491, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13001, loss 0.1482, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13101, loss 0.1473, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13201, loss 0.1464, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13301, loss 0.1456, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13401, loss 0.1447, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13501, loss 0.1439, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13601, loss 0.1430, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13701, loss 0.1422, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13801, loss 0.1414, train acc 95.28%, f1 0.9324, precision 0.9302, recall 0.9346, auc 0.9485
epoch 13901, loss 0.1406, train acc 95.44%, f1 0.9349, precision 0.9306, recall 0.9393, auc 0.9509
epoch 14001, loss 0.1398, train acc 95.44%, f1 0.9349, precision 0.9306, recall 0.9393, auc 0.9509
epoch 14101, loss 0.1391, train acc 95.44%, f1 0.9349, precision 0.9306, recall 0.9393, auc 0.9509
epoch 14201, loss 0.1383, train acc 95.60%, f1 0.9374, precision 0.9309, recall 0.9439, auc 0.9532
epoch 14301, loss 0.1375, train acc 95.60%, f1 0.9374, precision 0.9309, recall 0.9439, auc 0.9532
epoch 14401, loss 0.1368, train acc 95.77%, f1 0.9395, precision 0.9352, recall 0.9439, auc 0.9545
epoch 14501, loss 0.1360, train acc 95.77%, f1 0.9395, precision 0.9352, recall 0.9439, auc 0.9545
epoch 14601, loss 0.1353, train acc 96.09%, f1 0.9439, precision 0.9439, recall 0.9439, auc 0.9570
epoch 14701, loss 0.1346, train acc 96.09%, f1 0.9439, precision 0.9439, recall 0.9439, auc 0.9570
epoch 14801, loss 0.1338, train acc 96.09%, f1 0.9439, precision 0.9439, recall 0.9439, auc 0.9570
epoch 14901, loss 0.1331, train acc 96.09%, f1 0.9439, precision 0.9439, recall 0.9439, auc 0.9570
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_normal_15000
normal
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_1
./test_pima/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.6435185185185186

the Fscore is 0.5370370370370371

the precision is 0.5370370370370371

the recall is 0.5370370370370371

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_1
----------------------



epoch 1, loss 0.6919, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6228, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5957, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5702, train acc 65.15%, f1 0.0093, precision 0.5000, recall 0.0047, auc 0.5011
epoch 401, loss 0.5475, train acc 67.43%, f1 0.1525, precision 0.8182, recall 0.0841, auc 0.5371
epoch 501, loss 0.5293, train acc 71.34%, f1 0.3529, precision 0.8276, recall 0.2243, auc 0.5996
epoch 601, loss 0.5153, train acc 74.10%, f1 0.4952, precision 0.7723, recall 0.3645, auc 0.6535
epoch 701, loss 0.5049, train acc 74.92%, f1 0.5549, precision 0.7273, recall 0.4486, auc 0.6793
epoch 801, loss 0.4971, train acc 76.06%, f1 0.5995, precision 0.7190, recall 0.5140, auc 0.7033
epoch 901, loss 0.4914, train acc 76.87%, f1 0.6283, precision 0.7143, recall 0.5607, auc 0.7204
epoch 1001, loss 0.4873, train acc 76.71%, f1 0.6324, precision 0.7029, recall 0.5748, auc 0.7224
epoch 1101, loss 0.4843, train acc 76.55%, f1 0.6364, precision 0.6923, recall 0.5888, auc 0.7244
epoch 1201, loss 0.4819, train acc 76.71%, f1 0.6452, precision 0.6878, recall 0.6075, auc 0.7300
epoch 1301, loss 0.4796, train acc 77.20%, f1 0.6535, precision 0.6947, recall 0.6168, auc 0.7359
epoch 1401, loss 0.4768, train acc 77.36%, f1 0.6534, precision 0.7005, recall 0.6121, auc 0.7361
epoch 1501, loss 0.4735, train acc 77.20%, f1 0.6465, precision 0.7033, recall 0.5981, auc 0.7316
epoch 1601, loss 0.4701, train acc 77.20%, f1 0.6447, precision 0.7056, recall 0.5935, auc 0.7305
epoch 1701, loss 0.4669, train acc 77.36%, f1 0.6463, precision 0.7095, recall 0.5935, auc 0.7317
epoch 1801, loss 0.4639, train acc 77.20%, f1 0.6482, precision 0.7011, recall 0.6028, auc 0.7327
epoch 1901, loss 0.4612, train acc 77.52%, f1 0.6567, precision 0.7021, recall 0.6168, auc 0.7384
epoch 2001, loss 0.4588, train acc 77.52%, f1 0.6567, precision 0.7021, recall 0.6168, auc 0.7384
epoch 2101, loss 0.4565, train acc 77.69%, f1 0.6566, precision 0.7081, recall 0.6121, auc 0.7386
epoch 2201, loss 0.4544, train acc 77.85%, f1 0.6583, precision 0.7120, recall 0.6121, auc 0.7398
epoch 2301, loss 0.4523, train acc 77.85%, f1 0.6583, precision 0.7120, recall 0.6121, auc 0.7398
epoch 2401, loss 0.4499, train acc 77.69%, f1 0.6549, precision 0.7104, recall 0.6075, auc 0.7375
epoch 2501, loss 0.4475, train acc 77.69%, f1 0.6532, precision 0.7127, recall 0.6028, auc 0.7364
epoch 2601, loss 0.4452, train acc 77.52%, f1 0.6515, precision 0.7088, recall 0.6028, auc 0.7352
epoch 2701, loss 0.4428, train acc 78.18%, f1 0.6616, precision 0.7198, recall 0.6121, auc 0.7423
epoch 2801, loss 0.4403, train acc 78.18%, f1 0.6650, precision 0.7151, recall 0.6215, auc 0.7445
epoch 2901, loss 0.4379, train acc 78.18%, f1 0.6650, precision 0.7151, recall 0.6215, auc 0.7445
epoch 3001, loss 0.4356, train acc 78.01%, f1 0.6633, precision 0.7112, recall 0.6215, auc 0.7432
epoch 3101, loss 0.4334, train acc 78.18%, f1 0.6667, precision 0.7128, recall 0.6262, auc 0.7456
epoch 3201, loss 0.4312, train acc 78.83%, f1 0.6766, precision 0.7234, recall 0.6355, auc 0.7528
epoch 3301, loss 0.4291, train acc 79.48%, f1 0.6866, precision 0.7340, recall 0.6449, auc 0.7599
epoch 3401, loss 0.4271, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 3501, loss 0.4252, train acc 79.97%, f1 0.6917, precision 0.7459, recall 0.6449, auc 0.7637
epoch 3601, loss 0.4232, train acc 79.97%, f1 0.6902, precision 0.7486, recall 0.6402, auc 0.7626
epoch 3701, loss 0.4212, train acc 79.80%, f1 0.6900, precision 0.7419, recall 0.6449, auc 0.7624
epoch 3801, loss 0.4193, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 3901, loss 0.4173, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 4001, loss 0.4151, train acc 79.80%, f1 0.6915, precision 0.7394, recall 0.6495, auc 0.7635
epoch 4101, loss 0.4128, train acc 80.13%, f1 0.6965, precision 0.7447, recall 0.6542, auc 0.7671
epoch 4201, loss 0.4100, train acc 79.97%, f1 0.6948, precision 0.7407, recall 0.6542, auc 0.7659
epoch 4301, loss 0.4065, train acc 80.78%, f1 0.7094, precision 0.7500, recall 0.6729, auc 0.7764
epoch 4401, loss 0.4028, train acc 81.11%, f1 0.7129, precision 0.7579, recall 0.6729, auc 0.7789
epoch 4501, loss 0.3989, train acc 81.11%, f1 0.7157, precision 0.7526, recall 0.6822, auc 0.7811
epoch 4601, loss 0.3949, train acc 81.11%, f1 0.7157, precision 0.7526, recall 0.6822, auc 0.7811
epoch 4701, loss 0.3912, train acc 81.27%, f1 0.7174, precision 0.7565, recall 0.6822, auc 0.7824
epoch 4801, loss 0.3869, train acc 81.76%, f1 0.7241, precision 0.7656, recall 0.6869, auc 0.7872
epoch 4901, loss 0.3819, train acc 81.43%, f1 0.7192, precision 0.7604, recall 0.6822, auc 0.7836
epoch 5001, loss 0.3764, train acc 82.41%, f1 0.7353, precision 0.7732, recall 0.7009, auc 0.7955
epoch 5101, loss 0.3693, train acc 82.90%, f1 0.7445, precision 0.7766, recall 0.7150, auc 0.8025
epoch 5201, loss 0.3613, train acc 83.55%, f1 0.7554, precision 0.7839, recall 0.7290, auc 0.8107
epoch 5301, loss 0.3537, train acc 84.04%, f1 0.7621, precision 0.7929, recall 0.7336, auc 0.8156
epoch 5401, loss 0.3462, train acc 84.36%, f1 0.7670, precision 0.7980, recall 0.7383, auc 0.8192
epoch 5501, loss 0.3384, train acc 85.34%, f1 0.7816, precision 0.8131, recall 0.7523, auc 0.8299
epoch 5601, loss 0.3314, train acc 85.99%, f1 0.7913, precision 0.8232, recall 0.7617, auc 0.8371
epoch 5701, loss 0.3247, train acc 85.99%, f1 0.7913, precision 0.8232, recall 0.7617, auc 0.8371
epoch 5801, loss 0.3182, train acc 86.48%, f1 0.8010, precision 0.8227, recall 0.7804, auc 0.8452
epoch 5901, loss 0.3122, train acc 86.97%, f1 0.8086, precision 0.8284, recall 0.7897, auc 0.8511
epoch 6001, loss 0.3068, train acc 87.30%, f1 0.8134, precision 0.8333, recall 0.7944, auc 0.8547
epoch 6101, loss 0.3019, train acc 87.79%, f1 0.8210, precision 0.8390, recall 0.8037, auc 0.8606
epoch 6201, loss 0.2973, train acc 87.95%, f1 0.8230, precision 0.8431, recall 0.8037, auc 0.8619
epoch 6301, loss 0.2930, train acc 88.27%, f1 0.8278, precision 0.8480, recall 0.8084, auc 0.8655
epoch 6401, loss 0.2889, train acc 88.60%, f1 0.8333, precision 0.8495, recall 0.8178, auc 0.8701
epoch 6501, loss 0.2850, train acc 89.09%, f1 0.8401, precision 0.8585, recall 0.8224, auc 0.8750
epoch 6601, loss 0.2810, train acc 89.09%, f1 0.8401, precision 0.8585, recall 0.8224, auc 0.8750
epoch 6701, loss 0.2771, train acc 89.58%, f1 0.8476, precision 0.8641, recall 0.8318, auc 0.8809
epoch 6801, loss 0.2730, train acc 89.90%, f1 0.8531, precision 0.8654, recall 0.8411, auc 0.8856
epoch 6901, loss 0.2692, train acc 90.23%, f1 0.8578, precision 0.8702, recall 0.8458, auc 0.8891
epoch 7001, loss 0.2656, train acc 90.07%, f1 0.8558, precision 0.8660, recall 0.8458, auc 0.8879
epoch 7101, loss 0.2622, train acc 90.07%, f1 0.8558, precision 0.8660, recall 0.8458, auc 0.8879
epoch 7201, loss 0.2588, train acc 90.07%, f1 0.8565, precision 0.8626, recall 0.8505, auc 0.8890
epoch 7301, loss 0.2553, train acc 90.23%, f1 0.8585, precision 0.8667, recall 0.8505, auc 0.8902
epoch 7401, loss 0.2514, train acc 89.90%, f1 0.8531, precision 0.8654, recall 0.8411, auc 0.8856
epoch 7501, loss 0.2474, train acc 90.07%, f1 0.8558, precision 0.8660, recall 0.8458, auc 0.8879
epoch 7601, loss 0.2431, train acc 89.90%, f1 0.8538, precision 0.8619, recall 0.8458, auc 0.8866
epoch 7701, loss 0.2388, train acc 90.88%, f1 0.8667, precision 0.8835, recall 0.8505, auc 0.8952
epoch 7801, loss 0.2347, train acc 90.88%, f1 0.8667, precision 0.8835, recall 0.8505, auc 0.8952
epoch 7901, loss 0.2310, train acc 91.04%, f1 0.8694, precision 0.8841, recall 0.8551, auc 0.8976
epoch 8001, loss 0.2276, train acc 91.04%, f1 0.8700, precision 0.8804, recall 0.8598, auc 0.8987
epoch 8101, loss 0.2244, train acc 91.21%, f1 0.8726, precision 0.8810, recall 0.8645, auc 0.9010
epoch 8201, loss 0.2214, train acc 91.21%, f1 0.8726, precision 0.8810, recall 0.8645, auc 0.9010/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2185, train acc 91.04%, f1 0.8700, precision 0.8804, recall 0.8598, auc 0.8987
epoch 8401, loss 0.2157, train acc 90.88%, f1 0.8685, precision 0.8726, recall 0.8645, auc 0.8985
epoch 8501, loss 0.2131, train acc 90.72%, f1 0.8665, precision 0.8685, recall 0.8645, auc 0.8972
epoch 8601, loss 0.2105, train acc 90.72%, f1 0.8665, precision 0.8685, recall 0.8645, auc 0.8972
epoch 8701, loss 0.2078, train acc 91.04%, f1 0.8700, precision 0.8804, recall 0.8598, auc 0.8987
epoch 8801, loss 0.2052, train acc 90.88%, f1 0.8673, precision 0.8798, recall 0.8551, auc 0.8963
epoch 8901, loss 0.2028, train acc 90.88%, f1 0.8673, precision 0.8798, recall 0.8551, auc 0.8963
epoch 9001, loss 0.2005, train acc 91.04%, f1 0.8700, precision 0.8804, recall 0.8598, auc 0.8987
epoch 9101, loss 0.1982, train acc 91.37%, f1 0.8747, precision 0.8852, recall 0.8645, auc 0.9022
epoch 9201, loss 0.1960, train acc 91.69%, f1 0.8794, precision 0.8900, recall 0.8692, auc 0.9058
epoch 9301, loss 0.1937, train acc 92.02%, f1 0.8842, precision 0.8947, recall 0.8738, auc 0.9094
epoch 9401, loss 0.1913, train acc 92.67%, f1 0.8941, precision 0.9005, recall 0.8879, auc 0.9177
epoch 9501, loss 0.1889, train acc 92.51%, f1 0.8915, precision 0.9000, recall 0.8832, auc 0.9153
epoch 9601, loss 0.1862, train acc 93.00%, f1 0.8988, precision 0.9052, recall 0.8925, auc 0.9213
epoch 9701, loss 0.1837, train acc 93.49%, f1 0.9057, precision 0.9143, recall 0.8972, auc 0.9261
epoch 9801, loss 0.1812, train acc 93.49%, f1 0.9057, precision 0.9143, recall 0.8972, auc 0.9261
epoch 9901, loss 0.1791, train acc 93.32%, f1 0.9031, precision 0.9139, recall 0.8925, auc 0.9238
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_normal_10000
normal
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_1
./test_pima/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.7988888888888889

the Fscore is 0.7368421052631577

the precision is 0.7

the recall is 0.7777777777777778

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_1
----------------------



epoch 1, loss 0.6963, train acc 34.85%, f1 0.5169, precision 0.3485, recall 1.0000, auc 0.5000
epoch 101, loss 0.6235, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5962, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5711, train acc 64.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5486, train acc 67.10%, f1 0.1368, precision 0.8000, recall 0.0748, auc 0.5324
epoch 501, loss 0.5302, train acc 71.34%, f1 0.3481, precision 0.8393, recall 0.2196, auc 0.5986
epoch 601, loss 0.5152, train acc 74.43%, f1 0.5078, precision 0.7714, recall 0.3785, auc 0.6593
epoch 701, loss 0.5006, train acc 75.24%, f1 0.5801, precision 0.7095, recall 0.4907, auc 0.6916
epoch 801, loss 0.4888, train acc 76.87%, f1 0.6340, precision 0.7069, recall 0.5748, auc 0.7236
epoch 901, loss 0.4813, train acc 77.36%, f1 0.6499, precision 0.7049, recall 0.6028, auc 0.7339
epoch 1001, loss 0.4766, train acc 77.69%, f1 0.6584, precision 0.7059, recall 0.6168, auc 0.7397
epoch 1101, loss 0.4735, train acc 78.18%, f1 0.6700, precision 0.7083, recall 0.6355, auc 0.7478
epoch 1201, loss 0.4710, train acc 78.18%, f1 0.6716, precision 0.7062, recall 0.6402, auc 0.7488
epoch 1301, loss 0.4681, train acc 77.52%, f1 0.6618, precision 0.6959, recall 0.6308, auc 0.7417
epoch 1401, loss 0.4644, train acc 77.52%, f1 0.6567, precision 0.7021, recall 0.6168, auc 0.7384
epoch 1501, loss 0.4606, train acc 76.71%, f1 0.6416, precision 0.6919, recall 0.5981, auc 0.7278
epoch 1601, loss 0.4568, train acc 76.87%, f1 0.6432, precision 0.6957, recall 0.5981, auc 0.7291
epoch 1701, loss 0.4536, train acc 78.34%, f1 0.6633, precision 0.7238, recall 0.6121, auc 0.7436
epoch 1801, loss 0.4511, train acc 77.52%, f1 0.6515, precision 0.7088, recall 0.6028, auc 0.7352
epoch 1901, loss 0.4490, train acc 77.69%, f1 0.6532, precision 0.7127, recall 0.6028, auc 0.7364
epoch 2001, loss 0.4471, train acc 78.18%, f1 0.6616, precision 0.7198, recall 0.6121, auc 0.7423
epoch 2101, loss 0.4454, train acc 78.18%, f1 0.6650, precision 0.7151, recall 0.6215, auc 0.7445
epoch 2201, loss 0.4437, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 2301, loss 0.4420, train acc 78.18%, f1 0.6650, precision 0.7151, recall 0.6215, auc 0.7445
epoch 2401, loss 0.4400, train acc 78.18%, f1 0.6667, precision 0.7128, recall 0.6262, auc 0.7456
epoch 2501, loss 0.4376, train acc 77.85%, f1 0.6617, precision 0.7074, recall 0.6215, auc 0.7420
epoch 2601, loss 0.4349, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 2701, loss 0.4324, train acc 77.69%, f1 0.6600, precision 0.7037, recall 0.6215, auc 0.7407
epoch 2801, loss 0.4302, train acc 77.85%, f1 0.6634, precision 0.7053, recall 0.6262, auc 0.7431
epoch 2901, loss 0.4281, train acc 78.01%, f1 0.6650, precision 0.7090, recall 0.6262, auc 0.7443
epoch 3001, loss 0.4262, train acc 78.01%, f1 0.6650, precision 0.7090, recall 0.6262, auc 0.7443
epoch 3101, loss 0.4243, train acc 78.18%, f1 0.6667, precision 0.7128, recall 0.6262, auc 0.7456
epoch 3201, loss 0.4226, train acc 78.18%, f1 0.6667, precision 0.7128, recall 0.6262, auc 0.7456
epoch 3301, loss 0.4206, train acc 78.66%, f1 0.6749, precision 0.7196, recall 0.6355, auc 0.7515
epoch 3401, loss 0.4183, train acc 79.15%, f1 0.6816, precision 0.7287, recall 0.6402, auc 0.7563
epoch 3501, loss 0.4161, train acc 79.32%, f1 0.6817, precision 0.7351, recall 0.6355, auc 0.7565
epoch 3601, loss 0.4140, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 3701, loss 0.4118, train acc 79.97%, f1 0.6933, precision 0.7433, recall 0.6495, auc 0.7648
epoch 3801, loss 0.4097, train acc 80.29%, f1 0.6967, precision 0.7514, recall 0.6495, auc 0.7673
epoch 3901, loss 0.4076, train acc 80.62%, f1 0.7032, precision 0.7540, recall 0.6589, auc 0.7719
epoch 4001, loss 0.4050, train acc 81.11%, f1 0.7114, precision 0.7606, recall 0.6682, auc 0.7779
epoch 4101, loss 0.4022, train acc 81.27%, f1 0.7146, precision 0.7619, recall 0.6729, auc 0.7802
epoch 4201, loss 0.3994, train acc 81.43%, f1 0.7164, precision 0.7660, recall 0.6729, auc 0.7814
epoch 4301, loss 0.3966, train acc 81.11%, f1 0.7114, precision 0.7606, recall 0.6682, auc 0.7779
epoch 4401, loss 0.3938, train acc 80.94%, f1 0.7082, precision 0.7594, recall 0.6636, auc 0.7755
epoch 4501, loss 0.3907, train acc 81.43%, f1 0.7164, precision 0.7660, recall 0.6729, auc 0.7814
epoch 4601, loss 0.3871, train acc 81.60%, f1 0.7182, precision 0.7701, recall 0.6729, auc 0.7827
epoch 4701, loss 0.3838, train acc 82.57%, f1 0.7332, precision 0.7861, recall 0.6869, auc 0.7935
epoch 4801, loss 0.3804, train acc 82.74%, f1 0.7350, precision 0.7903, recall 0.6869, auc 0.7947
epoch 4901, loss 0.3764, train acc 82.90%, f1 0.7382, precision 0.7914, recall 0.6916, auc 0.7970
epoch 5001, loss 0.3713, train acc 82.74%, f1 0.7350, precision 0.7903, recall 0.6869, auc 0.7947
epoch 5101, loss 0.3656, train acc 83.39%, f1 0.7463, precision 0.7979, recall 0.7009, auc 0.8030
epoch 5201, loss 0.3592, train acc 83.55%, f1 0.7518, precision 0.7927, recall 0.7150, auc 0.8075
epoch 5301, loss 0.3537, train acc 83.55%, f1 0.7518, precision 0.7927, recall 0.7150, auc 0.8075
epoch 5401, loss 0.3491, train acc 83.88%, f1 0.7568, precision 0.7979, recall 0.7196, auc 0.8111
epoch 5501, loss 0.3446, train acc 83.39%, f1 0.7488, precision 0.7917, recall 0.7103, auc 0.8051
epoch 5601, loss 0.3400, train acc 83.88%, f1 0.7568, precision 0.7979, recall 0.7196, auc 0.8111
epoch 5701, loss 0.3352, train acc 82.90%, f1 0.7407, precision 0.7853, recall 0.7009, auc 0.7992
epoch 5801, loss 0.3305, train acc 83.88%, f1 0.7568, precision 0.7979, recall 0.7196, auc 0.8111
epoch 5901, loss 0.3254, train acc 84.20%, f1 0.7605, precision 0.8063, recall 0.7196, auc 0.8136
epoch 6001, loss 0.3186, train acc 85.83%, f1 0.7873, precision 0.8256, recall 0.7523, auc 0.8337
epoch 6101, loss 0.3120, train acc 85.67%, f1 0.7854, precision 0.8214, recall 0.7523, auc 0.8324
epoch 6201, loss 0.3059, train acc 86.32%, f1 0.7961, precision 0.8283, recall 0.7664, auc 0.8407
epoch 6301, loss 0.3003, train acc 86.81%, f1 0.8039, precision 0.8342, recall 0.7757, auc 0.8466
epoch 6401, loss 0.2950, train acc 87.46%, f1 0.8127, precision 0.8477, recall 0.7804, auc 0.8527
epoch 6501, loss 0.2901, train acc 87.95%, f1 0.8204, precision 0.8535, recall 0.7897, auc 0.8586
epoch 6601, loss 0.2856, train acc 88.27%, f1 0.8261, precision 0.8550, recall 0.7991, auc 0.8633
epoch 6701, loss 0.2815, train acc 88.76%, f1 0.8345, precision 0.8571, recall 0.8131, auc 0.8703
epoch 6801, loss 0.2777, train acc 89.09%, f1 0.8393, precision 0.8621, recall 0.8178, auc 0.8739
epoch 6901, loss 0.2743, train acc 89.09%, f1 0.8401, precision 0.8585, recall 0.8224, auc 0.8750
epoch 7001, loss 0.2709, train acc 89.25%, f1 0.8429, precision 0.8592, recall 0.8271, auc 0.8773
epoch 7101, loss 0.2678, train acc 88.93%, f1 0.8373, precision 0.8578, recall 0.8178, auc 0.8726
epoch 7201, loss 0.2649, train acc 89.09%, f1 0.8401, precision 0.8585, recall 0.8224, auc 0.8750
epoch 7301, loss 0.2622, train acc 88.93%, f1 0.8373, precision 0.8578, recall 0.8178, auc 0.8726
epoch 7401, loss 0.2597, train acc 88.60%, f1 0.8325, precision 0.8529, recall 0.8131, auc 0.8690
epoch 7501, loss 0.2573, train acc 88.76%, f1 0.8353, precision 0.8537, recall 0.8178, auc 0.8714
epoch 7601, loss 0.2551, train acc 89.09%, f1 0.8401, precision 0.8585, recall 0.8224, auc 0.8750
epoch 7701, loss 0.2530, train acc 89.58%, f1 0.8476, precision 0.8641, recall 0.8318, auc 0.8809
epoch 7801, loss 0.2509, train acc 89.41%, f1 0.8449, precision 0.8634, recall 0.8271, auc 0.8786
epoch 7901, loss 0.2489, train acc 89.41%, f1 0.8449, precision 0.8634, recall 0.8271, auc 0.8786
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_normal_8000
normal
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_1
./test_pima/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.6635185185185186

the Fscore is 0.5576923076923077

the precision is 0.58

the recall is 0.5370370370370371

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_1
----------------------



epoch 1, loss 0.6888, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6226, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5956, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5699, train acc 65.15%, f1 0.0093, precision 0.5000, recall 0.0047, auc 0.5011
epoch 401, loss 0.5470, train acc 67.75%, f1 0.1750, precision 0.8077, recall 0.0981, auc 0.5428
epoch 501, loss 0.5288, train acc 71.34%, f1 0.3577, precision 0.8167, recall 0.2290, auc 0.6007
epoch 601, loss 0.5148, train acc 74.27%, f1 0.5031, precision 0.7692, recall 0.3738, auc 0.6569
epoch 701, loss 0.5044, train acc 75.08%, f1 0.5591, precision 0.7293, recall 0.4533, auc 0.6816
epoch 801, loss 0.4967, train acc 76.06%, f1 0.5995, precision 0.7190, recall 0.5140, auc 0.7033
epoch 901, loss 0.4910, train acc 76.87%, f1 0.6302, precision 0.7118, recall 0.5654, auc 0.7215
epoch 1001, loss 0.4870, train acc 76.71%, f1 0.6324, precision 0.7029, recall 0.5748, auc 0.7224
epoch 1101, loss 0.4841, train acc 76.55%, f1 0.6364, precision 0.6923, recall 0.5888, auc 0.7244
epoch 1201, loss 0.4818, train acc 76.55%, f1 0.6436, precision 0.6842, recall 0.6075, auc 0.7287
epoch 1301, loss 0.4797, train acc 77.04%, f1 0.6519, precision 0.6911, recall 0.6168, auc 0.7347
epoch 1401, loss 0.4768, train acc 77.52%, f1 0.6550, precision 0.7043, recall 0.6121, auc 0.7373
epoch 1501, loss 0.4730, train acc 77.20%, f1 0.6465, precision 0.7033, recall 0.5981, auc 0.7316
epoch 1601, loss 0.4692, train acc 77.04%, f1 0.6412, precision 0.7039, recall 0.5888, auc 0.7281
epoch 1701, loss 0.4658, train acc 77.20%, f1 0.6447, precision 0.7056, recall 0.5935, auc 0.7305
epoch 1801, loss 0.4627, train acc 77.04%, f1 0.6484, precision 0.6952, recall 0.6075, auc 0.7325
epoch 1901, loss 0.4601, train acc 77.52%, f1 0.6567, precision 0.7021, recall 0.6168, auc 0.7384
epoch 2001, loss 0.4578, train acc 77.52%, f1 0.6550, precision 0.7043, recall 0.6121, auc 0.7373
epoch 2101, loss 0.4557, train acc 77.85%, f1 0.6583, precision 0.7120, recall 0.6121, auc 0.7398
epoch 2201, loss 0.4534, train acc 78.01%, f1 0.6633, precision 0.7112, recall 0.6215, auc 0.7432
epoch 2301, loss 0.4507, train acc 77.85%, f1 0.6583, precision 0.7120, recall 0.6121, auc 0.7398
epoch 2401, loss 0.4476, train acc 77.85%, f1 0.6548, precision 0.7167, recall 0.6028, auc 0.7377
epoch 2501, loss 0.4444, train acc 77.52%, f1 0.6497, precision 0.7111, recall 0.5981, auc 0.7341
epoch 2601, loss 0.4417, train acc 77.69%, f1 0.6532, precision 0.7127, recall 0.6028, auc 0.7364
epoch 2701, loss 0.4392, train acc 77.85%, f1 0.6583, precision 0.7120, recall 0.6121, auc 0.7398
epoch 2801, loss 0.4369, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 2901, loss 0.4348, train acc 78.01%, f1 0.6617, precision 0.7135, recall 0.6168, auc 0.7422
epoch 3001, loss 0.4328, train acc 78.34%, f1 0.6667, precision 0.7189, recall 0.6215, auc 0.7457
epoch 3101, loss 0.4309, train acc 78.66%, f1 0.6733, precision 0.7219, recall 0.6308, auc 0.7504
epoch 3201, loss 0.4290, train acc 78.99%, f1 0.6783, precision 0.7273, recall 0.6355, auc 0.7540
epoch 3301, loss 0.4270, train acc 79.32%, f1 0.6833, precision 0.7326, recall 0.6402, auc 0.7576
epoch 3401, loss 0.4250, train acc 79.80%, f1 0.6900, precision 0.7419, recall 0.6449, auc 0.7624
epoch 3501, loss 0.4229, train acc 79.80%, f1 0.6900, precision 0.7419, recall 0.6449, auc 0.7624
epoch 3601, loss 0.4207, train acc 79.64%, f1 0.6867, precision 0.7405, recall 0.6402, auc 0.7601
epoch 3701, loss 0.4185, train acc 79.64%, f1 0.6883, precision 0.7380, recall 0.6449, auc 0.7612
epoch 3801, loss 0.4164, train acc 79.97%, f1 0.6933, precision 0.7433, recall 0.6495, auc 0.7648
epoch 3901, loss 0.4142, train acc 79.97%, f1 0.6948, precision 0.7407, recall 0.6542, auc 0.7659
epoch 4001, loss 0.4120, train acc 80.46%, f1 0.7015, precision 0.7500, recall 0.6589, auc 0.7707
epoch 4101, loss 0.4097, train acc 80.46%, f1 0.7015, precision 0.7500, recall 0.6589, auc 0.7707
epoch 4201, loss 0.4068, train acc 80.78%, f1 0.7065, precision 0.7553, recall 0.6636, auc 0.7743
epoch 4301, loss 0.4038, train acc 81.11%, f1 0.7114, precision 0.7606, recall 0.6682, auc 0.7779
epoch 4401, loss 0.4003, train acc 80.46%, f1 0.7015, precision 0.7500, recall 0.6589, auc 0.7707
epoch 4501, loss 0.3955, train acc 80.46%, f1 0.7015, precision 0.7500, recall 0.6589, auc 0.7707
epoch 4601, loss 0.3904, train acc 80.46%, f1 0.7000, precision 0.7527, recall 0.6542, auc 0.7696
epoch 4701, loss 0.3856, train acc 80.94%, f1 0.7097, precision 0.7566, recall 0.6682, auc 0.7766
epoch 4801, loss 0.3809, train acc 80.94%, f1 0.7068, precision 0.7622, recall 0.6589, auc 0.7744
epoch 4901, loss 0.3759, train acc 81.60%, f1 0.7168, precision 0.7730, recall 0.6682, auc 0.7816
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_normal_5000
normal
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_1
./test_pima/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.7348148148148149

the Fscore is 0.6538461538461539

the precision is 0.68

the recall is 0.6296296296296297

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_1
----------------------



epoch 1, loss 0.6926, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6227, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5957, train acc 65.15%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5704, train acc 65.15%, f1 0.0093, precision 0.5000, recall 0.0047, auc 0.5011
epoch 401, loss 0.5477, train acc 67.26%, f1 0.1447, precision 0.8095, recall 0.0794, auc 0.5347
epoch 501, loss 0.5296, train acc 71.34%, f1 0.3529, precision 0.8276, recall 0.2243, auc 0.5996
epoch 601, loss 0.5156, train acc 73.94%, f1 0.4872, precision 0.7755, recall 0.3551, auc 0.6501
epoch 701, loss 0.5051, train acc 75.24%, f1 0.5581, precision 0.7385, recall 0.4486, auc 0.6818
epoch 801, loss 0.4973, train acc 76.06%, f1 0.5995, precision 0.7190, recall 0.5140, auc 0.7033
epoch 901, loss 0.4915, train acc 76.87%, f1 0.6283, precision 0.7143, recall 0.5607, auc 0.7204
epoch 1001, loss 0.4874, train acc 76.71%, f1 0.6324, precision 0.7029, recall 0.5748, auc 0.7224
epoch 1101, loss 0.4844, train acc 76.55%, f1 0.6364, precision 0.6923, recall 0.5888, auc 0.7244
epoch 1201, loss 0.4819, train acc 76.87%, f1 0.6450, precision 0.6935, recall 0.6028, auc 0.7302
epoch 1301, loss 0.4795, train acc 77.20%, f1 0.6517, precision 0.6968, recall 0.6121, auc 0.7348
epoch 1401, loss 0.4764, train acc 77.52%, f1 0.6533, precision 0.7065, recall 0.6075, auc 0.7362
epoch 1501, loss 0.4730, train acc 76.87%, f1 0.6414, precision 0.6978, recall 0.5935, auc 0.7280
epoch 1601, loss 0.4697, train acc 77.36%, f1 0.6463, precision 0.7095, recall 0.5935, auc 0.7317
epoch 1701, loss 0.4665, train acc 77.20%, f1 0.6447, precision 0.7056, recall 0.5935, auc 0.7305
epoch 1801, loss 0.4635, train acc 77.36%, f1 0.6516, precision 0.7027, recall 0.6075, auc 0.7350
epoch 1901, loss 0.4609, train acc 77.52%, f1 0.6567, precision 0.7021, recall 0.6168, auc 0.7384
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_normal_2000
normal
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_1
./test_pima/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.764074074074074

the Fscore is 0.6930693069306931

the precision is 0.7446808510638298

the recall is 0.6481481481481481

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_1
----------------------



epoch 1, loss 0.6934, train acc 55.66%, f1 0.6912, precision 0.5302, recall 0.9925, auc 0.5566
epoch 101, loss 0.3199, train acc 87.29%, f1 0.8729, precision 0.8731, recall 0.8727, auc 0.8729
epoch 201, loss 0.1553, train acc 96.29%, f1 0.9629, precision 0.9630, recall 0.9628, auc 0.9629
epoch 301, loss 0.1086, train acc 97.99%, f1 0.9799, precision 0.9800, recall 0.9798, auc 0.9799
epoch 401, loss 0.0608, train acc 98.50%, f1 0.9850, precision 0.9851, recall 0.9850, auc 0.9850
epoch 501, loss 0.0433, train acc 98.84%, f1 0.9884, precision 0.9884, recall 0.9884, auc 0.9884
epoch 601, loss 0.0366, train acc 99.00%, f1 0.9900, precision 0.9901, recall 0.9900, auc 0.9900
epoch 701, loss 0.0357, train acc 99.14%, f1 0.9914, precision 0.9914, recall 0.9913, auc 0.9914
epoch 801, loss 0.0352, train acc 99.27%, f1 0.9927, precision 0.9927, recall 0.9926, auc 0.9927
epoch 901, loss 0.0208, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9933, auc 0.9934
epoch 1001, loss 0.0223, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1101, loss 0.0198, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9945, auc 0.9946
epoch 1201, loss 0.0133, train acc 99.52%, f1 0.9952, precision 0.9953, recall 0.9952, auc 0.9952
epoch 1301, loss 0.0172, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9956, auc 0.9956
epoch 1401, loss 0.0157, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1501, loss 0.0264, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1601, loss 0.0207, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9964, auc 0.9964
epoch 1701, loss 0.0111, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1801, loss 0.0143, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9968, auc 0.9968
epoch 1901, loss 0.0113, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2001, loss 0.0103, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2101, loss 0.0089, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 2201, loss 0.0047, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2301, loss 0.0121, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2401, loss 0.0067, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0050, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9976, auc 0.9977
epoch 2601, loss 0.0076, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2701, loss 0.0082, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2801, loss 0.0086, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2901, loss 0.0076, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3001, loss 0.0057, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3101, loss 0.0079, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3201, loss 0.0057, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3301, loss 0.0130, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 3401, loss 0.0045, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 3501, loss 0.0032, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3601, loss 0.0030, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3701, loss 0.0024, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3801, loss 0.0078, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3901, loss 0.0046, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4001, loss 0.0046, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4101, loss 0.0026, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4201, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4301, loss 0.0027, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 4401, loss 0.0026, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 4501, loss 0.0058, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4601, loss 0.0057, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4701, loss 0.0058, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4801, loss 0.0007, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4901, loss 0.0024, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5001, loss 0.0021, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5101, loss 0.0041, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 5201, loss 0.0033, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5301, loss 0.0011, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5401, loss 0.0048, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5501, loss 0.0037, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5601, loss 0.0033, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5701, loss 0.0052, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 5801, loss 0.0034, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5901, loss 0.0070, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 6001, loss 0.0014, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 6101, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 6201, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6301, loss 0.0008, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6401, loss 0.0012, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6501, loss 0.0030, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6601, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 6701, loss 0.0047, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6801, loss 0.0042, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 6901, loss 0.0013, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 7001, loss 0.0055, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 7101, loss 0.0026, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7201, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7301, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7401, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 7501, loss 0.0067, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7601, loss 0.0034, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 7701, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7801, loss 0.0059, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 7901, loss 0.0011, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 8001, loss 0.0038, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 8101, loss 0.0006, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8201, loss 0.0005, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8301, loss 0.0020, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8401, loss 0.0040, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8501, loss 0.0033, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 8601, loss 0.0050, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8701, loss 0.0054, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8801, loss 0.0033, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 8901, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 9001, loss 0.0053, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 9101, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9201, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9301, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9401, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 9501, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 9601, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 9701, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 9801, loss 0.0031, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9901, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 10001, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 10101, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10201, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10301, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 10401, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10501, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 10601, loss 0.0037, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 10701, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 10801, loss 0.0030, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10901, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11001, loss 0.0019, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 11101, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 11201, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11301, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 11401, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11501, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11601, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11701, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11801, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11901, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12001, loss 0.0005, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 12101, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12201, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12301, loss 0.0001, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12401, loss 0.0009, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12601, loss 0.0001, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 13101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 13301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 13401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_1
./test_vehicle0/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.925

the Fscore is 0.9189189189189189

the precision is 1.0

the recall is 0.85

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_1
----------------------



epoch 1, loss 0.6931, train acc 58.15%, f1 0.2917, precision 0.9488, recall 0.1724, auc 0.5815
epoch 101, loss 0.3457, train acc 87.03%, f1 0.8703, precision 0.8704, recall 0.8703, auc 0.8703
epoch 201, loss 0.1258, train acc 96.16%, f1 0.9616, precision 0.9617, recall 0.9615, auc 0.9616
epoch 301, loss 0.0823, train acc 97.99%, f1 0.9799, precision 0.9799, recall 0.9799, auc 0.9799
epoch 401, loss 0.0739, train acc 98.57%, f1 0.9857, precision 0.9857, recall 0.9857, auc 0.9857
epoch 501, loss 0.0470, train acc 98.82%, f1 0.9882, precision 0.9882, recall 0.9882, auc 0.9882
epoch 601, loss 0.0336, train acc 99.01%, f1 0.9901, precision 0.9900, recall 0.9901, auc 0.9901
epoch 701, loss 0.0494, train acc 99.15%, f1 0.9915, precision 0.9914, recall 0.9915, auc 0.9915
epoch 801, loss 0.0268, train acc 99.27%, f1 0.9927, precision 0.9927, recall 0.9928, auc 0.9927
epoch 901, loss 0.0134, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9935, auc 0.9934
epoch 1001, loss 0.0272, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1101, loss 0.0170, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9947, auc 0.9947
epoch 1201, loss 0.0188, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1301, loss 0.0210, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1401, loss 0.0175, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1501, loss 0.0158, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1601, loss 0.0120, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1701, loss 0.0211, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1801, loss 0.0064, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 1901, loss 0.0139, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2001, loss 0.0071, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2101, loss 0.0121, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2201, loss 0.0114, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9974
epoch 2301, loss 0.0175, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2401, loss 0.0069, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2501, loss 0.0044, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2601, loss 0.0114, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2701, loss 0.0079, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2801, loss 0.0091, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2901, loss 0.0057, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3001, loss 0.0104, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3101, loss 0.0055, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3201, loss 0.0094, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3301, loss 0.0030, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9980
epoch 3401, loss 0.0050, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3501, loss 0.0050, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3601, loss 0.0047, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 3701, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 3801, loss 0.0057, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3901, loss 0.0042, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4001, loss 0.0080, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4101, loss 0.0039, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 4201, loss 0.0079, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4301, loss 0.0087, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 4401, loss 0.0086, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 4501, loss 0.0058, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4601, loss 0.0021, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 4701, loss 0.0030, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4801, loss 0.0026, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4901, loss 0.0040, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 5001, loss 0.0022, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5101, loss 0.0008, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5201, loss 0.0032, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5301, loss 0.0087, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 5401, loss 0.0030, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5501, loss 0.0052, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5601, loss 0.0016, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5701, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5801, loss 0.0045, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5901, loss 0.0036, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 6001, loss 0.0013, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6101, loss 0.0059, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6201, loss 0.0064, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6301, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6401, loss 0.0068, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6501, loss 0.0019, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6601, loss 0.0057, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 6701, loss 0.0094, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 6801, loss 0.0046, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6901, loss 0.0046, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 7001, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 7101, loss 0.0057, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 7201, loss 0.0012, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 7301, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 7401, loss 0.0051, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7501, loss 0.0015, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7601, loss 0.0043, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7701, loss 0.0005, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 7801, loss 0.0016, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7901, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8001, loss 0.0052, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8101, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8201, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0003, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8401, loss 0.0009, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 8501, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8601, loss 0.0044, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8701, loss 0.0034, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8801, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8901, loss 0.0041, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9001, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9101, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9201, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9301, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 9401, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 9501, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 9601, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 9701, loss 0.0028, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9801, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9901, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 10001, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 10101, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 10201, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10301, loss 0.0028, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10401, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10501, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10601, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10701, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10801, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10901, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 11001, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11101, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11201, loss 0.0017, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11301, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11401, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11501, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11601, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11701, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11801, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11901, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12001, loss 0.0001, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_1
./test_vehicle0/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9625

the Fscore is 0.961038961038961

the precision is 1.0

the recall is 0.925

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_1
----------------------



epoch 1, loss 0.6934, train acc 49.84%, f1 0.6648, precision 0.4992, recall 0.9947, auc 0.4984
epoch 101, loss 0.3629, train acc 87.17%, f1 0.8715, precision 0.8732, recall 0.8697, auc 0.8717
epoch 201, loss 0.1471, train acc 96.32%, f1 0.9632, precision 0.9634, recall 0.9631, auc 0.9632
epoch 301, loss 0.0860, train acc 98.07%, f1 0.9807, precision 0.9807, recall 0.9808, auc 0.9807
epoch 401, loss 0.0599, train acc 98.55%, f1 0.9855, precision 0.9854, recall 0.9855, auc 0.9855
epoch 501, loss 0.0467, train acc 98.81%, f1 0.9881, precision 0.9880, recall 0.9881, auc 0.9881
epoch 601, loss 0.0255, train acc 98.98%, f1 0.9898, precision 0.9898, recall 0.9899, auc 0.9898
epoch 701, loss 0.0286, train acc 99.11%, f1 0.9911, precision 0.9910, recall 0.9912, auc 0.9911
epoch 801, loss 0.0349, train acc 99.24%, f1 0.9924, precision 0.9923, recall 0.9924, auc 0.9924
epoch 901, loss 0.0231, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9933, auc 0.9933
epoch 1001, loss 0.0181, train acc 99.39%, f1 0.9939, precision 0.9938, recall 0.9939, auc 0.9939
epoch 1101, loss 0.0092, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9946
epoch 1201, loss 0.0198, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9952, auc 0.9951
epoch 1301, loss 0.0173, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1401, loss 0.0090, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9958, auc 0.9958
epoch 1501, loss 0.0152, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9962, auc 0.9961
epoch 1601, loss 0.0248, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1701, loss 0.0102, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 1801, loss 0.0092, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 1901, loss 0.0073, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 2001, loss 0.0137, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2101, loss 0.0054, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9973
epoch 2201, loss 0.0047, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2301, loss 0.0071, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2401, loss 0.0070, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0101, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2601, loss 0.0063, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2701, loss 0.0064, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2801, loss 0.0042, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 2901, loss 0.0057, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3001, loss 0.0062, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 3101, loss 0.0075, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 3201, loss 0.0093, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 3301, loss 0.0083, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3401, loss 0.0056, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 3501, loss 0.0106, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 3601, loss 0.0066, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3701, loss 0.0041, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3801, loss 0.0090, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 3901, loss 0.0052, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4001, loss 0.0056, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4101, loss 0.0024, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 4201, loss 0.0034, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4301, loss 0.0011, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 4401, loss 0.0028, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4501, loss 0.0068, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4601, loss 0.0041, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4701, loss 0.0038, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4801, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 4901, loss 0.0055, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5001, loss 0.0025, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 5101, loss 0.0053, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5201, loss 0.0009, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 5301, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5401, loss 0.0050, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 5501, loss 0.0037, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5601, loss 0.0038, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 5701, loss 0.0004, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 5801, loss 0.0018, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 5901, loss 0.0020, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6001, loss 0.0036, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 6101, loss 0.0036, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 6201, loss 0.0058, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 6301, loss 0.0013, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 6401, loss 0.0007, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 6501, loss 0.0049, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 6601, loss 0.0050, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6701, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6801, loss 0.0045, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6901, loss 0.0009, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7001, loss 0.0034, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7101, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 7201, loss 0.0008, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7301, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 7401, loss 0.0032, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 7501, loss 0.0028, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 7601, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7701, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7801, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 7901, loss 0.0035, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 8001, loss 0.0029, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 8101, loss 0.0035, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 8201, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0005, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 8401, loss 0.0036, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 8501, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 8601, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 8701, loss 0.0038, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8801, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 8901, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9001, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 9101, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 9201, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9301, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 9401, loss 0.0024, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9501, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9601, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 9701, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9801, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9999, auc 0.9997
epoch 9901, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_1
./test_vehicle0/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9673076923076924

the Fscore is 0.9500000000000001

the precision is 0.95

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_1
----------------------



epoch 1, loss 0.6935, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.3166, train acc 87.31%, f1 0.8731, precision 0.8731, recall 0.8730, auc 0.8731
epoch 201, loss 0.1416, train acc 96.15%, f1 0.9615, precision 0.9615, recall 0.9615, auc 0.9615
epoch 301, loss 0.1146, train acc 98.08%, f1 0.9808, precision 0.9807, recall 0.9808, auc 0.9808
epoch 401, loss 0.0688, train acc 98.54%, f1 0.9854, precision 0.9854, recall 0.9855, auc 0.9854
epoch 501, loss 0.0727, train acc 98.84%, f1 0.9884, precision 0.9884, recall 0.9884, auc 0.9884
epoch 601, loss 0.0343, train acc 99.00%, f1 0.9900, precision 0.9900, recall 0.9900, auc 0.9900
epoch 701, loss 0.0252, train acc 99.13%, f1 0.9913, precision 0.9914, recall 0.9913, auc 0.9913
epoch 801, loss 0.0293, train acc 99.26%, f1 0.9926, precision 0.9926, recall 0.9926, auc 0.9926
epoch 901, loss 0.0150, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9934, auc 0.9934
epoch 1001, loss 0.0165, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9942, auc 0.9943
epoch 1101, loss 0.0209, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1201, loss 0.0167, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1301, loss 0.0109, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1401, loss 0.0160, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9959, auc 0.9958
epoch 1501, loss 0.0147, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 1601, loss 0.0154, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1701, loss 0.0098, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1801, loss 0.0127, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1901, loss 0.0080, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2001, loss 0.0122, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2101, loss 0.0031, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 2201, loss 0.0062, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9973, auc 0.9973
epoch 2301, loss 0.0107, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2401, loss 0.0102, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0112, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2601, loss 0.0037, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2701, loss 0.0082, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2801, loss 0.0059, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2901, loss 0.0178, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 3001, loss 0.0071, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3101, loss 0.0037, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3201, loss 0.0023, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3301, loss 0.0026, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3401, loss 0.0103, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3501, loss 0.0049, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3601, loss 0.0071, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3701, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3801, loss 0.0066, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3901, loss 0.0070, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4001, loss 0.0033, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4101, loss 0.0039, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4201, loss 0.0013, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 4301, loss 0.0029, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4401, loss 0.0077, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 4501, loss 0.0041, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 4601, loss 0.0050, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 4701, loss 0.0021, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4801, loss 0.0061, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 4901, loss 0.0064, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5001, loss 0.0018, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5101, loss 0.0021, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 5201, loss 0.0039, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5301, loss 0.0065, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5401, loss 0.0069, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 5501, loss 0.0060, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 5601, loss 0.0019, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5701, loss 0.0032, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5801, loss 0.0016, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5901, loss 0.0038, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 6001, loss 0.0036, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6101, loss 0.0111, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6201, loss 0.0018, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 6301, loss 0.0053, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6401, loss 0.0012, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 6501, loss 0.0035, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6601, loss 0.0013, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 6701, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6801, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6901, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 7001, loss 0.0037, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 7101, loss 0.0035, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 7201, loss 0.0048, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 7301, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7401, loss 0.0033, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 7501, loss 0.0012, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7601, loss 0.0044, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7701, loss 0.0039, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7801, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7901, loss 0.0024, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_1
./test_vehicle0/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9423076923076923

the Fscore is 0.9230769230769231

the precision is 0.9473684210526315

the recall is 0.9

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_1
----------------------



epoch 1, loss 0.6935, train acc 49.84%, f1 0.0000, precision 0.0039, recall 0.0000, auc 0.4984
epoch 101, loss 0.3049, train acc 87.01%, f1 0.8708, precision 0.8662, recall 0.8755, auc 0.8701
epoch 201, loss 0.1589, train acc 96.16%, f1 0.9616, precision 0.9614, recall 0.9619, auc 0.9616
epoch 301, loss 0.0913, train acc 98.04%, f1 0.9804, precision 0.9805, recall 0.9803, auc 0.9804
epoch 401, loss 0.0697, train acc 98.52%, f1 0.9852, precision 0.9854, recall 0.9850, auc 0.9852
epoch 501, loss 0.0604, train acc 98.75%, f1 0.9875, precision 0.9877, recall 0.9872, auc 0.9875
epoch 601, loss 0.0403, train acc 98.98%, f1 0.9898, precision 0.9899, recall 0.9897, auc 0.9898
epoch 701, loss 0.0247, train acc 99.11%, f1 0.9911, precision 0.9913, recall 0.9909, auc 0.9911
epoch 801, loss 0.0203, train acc 99.24%, f1 0.9924, precision 0.9925, recall 0.9923, auc 0.9924
epoch 901, loss 0.0191, train acc 99.35%, f1 0.9935, precision 0.9936, recall 0.9934, auc 0.9935
epoch 1001, loss 0.0209, train acc 99.40%, f1 0.9940, precision 0.9941, recall 0.9940, auc 0.9940
epoch 1101, loss 0.0220, train acc 99.47%, f1 0.9947, precision 0.9948, recall 0.9947, auc 0.9947
epoch 1201, loss 0.0198, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1301, loss 0.0167, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1401, loss 0.0161, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9958, auc 0.9959
epoch 1501, loss 0.0204, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1601, loss 0.0148, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1701, loss 0.0116, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9967, auc 0.9967
epoch 1801, loss 0.0088, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1901, loss 0.0106, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2001, loss 0.0183, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 2101, loss 0.0071, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2201, loss 0.0070, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2301, loss 0.0095, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2401, loss 0.0072, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9977, auc 0.9977
epoch 2501, loss 0.0094, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9977, auc 0.9977
epoch 2601, loss 0.0076, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 2701, loss 0.0064, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 2801, loss 0.0073, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2901, loss 0.0058, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 3001, loss 0.0023, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3101, loss 0.0060, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 3201, loss 0.0085, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 3301, loss 0.0049, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9982
epoch 3401, loss 0.0076, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 3501, loss 0.0049, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 3601, loss 0.0047, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 3701, loss 0.0041, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9982, auc 0.9983
epoch 3801, loss 0.0024, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3901, loss 0.0032, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 4001, loss 0.0018, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 4101, loss 0.0039, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 4201, loss 0.0097, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9984, auc 0.9985
epoch 4301, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 4401, loss 0.0064, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 4501, loss 0.0023, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4601, loss 0.0066, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 4701, loss 0.0035, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4801, loss 0.0042, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 4901, loss 0.0010, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_1
./test_vehicle0/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9548076923076924

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_1
----------------------



epoch 1, loss 0.6931, train acc 50.02%, f1 0.0009, precision 1.0000, recall 0.0005, auc 0.5002
epoch 101, loss 0.3286, train acc 87.40%, f1 0.8740, precision 0.8738, recall 0.8742, auc 0.8740
epoch 201, loss 0.1482, train acc 96.34%, f1 0.9634, precision 0.9635, recall 0.9633, auc 0.9634
epoch 301, loss 0.0967, train acc 98.00%, f1 0.9800, precision 0.9800, recall 0.9800, auc 0.9800
epoch 401, loss 0.0484, train acc 98.57%, f1 0.9857, precision 0.9856, recall 0.9857, auc 0.9857
epoch 501, loss 0.0435, train acc 98.81%, f1 0.9881, precision 0.9880, recall 0.9881, auc 0.9881
epoch 601, loss 0.0372, train acc 99.00%, f1 0.9900, precision 0.9899, recall 0.9900, auc 0.9900
epoch 701, loss 0.0351, train acc 99.11%, f1 0.9911, precision 0.9910, recall 0.9912, auc 0.9911
epoch 801, loss 0.0317, train acc 99.23%, f1 0.9923, precision 0.9922, recall 0.9923, auc 0.9923
epoch 901, loss 0.0203, train acc 99.35%, f1 0.9935, precision 0.9935, recall 0.9935, auc 0.9935
epoch 1001, loss 0.0217, train acc 99.42%, f1 0.9942, precision 0.9942, recall 0.9943, auc 0.9942
epoch 1101, loss 0.0243, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9947, auc 0.9947
epoch 1201, loss 0.0146, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1301, loss 0.0153, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1401, loss 0.0157, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1501, loss 0.0158, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1601, loss 0.0115, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1701, loss 0.0077, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9967, auc 0.9966
epoch 1801, loss 0.0109, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1901, loss 0.0065, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_1
./test_vehicle0/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9846153846153847

the Fscore is 0.9523809523809523

the precision is 0.9090909090909091

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_1
----------------------



epoch 1, loss 0.6930, train acc 79.71%, f1 0.7686, precision 0.8961, recall 0.6728, auc 0.7973
epoch 101, loss 0.3539, train acc 87.14%, f1 0.8714, precision 0.8731, recall 0.8696, auc 0.8714
epoch 201, loss 0.1512, train acc 96.47%, f1 0.9647, precision 0.9662, recall 0.9631, auc 0.9647
epoch 301, loss 0.0803, train acc 98.11%, f1 0.9811, precision 0.9811, recall 0.9811, auc 0.9811
epoch 401, loss 0.0624, train acc 98.51%, f1 0.9851, precision 0.9858, recall 0.9845, auc 0.9851
epoch 501, loss 0.0420, train acc 98.87%, f1 0.9888, precision 0.9889, recall 0.9886, auc 0.9887
epoch 601, loss 0.0238, train acc 99.02%, f1 0.9902, precision 0.9904, recall 0.9899, auc 0.9902
epoch 701, loss 0.0239, train acc 99.14%, f1 0.9914, precision 0.9913, recall 0.9915, auc 0.9914
epoch 801, loss 0.0188, train acc 99.25%, f1 0.9925, precision 0.9924, recall 0.9925, auc 0.9925
epoch 901, loss 0.0157, train acc 99.35%, f1 0.9935, precision 0.9936, recall 0.9933, auc 0.9935
epoch 1001, loss 0.0173, train acc 99.41%, f1 0.9942, precision 0.9945, recall 0.9939, auc 0.9941
epoch 1101, loss 0.0176, train acc 99.49%, f1 0.9949, precision 0.9950, recall 0.9949, auc 0.9949
epoch 1201, loss 0.0217, train acc 99.51%, f1 0.9951, precision 0.9949, recall 0.9953, auc 0.9951
epoch 1301, loss 0.0229, train acc 99.55%, f1 0.9955, precision 0.9956, recall 0.9955, auc 0.9955
epoch 1401, loss 0.0209, train acc 99.59%, f1 0.9959, precision 0.9965, recall 0.9953, auc 0.9959
epoch 1501, loss 0.0144, train acc 99.63%, f1 0.9963, precision 0.9968, recall 0.9958, auc 0.9963
epoch 1601, loss 0.0140, train acc 99.63%, f1 0.9963, precision 0.9969, recall 0.9957, auc 0.9963
epoch 1701, loss 0.0132, train acc 99.66%, f1 0.9966, precision 0.9973, recall 0.9959, auc 0.9966
epoch 1801, loss 0.0073, train acc 99.67%, f1 0.9967, precision 0.9971, recall 0.9964, auc 0.9967
epoch 1901, loss 0.0161, train acc 99.70%, f1 0.9970, precision 0.9974, recall 0.9965, auc 0.9970
epoch 2001, loss 0.0037, train acc 99.70%, f1 0.9970, precision 0.9976, recall 0.9965, auc 0.9970
epoch 2101, loss 0.0080, train acc 99.71%, f1 0.9971, precision 0.9977, recall 0.9964, auc 0.9971
epoch 2201, loss 0.0117, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 2301, loss 0.0105, train acc 99.74%, f1 0.9974, precision 0.9979, recall 0.9970, auc 0.9974
epoch 2401, loss 0.0049, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0113, train acc 99.77%, f1 0.9977, precision 0.9982, recall 0.9972, auc 0.9977
epoch 2601, loss 0.0040, train acc 99.77%, f1 0.9977, precision 0.9981, recall 0.9973, auc 0.9977
epoch 2701, loss 0.0055, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9977, auc 0.9979
epoch 2801, loss 0.0014, train acc 99.78%, f1 0.9978, precision 0.9982, recall 0.9975, auc 0.9978
epoch 2901, loss 0.0040, train acc 99.77%, f1 0.9978, precision 0.9985, recall 0.9970, auc 0.9978
epoch 3001, loss 0.0044, train acc 99.80%, f1 0.9980, precision 0.9982, recall 0.9977, auc 0.9980
epoch 3101, loss 0.0079, train acc 99.80%, f1 0.9980, precision 0.9984, recall 0.9976, auc 0.9980
epoch 3201, loss 0.0021, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3301, loss 0.0047, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 3401, loss 0.0022, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9978, auc 0.9981
epoch 3501, loss 0.0037, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3601, loss 0.0015, train acc 99.82%, f1 0.9983, precision 0.9981, recall 0.9984, auc 0.9982
epoch 3701, loss 0.0075, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9980, auc 0.9982
epoch 3801, loss 0.0069, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 3901, loss 0.0165, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9984, auc 0.9982
epoch 4001, loss 0.0039, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 4101, loss 0.0036, train acc 99.84%, f1 0.9984, precision 0.9987, recall 0.9980, auc 0.9984
epoch 4201, loss 0.0092, train acc 99.84%, f1 0.9984, precision 0.9987, recall 0.9981, auc 0.9984
epoch 4301, loss 0.0033, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9982, auc 0.9985
epoch 4401, loss 0.0047, train acc 99.84%, f1 0.9984, precision 0.9987, recall 0.9982, auc 0.9984
epoch 4501, loss 0.0013, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 4601, loss 0.0021, train acc 99.85%, f1 0.9985, precision 0.9988, recall 0.9982, auc 0.9985
epoch 4701, loss 0.0046, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9986, auc 0.9985
epoch 4801, loss 0.0105, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9983, auc 0.9985
epoch 4901, loss 0.0044, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9983, auc 0.9985
epoch 5001, loss 0.0019, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 5101, loss 0.0019, train acc 99.86%, f1 0.9986, precision 0.9989, recall 0.9983, auc 0.9986
epoch 5201, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5301, loss 0.0069, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9983, auc 0.9986
epoch 5401, loss 0.0059, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 5501, loss 0.0047, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 5601, loss 0.0039, train acc 99.87%, f1 0.9987, precision 0.9984, recall 0.9989, auc 0.9987
epoch 5701, loss 0.0028, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9985, auc 0.9987
epoch 5801, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9984, auc 0.9987
epoch 5901, loss 0.0016, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9988, auc 0.9986
epoch 6001, loss 0.0027, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9985, auc 0.9987
epoch 6101, loss 0.0030, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6201, loss 0.0013, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6301, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9992, recall 0.9985, auc 0.9988
epoch 6401, loss 0.0026, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9987, auc 0.9988
epoch 6501, loss 0.0042, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 6601, loss 0.0019, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 6701, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 6801, loss 0.0037, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 6901, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9988, auc 0.9990
epoch 7001, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7101, loss 0.0026, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9988, auc 0.9990
epoch 7201, loss 0.0029, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9987, auc 0.9989
epoch 7301, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 7401, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 7501, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9987, recall 0.9992, auc 0.9990
epoch 7601, loss 0.0035, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 7701, loss 0.0045, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9989, auc 0.9990
epoch 7801, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7901, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9995, recall 0.9986, auc 0.9990
epoch 8001, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9989, auc 0.9991
epoch 8101, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9989, auc 0.9991
epoch 8201, loss 0.0007, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 8301, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 8401, loss 0.0038, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 8501, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 8601, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 8701, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9990, auc 0.9992
epoch 8801, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9995, recall 0.9989, auc 0.9992
epoch 8901, loss 0.0028, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 9001, loss 0.0002, train acc 99.93%, f1 0.9993, precision 0.9996, recall 0.9990, auc 0.9993
epoch 9101, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9991, auc 0.9993
epoch 9201, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 9301, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9996, recall 0.9990, auc 0.9993
epoch 9401, loss 0.0027, train acc 99.93%, f1 0.9993, precision 0.9995, recall 0.9992, auc 0.9993
epoch 9501, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 9601, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9996, recall 0.9990, auc 0.9993
epoch 9701, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 9801, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9996, recall 0.9991, auc 0.9993
epoch 9901, loss 0.0030, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 10001, loss 0.0047, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9991, auc 0.9994
epoch 10101, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9993, auc 0.9994
epoch 10201, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 10301, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9993, auc 0.9995
epoch 10401, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 10501, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9993, auc 0.9995
epoch 10601, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9994, auc 0.9995
epoch 10701, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 10801, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 10901, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 11001, loss 0.0029, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9994, auc 0.9996
epoch 11101, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 11201, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 11301, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9995, auc 0.9996
epoch 11401, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 11501, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 11601, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 11701, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 11801, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 11901, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 12001, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 12101, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12201, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12301, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 12401, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12501, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 12601, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 12701, loss 0.0023, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 12801, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12901, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 13001, loss 0.0023, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 13101, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 13201, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 13301, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 13401, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 13501, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 13601, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 13701, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 13801, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13901, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 14001, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14101, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14201, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14301, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14401, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14501, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 14601, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14701, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14801, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 14901, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15001, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15101, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15201, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15301, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15401, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15501, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15601, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15701, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_1
./test_vehicle0/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9836538461538462

the Fscore is 0.975

the precision is 0.975

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_1
----------------------



epoch 1, loss 0.6934, train acc 64.43%, f1 0.4492, precision 0.9982, recall 0.2898, auc 0.6446
epoch 101, loss 0.3451, train acc 86.92%, f1 0.8692, precision 0.8700, recall 0.8684, auc 0.8692
epoch 201, loss 0.1459, train acc 96.24%, f1 0.9625, precision 0.9624, recall 0.9625, auc 0.9624
epoch 301, loss 0.0984, train acc 98.00%, f1 0.9800, precision 0.9799, recall 0.9801, auc 0.9800
epoch 401, loss 0.0718, train acc 98.55%, f1 0.9856, precision 0.9849, recall 0.9862, auc 0.9855
epoch 501, loss 0.0488, train acc 98.79%, f1 0.9879, precision 0.9876, recall 0.9883, auc 0.9879
epoch 601, loss 0.0307, train acc 98.99%, f1 0.9899, precision 0.9896, recall 0.9903, auc 0.9899
epoch 701, loss 0.0261, train acc 99.12%, f1 0.9912, precision 0.9905, recall 0.9920, auc 0.9912
epoch 801, loss 0.0304, train acc 99.24%, f1 0.9924, precision 0.9921, recall 0.9927, auc 0.9924
epoch 901, loss 0.0299, train acc 99.31%, f1 0.9931, precision 0.9930, recall 0.9932, auc 0.9931
epoch 1001, loss 0.0092, train acc 99.40%, f1 0.9940, precision 0.9938, recall 0.9943, auc 0.9940
epoch 1101, loss 0.0300, train acc 99.44%, f1 0.9944, precision 0.9945, recall 0.9944, auc 0.9944
epoch 1201, loss 0.0158, train acc 99.52%, f1 0.9952, precision 0.9951, recall 0.9953, auc 0.9952
epoch 1301, loss 0.0149, train acc 99.54%, f1 0.9954, precision 0.9955, recall 0.9954, auc 0.9954
epoch 1401, loss 0.0176, train acc 99.59%, f1 0.9959, precision 0.9957, recall 0.9960, auc 0.9959
epoch 1501, loss 0.0194, train acc 99.61%, f1 0.9961, precision 0.9959, recall 0.9962, auc 0.9961
epoch 1601, loss 0.0112, train acc 99.64%, f1 0.9964, precision 0.9962, recall 0.9967, auc 0.9964
epoch 1701, loss 0.0152, train acc 99.66%, f1 0.9966, precision 0.9963, recall 0.9968, auc 0.9966
epoch 1801, loss 0.0057, train acc 99.68%, f1 0.9968, precision 0.9966, recall 0.9970, auc 0.9968
epoch 1901, loss 0.0082, train acc 99.69%, f1 0.9969, precision 0.9967, recall 0.9971, auc 0.9969
epoch 2001, loss 0.0108, train acc 99.71%, f1 0.9971, precision 0.9969, recall 0.9972, auc 0.9971
epoch 2101, loss 0.0098, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 2201, loss 0.0079, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9974, auc 0.9974
epoch 2301, loss 0.0089, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 2401, loss 0.0050, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 2501, loss 0.0092, train acc 99.77%, f1 0.9978, precision 0.9975, recall 0.9980, auc 0.9977
epoch 2601, loss 0.0049, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9976, auc 0.9978
epoch 2701, loss 0.0053, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9976, auc 0.9978
epoch 2801, loss 0.0036, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 2901, loss 0.0090, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9981, auc 0.9978
epoch 3001, loss 0.0101, train acc 99.80%, f1 0.9980, precision 0.9977, recall 0.9983, auc 0.9980
epoch 3101, loss 0.0040, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 3201, loss 0.0075, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 3301, loss 0.0126, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 3401, loss 0.0053, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3501, loss 0.0096, train acc 99.82%, f1 0.9982, precision 0.9979, recall 0.9984, auc 0.9982
epoch 3601, loss 0.0046, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9984, auc 0.9981
epoch 3701, loss 0.0058, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9984, auc 0.9982
epoch 3801, loss 0.0027, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 3901, loss 0.0067, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4001, loss 0.0064, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9985, auc 0.9983
epoch 4101, loss 0.0065, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9986, auc 0.9985
epoch 4201, loss 0.0097, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4301, loss 0.0046, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 4401, loss 0.0018, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 4501, loss 0.0110, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4601, loss 0.0045, train acc 99.84%, f1 0.9984, precision 0.9979, recall 0.9990, auc 0.9984
epoch 4701, loss 0.0021, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 4801, loss 0.0065, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9989, auc 0.9986
epoch 4901, loss 0.0035, train acc 99.86%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9986
epoch 5001, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9989, auc 0.9986
epoch 5101, loss 0.0049, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 5201, loss 0.0070, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 5301, loss 0.0010, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 5401, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 5501, loss 0.0043, train acc 99.87%, f1 0.9987, precision 0.9983, recall 0.9991, auc 0.9987
epoch 5601, loss 0.0011, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 5701, loss 0.0084, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 5801, loss 0.0030, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 5901, loss 0.0032, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9986, auc 0.9987
epoch 6001, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9990, auc 0.9987
epoch 6101, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9984, recall 0.9990, auc 0.9987
epoch 6201, loss 0.0030, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 6301, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9987, auc 0.9989
epoch 6401, loss 0.0060, train acc 99.87%, f1 0.9987, precision 0.9984, recall 0.9991, auc 0.9987
epoch 6501, loss 0.0027, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 6601, loss 0.0023, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 6701, loss 0.0030, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6801, loss 0.0010, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 6901, loss 0.0100, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9991, auc 0.9989
epoch 7001, loss 0.0037, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9989, auc 0.9991
epoch 7101, loss 0.0044, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 7201, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 7301, loss 0.0023, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 7401, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7501, loss 0.0098, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 7601, loss 0.0040, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7701, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 7801, loss 0.0005, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7901, loss 0.0040, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 8001, loss 0.0012, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 8101, loss 0.0059, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0024, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9990, auc 0.9992
epoch 8301, loss 0.0030, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9992
epoch 8401, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 8501, loss 0.0039, train acc 99.92%, f1 0.9992, precision 0.9995, recall 0.9990, auc 0.9992
epoch 8601, loss 0.0003, train acc 99.92%, f1 0.9992, precision 0.9996, recall 0.9988, auc 0.9992
epoch 8701, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8801, loss 0.0042, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9991, auc 0.9993
epoch 8901, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9001, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 9101, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9996, recall 0.9991, auc 0.9993
epoch 9201, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 9301, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9401, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9993, auc 0.9995
epoch 9501, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 9601, loss 0.0012, train acc 99.95%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9995
epoch 9701, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 9801, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 9901, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 10001, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 10101, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 10201, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 10301, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10401, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 10501, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 10601, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 10701, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 10801, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 10901, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 11001, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 11101, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 11201, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11301, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 11401, loss 0.0021, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 11501, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 11601, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 11701, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11801, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11901, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12001, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 12101, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12201, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 12301, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12401, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12501, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12601, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12701, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12801, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12901, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13001, loss 0.0020, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 13101, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13201, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 13301, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13401, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13501, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13601, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13701, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 13801, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 13901, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14001, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14101, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 14201, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14301, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_1
./test_vehicle0/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.975

the Fscore is 0.9743589743589743

the precision is 1.0

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_1
----------------------



epoch 1, loss 0.6956, train acc 50.06%, f1 0.6672, precision 0.5006, recall 1.0000, auc 0.5000
epoch 101, loss 0.3650, train acc 86.27%, f1 0.8639, precision 0.8576, recall 0.8702, auc 0.8627
epoch 201, loss 0.1520, train acc 95.91%, f1 0.9592, precision 0.9576, recall 0.9609, auc 0.9591
epoch 301, loss 0.0732, train acc 97.94%, f1 0.9794, precision 0.9799, recall 0.9789, auc 0.9794
epoch 401, loss 0.0654, train acc 98.47%, f1 0.9847, precision 0.9852, recall 0.9842, auc 0.9847
epoch 501, loss 0.0432, train acc 98.71%, f1 0.9871, precision 0.9876, recall 0.9866, auc 0.9871
epoch 601, loss 0.0594, train acc 98.95%, f1 0.9895, precision 0.9897, recall 0.9892, auc 0.9895
epoch 701, loss 0.0227, train acc 99.09%, f1 0.9909, precision 0.9921, recall 0.9898, auc 0.9909
epoch 801, loss 0.0341, train acc 99.22%, f1 0.9922, precision 0.9932, recall 0.9912, auc 0.9922
epoch 901, loss 0.0297, train acc 99.32%, f1 0.9932, precision 0.9941, recall 0.9924, auc 0.9932
epoch 1001, loss 0.0213, train acc 99.39%, f1 0.9939, precision 0.9952, recall 0.9927, auc 0.9939
epoch 1101, loss 0.0204, train acc 99.45%, f1 0.9945, precision 0.9953, recall 0.9936, auc 0.9945
epoch 1201, loss 0.0249, train acc 99.50%, f1 0.9950, precision 0.9955, recall 0.9944, auc 0.9950
epoch 1301, loss 0.0148, train acc 99.53%, f1 0.9953, precision 0.9960, recall 0.9946, auc 0.9953
epoch 1401, loss 0.0323, train acc 99.56%, f1 0.9956, precision 0.9963, recall 0.9949, auc 0.9956
epoch 1501, loss 0.0112, train acc 99.60%, f1 0.9960, precision 0.9970, recall 0.9950, auc 0.9960
epoch 1601, loss 0.0134, train acc 99.62%, f1 0.9962, precision 0.9966, recall 0.9957, auc 0.9962
epoch 1701, loss 0.0171, train acc 99.65%, f1 0.9965, precision 0.9971, recall 0.9959, auc 0.9965
epoch 1801, loss 0.0052, train acc 99.67%, f1 0.9967, precision 0.9972, recall 0.9963, auc 0.9967
epoch 1901, loss 0.0111, train acc 99.69%, f1 0.9969, precision 0.9974, recall 0.9964, auc 0.9969
epoch 2001, loss 0.0139, train acc 99.70%, f1 0.9970, precision 0.9977, recall 0.9964, auc 0.9970
epoch 2101, loss 0.0054, train acc 99.72%, f1 0.9972, precision 0.9977, recall 0.9968, auc 0.9972
epoch 2201, loss 0.0198, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9973, auc 0.9975
epoch 2301, loss 0.0074, train acc 99.75%, f1 0.9975, precision 0.9977, recall 0.9974, auc 0.9975
epoch 2401, loss 0.0083, train acc 99.76%, f1 0.9976, precision 0.9978, recall 0.9974, auc 0.9976
epoch 2501, loss 0.0112, train acc 99.77%, f1 0.9977, precision 0.9980, recall 0.9974, auc 0.9977
epoch 2601, loss 0.0069, train acc 99.78%, f1 0.9978, precision 0.9982, recall 0.9973, auc 0.9978
epoch 2701, loss 0.0098, train acc 99.78%, f1 0.9978, precision 0.9982, recall 0.9975, auc 0.9978
epoch 2801, loss 0.0086, train acc 99.78%, f1 0.9978, precision 0.9985, recall 0.9971, auc 0.9978
epoch 2901, loss 0.0015, train acc 99.79%, f1 0.9979, precision 0.9986, recall 0.9971, auc 0.9979
epoch 3001, loss 0.0053, train acc 99.80%, f1 0.9980, precision 0.9984, recall 0.9976, auc 0.9980
epoch 3101, loss 0.0086, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 3201, loss 0.0076, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 3301, loss 0.0022, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9980, auc 0.9982
epoch 3401, loss 0.0084, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 3501, loss 0.0059, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 3601, loss 0.0050, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 3701, loss 0.0052, train acc 99.82%, f1 0.9982, precision 0.9986, recall 0.9979, auc 0.9982
epoch 3801, loss 0.0092, train acc 99.82%, f1 0.9982, precision 0.9988, recall 0.9976, auc 0.9982
epoch 3901, loss 0.0044, train acc 99.82%, f1 0.9982, precision 0.9987, recall 0.9977, auc 0.9982
epoch 4001, loss 0.0011, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9983, auc 0.9985
epoch 4101, loss 0.0056, train acc 99.84%, f1 0.9984, precision 0.9989, recall 0.9979, auc 0.9984
epoch 4201, loss 0.0022, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 4301, loss 0.0080, train acc 99.84%, f1 0.9984, precision 0.9988, recall 0.9980, auc 0.9984
epoch 4401, loss 0.0140, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4501, loss 0.0098, train acc 99.85%, f1 0.9985, precision 0.9988, recall 0.9983, auc 0.9985
epoch 4601, loss 0.0075, train acc 99.85%, f1 0.9985, precision 0.9989, recall 0.9980, auc 0.9985
epoch 4701, loss 0.0108, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 4801, loss 0.0039, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9987, auc 0.9985
epoch 4901, loss 0.0047, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 5001, loss 0.0083, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9985, auc 0.9987
epoch 5101, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 5201, loss 0.0019, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9984, auc 0.9987
epoch 5301, loss 0.0030, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 5401, loss 0.0034, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 5501, loss 0.0052, train acc 99.87%, f1 0.9987, precision 0.9991, recall 0.9983, auc 0.9987
epoch 5601, loss 0.0048, train acc 99.87%, f1 0.9987, precision 0.9991, recall 0.9983, auc 0.9987
epoch 5701, loss 0.0026, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 5801, loss 0.0017, train acc 99.87%, f1 0.9987, precision 0.9991, recall 0.9983, auc 0.9987
epoch 5901, loss 0.0038, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 6001, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9986, auc 0.9988
epoch 6101, loss 0.0024, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 6201, loss 0.0075, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 6301, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 6401, loss 0.0024, train acc 99.89%, f1 0.9989, precision 0.9992, recall 0.9986, auc 0.9989
epoch 6501, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 6601, loss 0.0048, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 6701, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 6801, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9994, recall 0.9986, auc 0.9990
epoch 6901, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9992, auc 0.9990
epoch 7001, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7101, loss 0.0031, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 7201, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9994, recall 0.9989, auc 0.9991
epoch 7301, loss 0.0032, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9990, auc 0.9992
epoch 7401, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7501, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 7601, loss 0.0035, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 7701, loss 0.0004, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 7801, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 7901, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 8001, loss 0.0021, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 8101, loss 0.0032, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 8301, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9992, auc 0.9994
epoch 8401, loss 0.0030, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 8501, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8601, loss 0.0029, train acc 99.95%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9995
epoch 8701, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9997, auc 0.9995
epoch 8801, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8901, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 9001, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9997, auc 0.9996
epoch 9101, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 9201, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 9301, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 9401, loss 0.0035, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 9501, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 9601, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 9701, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 9801, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 9901, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_1
./test_vehicle0/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9461538461538462

the Fscore is 0.935064935064935

the precision is 0.972972972972973

the recall is 0.9

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_1
----------------------



epoch 1, loss 0.6938, train acc 49.99%, f1 0.6666, precision 0.4999, recall 1.0000, auc 0.5000
epoch 101, loss 0.3749, train acc 87.26%, f1 0.8723, precision 0.8740, recall 0.8706, auc 0.8726
epoch 201, loss 0.1525, train acc 96.31%, f1 0.9631, precision 0.9642, recall 0.9620, auc 0.9631
epoch 301, loss 0.1099, train acc 98.08%, f1 0.9808, precision 0.9811, recall 0.9805, auc 0.9808
epoch 401, loss 0.0836, train acc 98.53%, f1 0.9853, precision 0.9859, recall 0.9846, auc 0.9853
epoch 501, loss 0.0415, train acc 98.81%, f1 0.9881, precision 0.9883, recall 0.9879, auc 0.9881
epoch 601, loss 0.0300, train acc 99.00%, f1 0.9900, precision 0.9907, recall 0.9892, auc 0.9900
epoch 701, loss 0.0212, train acc 99.13%, f1 0.9913, precision 0.9911, recall 0.9914, auc 0.9913
epoch 801, loss 0.0231, train acc 99.25%, f1 0.9925, precision 0.9919, recall 0.9932, auc 0.9925
epoch 901, loss 0.0150, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9934, auc 0.9934
epoch 1001, loss 0.0214, train acc 99.40%, f1 0.9940, precision 0.9940, recall 0.9940, auc 0.9940
epoch 1101, loss 0.0208, train acc 99.46%, f1 0.9946, precision 0.9944, recall 0.9948, auc 0.9946
epoch 1201, loss 0.0181, train acc 99.52%, f1 0.9952, precision 0.9953, recall 0.9952, auc 0.9952
epoch 1301, loss 0.0181, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9957, auc 0.9956
epoch 1401, loss 0.0176, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1501, loss 0.0100, train acc 99.61%, f1 0.9961, precision 0.9964, recall 0.9959, auc 0.9961
epoch 1601, loss 0.0124, train acc 99.63%, f1 0.9963, precision 0.9969, recall 0.9958, auc 0.9963
epoch 1701, loss 0.0117, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9967, auc 0.9966
epoch 1801, loss 0.0195, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1901, loss 0.0129, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9969, auc 0.9970
epoch 2001, loss 0.0107, train acc 99.70%, f1 0.9970, precision 0.9972, recall 0.9969, auc 0.9970
epoch 2101, loss 0.0082, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9972, auc 0.9973
epoch 2201, loss 0.0090, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 2301, loss 0.0188, train acc 99.75%, f1 0.9975, precision 0.9973, recall 0.9977, auc 0.9975
epoch 2401, loss 0.0137, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9976, auc 0.9977
epoch 2501, loss 0.0057, train acc 99.76%, f1 0.9976, precision 0.9974, recall 0.9978, auc 0.9976
epoch 2601, loss 0.0041, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 2701, loss 0.0081, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9977, auc 0.9978
epoch 2801, loss 0.0033, train acc 99.78%, f1 0.9978, precision 0.9981, recall 0.9976, auc 0.9978
epoch 2901, loss 0.0020, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9982, auc 0.9979
epoch 3001, loss 0.0049, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9982, auc 0.9980
epoch 3101, loss 0.0068, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9978, auc 0.9980
epoch 3201, loss 0.0041, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 3301, loss 0.0061, train acc 99.81%, f1 0.9981, precision 0.9978, recall 0.9983, auc 0.9981
epoch 3401, loss 0.0050, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 3501, loss 0.0076, train acc 99.82%, f1 0.9982, precision 0.9979, recall 0.9986, auc 0.9982
epoch 3601, loss 0.0050, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 3701, loss 0.0057, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 3801, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 3901, loss 0.0047, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9984, auc 0.9983
epoch 4001, loss 0.0038, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 4101, loss 0.0066, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4201, loss 0.0055, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 4301, loss 0.0051, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 4401, loss 0.0011, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9988, auc 0.9985
epoch 4501, loss 0.0094, train acc 99.85%, f1 0.9985, precision 0.9982, recall 0.9987, auc 0.9985
epoch 4601, loss 0.0018, train acc 99.85%, f1 0.9985, precision 0.9982, recall 0.9989, auc 0.9985
epoch 4701, loss 0.0053, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9986, auc 0.9985
epoch 4801, loss 0.0059, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 4901, loss 0.0051, train acc 99.86%, f1 0.9986, precision 0.9981, recall 0.9990, auc 0.9986
epoch 5001, loss 0.0023, train acc 99.86%, f1 0.9986, precision 0.9982, recall 0.9989, auc 0.9986
epoch 5101, loss 0.0017, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9989, auc 0.9986
epoch 5201, loss 0.0018, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 5301, loss 0.0018, train acc 99.86%, f1 0.9986, precision 0.9982, recall 0.9991, auc 0.9986
epoch 5401, loss 0.0031, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9989, auc 0.9986
epoch 5501, loss 0.0039, train acc 99.87%, f1 0.9987, precision 0.9983, recall 0.9990, auc 0.9987
epoch 5601, loss 0.0095, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 5701, loss 0.0022, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9989, auc 0.9986
epoch 5801, loss 0.0010, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 5901, loss 0.0020, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 6001, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9991, auc 0.9988
epoch 6101, loss 0.0011, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 6201, loss 0.0039, train acc 99.88%, f1 0.9988, precision 0.9985, recall 0.9991, auc 0.9988
epoch 6301, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9991, auc 0.9988
epoch 6401, loss 0.0023, train acc 99.89%, f1 0.9989, precision 0.9986, recall 0.9991, auc 0.9989
epoch 6501, loss 0.0026, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6601, loss 0.0013, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 6701, loss 0.0020, train acc 99.88%, f1 0.9988, precision 0.9991, recall 0.9985, auc 0.9988
epoch 6801, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 6901, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7001, loss 0.0020, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 7101, loss 0.0015, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 7201, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7301, loss 0.0057, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 7401, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 7501, loss 0.0020, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9992, auc 0.9990
epoch 7601, loss 0.0015, train acc 99.90%, f1 0.9990, precision 0.9987, recall 0.9994, auc 0.9990
epoch 7701, loss 0.0062, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 7801, loss 0.0043, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9992
epoch 7901, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9989, auc 0.9991
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_1
./test_vehicle0/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9798076923076924

the Fscore is 0.9629629629629629

the precision is 0.9512195121951219

the recall is 0.975

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_1
----------------------



epoch 1, loss 0.6932, train acc 50.08%, f1 0.6674, precision 0.5008, recall 1.0000, auc 0.5000
epoch 101, loss 0.3186, train acc 86.61%, f1 0.8657, precision 0.8695, recall 0.8620, auc 0.8661
epoch 201, loss 0.1596, train acc 95.82%, f1 0.9582, precision 0.9592, recall 0.9573, auc 0.9582
epoch 301, loss 0.0921, train acc 98.15%, f1 0.9815, precision 0.9815, recall 0.9815, auc 0.9815
epoch 401, loss 0.0761, train acc 98.55%, f1 0.9856, precision 0.9855, recall 0.9856, auc 0.9855
epoch 501, loss 0.0432, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9881, auc 0.9880
epoch 601, loss 0.0231, train acc 98.99%, f1 0.9899, precision 0.9894, recall 0.9904, auc 0.9899
epoch 701, loss 0.0339, train acc 99.13%, f1 0.9913, precision 0.9912, recall 0.9914, auc 0.9913
epoch 801, loss 0.0247, train acc 99.25%, f1 0.9925, precision 0.9925, recall 0.9925, auc 0.9925
epoch 901, loss 0.0136, train acc 99.33%, f1 0.9933, precision 0.9934, recall 0.9932, auc 0.9933
epoch 1001, loss 0.0162, train acc 99.40%, f1 0.9940, precision 0.9938, recall 0.9942, auc 0.9940
epoch 1101, loss 0.0239, train acc 99.45%, f1 0.9945, precision 0.9948, recall 0.9942, auc 0.9945
epoch 1201, loss 0.0234, train acc 99.50%, f1 0.9951, precision 0.9948, recall 0.9953, auc 0.9950
epoch 1301, loss 0.0194, train acc 99.55%, f1 0.9956, precision 0.9957, recall 0.9954, auc 0.9955
epoch 1401, loss 0.0157, train acc 99.57%, f1 0.9957, precision 0.9956, recall 0.9959, auc 0.9957
epoch 1501, loss 0.0099, train acc 99.59%, f1 0.9959, precision 0.9958, recall 0.9961, auc 0.9959
epoch 1601, loss 0.0161, train acc 99.63%, f1 0.9963, precision 0.9965, recall 0.9962, auc 0.9963
epoch 1701, loss 0.0136, train acc 99.66%, f1 0.9966, precision 0.9968, recall 0.9963, auc 0.9966
epoch 1801, loss 0.0086, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9966, auc 0.9967
epoch 1901, loss 0.0126, train acc 99.68%, f1 0.9968, precision 0.9966, recall 0.9971, auc 0.9968
epoch 2001, loss 0.0127, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9972, auc 0.9970
epoch 2101, loss 0.0135, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2201, loss 0.0046, train acc 99.74%, f1 0.9974, precision 0.9977, recall 0.9971, auc 0.9974
epoch 2301, loss 0.0183, train acc 99.75%, f1 0.9975, precision 0.9973, recall 0.9977, auc 0.9975
epoch 2401, loss 0.0080, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 2501, loss 0.0044, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2601, loss 0.0108, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2701, loss 0.0036, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 2801, loss 0.0075, train acc 99.78%, f1 0.9979, precision 0.9977, recall 0.9980, auc 0.9978
epoch 2901, loss 0.0044, train acc 99.78%, f1 0.9979, precision 0.9977, recall 0.9980, auc 0.9978
epoch 3001, loss 0.0062, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9979, auc 0.9981
epoch 3101, loss 0.0091, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 3201, loss 0.0023, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3301, loss 0.0068, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 3401, loss 0.0045, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 3501, loss 0.0041, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 3601, loss 0.0077, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 3701, loss 0.0074, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 3801, loss 0.0040, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9984, auc 0.9983
epoch 3901, loss 0.0070, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 4001, loss 0.0106, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4101, loss 0.0074, train acc 99.83%, f1 0.9983, precision 0.9987, recall 0.9980, auc 0.9983
epoch 4201, loss 0.0054, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9986, auc 0.9983
epoch 4301, loss 0.0078, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 4401, loss 0.0056, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9986, auc 0.9985
epoch 4501, loss 0.0017, train acc 99.84%, f1 0.9984, precision 0.9981, recall 0.9987, auc 0.9984
epoch 4601, loss 0.0032, train acc 99.85%, f1 0.9985, precision 0.9982, recall 0.9987, auc 0.9985
epoch 4701, loss 0.0039, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 4801, loss 0.0011, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 4901, loss 0.0034, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9988, auc 0.9986
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_1
./test_vehicle0/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9548076923076924

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_1
----------------------



epoch 1, loss 0.6934, train acc 50.03%, f1 0.6669, precision 0.5003, recall 1.0000, auc 0.5000
epoch 101, loss 0.3106, train acc 87.11%, f1 0.8710, precision 0.8726, recall 0.8694, auc 0.8711
epoch 201, loss 0.1317, train acc 96.09%, f1 0.9609, precision 0.9613, recall 0.9606, auc 0.9609
epoch 301, loss 0.0961, train acc 98.10%, f1 0.9810, precision 0.9813, recall 0.9807, auc 0.9810
epoch 401, loss 0.0769, train acc 98.49%, f1 0.9849, precision 0.9846, recall 0.9853, auc 0.9849
epoch 501, loss 0.0731, train acc 98.82%, f1 0.9883, precision 0.9881, recall 0.9884, auc 0.9882
epoch 601, loss 0.0274, train acc 98.99%, f1 0.9899, precision 0.9900, recall 0.9897, auc 0.9899
epoch 701, loss 0.0383, train acc 99.13%, f1 0.9913, precision 0.9914, recall 0.9913, auc 0.9913
epoch 801, loss 0.0253, train acc 99.24%, f1 0.9924, precision 0.9923, recall 0.9924, auc 0.9924
epoch 901, loss 0.0458, train acc 99.32%, f1 0.9932, precision 0.9926, recall 0.9939, auc 0.9932
epoch 1001, loss 0.0182, train acc 99.39%, f1 0.9939, precision 0.9936, recall 0.9942, auc 0.9939
epoch 1101, loss 0.0211, train acc 99.44%, f1 0.9944, precision 0.9943, recall 0.9946, auc 0.9944
epoch 1201, loss 0.0184, train acc 99.51%, f1 0.9951, precision 0.9948, recall 0.9953, auc 0.9951
epoch 1301, loss 0.0106, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1401, loss 0.0161, train acc 99.58%, f1 0.9958, precision 0.9956, recall 0.9960, auc 0.9958
epoch 1501, loss 0.0189, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1601, loss 0.0159, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9964, auc 0.9963
epoch 1701, loss 0.0101, train acc 99.65%, f1 0.9965, precision 0.9960, recall 0.9970, auc 0.9965
epoch 1801, loss 0.0140, train acc 99.68%, f1 0.9968, precision 0.9967, recall 0.9970, auc 0.9968
epoch 1901, loss 0.0093, train acc 99.68%, f1 0.9968, precision 0.9965, recall 0.9972, auc 0.9968
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_1
./test_vehicle0/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9596153846153845

the Fscore is 0.9268292682926829

the precision is 0.9047619047619048

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4343, train acc 83.99%, f1 0.8398, precision 0.8407, recall 0.8389, auc 0.8399
epoch 201, loss 0.2084, train acc 93.94%, f1 0.9394, precision 0.9396, recall 0.9393, auc 0.9394
epoch 301, loss 0.1334, train acc 97.21%, f1 0.9721, precision 0.9720, recall 0.9721, auc 0.9721
epoch 401, loss 0.1130, train acc 98.16%, f1 0.9816, precision 0.9815, recall 0.9816, auc 0.9816
epoch 501, loss 0.0684, train acc 98.45%, f1 0.9845, precision 0.9845, recall 0.9846, auc 0.9845
epoch 601, loss 0.0485, train acc 98.71%, f1 0.9871, precision 0.9871, recall 0.9871, auc 0.9871
epoch 701, loss 0.0445, train acc 98.88%, f1 0.9888, precision 0.9888, recall 0.9888, auc 0.9888
epoch 801, loss 0.0297, train acc 99.02%, f1 0.9903, precision 0.9902, recall 0.9903, auc 0.9902
epoch 901, loss 0.0353, train acc 99.11%, f1 0.9911, precision 0.9910, recall 0.9911, auc 0.9911
epoch 1001, loss 0.0271, train acc 99.18%, f1 0.9918, precision 0.9918, recall 0.9919, auc 0.9918
epoch 1101, loss 0.0209, train acc 99.25%, f1 0.9925, precision 0.9925, recall 0.9925, auc 0.9925
epoch 1201, loss 0.0309, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9933, auc 0.9933
epoch 1301, loss 0.0224, train acc 99.40%, f1 0.9940, precision 0.9940, recall 0.9940, auc 0.9940
epoch 1401, loss 0.0204, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9943, auc 0.9943
epoch 1501, loss 0.0144, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9946, auc 0.9946
epoch 1601, loss 0.0136, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9950, auc 0.9949
epoch 1701, loss 0.0143, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1801, loss 0.0095, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1901, loss 0.0102, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 2001, loss 0.0117, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2101, loss 0.0109, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 2201, loss 0.0103, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9966, auc 0.9966
epoch 2301, loss 0.0132, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2401, loss 0.0076, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2501, loss 0.0163, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2601, loss 0.0131, train acc 99.70%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9970
epoch 2701, loss 0.0073, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2801, loss 0.0099, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2901, loss 0.0119, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3001, loss 0.0157, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3101, loss 0.0132, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3201, loss 0.0044, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 3301, loss 0.0041, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3401, loss 0.0123, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 3501, loss 0.0074, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3601, loss 0.0033, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3701, loss 0.0050, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 3801, loss 0.0067, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3901, loss 0.0083, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4001, loss 0.0035, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4101, loss 0.0092, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4201, loss 0.0101, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4301, loss 0.0063, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4401, loss 0.0046, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4501, loss 0.0124, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4601, loss 0.0088, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4701, loss 0.0066, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4801, loss 0.0032, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4901, loss 0.0120, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 5001, loss 0.0052, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5101, loss 0.0044, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5201, loss 0.0056, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 5301, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5401, loss 0.0111, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5501, loss 0.0041, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5601, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5701, loss 0.0029, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5801, loss 0.0080, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5901, loss 0.0041, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6001, loss 0.0044, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6101, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6201, loss 0.0048, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6301, loss 0.0017, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6401, loss 0.0048, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6501, loss 0.0078, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6601, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6701, loss 0.0045, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 6801, loss 0.0015, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 6901, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7001, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7101, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7201, loss 0.0007, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7301, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7401, loss 0.0015, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7501, loss 0.0021, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7601, loss 0.0034, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7701, loss 0.0016, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7801, loss 0.0035, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 7901, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8001, loss 0.0057, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8101, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8201, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8301, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 8401, loss 0.0007, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8501, loss 0.0040, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8601, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8701, loss 0.0025, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8801, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8901, loss 0.0056, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9001, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9101, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9201, loss 0.0033, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9301, loss 0.0041, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9401, loss 0.0012, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9501, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9601, loss 0.0041, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9701, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9801, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9901, loss 0.0054, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10001, loss 0.0007, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10101, loss 0.0040, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10201, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10301, loss 0.0057, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10401, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10501, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10601, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10701, loss 0.0036, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10801, loss 0.0004, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 10901, loss 0.0011, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 11001, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11101, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11201, loss 0.0032, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11301, loss 0.0029, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11401, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11501, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11601, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11701, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11801, loss 0.0002, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11901, loss 0.0018, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12001, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12101, loss 0.0036, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12201, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12301, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12401, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12501, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12601, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12701, loss 0.0037, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12801, loss 0.0002, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12901, loss 0.0019, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13001, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13101, loss 0.0049, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13201, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13301, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13401, loss 0.0002, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13501, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13601, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13701, loss 0.0023, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13801, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13901, loss 0.0002, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14001, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14101, loss 0.0001, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 14201, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14301, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14401, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14501, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14601, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14701, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14801, loss 0.0003, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14901, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15001, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15101, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15201, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15301, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15401, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15501, loss 0.0024, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15601, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15701, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15801, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15901, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16001, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16101, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16201, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16301, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16401, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16501, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16701, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16801, loss 0.0033, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16901, loss 0.0025, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17001, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17101, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17201, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17301, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17401, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17501, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17601, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 17701, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17801, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17901, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18001, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18101, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18201, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18301, loss 0.0000, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18401, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18501, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18601, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18701, loss 0.0024, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18801, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18901, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19001, loss 0.0020, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19101, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19201, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19301, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19401, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19501, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19601, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19701, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19801, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19901, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_1
./test_vehicle0/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9586538461538462

the Fscore is 0.9487179487179489

the precision is 0.9736842105263158

the recall is 0.925

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_1
----------------------



epoch 1, loss 0.6931, train acc 66.63%, f1 0.5180, precision 0.9323, recall 0.3586, auc 0.6663
epoch 101, loss 0.4086, train acc 83.95%, f1 0.8396, precision 0.8392, recall 0.8401, auc 0.8395
epoch 201, loss 0.2613, train acc 93.83%, f1 0.9383, precision 0.9382, recall 0.9383, auc 0.9383
epoch 301, loss 0.1329, train acc 97.02%, f1 0.9702, precision 0.9702, recall 0.9702, auc 0.9702
epoch 401, loss 0.0959, train acc 98.14%, f1 0.9814, precision 0.9814, recall 0.9814, auc 0.9814
epoch 501, loss 0.0604, train acc 98.40%, f1 0.9840, precision 0.9841, recall 0.9840, auc 0.9840
epoch 601, loss 0.0585, train acc 98.67%, f1 0.9867, precision 0.9867, recall 0.9867, auc 0.9867
epoch 701, loss 0.0370, train acc 98.83%, f1 0.9883, precision 0.9883, recall 0.9883, auc 0.9883
epoch 801, loss 0.0457, train acc 99.00%, f1 0.9900, precision 0.9900, recall 0.9900, auc 0.9900
epoch 901, loss 0.0302, train acc 99.10%, f1 0.9910, precision 0.9910, recall 0.9910, auc 0.9910
epoch 1001, loss 0.0266, train acc 99.18%, f1 0.9918, precision 0.9918, recall 0.9918, auc 0.9918
epoch 1101, loss 0.0221, train acc 99.29%, f1 0.9929, precision 0.9929, recall 0.9929, auc 0.9929
epoch 1201, loss 0.0211, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9934, auc 0.9934
epoch 1301, loss 0.0196, train acc 99.38%, f1 0.9938, precision 0.9938, recall 0.9937, auc 0.9938
epoch 1401, loss 0.0166, train acc 99.44%, f1 0.9944, precision 0.9944, recall 0.9944, auc 0.9944
epoch 1501, loss 0.0268, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9947, auc 0.9947
epoch 1601, loss 0.0160, train acc 99.50%, f1 0.9950, precision 0.9951, recall 0.9950, auc 0.9950
epoch 1701, loss 0.0141, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1801, loss 0.0203, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1901, loss 0.0195, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 2001, loss 0.0148, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 2101, loss 0.0161, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2201, loss 0.0153, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2301, loss 0.0082, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 2401, loss 0.0077, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2501, loss 0.0053, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2601, loss 0.0073, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2701, loss 0.0154, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2801, loss 0.0061, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2901, loss 0.0106, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 3001, loss 0.0072, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3101, loss 0.0039, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3201, loss 0.0060, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3301, loss 0.0134, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3401, loss 0.0110, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3501, loss 0.0107, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3601, loss 0.0112, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3701, loss 0.0076, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 3801, loss 0.0044, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3901, loss 0.0072, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4001, loss 0.0081, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4101, loss 0.0058, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4201, loss 0.0066, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4301, loss 0.0139, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4401, loss 0.0053, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4501, loss 0.0065, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4601, loss 0.0026, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4701, loss 0.0019, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4801, loss 0.0054, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4901, loss 0.0023, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5001, loss 0.0070, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5101, loss 0.0078, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 5201, loss 0.0009, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5301, loss 0.0044, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5401, loss 0.0021, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5501, loss 0.0041, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5601, loss 0.0050, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5701, loss 0.0054, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5801, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5901, loss 0.0072, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6001, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6101, loss 0.0037, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6201, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6301, loss 0.0048, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6401, loss 0.0049, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6501, loss 0.0006, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6601, loss 0.0019, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6701, loss 0.0052, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6801, loss 0.0079, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6901, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7001, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7101, loss 0.0055, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7201, loss 0.0063, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7301, loss 0.0042, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7401, loss 0.0018, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7501, loss 0.0081, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7601, loss 0.0046, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7701, loss 0.0007, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7801, loss 0.0043, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7901, loss 0.0010, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8001, loss 0.0050, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8101, loss 0.0043, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8201, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0038, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8401, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8501, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8601, loss 0.0032, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8701, loss 0.0005, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8801, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8901, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9001, loss 0.0006, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9101, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9201, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9301, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9401, loss 0.0050, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9501, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9601, loss 0.0011, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9701, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9801, loss 0.0006, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9901, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10001, loss 0.0049, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10101, loss 0.0052, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10201, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10301, loss 0.0009, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10401, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10501, loss 0.0040, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10601, loss 0.0024, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 10701, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10801, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 10901, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11001, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11101, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11201, loss 0.0023, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11301, loss 0.0023, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11401, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 11501, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11601, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11701, loss 0.0024, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11801, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11901, loss 0.0047, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12001, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12101, loss 0.0034, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12201, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 12301, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12401, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12501, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12601, loss 0.0003, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12701, loss 0.0035, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12801, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12901, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13001, loss 0.0034, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13101, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13201, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13301, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13401, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13501, loss 0.0024, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13601, loss 0.0003, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13701, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13801, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13901, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14001, loss 0.0002, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14101, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14201, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 14301, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14401, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 14501, loss 0.0001, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14601, loss 0.0035, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14701, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14801, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14901, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_1
./test_vehicle0/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9548076923076924

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_1
----------------------



epoch 1, loss 0.6929, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.4133, train acc 84.38%, f1 0.8436, precision 0.8447, recall 0.8426, auc 0.8438
epoch 201, loss 0.2307, train acc 93.89%, f1 0.9389, precision 0.9390, recall 0.9389, auc 0.9389
epoch 301, loss 0.1428, train acc 97.13%, f1 0.9713, precision 0.9713, recall 0.9714, auc 0.9713
epoch 401, loss 0.0687, train acc 98.06%, f1 0.9806, precision 0.9805, recall 0.9806, auc 0.9806
epoch 501, loss 0.0648, train acc 98.49%, f1 0.9849, precision 0.9848, recall 0.9850, auc 0.9849
epoch 601, loss 0.0687, train acc 98.71%, f1 0.9871, precision 0.9870, recall 0.9871, auc 0.9871
epoch 701, loss 0.0486, train acc 98.89%, f1 0.9889, precision 0.9888, recall 0.9889, auc 0.9889
epoch 801, loss 0.0476, train acc 99.01%, f1 0.9901, precision 0.9901, recall 0.9902, auc 0.9901
epoch 901, loss 0.0342, train acc 99.12%, f1 0.9912, precision 0.9911, recall 0.9912, auc 0.9912
epoch 1001, loss 0.0270, train acc 99.22%, f1 0.9922, precision 0.9921, recall 0.9922, auc 0.9922
epoch 1101, loss 0.0144, train acc 99.28%, f1 0.9928, precision 0.9927, recall 0.9928, auc 0.9928
epoch 1201, loss 0.0268, train acc 99.35%, f1 0.9935, precision 0.9934, recall 0.9935, auc 0.9935
epoch 1301, loss 0.0235, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1401, loss 0.0137, train acc 99.45%, f1 0.9945, precision 0.9944, recall 0.9945, auc 0.9945
epoch 1501, loss 0.0104, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 1601, loss 0.0202, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9951, auc 0.9951
epoch 1701, loss 0.0203, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1801, loss 0.0174, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9958, auc 0.9958
epoch 1901, loss 0.0195, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 2001, loss 0.0164, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 2101, loss 0.0089, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9964, auc 0.9963
epoch 2201, loss 0.0247, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9966, auc 0.9965
epoch 2301, loss 0.0180, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2401, loss 0.0144, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2501, loss 0.0089, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2601, loss 0.0069, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2701, loss 0.0101, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 2801, loss 0.0114, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2901, loss 0.0078, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 3001, loss 0.0055, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3101, loss 0.0070, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 3201, loss 0.0083, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3301, loss 0.0152, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 3401, loss 0.0139, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 3501, loss 0.0102, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3601, loss 0.0048, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 3701, loss 0.0086, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3801, loss 0.0043, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3901, loss 0.0105, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4001, loss 0.0081, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4101, loss 0.0093, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4201, loss 0.0051, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4301, loss 0.0056, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4401, loss 0.0049, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4501, loss 0.0029, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4601, loss 0.0062, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4701, loss 0.0088, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4801, loss 0.0054, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4901, loss 0.0031, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5001, loss 0.0054, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5101, loss 0.0026, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5201, loss 0.0043, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5301, loss 0.0054, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5401, loss 0.0042, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5501, loss 0.0026, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5601, loss 0.0027, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5701, loss 0.0046, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5801, loss 0.0032, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5901, loss 0.0015, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6001, loss 0.0035, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6101, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6201, loss 0.0071, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6301, loss 0.0008, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6401, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6501, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6601, loss 0.0038, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6701, loss 0.0037, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6801, loss 0.0037, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6901, loss 0.0029, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 7001, loss 0.0021, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7101, loss 0.0010, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7201, loss 0.0019, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 7301, loss 0.0034, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7401, loss 0.0040, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7501, loss 0.0087, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7601, loss 0.0030, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7701, loss 0.0053, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7801, loss 0.0022, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7901, loss 0.0016, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 8001, loss 0.0042, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8101, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8201, loss 0.0011, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0024, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8401, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8501, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8601, loss 0.0026, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 8701, loss 0.0052, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8801, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8901, loss 0.0030, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9001, loss 0.0004, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9101, loss 0.0035, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9201, loss 0.0039, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9301, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 9401, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9501, loss 0.0028, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9601, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 9701, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9801, loss 0.0042, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 9901, loss 0.0053, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_1
./test_vehicle0/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9634615384615384

the Fscore is 0.9382716049382716

the precision is 0.926829268292683

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_1
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4316, train acc 84.23%, f1 0.8394, precision 0.8553, recall 0.8240, auc 0.8423
epoch 201, loss 0.2355, train acc 93.85%, f1 0.9384, precision 0.9399, recall 0.9369, auc 0.9385
epoch 301, loss 0.1311, train acc 97.10%, f1 0.9710, precision 0.9711, recall 0.9709, auc 0.9710
epoch 401, loss 0.0925, train acc 98.11%, f1 0.9811, precision 0.9808, recall 0.9814, auc 0.9811
epoch 501, loss 0.0707, train acc 98.40%, f1 0.9841, precision 0.9836, recall 0.9845, auc 0.9840
epoch 601, loss 0.0571, train acc 98.71%, f1 0.9871, precision 0.9867, recall 0.9874, auc 0.9871
epoch 701, loss 0.0384, train acc 98.88%, f1 0.9888, precision 0.9884, recall 0.9891, auc 0.9888
epoch 801, loss 0.0414, train acc 99.01%, f1 0.9901, precision 0.9898, recall 0.9905, auc 0.9901
epoch 901, loss 0.0318, train acc 99.13%, f1 0.9913, precision 0.9911, recall 0.9916, auc 0.9913
epoch 1001, loss 0.0282, train acc 99.21%, f1 0.9921, precision 0.9919, recall 0.9923, auc 0.9921
epoch 1101, loss 0.0394, train acc 99.28%, f1 0.9928, precision 0.9925, recall 0.9930, auc 0.9928
epoch 1201, loss 0.0354, train acc 99.34%, f1 0.9934, precision 0.9933, recall 0.9936, auc 0.9934
epoch 1301, loss 0.0232, train acc 99.40%, f1 0.9940, precision 0.9939, recall 0.9942, auc 0.9940
epoch 1401, loss 0.0150, train acc 99.43%, f1 0.9943, precision 0.9941, recall 0.9945, auc 0.9943
epoch 1501, loss 0.0119, train acc 99.48%, f1 0.9948, precision 0.9947, recall 0.9950, auc 0.9948
epoch 1601, loss 0.0152, train acc 99.52%, f1 0.9952, precision 0.9950, recall 0.9953, auc 0.9952
epoch 1701, loss 0.0058, train acc 99.55%, f1 0.9955, precision 0.9954, recall 0.9956, auc 0.9955
epoch 1801, loss 0.0128, train acc 99.59%, f1 0.9959, precision 0.9957, recall 0.9960, auc 0.9959
epoch 1901, loss 0.0147, train acc 99.60%, f1 0.9960, precision 0.9958, recall 0.9961, auc 0.9960
epoch 2001, loss 0.0087, train acc 99.62%, f1 0.9962, precision 0.9961, recall 0.9963, auc 0.9962
epoch 2101, loss 0.0207, train acc 99.64%, f1 0.9964, precision 0.9962, recall 0.9965, auc 0.9964
epoch 2201, loss 0.0113, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9966, auc 0.9965
epoch 2301, loss 0.0170, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2401, loss 0.0125, train acc 99.68%, f1 0.9968, precision 0.9967, recall 0.9969, auc 0.9968
epoch 2501, loss 0.0080, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9970, auc 0.9969
epoch 2601, loss 0.0133, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9971, auc 0.9970
epoch 2701, loss 0.0150, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 2801, loss 0.0066, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 2901, loss 0.0128, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 3001, loss 0.0126, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 3101, loss 0.0074, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 3201, loss 0.0060, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9976
epoch 3301, loss 0.0141, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 3401, loss 0.0102, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3501, loss 0.0133, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 3601, loss 0.0100, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 3701, loss 0.0068, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 3801, loss 0.0039, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 3901, loss 0.0028, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 4001, loss 0.0017, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 4101, loss 0.0090, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 4201, loss 0.0035, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 4301, loss 0.0079, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9980
epoch 4401, loss 0.0024, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 4501, loss 0.0104, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 4601, loss 0.0044, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4701, loss 0.0066, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 4801, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4901, loss 0.0055, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 5001, loss 0.0110, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 5101, loss 0.0042, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 5201, loss 0.0078, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 5301, loss 0.0089, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 5401, loss 0.0068, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 5501, loss 0.0034, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5601, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 5701, loss 0.0031, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 5801, loss 0.0026, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5901, loss 0.0076, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 6001, loss 0.0094, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 6101, loss 0.0032, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 6201, loss 0.0099, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 6301, loss 0.0036, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 6401, loss 0.0032, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6501, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6601, loss 0.0017, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6701, loss 0.0056, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6801, loss 0.0139, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6901, loss 0.0055, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 7001, loss 0.0033, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 7101, loss 0.0017, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 7201, loss 0.0054, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 7301, loss 0.0036, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 7401, loss 0.0025, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 7501, loss 0.0096, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 7601, loss 0.0039, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 7701, loss 0.0068, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 7801, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 7901, loss 0.0021, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_1
./test_vehicle0/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9471153846153847

the Fscore is 0.9135802469135802

the precision is 0.9024390243902439

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_1
----------------------



epoch 1, loss 0.6933, train acc 51.02%, f1 0.6704, precision 0.5052, recall 0.9963, auc 0.5102
epoch 101, loss 0.3989, train acc 84.20%, f1 0.8449, precision 0.8296, recall 0.8608, auc 0.8420
epoch 201, loss 0.2701, train acc 93.61%, f1 0.9362, precision 0.9349, recall 0.9375, auc 0.9361
epoch 301, loss 0.1457, train acc 97.14%, f1 0.9714, precision 0.9713, recall 0.9714, auc 0.9714
epoch 401, loss 0.0876, train acc 98.14%, f1 0.9814, precision 0.9817, recall 0.9812, auc 0.9814
epoch 501, loss 0.0575, train acc 98.39%, f1 0.9839, precision 0.9842, recall 0.9836, auc 0.9839
epoch 601, loss 0.0523, train acc 98.66%, f1 0.9866, precision 0.9870, recall 0.9862, auc 0.9866
epoch 701, loss 0.0397, train acc 98.87%, f1 0.9887, precision 0.9890, recall 0.9884, auc 0.9887
epoch 801, loss 0.0505, train acc 99.00%, f1 0.9900, precision 0.9903, recall 0.9898, auc 0.9900
epoch 901, loss 0.0434, train acc 99.10%, f1 0.9910, precision 0.9912, recall 0.9908, auc 0.9910
epoch 1001, loss 0.0382, train acc 99.17%, f1 0.9917, precision 0.9919, recall 0.9914, auc 0.9917
epoch 1101, loss 0.0359, train acc 99.27%, f1 0.9927, precision 0.9928, recall 0.9925, auc 0.9927
epoch 1201, loss 0.0377, train acc 99.33%, f1 0.9933, precision 0.9935, recall 0.9931, auc 0.9933
epoch 1301, loss 0.0314, train acc 99.38%, f1 0.9938, precision 0.9940, recall 0.9936, auc 0.9938
epoch 1401, loss 0.0173, train acc 99.44%, f1 0.9944, precision 0.9945, recall 0.9942, auc 0.9944
epoch 1501, loss 0.0193, train acc 99.46%, f1 0.9946, precision 0.9947, recall 0.9944, auc 0.9946
epoch 1601, loss 0.0088, train acc 99.51%, f1 0.9951, precision 0.9952, recall 0.9949, auc 0.9951
epoch 1701, loss 0.0195, train acc 99.53%, f1 0.9953, precision 0.9954, recall 0.9952, auc 0.9953
epoch 1801, loss 0.0151, train acc 99.57%, f1 0.9957, precision 0.9958, recall 0.9955, auc 0.9957
epoch 1901, loss 0.0177, train acc 99.59%, f1 0.9959, precision 0.9960, recall 0.9958, auc 0.9959
epoch 2001, loss 0.0188, train acc 99.61%, f1 0.9961, precision 0.9962, recall 0.9959, auc 0.9961
epoch 2101, loss 0.0115, train acc 99.63%, f1 0.9963, precision 0.9964, recall 0.9962, auc 0.9963
epoch 2201, loss 0.0105, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9964, auc 0.9965
epoch 2301, loss 0.0143, train acc 99.66%, f1 0.9966, precision 0.9967, recall 0.9965, auc 0.9966
epoch 2401, loss 0.0121, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9967, auc 0.9968
epoch 2501, loss 0.0128, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9968, auc 0.9969
epoch 2601, loss 0.0065, train acc 99.71%, f1 0.9971, precision 0.9972, recall 0.9970, auc 0.9971
epoch 2701, loss 0.0166, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9971, auc 0.9972
epoch 2801, loss 0.0078, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 2901, loss 0.0068, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9974
epoch 3001, loss 0.0093, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9974, auc 0.9975
epoch 3101, loss 0.0051, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 3201, loss 0.0058, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9975, auc 0.9976
epoch 3301, loss 0.0044, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9976, auc 0.9977
epoch 3401, loss 0.0113, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9977, auc 0.9978
epoch 3501, loss 0.0060, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 3601, loss 0.0076, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 3701, loss 0.0055, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 3801, loss 0.0067, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 3901, loss 0.0057, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 4001, loss 0.0057, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 4101, loss 0.0054, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 4201, loss 0.0086, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 4301, loss 0.0061, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 4401, loss 0.0023, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 4501, loss 0.0064, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 4601, loss 0.0059, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9982
epoch 4701, loss 0.0081, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4801, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 4901, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_1
./test_vehicle0/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9673076923076924

the Fscore is 0.9500000000000001

the precision is 0.95

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_1
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4069, train acc 84.25%, f1 0.8400, precision 0.8538, recall 0.8267, auc 0.8425
epoch 201, loss 0.2352, train acc 93.70%, f1 0.9369, precision 0.9380, recall 0.9358, auc 0.9370
epoch 301, loss 0.1400, train acc 97.23%, f1 0.9723, precision 0.9723, recall 0.9723, auc 0.9723
epoch 401, loss 0.1133, train acc 98.14%, f1 0.9814, precision 0.9811, recall 0.9816, auc 0.9814
epoch 501, loss 0.0798, train acc 98.43%, f1 0.9843, precision 0.9841, recall 0.9846, auc 0.9843
epoch 601, loss 0.0629, train acc 98.72%, f1 0.9872, precision 0.9870, recall 0.9874, auc 0.9872
epoch 701, loss 0.0414, train acc 98.86%, f1 0.9886, precision 0.9883, recall 0.9890, auc 0.9886
epoch 801, loss 0.0265, train acc 99.01%, f1 0.9901, precision 0.9900, recall 0.9903, auc 0.9901
epoch 901, loss 0.0408, train acc 99.12%, f1 0.9912, precision 0.9910, recall 0.9914, auc 0.9912
epoch 1001, loss 0.0268, train acc 99.17%, f1 0.9917, precision 0.9915, recall 0.9919, auc 0.9917
epoch 1101, loss 0.0204, train acc 99.26%, f1 0.9926, precision 0.9924, recall 0.9928, auc 0.9926
epoch 1201, loss 0.0299, train acc 99.33%, f1 0.9933, precision 0.9931, recall 0.9935, auc 0.9933
epoch 1301, loss 0.0274, train acc 99.38%, f1 0.9938, precision 0.9937, recall 0.9939, auc 0.9938
epoch 1401, loss 0.0392, train acc 99.43%, f1 0.9943, precision 0.9941, recall 0.9944, auc 0.9943
epoch 1501, loss 0.0201, train acc 99.48%, f1 0.9948, precision 0.9947, recall 0.9950, auc 0.9948
epoch 1601, loss 0.0115, train acc 99.51%, f1 0.9951, precision 0.9950, recall 0.9952, auc 0.9951
epoch 1701, loss 0.0159, train acc 99.54%, f1 0.9954, precision 0.9953, recall 0.9956, auc 0.9954
epoch 1801, loss 0.0207, train acc 99.57%, f1 0.9957, precision 0.9955, recall 0.9958, auc 0.9957
epoch 1901, loss 0.0074, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9959, auc 0.9958
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_1
./test_vehicle0/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9644230769230768

the Fscore is 0.9176470588235294

the precision is 0.8666666666666667

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_1
----------------------



epoch 1, loss 0.6930, train acc 50.20%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4233, train acc 83.93%, f1 0.8385, precision 0.8392, recall 0.8378, auc 0.8393
epoch 201, loss 0.2323, train acc 93.91%, f1 0.9389, precision 0.9377, recall 0.9402, auc 0.9391
epoch 301, loss 0.1389, train acc 97.28%, f1 0.9727, precision 0.9718, recall 0.9735, auc 0.9728
epoch 401, loss 0.1063, train acc 98.13%, f1 0.9812, precision 0.9807, recall 0.9818, auc 0.9813
epoch 501, loss 0.0495, train acc 98.44%, f1 0.9844, precision 0.9840, recall 0.9847, auc 0.9844
epoch 601, loss 0.0650, train acc 98.73%, f1 0.9872, precision 0.9870, recall 0.9874, auc 0.9873
epoch 701, loss 0.0437, train acc 98.86%, f1 0.9885, precision 0.9884, recall 0.9887, auc 0.9886
epoch 801, loss 0.0454, train acc 98.99%, f1 0.9899, precision 0.9900, recall 0.9898, auc 0.9899
epoch 901, loss 0.0280, train acc 99.11%, f1 0.9911, precision 0.9909, recall 0.9913, auc 0.9911
epoch 1001, loss 0.0377, train acc 99.20%, f1 0.9920, precision 0.9920, recall 0.9920, auc 0.9920
epoch 1101, loss 0.0343, train acc 99.26%, f1 0.9926, precision 0.9925, recall 0.9926, auc 0.9926
epoch 1201, loss 0.0240, train acc 99.34%, f1 0.9933, precision 0.9932, recall 0.9935, auc 0.9934
epoch 1301, loss 0.0221, train acc 99.39%, f1 0.9939, precision 0.9938, recall 0.9939, auc 0.9939
epoch 1401, loss 0.0167, train acc 99.41%, f1 0.9941, precision 0.9940, recall 0.9942, auc 0.9941
epoch 1501, loss 0.0200, train acc 99.46%, f1 0.9945, precision 0.9944, recall 0.9947, auc 0.9946
epoch 1601, loss 0.0210, train acc 99.50%, f1 0.9950, precision 0.9948, recall 0.9952, auc 0.9950
epoch 1701, loss 0.0193, train acc 99.53%, f1 0.9953, precision 0.9952, recall 0.9955, auc 0.9953
epoch 1801, loss 0.0103, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9957, auc 0.9956
epoch 1901, loss 0.0125, train acc 99.58%, f1 0.9958, precision 0.9956, recall 0.9961, auc 0.9958
epoch 2001, loss 0.0180, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9962, auc 0.9961
epoch 2101, loss 0.0131, train acc 99.63%, f1 0.9963, precision 0.9961, recall 0.9964, auc 0.9963
epoch 2201, loss 0.0185, train acc 99.65%, f1 0.9964, precision 0.9963, recall 0.9966, auc 0.9965
epoch 2301, loss 0.0093, train acc 99.67%, f1 0.9967, precision 0.9966, recall 0.9968, auc 0.9967
epoch 2401, loss 0.0109, train acc 99.68%, f1 0.9968, precision 0.9967, recall 0.9968, auc 0.9968
epoch 2501, loss 0.0102, train acc 99.69%, f1 0.9969, precision 0.9966, recall 0.9971, auc 0.9969
epoch 2601, loss 0.0138, train acc 99.70%, f1 0.9970, precision 0.9968, recall 0.9972, auc 0.9970
epoch 2701, loss 0.0112, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9974, auc 0.9973
epoch 2801, loss 0.0062, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 2901, loss 0.0090, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 3001, loss 0.0134, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9976, auc 0.9974
epoch 3101, loss 0.0059, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9977, auc 0.9976
epoch 3201, loss 0.0137, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9976
epoch 3301, loss 0.0078, train acc 99.77%, f1 0.9977, precision 0.9975, recall 0.9979, auc 0.9977
epoch 3401, loss 0.0108, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9979, auc 0.9977
epoch 3501, loss 0.0084, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 3601, loss 0.0080, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9980, auc 0.9978
epoch 3701, loss 0.0094, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 3801, loss 0.0082, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9980
epoch 3901, loss 0.0032, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 4001, loss 0.0078, train acc 99.81%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9981
epoch 4101, loss 0.0033, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4201, loss 0.0075, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 4301, loss 0.0077, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 4401, loss 0.0068, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 4501, loss 0.0034, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 4601, loss 0.0048, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 4701, loss 0.0048, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9984, auc 0.9983
epoch 4801, loss 0.0029, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 4901, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 5001, loss 0.0061, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 5101, loss 0.0038, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 5201, loss 0.0037, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 5301, loss 0.0092, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 5401, loss 0.0097, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 5501, loss 0.0075, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 5601, loss 0.0035, train acc 99.85%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9985
epoch 5701, loss 0.0028, train acc 99.86%, f1 0.9985, precision 0.9984, recall 0.9987, auc 0.9986
epoch 5801, loss 0.0027, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 5901, loss 0.0059, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6001, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9988, auc 0.9987
epoch 6101, loss 0.0042, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 6201, loss 0.0023, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 6301, loss 0.0056, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 6401, loss 0.0014, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9987
epoch 6501, loss 0.0019, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 6601, loss 0.0036, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6701, loss 0.0026, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 6801, loss 0.0062, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 6901, loss 0.0068, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 7001, loss 0.0015, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 7101, loss 0.0015, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 7201, loss 0.0025, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9990, auc 0.9988
epoch 7301, loss 0.0006, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 7401, loss 0.0056, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 7501, loss 0.0026, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 7601, loss 0.0013, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 7701, loss 0.0035, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 7801, loss 0.0068, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7901, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 8001, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 8101, loss 0.0012, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 8201, loss 0.0033, train acc 99.91%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9991
epoch 8301, loss 0.0015, train acc 99.91%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9991
epoch 8401, loss 0.0023, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 8501, loss 0.0055, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 8601, loss 0.0049, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 8701, loss 0.0032, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 8801, loss 0.0007, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 8901, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9001, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9101, loss 0.0011, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9201, loss 0.0034, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9301, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9401, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9501, loss 0.0030, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 9601, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 9701, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 9801, loss 0.0004, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9901, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 10001, loss 0.0001, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 10101, loss 0.0062, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10201, loss 0.0043, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10301, loss 0.0062, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10401, loss 0.0037, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10501, loss 0.0007, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 10601, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 10701, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10801, loss 0.0004, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 10901, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 11001, loss 0.0003, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 11101, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 11201, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11301, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 11401, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11501, loss 0.0005, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11601, loss 0.0029, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 11701, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 11801, loss 0.0042, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11901, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12001, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 12101, loss 0.0044, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 12201, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 12301, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 12401, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 12501, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 12601, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 12701, loss 0.0044, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 12801, loss 0.0028, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9993, auc 0.9994
epoch 12901, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 13001, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9993, auc 0.9994
epoch 13101, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 13201, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9993, auc 0.9995
epoch 13301, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 13401, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 13501, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13601, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 13701, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9993, auc 0.9995
epoch 13801, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 13901, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 14001, loss 0.0002, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9993, auc 0.9994
epoch 14101, loss 0.0005, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 14201, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 14301, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 14401, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 14501, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9993, auc 0.9995
epoch 14601, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9993, auc 0.9994
epoch 14701, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 14801, loss 0.0032, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9993, auc 0.9994
epoch 14901, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 15001, loss 0.0003, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 15101, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 15201, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9993, auc 0.9995
epoch 15301, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9994, auc 0.9995
epoch 15401, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 15501, loss 0.0002, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9994, auc 0.9995
epoch 15601, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 15701, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9994, auc 0.9995
epoch 15801, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 15901, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9994, auc 0.9995
epoch 16001, loss 0.0027, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9994, auc 0.9995
epoch 16101, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 16201, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 16301, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9994, auc 0.9995
epoch 16401, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9994, auc 0.9995/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 16601, loss 0.0027, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 16701, loss 0.0001, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 16801, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 16901, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9994, auc 0.9996
epoch 17001, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9994, auc 0.9996
epoch 17101, loss 0.0030, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17201, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 17301, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9994, auc 0.9996
epoch 17401, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9994, auc 0.9996
epoch 17501, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 17601, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9994, auc 0.9996
epoch 17701, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 17801, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 17901, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 18001, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 18101, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 18201, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 18301, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 18401, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 18501, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 18601, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 18701, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 18801, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 18901, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 19001, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 19101, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9995, auc 0.9997
epoch 19201, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 19301, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 19401, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 19501, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 19601, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19701, loss 0.0038, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 19801, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9995, auc 0.9996
epoch 19901, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_1
./test_vehicle0/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9711538461538461

the Fscore is 0.9620253164556962

the precision is 0.9743589743589743

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_1
----------------------



epoch 1, loss 0.6941, train acc 49.95%, f1 0.6662, precision 0.4995, recall 1.0000, auc 0.5000
epoch 101, loss 0.4400, train acc 83.48%, f1 0.8412, precision 0.8089, recall 0.8762, auc 0.8348
epoch 201, loss 0.2728, train acc 93.38%, f1 0.9342, precision 0.9276, recall 0.9409, auc 0.9338
epoch 301, loss 0.1569, train acc 97.04%, f1 0.9704, precision 0.9695, recall 0.9713, auc 0.9704
epoch 401, loss 0.0789, train acc 97.99%, f1 0.9799, precision 0.9801, recall 0.9797, auc 0.9799
epoch 501, loss 0.0927, train acc 98.34%, f1 0.9834, precision 0.9836, recall 0.9832, auc 0.9834
epoch 601, loss 0.0537, train acc 98.63%, f1 0.9862, precision 0.9865, recall 0.9859, auc 0.9863
epoch 701, loss 0.0529, train acc 98.87%, f1 0.9887, precision 0.9890, recall 0.9884, auc 0.9887
epoch 801, loss 0.0348, train acc 98.98%, f1 0.9898, precision 0.9901, recall 0.9896, auc 0.9898
epoch 901, loss 0.0428, train acc 99.09%, f1 0.9909, precision 0.9911, recall 0.9907, auc 0.9909
epoch 1001, loss 0.0357, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9915, auc 0.9916
epoch 1101, loss 0.0266, train acc 99.26%, f1 0.9926, precision 0.9926, recall 0.9925, auc 0.9926
epoch 1201, loss 0.0274, train acc 99.34%, f1 0.9934, precision 0.9935, recall 0.9933, auc 0.9934
epoch 1301, loss 0.0235, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9939, auc 0.9939
epoch 1401, loss 0.0174, train acc 99.45%, f1 0.9944, precision 0.9945, recall 0.9944, auc 0.9945
epoch 1501, loss 0.0206, train acc 99.48%, f1 0.9948, precision 0.9950, recall 0.9945, auc 0.9948
epoch 1601, loss 0.0194, train acc 99.50%, f1 0.9950, precision 0.9952, recall 0.9949, auc 0.9950
epoch 1701, loss 0.0227, train acc 99.53%, f1 0.9953, precision 0.9954, recall 0.9952, auc 0.9953
epoch 1801, loss 0.0183, train acc 99.56%, f1 0.9956, precision 0.9957, recall 0.9956, auc 0.9956
epoch 1901, loss 0.0263, train acc 99.59%, f1 0.9959, precision 0.9961, recall 0.9957, auc 0.9959
epoch 2001, loss 0.0053, train acc 99.61%, f1 0.9961, precision 0.9962, recall 0.9960, auc 0.9961
epoch 2101, loss 0.0072, train acc 99.63%, f1 0.9963, precision 0.9964, recall 0.9962, auc 0.9963
epoch 2201, loss 0.0104, train acc 99.66%, f1 0.9966, precision 0.9967, recall 0.9964, auc 0.9966
epoch 2301, loss 0.0120, train acc 99.67%, f1 0.9967, precision 0.9969, recall 0.9964, auc 0.9967
epoch 2401, loss 0.0060, train acc 99.68%, f1 0.9968, precision 0.9970, recall 0.9966, auc 0.9968
epoch 2501, loss 0.0135, train acc 99.70%, f1 0.9970, precision 0.9972, recall 0.9967, auc 0.9970
epoch 2601, loss 0.0066, train acc 99.70%, f1 0.9970, precision 0.9972, recall 0.9968, auc 0.9970
epoch 2701, loss 0.0122, train acc 99.72%, f1 0.9972, precision 0.9975, recall 0.9968, auc 0.9972
epoch 2801, loss 0.0086, train acc 99.74%, f1 0.9974, precision 0.9976, recall 0.9971, auc 0.9974
epoch 2901, loss 0.0101, train acc 99.75%, f1 0.9975, precision 0.9977, recall 0.9972, auc 0.9975
epoch 3001, loss 0.0134, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9973, auc 0.9975
epoch 3101, loss 0.0105, train acc 99.76%, f1 0.9976, precision 0.9978, recall 0.9974, auc 0.9976
epoch 3201, loss 0.0106, train acc 99.77%, f1 0.9977, precision 0.9979, recall 0.9975, auc 0.9977
epoch 3301, loss 0.0110, train acc 99.77%, f1 0.9977, precision 0.9979, recall 0.9976, auc 0.9977
epoch 3401, loss 0.0043, train acc 99.77%, f1 0.9977, precision 0.9980, recall 0.9975, auc 0.9977
epoch 3501, loss 0.0079, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9977, auc 0.9978
epoch 3601, loss 0.0031, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 3701, loss 0.0090, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 3801, loss 0.0141, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 3901, loss 0.0064, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9978, auc 0.9980
epoch 4001, loss 0.0115, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9978, auc 0.9979
epoch 4101, loss 0.0035, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 4201, loss 0.0085, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 4301, loss 0.0047, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 4401, loss 0.0080, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4501, loss 0.0023, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9981, auc 0.9983
epoch 4601, loss 0.0072, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9982, auc 0.9983
epoch 4701, loss 0.0040, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 4801, loss 0.0063, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 4901, loss 0.0046, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9981, auc 0.9983
epoch 5001, loss 0.0050, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9982, auc 0.9983
epoch 5101, loss 0.0024, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 5201, loss 0.0028, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9982, auc 0.9984
epoch 5301, loss 0.0067, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 5401, loss 0.0033, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5501, loss 0.0049, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 5601, loss 0.0010, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5701, loss 0.0056, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 5801, loss 0.0014, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 5901, loss 0.0053, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 6001, loss 0.0069, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 6101, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 6201, loss 0.0025, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 6301, loss 0.0027, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 6401, loss 0.0045, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6501, loss 0.0068, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 6601, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6701, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6801, loss 0.0009, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 6901, loss 0.0055, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 7001, loss 0.0070, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 7101, loss 0.0050, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 7201, loss 0.0012, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 7301, loss 0.0030, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 7401, loss 0.0054, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 7501, loss 0.0015, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 7601, loss 0.0046, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 7701, loss 0.0012, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9991, auc 0.9990
epoch 7801, loss 0.0055, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7901, loss 0.0026, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 8001, loss 0.0046, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 8101, loss 0.0012, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0026, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9990
epoch 8301, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 8401, loss 0.0034, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 8501, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 8601, loss 0.0025, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 8701, loss 0.0016, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 8801, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9992, auc 0.9991
epoch 8901, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 9001, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9101, loss 0.0031, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 9201, loss 0.0032, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 9301, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 9401, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 9501, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9601, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9701, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 9801, loss 0.0051, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 9901, loss 0.0047, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 10001, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 10101, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 10201, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 10301, loss 0.0006, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 10401, loss 0.0053, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 10501, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 10601, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 10701, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9994, auc 0.9992
epoch 10801, loss 0.0038, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 10901, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 11001, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 11101, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 11201, loss 0.0041, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11301, loss 0.0029, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9994, auc 0.9992
epoch 11401, loss 0.0051, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 11501, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 11601, loss 0.0032, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 11701, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 11801, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 11901, loss 0.0042, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 12001, loss 0.0057, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 12101, loss 0.0045, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 12201, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 12301, loss 0.0029, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 12401, loss 0.0018, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9995, auc 0.9993
epoch 12501, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 12601, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 12701, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 12801, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 12901, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9995, auc 0.9993
epoch 13001, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 13101, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9995, auc 0.9993
epoch 13201, loss 0.0002, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 13301, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 13401, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 13501, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 13601, loss 0.0031, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 13701, loss 0.0032, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9996, auc 0.9994
epoch 13801, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 13901, loss 0.0031, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 14001, loss 0.0002, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 14101, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 14201, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 14301, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 14401, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 14501, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 14601, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 14701, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 14801, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 14901, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9996, auc 0.9994
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_1
./test_vehicle0/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9711538461538461

the Fscore is 0.9620253164556962

the precision is 0.9743589743589743

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_1
----------------------



epoch 1, loss 0.6927, train acc 50.06%, f1 0.6667, precision 0.5001, recall 1.0000, auc 0.5010
epoch 101, loss 0.3473, train acc 83.41%, f1 0.8344, precision 0.8321, recall 0.8368, auc 0.8341
epoch 201, loss 0.2295, train acc 93.77%, f1 0.9376, precision 0.9373, recall 0.9379, auc 0.9377
epoch 301, loss 0.1418, train acc 97.13%, f1 0.9713, precision 0.9720, recall 0.9706, auc 0.9713
epoch 401, loss 0.0924, train acc 98.07%, f1 0.9807, precision 0.9809, recall 0.9805, auc 0.9807
epoch 501, loss 0.0471, train acc 98.50%, f1 0.9849, precision 0.9851, recall 0.9848, auc 0.9850
epoch 601, loss 0.0530, train acc 98.69%, f1 0.9868, precision 0.9871, recall 0.9866, auc 0.9869
epoch 701, loss 0.0351, train acc 98.85%, f1 0.9885, precision 0.9888, recall 0.9881, auc 0.9885
epoch 801, loss 0.0359, train acc 98.98%, f1 0.9898, precision 0.9899, recall 0.9897, auc 0.9898
epoch 901, loss 0.0388, train acc 99.08%, f1 0.9908, precision 0.9910, recall 0.9906, auc 0.9908
epoch 1001, loss 0.0248, train acc 99.18%, f1 0.9918, precision 0.9920, recall 0.9916, auc 0.9918
epoch 1101, loss 0.0259, train acc 99.26%, f1 0.9926, precision 0.9929, recall 0.9923, auc 0.9926
epoch 1201, loss 0.0363, train acc 99.32%, f1 0.9932, precision 0.9935, recall 0.9929, auc 0.9932
epoch 1301, loss 0.0323, train acc 99.38%, f1 0.9938, precision 0.9942, recall 0.9935, auc 0.9938
epoch 1401, loss 0.0164, train acc 99.43%, f1 0.9943, precision 0.9945, recall 0.9940, auc 0.9943
epoch 1501, loss 0.0187, train acc 99.47%, f1 0.9947, precision 0.9948, recall 0.9945, auc 0.9947
epoch 1601, loss 0.0117, train acc 99.50%, f1 0.9950, precision 0.9950, recall 0.9950, auc 0.9950
epoch 1701, loss 0.0201, train acc 99.53%, f1 0.9953, precision 0.9956, recall 0.9951, auc 0.9953
epoch 1801, loss 0.0275, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9955, auc 0.9956
epoch 1901, loss 0.0100, train acc 99.59%, f1 0.9959, precision 0.9961, recall 0.9956, auc 0.9959
epoch 2001, loss 0.0076, train acc 99.60%, f1 0.9960, precision 0.9962, recall 0.9959, auc 0.9960
epoch 2101, loss 0.0166, train acc 99.62%, f1 0.9962, precision 0.9964, recall 0.9961, auc 0.9962
epoch 2201, loss 0.0170, train acc 99.64%, f1 0.9964, precision 0.9966, recall 0.9962, auc 0.9964
epoch 2301, loss 0.0074, train acc 99.66%, f1 0.9966, precision 0.9967, recall 0.9964, auc 0.9966
epoch 2401, loss 0.0101, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9967, auc 0.9968
epoch 2501, loss 0.0068, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9968, auc 0.9969
epoch 2601, loss 0.0064, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9969, auc 0.9969
epoch 2701, loss 0.0063, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9970, auc 0.9971
epoch 2801, loss 0.0050, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9971, auc 0.9972
epoch 2901, loss 0.0117, train acc 99.74%, f1 0.9974, precision 0.9975, recall 0.9973, auc 0.9974
epoch 3001, loss 0.0063, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 3101, loss 0.0045, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 3201, loss 0.0102, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9974, auc 0.9976
epoch 3301, loss 0.0028, train acc 99.75%, f1 0.9975, precision 0.9977, recall 0.9974, auc 0.9975
epoch 3401, loss 0.0156, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 3501, loss 0.0046, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9975, auc 0.9976
epoch 3601, loss 0.0066, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9976, auc 0.9977
epoch 3701, loss 0.0068, train acc 99.78%, f1 0.9978, precision 0.9981, recall 0.9975, auc 0.9978
epoch 3801, loss 0.0016, train acc 99.78%, f1 0.9978, precision 0.9981, recall 0.9976, auc 0.9978
epoch 3901, loss 0.0019, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9976, auc 0.9979
epoch 4001, loss 0.0147, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9976, auc 0.9979
epoch 4101, loss 0.0071, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9976, auc 0.9979
epoch 4201, loss 0.0088, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9978, auc 0.9979
epoch 4301, loss 0.0048, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9977, auc 0.9979
epoch 4401, loss 0.0083, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9977, auc 0.9979
epoch 4501, loss 0.0007, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 4601, loss 0.0059, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9978, auc 0.9980
epoch 4701, loss 0.0066, train acc 99.80%, f1 0.9980, precision 0.9982, recall 0.9978, auc 0.9980
epoch 4801, loss 0.0064, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9978, auc 0.9981
epoch 4901, loss 0.0041, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 5001, loss 0.0074, train acc 99.81%, f1 0.9981, precision 0.9984, recall 0.9979, auc 0.9981
epoch 5101, loss 0.0021, train acc 99.81%, f1 0.9981, precision 0.9984, recall 0.9979, auc 0.9981
epoch 5201, loss 0.0057, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9979, auc 0.9981
epoch 5301, loss 0.0030, train acc 99.82%, f1 0.9981, precision 0.9983, recall 0.9980, auc 0.9982
epoch 5401, loss 0.0056, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 5501, loss 0.0041, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9980, auc 0.9981
epoch 5601, loss 0.0046, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9979, auc 0.9982
epoch 5701, loss 0.0033, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9980, auc 0.9982
epoch 5801, loss 0.0037, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 5901, loss 0.0046, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 6001, loss 0.0018, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9980, auc 0.9982
epoch 6101, loss 0.0076, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 6201, loss 0.0028, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9981, auc 0.9983
epoch 6301, loss 0.0051, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 6401, loss 0.0088, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9980, auc 0.9982
epoch 6501, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9981, auc 0.9983
epoch 6601, loss 0.0026, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9981, auc 0.9983
epoch 6701, loss 0.0032, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 6801, loss 0.0108, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 6901, loss 0.0037, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 7001, loss 0.0048, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9981, auc 0.9983
epoch 7101, loss 0.0037, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9982, auc 0.9984
epoch 7201, loss 0.0072, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 7301, loss 0.0065, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 7401, loss 0.0078, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9983, auc 0.9985
epoch 7501, loss 0.0068, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9982, auc 0.9983
epoch 7601, loss 0.0023, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 7701, loss 0.0031, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 7801, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9982, auc 0.9984
epoch 7901, loss 0.0079, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 8001, loss 0.0051, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 8101, loss 0.0082, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0018, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9982, auc 0.9984
epoch 8301, loss 0.0058, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 8401, loss 0.0021, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9983, auc 0.9984
epoch 8501, loss 0.0049, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9982, auc 0.9984
epoch 8601, loss 0.0064, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 8701, loss 0.0025, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 8801, loss 0.0062, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 8901, loss 0.0019, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 9001, loss 0.0015, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 9101, loss 0.0020, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 9201, loss 0.0088, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 9301, loss 0.0122, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 9401, loss 0.0031, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 9501, loss 0.0011, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 9601, loss 0.0105, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 9701, loss 0.0055, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 9801, loss 0.0037, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 9901, loss 0.0051, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_1
./test_vehicle0/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9509615384615385

the Fscore is 0.925

the precision is 0.925

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_1
----------------------



epoch 1, loss 0.6931, train acc 50.27%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4012, train acc 83.92%, f1 0.8373, precision 0.8425, recall 0.8323, auc 0.8392
epoch 201, loss 0.2286, train acc 93.87%, f1 0.9384, precision 0.9388, recall 0.9379, auc 0.9387
epoch 301, loss 0.1649, train acc 97.19%, f1 0.9718, precision 0.9721, recall 0.9714, auc 0.9719
epoch 401, loss 0.0955, train acc 98.14%, f1 0.9813, precision 0.9816, recall 0.9809, auc 0.9814
epoch 501, loss 0.0788, train acc 98.40%, f1 0.9839, precision 0.9843, recall 0.9835, auc 0.9840
epoch 601, loss 0.0517, train acc 98.62%, f1 0.9861, precision 0.9863, recall 0.9860, auc 0.9862
epoch 701, loss 0.0619, train acc 98.85%, f1 0.9884, precision 0.9888, recall 0.9880, auc 0.9885
epoch 801, loss 0.0333, train acc 98.98%, f1 0.9897, precision 0.9896, recall 0.9898, auc 0.9898
epoch 901, loss 0.0362, train acc 99.08%, f1 0.9907, precision 0.9908, recall 0.9906, auc 0.9908
epoch 1001, loss 0.0295, train acc 99.17%, f1 0.9916, precision 0.9917, recall 0.9916, auc 0.9917
epoch 1101, loss 0.0370, train acc 99.24%, f1 0.9924, precision 0.9924, recall 0.9924, auc 0.9924
epoch 1201, loss 0.0236, train acc 99.32%, f1 0.9932, precision 0.9931, recall 0.9933, auc 0.9932
epoch 1301, loss 0.0181, train acc 99.40%, f1 0.9940, precision 0.9937, recall 0.9942, auc 0.9940
epoch 1401, loss 0.0147, train acc 99.44%, f1 0.9944, precision 0.9942, recall 0.9945, auc 0.9944
epoch 1501, loss 0.0188, train acc 99.47%, f1 0.9947, precision 0.9948, recall 0.9945, auc 0.9947
epoch 1601, loss 0.0270, train acc 99.50%, f1 0.9950, precision 0.9950, recall 0.9950, auc 0.9950
epoch 1701, loss 0.0106, train acc 99.55%, f1 0.9954, precision 0.9953, recall 0.9956, auc 0.9955
epoch 1801, loss 0.0116, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9957, auc 0.9956
epoch 1901, loss 0.0100, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9959, auc 0.9958
epoch 2001, loss 0.0123, train acc 99.61%, f1 0.9961, precision 0.9959, recall 0.9963, auc 0.9961
epoch 2101, loss 0.0133, train acc 99.63%, f1 0.9962, precision 0.9962, recall 0.9963, auc 0.9963
epoch 2201, loss 0.0145, train acc 99.65%, f1 0.9964, precision 0.9964, recall 0.9965, auc 0.9965
epoch 2301, loss 0.0096, train acc 99.68%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9968
epoch 2401, loss 0.0092, train acc 99.69%, f1 0.9968, precision 0.9969, recall 0.9968, auc 0.9969
epoch 2501, loss 0.0102, train acc 99.70%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9970
epoch 2601, loss 0.0097, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9969, auc 0.9970
epoch 2701, loss 0.0096, train acc 99.72%, f1 0.9972, precision 0.9974, recall 0.9969, auc 0.9972
epoch 2801, loss 0.0099, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9973, auc 0.9973
epoch 2901, loss 0.0105, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9973, auc 0.9975
epoch 3001, loss 0.0104, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9975, auc 0.9976
epoch 3101, loss 0.0078, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 3201, loss 0.0038, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9976, auc 0.9977
epoch 3301, loss 0.0091, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9975, auc 0.9977
epoch 3401, loss 0.0096, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9976, auc 0.9978
epoch 3501, loss 0.0033, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 3601, loss 0.0064, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9977, auc 0.9979
epoch 3701, loss 0.0048, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 3801, loss 0.0029, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 3901, loss 0.0033, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 4001, loss 0.0118, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 4101, loss 0.0046, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 4201, loss 0.0126, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9979, auc 0.9981
epoch 4301, loss 0.0057, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9980, auc 0.9982
epoch 4401, loss 0.0067, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 4501, loss 0.0048, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9982
epoch 4601, loss 0.0059, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9981, auc 0.9983
epoch 4701, loss 0.0038, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 4801, loss 0.0017, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 4901, loss 0.0062, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 5001, loss 0.0071, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9982, auc 0.9983
epoch 5101, loss 0.0026, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 5201, loss 0.0043, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9983, auc 0.9984
epoch 5301, loss 0.0023, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 5401, loss 0.0070, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 5501, loss 0.0020, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9983, auc 0.9985
epoch 5601, loss 0.0068, train acc 99.85%, f1 0.9984, precision 0.9986, recall 0.9983, auc 0.9985
epoch 5701, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9985, auc 0.9986
epoch 5801, loss 0.0103, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5901, loss 0.0042, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 6001, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 6101, loss 0.0055, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 6201, loss 0.0055, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 6301, loss 0.0034, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9986, auc 0.9987
epoch 6401, loss 0.0026, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 6501, loss 0.0040, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 6601, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 6701, loss 0.0013, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9985, auc 0.9987
epoch 6801, loss 0.0011, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 6901, loss 0.0014, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 7001, loss 0.0034, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 7101, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 7201, loss 0.0024, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9987, auc 0.9989
epoch 7301, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9986, auc 0.9988
epoch 7401, loss 0.0028, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 7501, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9987, auc 0.9989
epoch 7601, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 7701, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 7801, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9988, auc 0.9990
epoch 7901, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_1
./test_vehicle0/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9423076923076923

the Fscore is 0.9230769230769231

the precision is 0.9473684210526315

the recall is 0.9

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_1
----------------------



epoch 1, loss 0.6936, train acc 49.94%, f1 0.6661, precision 0.4994, recall 1.0000, auc 0.5000
epoch 101, loss 0.4175, train acc 83.71%, f1 0.8377, precision 0.8335, recall 0.8420, auc 0.8371
epoch 201, loss 0.2409, train acc 93.80%, f1 0.9380, precision 0.9363, recall 0.9398, auc 0.9380
epoch 301, loss 0.1418, train acc 97.08%, f1 0.9708, precision 0.9696, recall 0.9719, auc 0.9708
epoch 401, loss 0.0990, train acc 98.11%, f1 0.9810, precision 0.9805, recall 0.9816, auc 0.9811
epoch 501, loss 0.0865, train acc 98.42%, f1 0.9841, precision 0.9838, recall 0.9845, auc 0.9842
epoch 601, loss 0.0591, train acc 98.70%, f1 0.9870, precision 0.9863, recall 0.9876, auc 0.9870
epoch 701, loss 0.0479, train acc 98.84%, f1 0.9884, precision 0.9879, recall 0.9889, auc 0.9884
epoch 801, loss 0.0313, train acc 99.00%, f1 0.9899, precision 0.9897, recall 0.9901, auc 0.9900
epoch 901, loss 0.0301, train acc 99.09%, f1 0.9909, precision 0.9907, recall 0.9910, auc 0.9909
epoch 1001, loss 0.0350, train acc 99.17%, f1 0.9917, precision 0.9918, recall 0.9916, auc 0.9917
epoch 1101, loss 0.0270, train acc 99.25%, f1 0.9925, precision 0.9927, recall 0.9924, auc 0.9925
epoch 1201, loss 0.0205, train acc 99.31%, f1 0.9931, precision 0.9932, recall 0.9930, auc 0.9931
epoch 1301, loss 0.0252, train acc 99.37%, f1 0.9937, precision 0.9939, recall 0.9936, auc 0.9937
epoch 1401, loss 0.0274, train acc 99.43%, f1 0.9943, precision 0.9944, recall 0.9942, auc 0.9943
epoch 1501, loss 0.0241, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9947, auc 0.9948
epoch 1601, loss 0.0166, train acc 99.50%, f1 0.9950, precision 0.9951, recall 0.9949, auc 0.9950
epoch 1701, loss 0.0169, train acc 99.54%, f1 0.9953, precision 0.9955, recall 0.9952, auc 0.9954
epoch 1801, loss 0.0089, train acc 99.56%, f1 0.9956, precision 0.9957, recall 0.9955, auc 0.9956
epoch 1901, loss 0.0116, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9958, auc 0.9959
epoch 2001, loss 0.0267, train acc 99.61%, f1 0.9961, precision 0.9964, recall 0.9958, auc 0.9961
epoch 2101, loss 0.0162, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9963, auc 0.9964
epoch 2201, loss 0.0177, train acc 99.65%, f1 0.9965, precision 0.9966, recall 0.9963, auc 0.9965
epoch 2301, loss 0.0110, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 2401, loss 0.0099, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2501, loss 0.0137, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 2601, loss 0.0120, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 2701, loss 0.0111, train acc 99.73%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9973
epoch 2801, loss 0.0064, train acc 99.74%, f1 0.9974, precision 0.9972, recall 0.9976, auc 0.9974
epoch 2901, loss 0.0093, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9976, auc 0.9975
epoch 3001, loss 0.0058, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 3101, loss 0.0042, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 3201, loss 0.0072, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 3301, loss 0.0068, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 3401, loss 0.0064, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 3501, loss 0.0064, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 3601, loss 0.0112, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 3701, loss 0.0071, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 3801, loss 0.0062, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 3901, loss 0.0045, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 4001, loss 0.0059, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 4101, loss 0.0093, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9982
epoch 4201, loss 0.0057, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 4301, loss 0.0103, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 4401, loss 0.0044, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 4501, loss 0.0072, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4601, loss 0.0014, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4701, loss 0.0088, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 4801, loss 0.0027, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4901, loss 0.0089, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_1
./test_vehicle0/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9471153846153847

the Fscore is 0.9135802469135802

the precision is 0.9024390243902439

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_1
----------------------



epoch 1, loss 0.6930, train acc 49.86%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3911, train acc 83.63%, f1 0.8367, precision 0.8369, recall 0.8364, auc 0.8363
epoch 201, loss 0.2280, train acc 93.84%, f1 0.9384, precision 0.9409, recall 0.9359, auc 0.9384
epoch 301, loss 0.1311, train acc 97.14%, f1 0.9714, precision 0.9728, recall 0.9700, auc 0.9714
epoch 401, loss 0.0981, train acc 97.92%, f1 0.9793, precision 0.9799, recall 0.9787, auc 0.9792
epoch 501, loss 0.0635, train acc 98.37%, f1 0.9837, precision 0.9849, recall 0.9825, auc 0.9837
epoch 601, loss 0.0537, train acc 98.63%, f1 0.9863, precision 0.9872, recall 0.9854, auc 0.9863
epoch 701, loss 0.0554, train acc 98.86%, f1 0.9886, precision 0.9895, recall 0.9877, auc 0.9886
epoch 801, loss 0.0314, train acc 98.97%, f1 0.9897, precision 0.9901, recall 0.9894, auc 0.9897
epoch 901, loss 0.0388, train acc 99.09%, f1 0.9909, precision 0.9911, recall 0.9908, auc 0.9909
epoch 1001, loss 0.0361, train acc 99.17%, f1 0.9917, precision 0.9917, recall 0.9917, auc 0.9917
epoch 1101, loss 0.0182, train acc 99.25%, f1 0.9926, precision 0.9924, recall 0.9927, auc 0.9925
epoch 1201, loss 0.0199, train acc 99.32%, f1 0.9932, precision 0.9930, recall 0.9935, auc 0.9932
epoch 1301, loss 0.0224, train acc 99.38%, f1 0.9938, precision 0.9935, recall 0.9941, auc 0.9938
epoch 1401, loss 0.0121, train acc 99.41%, f1 0.9942, precision 0.9940, recall 0.9943, auc 0.9941
epoch 1501, loss 0.0216, train acc 99.48%, f1 0.9948, precision 0.9946, recall 0.9950, auc 0.9948
epoch 1601, loss 0.0236, train acc 99.50%, f1 0.9950, precision 0.9948, recall 0.9952, auc 0.9950
epoch 1701, loss 0.0161, train acc 99.53%, f1 0.9953, precision 0.9951, recall 0.9955, auc 0.9953
epoch 1801, loss 0.0268, train acc 99.57%, f1 0.9957, precision 0.9955, recall 0.9959, auc 0.9957
epoch 1901, loss 0.0204, train acc 99.59%, f1 0.9959, precision 0.9958, recall 0.9960, auc 0.9959
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_1
./test_vehicle0/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9721153846153847

the Fscore is 0.9397590361445783

the precision is 0.9069767441860465

the recall is 0.975

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_1
----------------------



epoch 1, loss 0.7054, train acc 23.52%, f1 0.3808, precision 0.2352, recall 1.0000, auc 0.5000
epoch 101, loss 0.4680, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4111, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3711, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3371, train acc 76.63%, f1 0.0125, precision 1.0000, recall 0.0063, auc 0.5031
epoch 501, loss 0.3076, train acc 77.22%, f1 0.0833, precision 0.7778, recall 0.0440, auc 0.5201
epoch 601, loss 0.2820, train acc 78.99%, f1 0.2198, precision 0.8696, recall 0.1258, auc 0.5600
epoch 701, loss 0.2595, train acc 81.51%, f1 0.3781, precision 0.9048, recall 0.2390, auc 0.6156
epoch 801, loss 0.2394, train acc 86.09%, f1 0.6017, precision 0.9221, recall 0.4465, auc 0.7175
epoch 901, loss 0.2215, train acc 89.05%, f1 0.7132, precision 0.9293, recall 0.5786, auc 0.7825
epoch 1001, loss 0.2056, train acc 92.01%, f1 0.8058, precision 0.9412, recall 0.7044, auc 0.8454
epoch 1101, loss 0.1914, train acc 94.23%, f1 0.8669, precision 0.9478, recall 0.7987, auc 0.8926
epoch 1201, loss 0.1787, train acc 94.97%, f1 0.8874, precision 0.9371, recall 0.8428, auc 0.9127
epoch 1301, loss 0.1674, train acc 96.45%, f1 0.9226, precision 0.9470, recall 0.8994, auc 0.9419
epoch 1401, loss 0.1570, train acc 96.89%, f1 0.9333, precision 0.9423, recall 0.9245, auc 0.9536
epoch 1501, loss 0.1474, train acc 97.04%, f1 0.9367, precision 0.9427, recall 0.9308, auc 0.9567
epoch 1601, loss 0.1385, train acc 97.49%, f1 0.9460, precision 0.9551, recall 0.9371, auc 0.9618
epoch 1701, loss 0.1303, train acc 97.78%, f1 0.9527, precision 0.9557, recall 0.9497, auc 0.9681
epoch 1801, loss 0.1226, train acc 98.22%, f1 0.9623, precision 0.9623, recall 0.9623, auc 0.9753
epoch 1901, loss 0.1154, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 2001, loss 0.1088, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 2101, loss 0.1026, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2201, loss 0.0968, train acc 98.67%, f1 0.9720, precision 0.9630, recall 0.9811, auc 0.9848
epoch 2301, loss 0.0913, train acc 98.67%, f1 0.9720, precision 0.9630, recall 0.9811, auc 0.9848
epoch 2401, loss 0.0863, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2501, loss 0.0815, train acc 98.96%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2601, loss 0.0770, train acc 98.96%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2701, loss 0.0728, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2801, loss 0.0689, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2901, loss 0.0653, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3001, loss 0.0619, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3101, loss 0.0586, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3201, loss 0.0556, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3301, loss 0.0528, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3401, loss 0.0501, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3501, loss 0.0476, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3601, loss 0.0452, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3701, loss 0.0430, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3801, loss 0.0408, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3901, loss 0.0388, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0369, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0351, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0333, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0317, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0301, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0286, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0271, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0257, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0244, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0232, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0220, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0209, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0198, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0188, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0179, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0169, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0161, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0152, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0145, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0137, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0130, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0123, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0117, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0111, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0105, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0100, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0094, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0089, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0085, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0080, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0076, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0072, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0069, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0065, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0062, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0059, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0050, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0045, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0043, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8201, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0035, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0030, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_normal_20000
normal
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_1
./test_vehicle0/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.9961538461538462

the Fscore is 0.9876543209876543

the precision is 0.975609756097561

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_1
----------------------



epoch 1, loss 0.6984, train acc 50.15%, f1 0.4807, precision 0.3184, recall 0.9811, auc 0.6675
epoch 101, loss 0.4661, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4101, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3701, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3361, train acc 76.63%, f1 0.0125, precision 1.0000, recall 0.0063, auc 0.5031
epoch 501, loss 0.3067, train acc 77.22%, f1 0.0833, precision 0.7778, recall 0.0440, auc 0.5201
epoch 601, loss 0.2811, train acc 78.99%, f1 0.2198, precision 0.8696, recall 0.1258, auc 0.5600
epoch 701, loss 0.2587, train acc 81.80%, f1 0.3941, precision 0.9091, recall 0.2516, auc 0.6219
epoch 801, loss 0.2387, train acc 86.24%, f1 0.6076, precision 0.9231, recall 0.4528, auc 0.7206
epoch 901, loss 0.2208, train acc 89.05%, f1 0.7132, precision 0.9293, recall 0.5786, auc 0.7825
epoch 1001, loss 0.2050, train acc 92.16%, f1 0.8100, precision 0.9417, recall 0.7107, auc 0.8486
epoch 1101, loss 0.1908, train acc 94.23%, f1 0.8669, precision 0.9478, recall 0.7987, auc 0.8926
epoch 1201, loss 0.1782, train acc 95.12%, f1 0.8904, precision 0.9437, recall 0.8428, auc 0.9136
epoch 1301, loss 0.1670, train acc 96.30%, f1 0.9191, precision 0.9467, recall 0.8931, auc 0.9388
epoch 1401, loss 0.1568, train acc 97.04%, f1 0.9367, precision 0.9427, recall 0.9308, auc 0.9567
epoch 1501, loss 0.1474, train acc 97.19%, f1 0.9397, precision 0.9487, recall 0.9308, auc 0.9577
epoch 1601, loss 0.1388, train acc 97.34%, f1 0.9430, precision 0.9490, recall 0.9371, auc 0.9608
epoch 1701, loss 0.1308, train acc 97.78%, f1 0.9527, precision 0.9557, recall 0.9497, auc 0.9681
epoch 1801, loss 0.1233, train acc 98.08%, f1 0.9592, precision 0.9563, recall 0.9623, auc 0.9744
epoch 1901, loss 0.1161, train acc 98.37%, f1 0.9655, precision 0.9625, recall 0.9686, auc 0.9785
epoch 2001, loss 0.1095, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 2101, loss 0.1034, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2201, loss 0.0975, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2301, loss 0.0919, train acc 98.96%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2401, loss 0.0868, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2501, loss 0.0819, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2601, loss 0.0774, train acc 98.96%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2701, loss 0.0732, train acc 98.96%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2801, loss 0.0692, train acc 98.96%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2901, loss 0.0655, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 3001, loss 0.0619, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 3101, loss 0.0587, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3201, loss 0.0556, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3301, loss 0.0527, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3401, loss 0.0500, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3501, loss 0.0475, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3601, loss 0.0451, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3701, loss 0.0428, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3801, loss 0.0407, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3901, loss 0.0386, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0367, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0348, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0331, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0314, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0298, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0283, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0269, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0255, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0242, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0230, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0218, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0207, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0196, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0186, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0177, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0168, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0159, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0151, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0143, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0136, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0129, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0122, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0116, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0110, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0104, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0099, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0093, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0089, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0084, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0080, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0076, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0072, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0068, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0065, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0061, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0058, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0055, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0050, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0047, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0045, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0043, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0035, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0033, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0030, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_normal_15000
normal
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_1
./test_vehicle0/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.9923076923076923

the Fscore is 0.975609756097561

the precision is 0.9523809523809523

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_1
----------------------



epoch 1, loss 0.6990, train acc 23.52%, f1 0.3808, precision 0.2352, recall 1.0000, auc 0.5000
epoch 101, loss 0.4672, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4109, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3709, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3368, train acc 76.63%, f1 0.0125, precision 1.0000, recall 0.0063, auc 0.5031
epoch 501, loss 0.3073, train acc 77.22%, f1 0.0833, precision 0.7778, recall 0.0440, auc 0.5201
epoch 601, loss 0.2815, train acc 78.99%, f1 0.2198, precision 0.8696, recall 0.1258, auc 0.5600
epoch 701, loss 0.2589, train acc 81.80%, f1 0.3941, precision 0.9091, recall 0.2516, auc 0.6219
epoch 801, loss 0.2388, train acc 86.24%, f1 0.6076, precision 0.9231, recall 0.4528, auc 0.7206
epoch 901, loss 0.2209, train acc 89.20%, f1 0.7181, precision 0.9300, recall 0.5849, auc 0.7857
epoch 1001, loss 0.2049, train acc 92.16%, f1 0.8100, precision 0.9417, recall 0.7107, auc 0.8486
epoch 1101, loss 0.1908, train acc 94.23%, f1 0.8678, precision 0.9412, recall 0.8050, auc 0.8948
epoch 1201, loss 0.1782, train acc 95.27%, f1 0.8947, precision 0.9379, recall 0.8553, auc 0.9190
epoch 1301, loss 0.1669, train acc 96.30%, f1 0.9196, precision 0.9408, recall 0.8994, auc 0.9410
epoch 1401, loss 0.1566, train acc 96.89%, f1 0.9333, precision 0.9423, recall 0.9245, auc 0.9536
epoch 1501, loss 0.1472, train acc 97.04%, f1 0.9367, precision 0.9427, recall 0.9308, auc 0.9567
epoch 1601, loss 0.1382, train acc 97.49%, f1 0.9464, precision 0.9494, recall 0.9434, auc 0.9640
epoch 1701, loss 0.1300, train acc 97.63%, f1 0.9497, precision 0.9497, recall 0.9497, auc 0.9671
epoch 1801, loss 0.1224, train acc 98.08%, f1 0.9590, precision 0.9620, recall 0.9560, auc 0.9722
epoch 1901, loss 0.1154, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 2001, loss 0.1088, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 2101, loss 0.1027, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2201, loss 0.0970, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2301, loss 0.0918, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2401, loss 0.0869, train acc 98.67%, f1 0.9720, precision 0.9630, recall 0.9811, auc 0.9848
epoch 2501, loss 0.0823, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2601, loss 0.0780, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2701, loss 0.0740, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2801, loss 0.0702, train acc 98.96%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2901, loss 0.0667, train acc 98.96%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 3001, loss 0.0634, train acc 98.96%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 3101, loss 0.0600, train acc 99.26%, f1 0.9845, precision 0.9695, recall 1.0000, auc 0.9952
epoch 3201, loss 0.0571, train acc 99.26%, f1 0.9845, precision 0.9695, recall 1.0000, auc 0.9952
epoch 3301, loss 0.0543, train acc 99.26%, f1 0.9845, precision 0.9695, recall 1.0000, auc 0.9952
epoch 3401, loss 0.0516, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3501, loss 0.0491, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3601, loss 0.0465, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3701, loss 0.0442, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3801, loss 0.0419, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3901, loss 0.0398, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0377, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0358, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0341, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0324, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0307, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0292, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0277, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0263, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0250, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0237, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0225, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0214, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0203, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0193, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0183, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0174, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0165, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0156, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0148, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0141, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0133, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0126, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0119, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0113, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0107, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0101, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0096, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0091, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0086, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0082, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0077, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0073, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0070, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0066, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0063, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0059, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0054, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0051, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0044, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0042, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0038, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0036, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0031, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_normal_10000
normal
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_1
./test_vehicle0/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.9961538461538462

the Fscore is 0.9876543209876543

the precision is 0.975609756097561

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_1
----------------------



epoch 1, loss 0.7015, train acc 23.52%, f1 0.3808, precision 0.2352, recall 1.0000, auc 0.5000
epoch 101, loss 0.4669, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4104, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3703, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3363, train acc 76.63%, f1 0.0125, precision 1.0000, recall 0.0063, auc 0.5031
epoch 501, loss 0.3069, train acc 77.22%, f1 0.0833, precision 0.7778, recall 0.0440, auc 0.5201
epoch 601, loss 0.2813, train acc 78.99%, f1 0.2198, precision 0.8696, recall 0.1258, auc 0.5600
epoch 701, loss 0.2587, train acc 81.80%, f1 0.3941, precision 0.9091, recall 0.2516, auc 0.6219
epoch 801, loss 0.2387, train acc 86.24%, f1 0.6076, precision 0.9231, recall 0.4528, auc 0.7206
epoch 901, loss 0.2209, train acc 89.20%, f1 0.7181, precision 0.9300, recall 0.5849, auc 0.7857
epoch 1001, loss 0.2050, train acc 92.16%, f1 0.8100, precision 0.9417, recall 0.7107, auc 0.8486
epoch 1101, loss 0.1909, train acc 94.38%, f1 0.8707, precision 0.9481, recall 0.8050, auc 0.8957
epoch 1201, loss 0.1782, train acc 95.12%, f1 0.8904, precision 0.9437, recall 0.8428, auc 0.9136
epoch 1301, loss 0.1669, train acc 96.30%, f1 0.9191, precision 0.9467, recall 0.8931, auc 0.9388
epoch 1401, loss 0.1566, train acc 96.89%, f1 0.9333, precision 0.9423, recall 0.9245, auc 0.9536
epoch 1501, loss 0.1471, train acc 97.19%, f1 0.9397, precision 0.9487, recall 0.9308, auc 0.9577
epoch 1601, loss 0.1384, train acc 97.34%, f1 0.9430, precision 0.9490, recall 0.9371, auc 0.9608
epoch 1701, loss 0.1303, train acc 97.78%, f1 0.9527, precision 0.9557, recall 0.9497, auc 0.9681
epoch 1801, loss 0.1227, train acc 98.08%, f1 0.9592, precision 0.9563, recall 0.9623, auc 0.9744
epoch 1901, loss 0.1156, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 2001, loss 0.1089, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 2101, loss 0.1028, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2201, loss 0.0970, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2301, loss 0.0916, train acc 98.67%, f1 0.9720, precision 0.9630, recall 0.9811, auc 0.9848
epoch 2401, loss 0.0865, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2501, loss 0.0818, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2601, loss 0.0774, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2701, loss 0.0733, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2801, loss 0.0695, train acc 98.96%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2901, loss 0.0659, train acc 98.96%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 3001, loss 0.0625, train acc 98.96%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 3101, loss 0.0594, train acc 99.11%, f1 0.9814, precision 0.9693, recall 0.9937, auc 0.9920
epoch 3201, loss 0.0564, train acc 99.26%, f1 0.9845, precision 0.9695, recall 1.0000, auc 0.9952
epoch 3301, loss 0.0535, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3401, loss 0.0509, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3501, loss 0.0484, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3601, loss 0.0460, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3701, loss 0.0437, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3801, loss 0.0416, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3901, loss 0.0395, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0376, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0357, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0340, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0323, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0307, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0292, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0277, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0263, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0250, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0237, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0225, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0214, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0203, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0193, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0183, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0173, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0164, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0156, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0148, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0140, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0133, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0126, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0119, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0113, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0107, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0102, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0096, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0091, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0087, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0082, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0078, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0074, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0070, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0067, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0063, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0060, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0057, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0054, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0051, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0049, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_normal_8000
normal
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_1
./test_vehicle0/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.9923076923076923

the Fscore is 0.975609756097561

the precision is 0.9523809523809523

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_1
----------------------



epoch 1, loss 0.6891, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4648, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4094, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3694, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3352, train acc 76.63%, f1 0.0125, precision 1.0000, recall 0.0063, auc 0.5031
epoch 501, loss 0.3055, train acc 77.22%, f1 0.0833, precision 0.7778, recall 0.0440, auc 0.5201
epoch 601, loss 0.2797, train acc 79.29%, f1 0.2391, precision 0.8800, recall 0.1384, auc 0.5663
epoch 701, loss 0.2569, train acc 82.25%, f1 0.4175, precision 0.9149, recall 0.2704, auc 0.6314
epoch 801, loss 0.2368, train acc 86.24%, f1 0.6076, precision 0.9231, recall 0.4528, auc 0.7206
epoch 901, loss 0.2189, train acc 89.94%, f1 0.7424, precision 0.9333, recall 0.6164, auc 0.8014
epoch 1001, loss 0.2031, train acc 92.60%, f1 0.8227, precision 0.9431, recall 0.7296, auc 0.8580
epoch 1101, loss 0.1890, train acc 94.38%, f1 0.8716, precision 0.9416, recall 0.8113, auc 0.8979
epoch 1201, loss 0.1765, train acc 95.27%, f1 0.8947, precision 0.9379, recall 0.8553, auc 0.9190
epoch 1301, loss 0.1653, train acc 96.75%, f1 0.9299, precision 0.9419, recall 0.9182, auc 0.9504
epoch 1401, loss 0.1551, train acc 97.04%, f1 0.9367, precision 0.9427, recall 0.9308, auc 0.9567
epoch 1501, loss 0.1456, train acc 97.04%, f1 0.9367, precision 0.9427, recall 0.9308, auc 0.9567
epoch 1601, loss 0.1368, train acc 97.49%, f1 0.9464, precision 0.9494, recall 0.9434, auc 0.9640
epoch 1701, loss 0.1287, train acc 97.78%, f1 0.9527, precision 0.9557, recall 0.9497, auc 0.9681
epoch 1801, loss 0.1211, train acc 98.37%, f1 0.9655, precision 0.9625, recall 0.9686, auc 0.9785
epoch 1901, loss 0.1140, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
epoch 2001, loss 0.1073, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
epoch 2101, loss 0.1011, train acc 98.82%, f1 0.9748, precision 0.9748, recall 0.9748, auc 0.9836
epoch 2201, loss 0.0953, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2301, loss 0.0898, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2401, loss 0.0847, train acc 98.96%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2501, loss 0.0799, train acc 98.96%, f1 0.9781, precision 0.9750, recall 0.9811, auc 0.9867
epoch 2601, loss 0.0754, train acc 99.11%, f1 0.9811, precision 0.9811, recall 0.9811, auc 0.9877
epoch 2701, loss 0.0711, train acc 99.11%, f1 0.9811, precision 0.9811, recall 0.9811, auc 0.9877
epoch 2801, loss 0.0672, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 2901, loss 0.0636, train acc 99.26%, f1 0.9843, precision 0.9812, recall 0.9874, auc 0.9908
epoch 3001, loss 0.0602, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 3101, loss 0.0571, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3201, loss 0.0542, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3301, loss 0.0514, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3401, loss 0.0488, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3501, loss 0.0463, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3601, loss 0.0440, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3701, loss 0.0418, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3801, loss 0.0398, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3901, loss 0.0378, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 4001, loss 0.0359, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0341, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0324, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0308, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0293, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0278, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0264, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0251, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0238, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0226, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_normal_5000
normal
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_1
./test_vehicle0/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.9961538461538462

the Fscore is 0.9876543209876543

the precision is 0.975609756097561

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_1
----------------------



epoch 1, loss 0.6895, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4638, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4086, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3687, train acc 76.48%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3346, train acc 76.63%, f1 0.0125, precision 1.0000, recall 0.0063, auc 0.5031
epoch 501, loss 0.3050, train acc 77.22%, f1 0.0833, precision 0.7778, recall 0.0440, auc 0.5201
epoch 601, loss 0.2793, train acc 79.29%, f1 0.2391, precision 0.8800, recall 0.1384, auc 0.5663
epoch 701, loss 0.2567, train acc 82.25%, f1 0.4175, precision 0.9149, recall 0.2704, auc 0.6314
epoch 801, loss 0.2367, train acc 86.24%, f1 0.6076, precision 0.9231, recall 0.4528, auc 0.7206
epoch 901, loss 0.2189, train acc 89.94%, f1 0.7424, precision 0.9333, recall 0.6164, auc 0.8014
epoch 1001, loss 0.2031, train acc 92.31%, f1 0.8143, precision 0.9421, recall 0.7170, auc 0.8517
epoch 1101, loss 0.1890, train acc 94.38%, f1 0.8716, precision 0.9416, recall 0.8113, auc 0.8979
epoch 1201, loss 0.1764, train acc 95.12%, f1 0.8911, precision 0.9375, recall 0.8491, auc 0.9158
epoch 1301, loss 0.1651, train acc 96.60%, f1 0.9265, precision 0.9416, recall 0.9119, auc 0.9473
epoch 1401, loss 0.1548, train acc 97.04%, f1 0.9367, precision 0.9427, recall 0.9308, auc 0.9567
epoch 1501, loss 0.1454, train acc 97.04%, f1 0.9367, precision 0.9427, recall 0.9308, auc 0.9567
epoch 1601, loss 0.1367, train acc 97.49%, f1 0.9464, precision 0.9494, recall 0.9434, auc 0.9640
epoch 1701, loss 0.1286, train acc 97.63%, f1 0.9497, precision 0.9497, recall 0.9497, auc 0.9671
epoch 1801, loss 0.1210, train acc 98.08%, f1 0.9592, precision 0.9563, recall 0.9623, auc 0.9744
epoch 1901, loss 0.1139, train acc 98.52%, f1 0.9687, precision 0.9627, recall 0.9748, auc 0.9816
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_1.csv
./test_vehicle0/standlization_data/vehicle0_std_test_1.csv
MLP_normal_2000
normal
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_1
./test_vehicle0/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.9836538461538462

the Fscore is 0.975

the precision is 0.975

the recall is 0.975

Done
./test_yeast3/standlization_data/yeast3_std_train_1.csv
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.3810, train acc 97.88%, f1 0.9787, precision 0.9791, recall 0.9784, auc 0.9788
epoch 201, loss 0.1461, train acc 98.44%, f1 0.9844, precision 0.9845, recall 0.9843, auc 0.9844
epoch 301, loss 0.0821, train acc 98.66%, f1 0.9866, precision 0.9867, recall 0.9865, auc 0.9866
epoch 401, loss 0.0602, train acc 98.71%, f1 0.9871, precision 0.9871, recall 0.9870, auc 0.9871
epoch 501, loss 0.0493, train acc 98.75%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9875
epoch 601, loss 0.0433, train acc 98.78%, f1 0.9878, precision 0.9877, recall 0.9878, auc 0.9878
epoch 701, loss 0.0394, train acc 98.78%, f1 0.9878, precision 0.9878, recall 0.9878, auc 0.9878
epoch 801, loss 0.0372, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9879, auc 0.9879
epoch 901, loss 0.0345, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 1001, loss 0.0343, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9881, auc 0.9880
epoch 1101, loss 0.0336, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9880, auc 0.9879
epoch 1201, loss 0.0311, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9879, auc 0.9879
epoch 1301, loss 0.0322, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9879, auc 0.9879
epoch 1401, loss 0.0311, train acc 98.79%, f1 0.9879, precision 0.9878, recall 0.9879, auc 0.9879
epoch 1501, loss 0.0317, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9879, auc 0.9879
epoch 1601, loss 0.0305, train acc 98.78%, f1 0.9878, precision 0.9879, recall 0.9878, auc 0.9878
epoch 1701, loss 0.0291, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9879, auc 0.9879
epoch 1801, loss 0.0310, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9879, auc 0.9879
epoch 1901, loss 0.0303, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9880, auc 0.9879
epoch 2001, loss 0.0282, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9879, auc 0.9879
epoch 2101, loss 0.0299, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 2201, loss 0.0280, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 2301, loss 0.0301, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9880, auc 0.9879
epoch 2401, loss 0.0290, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 2501, loss 0.0294, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 2601, loss 0.0295, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 2701, loss 0.0289, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9879, auc 0.9880
epoch 2801, loss 0.0287, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9881, auc 0.9880
epoch 2901, loss 0.0279, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 3001, loss 0.0280, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 3101, loss 0.0273, train acc 98.81%, f1 0.9881, precision 0.9881, recall 0.9881, auc 0.9881
epoch 3201, loss 0.0251, train acc 98.81%, f1 0.9881, precision 0.9881, recall 0.9881, auc 0.9881
epoch 3301, loss 0.0259, train acc 98.81%, f1 0.9881, precision 0.9881, recall 0.9881, auc 0.9881
epoch 3401, loss 0.0269, train acc 98.81%, f1 0.9881, precision 0.9881, recall 0.9881, auc 0.9881
epoch 3501, loss 0.0267, train acc 98.81%, f1 0.9881, precision 0.9881, recall 0.9881, auc 0.9881
epoch 3601, loss 0.0269, train acc 98.82%, f1 0.9882, precision 0.9882, recall 0.9882, auc 0.9882
epoch 3701, loss 0.0250, train acc 98.82%, f1 0.9882, precision 0.9882, recall 0.9882, auc 0.9882
epoch 3801, loss 0.0264, train acc 98.82%, f1 0.9882, precision 0.9882, recall 0.9882, auc 0.9882
epoch 3901, loss 0.0264, train acc 98.83%, f1 0.9883, precision 0.9883, recall 0.9882, auc 0.9883
epoch 4001, loss 0.0252, train acc 98.83%, f1 0.9883, precision 0.9883, recall 0.9883, auc 0.9883
epoch 4101, loss 0.0258, train acc 98.85%, f1 0.9885, precision 0.9885, recall 0.9884, auc 0.9885
epoch 4201, loss 0.0249, train acc 98.86%, f1 0.9886, precision 0.9886, recall 0.9887, auc 0.9886
epoch 4301, loss 0.0249, train acc 98.87%, f1 0.9887, precision 0.9887, recall 0.9887, auc 0.9887
epoch 4401, loss 0.0249, train acc 98.87%, f1 0.9887, precision 0.9887, recall 0.9888, auc 0.9887
epoch 4501, loss 0.0237, train acc 98.89%, f1 0.9889, precision 0.9889, recall 0.9889, auc 0.9889
epoch 4601, loss 0.0241, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9890, auc 0.9890
epoch 4701, loss 0.0229, train acc 98.92%, f1 0.9892, precision 0.9891, recall 0.9892, auc 0.9892
epoch 4801, loss 0.0234, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9894, auc 0.9894
epoch 4901, loss 0.0235, train acc 98.95%, f1 0.9895, precision 0.9895, recall 0.9895, auc 0.9895
epoch 5001, loss 0.0230, train acc 98.97%, f1 0.9897, precision 0.9897, recall 0.9897, auc 0.9897
epoch 5101, loss 0.0222, train acc 98.97%, f1 0.9897, precision 0.9896, recall 0.9898, auc 0.9897
epoch 5201, loss 0.0218, train acc 98.99%, f1 0.9899, precision 0.9898, recall 0.9899, auc 0.9899
epoch 5301, loss 0.0218, train acc 98.99%, f1 0.9899, precision 0.9900, recall 0.9899, auc 0.9899
epoch 5401, loss 0.0212, train acc 99.01%, f1 0.9901, precision 0.9901, recall 0.9900, auc 0.9901
epoch 5501, loss 0.0214, train acc 99.02%, f1 0.9902, precision 0.9902, recall 0.9902, auc 0.9902
epoch 5601, loss 0.0196, train acc 99.04%, f1 0.9904, precision 0.9903, recall 0.9904, auc 0.9904
epoch 5701, loss 0.0200, train acc 99.07%, f1 0.9907, precision 0.9907, recall 0.9906, auc 0.9907
epoch 5801, loss 0.0201, train acc 99.10%, f1 0.9910, precision 0.9910, recall 0.9910, auc 0.9910
epoch 5901, loss 0.0188, train acc 99.13%, f1 0.9913, precision 0.9913, recall 0.9913, auc 0.9913
epoch 6001, loss 0.0188, train acc 99.15%, f1 0.9915, precision 0.9915, recall 0.9916, auc 0.9915
epoch 6101, loss 0.0179, train acc 99.19%, f1 0.9919, precision 0.9920, recall 0.9919, auc 0.9919
epoch 6201, loss 0.0182, train acc 99.23%, f1 0.9923, precision 0.9923, recall 0.9924, auc 0.9923
epoch 6301, loss 0.0166, train acc 99.26%, f1 0.9926, precision 0.9927, recall 0.9925, auc 0.9926
epoch 6401, loss 0.0169, train acc 99.29%, f1 0.9929, precision 0.9930, recall 0.9928, auc 0.9929
epoch 6501, loss 0.0151, train acc 99.32%, f1 0.9932, precision 0.9932, recall 0.9931, auc 0.9932
epoch 6601, loss 0.0157, train acc 99.34%, f1 0.9934, precision 0.9935, recall 0.9933, auc 0.9934
epoch 6701, loss 0.0140, train acc 99.37%, f1 0.9937, precision 0.9938, recall 0.9937, auc 0.9937
epoch 6801, loss 0.0144, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9940, auc 0.9941
epoch 6901, loss 0.0138, train acc 99.45%, f1 0.9945, precision 0.9945, recall 0.9944, auc 0.9945
epoch 7001, loss 0.0122, train acc 99.48%, f1 0.9948, precision 0.9949, recall 0.9947, auc 0.9948
epoch 7101, loss 0.0127, train acc 99.50%, f1 0.9950, precision 0.9951, recall 0.9950, auc 0.9950
epoch 7201, loss 0.0122, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9954, auc 0.9955
epoch 7301, loss 0.0115, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 7401, loss 0.0112, train acc 99.58%, f1 0.9958, precision 0.9959, recall 0.9958, auc 0.9958
epoch 7501, loss 0.0109, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 7601, loss 0.0104, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 7701, loss 0.0101, train acc 99.62%, f1 0.9962, precision 0.9963, recall 0.9961, auc 0.9962
epoch 7801, loss 0.0094, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 7901, loss 0.0093, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9964, auc 0.9964
epoch 8001, loss 0.0083, train acc 99.65%, f1 0.9965, precision 0.9966, recall 0.9964, auc 0.9965
epoch 8101, loss 0.0085, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 8201, loss 0.0082, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9967, auc 0.9967
epoch 8301, loss 0.0081, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 8401, loss 0.0078, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 8501, loss 0.0075, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 8601, loss 0.0072, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 8701, loss 0.0070, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9973
epoch 8801, loss 0.0068, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 8901, loss 0.0064, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 9001, loss 0.0064, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 9101, loss 0.0062, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9979, auc 0.9978
epoch 9201, loss 0.0058, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 9301, loss 0.0056, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9980
epoch 9401, loss 0.0048, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 9501, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 9601, loss 0.0051, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 9701, loss 0.0044, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 9801, loss 0.0047, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 9901, loss 0.0042, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 10001, loss 0.0043, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 10101, loss 0.0042, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 10201, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 10301, loss 0.0037, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 10401, loss 0.0037, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10501, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 10601, loss 0.0030, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10701, loss 0.0032, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10801, loss 0.0031, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10901, loss 0.0029, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 11001, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 11101, loss 0.0027, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 11201, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 11301, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11401, loss 0.0024, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 11501, loss 0.0021, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 11601, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 11701, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 11801, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 11901, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12001, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12101, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12201, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 12301, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12401, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12501, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12601, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12701, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 12801, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 12901, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 13001, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 13101, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 13201, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 13301, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 13401, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13501, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13601, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13701, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13801, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13901, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14001, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14101, loss 0.0007, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_yeast3/standlization_data/yeast3_std_train_1.csv
./test_yeast3/standlization_data/yeast3_std_test_1.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_1
./test_yeast3/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.625

the Fscore is 0.4

the precision is 1.0

the recall is 0.25

Done
train_mlp_2_2.sh: line 433: 21676 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=yeast3 dataset_index=1 record_index=1 device_id=2 train_method=MLP_concat_Mirror_15000
