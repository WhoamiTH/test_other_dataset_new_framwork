nohup: ignoring input
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_1
----------------------



epoch 1, loss 0.6933, train acc 49.75%, f1 0.6644, precision 0.4987, recall 0.9950, auc 0.4975
Validation loss decreased (inf --> 0.693169).  Saving model ...
Validation loss decreased (0.693169 --> 0.693167).  Saving model ...
Validation loss decreased (0.693167 --> 0.693163).  Saving model ...
Validation loss decreased (0.693163 --> 0.693155).  Saving model ...
Validation loss decreased (0.693155 --> 0.693142).  Saving model ...
Validation loss decreased (0.693142 --> 0.693128).  Saving model ...
Validation loss decreased (0.693128 --> 0.693115).  Saving model ...
Validation loss decreased (0.693115 --> 0.693105).  Saving model ...
Validation loss decreased (0.693105 --> 0.693094).  Saving model ...
Validation loss decreased (0.693094 --> 0.693084).  Saving model ...
Validation loss decreased (0.693084 --> 0.693075).  Saving model ...
Validation loss decreased (0.693075 --> 0.693065).  Saving model ...
Validation loss decreased (0.693065 --> 0.693054).  Saving model ...
Validation loss decreased (0.693054 --> 0.693035).  Saving model ...
Validation loss decreased (0.693035 --> 0.693014).  Saving model ...
Validation loss decreased (0.693014 --> 0.692994).  Saving model ...
Validation loss decreased (0.692994 --> 0.692972).  Saving model ...
Validation loss decreased (0.692972 --> 0.692951).  Saving model ...
Validation loss decreased (0.692951 --> 0.692925).  Saving model ...
Validation loss decreased (0.692925 --> 0.692892).  Saving model ...
Validation loss decreased (0.692892 --> 0.692857).  Saving model ...
Validation loss decreased (0.692857 --> 0.692826).  Saving model ...
Validation loss decreased (0.692826 --> 0.692796).  Saving model ...
Validation loss decreased (0.692796 --> 0.692769).  Saving model ...
Validation loss decreased (0.692769 --> 0.692733).  Saving model ...
Validation loss decreased (0.692733 --> 0.692700).  Saving model ...
Validation loss decreased (0.692700 --> 0.692652).  Saving model ...
Validation loss decreased (0.692652 --> 0.692604).  Saving model ...
Validation loss decreased (0.692604 --> 0.692548).  Saving model ...
Validation loss decreased (0.692548 --> 0.692485).  Saving model ...
Validation loss decreased (0.692485 --> 0.692420).  Saving model ...
Validation loss decreased (0.692420 --> 0.692357).  Saving model ...
Validation loss decreased (0.692357 --> 0.692302).  Saving model ...
Validation loss decreased (0.692302 --> 0.692235).  Saving model ...
Validation loss decreased (0.692235 --> 0.692169).  Saving model ...
Validation loss decreased (0.692169 --> 0.692113).  Saving model ...
Validation loss decreased (0.692113 --> 0.692072).  Saving model ...
Validation loss decreased (0.692072 --> 0.692045).  Saving model ...
Validation loss decreased (0.692045 --> 0.692029).  Saving model ...
Validation loss decreased (0.692029 --> 0.692017).  Saving model ...
Validation loss decreased (0.692017 --> 0.692007).  Saving model ...
Validation loss decreased (0.692007 --> 0.691990).  Saving model ...
Validation loss decreased (0.691990 --> 0.691969).  Saving model ...
Validation loss decreased (0.691969 --> 0.691939).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
Validation loss decreased (0.691939 --> 0.691938).  Saving model ...
Validation loss decreased (0.691938 --> 0.691916).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 69, loss 0.6142, train acc 55.25%, f1 0.5536, precision 0.5522, recall 0.5550, auc 0.5525



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_True
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_1
./test_pima/result_MLP_concat_Mirror_True_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.56

the Fscore is 0.5510204081632654

the precision is 0.38028169014084506

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_1
----------------------



epoch 1, loss 0.6931, train acc 50.11%, f1 0.6671, precision 0.5005, recall 1.0000, auc 0.5011
epoch 101, loss 0.5816, train acc 78.37%, f1 0.7762, precision 0.8041, recall 0.7502, auc 0.7837
epoch 201, loss 0.4693, train acc 81.46%, f1 0.8148, precision 0.8141, recall 0.8154, auc 0.8146
epoch 301, loss 0.4450, train acc 82.76%, f1 0.8284, precision 0.8247, recall 0.8322, auc 0.8276
epoch 401, loss 0.3361, train acc 83.25%, f1 0.8334, precision 0.8287, recall 0.8383, auc 0.8325
epoch 501, loss 0.3638, train acc 83.37%, f1 0.8347, precision 0.8294, recall 0.8402, auc 0.8337
epoch 601, loss 0.3603, train acc 83.44%, f1 0.8353, precision 0.8308, recall 0.8399, auc 0.8344
epoch 701, loss 0.2999, train acc 83.42%, f1 0.8351, precision 0.8308, recall 0.8394, auc 0.8342
epoch 801, loss 0.5551, train acc 83.42%, f1 0.8350, precision 0.8308, recall 0.8393, auc 0.8342
epoch 901, loss 0.4305, train acc 83.46%, f1 0.8352, precision 0.8320, recall 0.8385, auc 0.8346
epoch 1001, loss 0.4704, train acc 83.41%, f1 0.8348, precision 0.8313, recall 0.8382, auc 0.8341
epoch 1101, loss 0.4381, train acc 83.46%, f1 0.8351, precision 0.8327, recall 0.8375, auc 0.8346
epoch 1201, loss 0.3584, train acc 83.48%, f1 0.8354, precision 0.8322, recall 0.8387, auc 0.8348
epoch 1301, loss 0.4559, train acc 83.47%, f1 0.8350, precision 0.8331, recall 0.8370, auc 0.8347
epoch 1401, loss 0.5430, train acc 83.46%, f1 0.8350, precision 0.8332, recall 0.8368, auc 0.8346
epoch 1501, loss 0.4798, train acc 83.46%, f1 0.8347, precision 0.8340, recall 0.8355, auc 0.8346
epoch 1601, loss 0.2523, train acc 83.41%, f1 0.8344, precision 0.8329, recall 0.8360, auc 0.8341
epoch 1701, loss 0.2864, train acc 83.38%, f1 0.8341, precision 0.8327, recall 0.8354, auc 0.8338
epoch 1801, loss 0.3387, train acc 83.46%, f1 0.8347, precision 0.8344, recall 0.8350, auc 0.8346
epoch 1901, loss 0.3975, train acc 83.45%, f1 0.8347, precision 0.8340, recall 0.8353, auc 0.8345
epoch 2001, loss 0.4471, train acc 83.52%, f1 0.8353, precision 0.8351, recall 0.8354, auc 0.8352
epoch 2101, loss 0.3821, train acc 83.51%, f1 0.8353, precision 0.8343, recall 0.8363, auc 0.8351
epoch 2201, loss 0.4955, train acc 83.50%, f1 0.8353, precision 0.8342, recall 0.8364, auc 0.8350
epoch 2301, loss 0.3807, train acc 83.51%, f1 0.8350, precision 0.8353, recall 0.8346, auc 0.8351
epoch 2401, loss 0.3835, train acc 83.62%, f1 0.8364, precision 0.8355, recall 0.8373, auc 0.8362
epoch 2501, loss 0.2340, train acc 83.62%, f1 0.8364, precision 0.8357, recall 0.8371, auc 0.8362
epoch 2601, loss 0.4135, train acc 83.72%, f1 0.8372, precision 0.8372, recall 0.8372, auc 0.8372
epoch 2701, loss 0.4980, train acc 83.76%, f1 0.8377, precision 0.8372, recall 0.8382, auc 0.8376
epoch 2801, loss 0.3938, train acc 83.96%, f1 0.8397, precision 0.8390, recall 0.8404, auc 0.8396
epoch 2901, loss 0.3602, train acc 84.03%, f1 0.8404, precision 0.8398, recall 0.8409, auc 0.8403
epoch 3001, loss 0.3982, train acc 84.17%, f1 0.8420, precision 0.8405, recall 0.8434, auc 0.8417
epoch 3101, loss 0.2957, train acc 84.24%, f1 0.8424, precision 0.8423, recall 0.8424, auc 0.8424
epoch 3201, loss 0.3137, train acc 84.40%, f1 0.8440, precision 0.8443, recall 0.8437, auc 0.8440
epoch 3301, loss 0.3165, train acc 84.43%, f1 0.8445, precision 0.8434, recall 0.8457, auc 0.8443
epoch 3401, loss 0.3267, train acc 84.54%, f1 0.8456, precision 0.8446, recall 0.8466, auc 0.8454
epoch 3501, loss 0.3001, train acc 84.62%, f1 0.8463, precision 0.8458, recall 0.8468, auc 0.8462
epoch 3601, loss 0.3555, train acc 84.84%, f1 0.8488, precision 0.8464, recall 0.8512, auc 0.8484
epoch 3701, loss 0.2787, train acc 84.93%, f1 0.8493, precision 0.8496, recall 0.8489, auc 0.8493
epoch 3801, loss 0.3147, train acc 85.04%, f1 0.8504, precision 0.8501, recall 0.8507, auc 0.8504
epoch 3901, loss 0.2924, train acc 85.13%, f1 0.8513, precision 0.8513, recall 0.8513, auc 0.8513
epoch 4001, loss 0.3100, train acc 85.20%, f1 0.8521, precision 0.8516, recall 0.8526, auc 0.8520
epoch 4101, loss 0.4271, train acc 85.29%, f1 0.8528, precision 0.8531, recall 0.8526, auc 0.8529
epoch 4201, loss 0.2195, train acc 85.32%, f1 0.8533, precision 0.8527, recall 0.8540, auc 0.8532
epoch 4301, loss 0.3909, train acc 85.51%, f1 0.8550, precision 0.8558, recall 0.8542, auc 0.8551
epoch 4401, loss 0.2873, train acc 85.60%, f1 0.8559, precision 0.8564, recall 0.8555, auc 0.8560
epoch 4501, loss 0.3683, train acc 85.71%, f1 0.8572, precision 0.8567, recall 0.8578, auc 0.8571
epoch 4601, loss 0.1936, train acc 85.72%, f1 0.8570, precision 0.8585, recall 0.8554, auc 0.8572
epoch 4701, loss 0.3590, train acc 85.83%, f1 0.8581, precision 0.8593, recall 0.8568, auc 0.8583
epoch 4801, loss 0.1991, train acc 85.96%, f1 0.8595, precision 0.8605, recall 0.8584, auc 0.8596
epoch 4901, loss 0.3191, train acc 86.00%, f1 0.8598, precision 0.8609, recall 0.8587, auc 0.8600
epoch 5001, loss 0.2949, train acc 86.08%, f1 0.8606, precision 0.8616, recall 0.8597, auc 0.8608
epoch 5101, loss 0.3697, train acc 86.12%, f1 0.8611, precision 0.8616, recall 0.8606, auc 0.8612
epoch 5201, loss 0.3553, train acc 86.21%, f1 0.8620, precision 0.8625, recall 0.8616, auc 0.8621
epoch 5301, loss 0.3174, train acc 86.32%, f1 0.8630, precision 0.8644, recall 0.8615, auc 0.8632
epoch 5401, loss 0.2377, train acc 86.36%, f1 0.8633, precision 0.8654, recall 0.8611, auc 0.8636
epoch 5501, loss 0.2423, train acc 86.48%, f1 0.8646, precision 0.8657, recall 0.8636, auc 0.8648
epoch 5601, loss 0.2841, train acc 86.51%, f1 0.8650, precision 0.8656, recall 0.8643, auc 0.8651
epoch 5701, loss 0.2389, train acc 86.55%, f1 0.8653, precision 0.8665, recall 0.8641, auc 0.8655
epoch 5801, loss 0.3361, train acc 86.64%, f1 0.8663, precision 0.8670, recall 0.8655, auc 0.8664
epoch 5901, loss 0.3193, train acc 86.65%, f1 0.8663, precision 0.8675, recall 0.8650, auc 0.8665
epoch 6001, loss 0.3228, train acc 86.68%, f1 0.8668, precision 0.8669, recall 0.8666, auc 0.8668
epoch 6101, loss 0.3225, train acc 86.75%, f1 0.8675, precision 0.8677, recall 0.8673, auc 0.8675
epoch 6201, loss 0.3753, train acc 86.82%, f1 0.8679, precision 0.8696, recall 0.8662, auc 0.8682
epoch 6301, loss 0.3467, train acc 86.88%, f1 0.8687, precision 0.8698, recall 0.8676, auc 0.8688
epoch 6401, loss 0.2246, train acc 86.82%, f1 0.8679, precision 0.8699, recall 0.8660, auc 0.8682
epoch 6501, loss 0.2869, train acc 86.87%, f1 0.8685, precision 0.8696, recall 0.8674, auc 0.8687
epoch 6601, loss 0.3887, train acc 86.98%, f1 0.8696, precision 0.8708, recall 0.8684, auc 0.8698
epoch 6701, loss 0.3470, train acc 87.04%, f1 0.8702, precision 0.8714, recall 0.8690, auc 0.8704
epoch 6801, loss 0.2864, train acc 87.13%, f1 0.8712, precision 0.8716, recall 0.8709, auc 0.8713
epoch 6901, loss 0.4057, train acc 87.12%, f1 0.8711, precision 0.8721, recall 0.8700, auc 0.8712
epoch 7001, loss 0.3723, train acc 87.19%, f1 0.8720, precision 0.8713, recall 0.8727, auc 0.8719
epoch 7101, loss 0.3374, train acc 87.25%, f1 0.8723, precision 0.8741, recall 0.8704, auc 0.8725
epoch 7201, loss 0.2745, train acc 87.29%, f1 0.8726, precision 0.8743, recall 0.8710, auc 0.8729
epoch 7301, loss 0.3379, train acc 87.34%, f1 0.8733, precision 0.8741, recall 0.8724, auc 0.8734
epoch 7401, loss 0.2476, train acc 87.36%, f1 0.8733, precision 0.8755, recall 0.8712, auc 0.8736
epoch 7501, loss 0.2448, train acc 87.43%, f1 0.8742, precision 0.8752, recall 0.8731, auc 0.8743
epoch 7601, loss 0.3045, train acc 87.51%, f1 0.8752, precision 0.8747, recall 0.8756, auc 0.8751
epoch 7701, loss 0.2137, train acc 87.57%, f1 0.8754, precision 0.8774, recall 0.8734, auc 0.8757
epoch 7801, loss 0.2778, train acc 87.57%, f1 0.8756, precision 0.8766, recall 0.8745, auc 0.8757
epoch 7901, loss 0.3206, train acc 87.64%, f1 0.8765, precision 0.8756, recall 0.8774, auc 0.8764
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_1
./test_pima/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.7172222222222222

the Fscore is 0.6538461538461539

the precision is 0.5

the recall is 0.9444444444444444

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4975, train acc 78.23%, f1 0.7819, precision 0.7833, recall 0.7806, auc 0.7823
epoch 201, loss 0.2956, train acc 81.54%, f1 0.8154, precision 0.8152, recall 0.8156, auc 0.8154
epoch 301, loss 0.3920, train acc 82.90%, f1 0.8290, precision 0.8288, recall 0.8292, auc 0.8290
epoch 401, loss 0.4923, train acc 83.19%, f1 0.8319, precision 0.8318, recall 0.8320, auc 0.8319
epoch 501, loss 0.3445, train acc 83.28%, f1 0.8329, precision 0.8325, recall 0.8332, auc 0.8328
epoch 601, loss 0.4699, train acc 83.33%, f1 0.8334, precision 0.8332, recall 0.8335, auc 0.8333
epoch 701, loss 0.3847, train acc 83.39%, f1 0.8340, precision 0.8338, recall 0.8341, auc 0.8339
epoch 801, loss 0.3194, train acc 83.42%, f1 0.8343, precision 0.8339, recall 0.8346, auc 0.8342
epoch 901, loss 0.3667, train acc 83.47%, f1 0.8348, precision 0.8345, recall 0.8350, auc 0.8347
epoch 1001, loss 0.2964, train acc 83.40%, f1 0.8341, precision 0.8340, recall 0.8341, auc 0.8340
epoch 1101, loss 0.5371, train acc 83.44%, f1 0.8344, precision 0.8341, recall 0.8348, auc 0.8344
epoch 1201, loss 0.4791, train acc 83.35%, f1 0.8336, precision 0.8331, recall 0.8342, auc 0.8335
epoch 1301, loss 0.4355, train acc 83.42%, f1 0.8342, precision 0.8339, recall 0.8346, auc 0.8342
epoch 1401, loss 0.2973, train acc 83.42%, f1 0.8343, precision 0.8338, recall 0.8349, auc 0.8342
epoch 1501, loss 0.4373, train acc 83.43%, f1 0.8344, precision 0.8339, recall 0.8348, auc 0.8343
epoch 1601, loss 0.3130, train acc 83.47%, f1 0.8349, precision 0.8341, recall 0.8356, auc 0.8347
epoch 1701, loss 0.3320, train acc 83.49%, f1 0.8350, precision 0.8345, recall 0.8355, auc 0.8349
epoch 1801, loss 0.3823, train acc 83.41%, f1 0.8343, precision 0.8334, recall 0.8352, auc 0.8341
epoch 1901, loss 0.3326, train acc 83.45%, f1 0.8346, precision 0.8339, recall 0.8352, auc 0.8345
epoch 2001, loss 0.3680, train acc 83.53%, f1 0.8354, precision 0.8347, recall 0.8361, auc 0.8353
epoch 2101, loss 0.3932, train acc 83.53%, f1 0.8355, precision 0.8346, recall 0.8364, auc 0.8353
epoch 2201, loss 0.3134, train acc 83.65%, f1 0.8366, precision 0.8360, recall 0.8371, auc 0.8365
epoch 2301, loss 0.2993, train acc 83.67%, f1 0.8368, precision 0.8363, recall 0.8373, auc 0.8367
epoch 2401, loss 0.2869, train acc 83.75%, f1 0.8377, precision 0.8367, recall 0.8387, auc 0.8375
epoch 2501, loss 0.3387, train acc 83.75%, f1 0.8375, precision 0.8375, recall 0.8376, auc 0.8375
epoch 2601, loss 0.3342, train acc 83.82%, f1 0.8383, precision 0.8375, recall 0.8391, auc 0.8382
epoch 2701, loss 0.2643, train acc 83.97%, f1 0.8399, precision 0.8388, recall 0.8410, auc 0.8397
epoch 2801, loss 0.3302, train acc 84.02%, f1 0.8403, precision 0.8402, recall 0.8403, auc 0.8402
epoch 2901, loss 0.3203, train acc 84.11%, f1 0.8412, precision 0.8403, recall 0.8421, auc 0.8411
epoch 3001, loss 0.3040, train acc 84.26%, f1 0.8425, precision 0.8429, recall 0.8422, auc 0.8426
epoch 3101, loss 0.2694, train acc 84.33%, f1 0.8434, precision 0.8428, recall 0.8440, auc 0.8433
epoch 3201, loss 0.3554, train acc 84.44%, f1 0.8444, precision 0.8441, recall 0.8447, auc 0.8444
epoch 3301, loss 0.2894, train acc 84.59%, f1 0.8459, precision 0.8458, recall 0.8461, auc 0.8459
epoch 3401, loss 0.3264, train acc 84.66%, f1 0.8466, precision 0.8467, recall 0.8464, auc 0.8466
epoch 3501, loss 0.3519, train acc 84.79%, f1 0.8480, precision 0.8474, recall 0.8486, auc 0.8479
epoch 3601, loss 0.3435, train acc 84.93%, f1 0.8493, precision 0.8493, recall 0.8493, auc 0.8493
epoch 3701, loss 0.3091, train acc 84.99%, f1 0.8500, precision 0.8493, recall 0.8507, auc 0.8499
epoch 3801, loss 0.3685, train acc 85.04%, f1 0.8504, precision 0.8503, recall 0.8505, auc 0.8504
epoch 3901, loss 0.4440, train acc 85.13%, f1 0.8512, precision 0.8515, recall 0.8510, auc 0.8513
epoch 4001, loss 0.3737, train acc 85.26%, f1 0.8526, precision 0.8526, recall 0.8525, auc 0.8526
epoch 4101, loss 0.3559, train acc 85.34%, f1 0.8533, precision 0.8534, recall 0.8533, auc 0.8534
epoch 4201, loss 0.2553, train acc 85.42%, f1 0.8543, precision 0.8539, recall 0.8547, auc 0.8542
epoch 4301, loss 0.2540, train acc 85.45%, f1 0.8543, precision 0.8552, recall 0.8534, auc 0.8545
epoch 4401, loss 0.3438, train acc 85.55%, f1 0.8555, precision 0.8553, recall 0.8557, auc 0.8555
epoch 4501, loss 0.3827, train acc 85.66%, f1 0.8566, precision 0.8566, recall 0.8566, auc 0.8566
epoch 4601, loss 0.2266, train acc 85.74%, f1 0.8573, precision 0.8576, recall 0.8571, auc 0.8574
epoch 4701, loss 0.2816, train acc 85.75%, f1 0.8574, precision 0.8577, recall 0.8571, auc 0.8575
epoch 4801, loss 0.2618, train acc 85.86%, f1 0.8587, precision 0.8584, recall 0.8589, auc 0.8586
epoch 4901, loss 0.1879, train acc 85.92%, f1 0.8592, precision 0.8592, recall 0.8592, auc 0.8592
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_1
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6772222222222222

the Fscore is 0.6219512195121951

the precision is 0.4636363636363636

the recall is 0.9444444444444444

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5233, train acc 78.85%, f1 0.7879, precision 0.7900, recall 0.7858, auc 0.7885
epoch 201, loss 0.3550, train acc 81.65%, f1 0.8166, precision 0.8165, recall 0.8166, auc 0.8165
epoch 301, loss 0.4323, train acc 82.82%, f1 0.8284, precision 0.8278, recall 0.8289, auc 0.8282
epoch 401, loss 0.4659, train acc 83.22%, f1 0.8324, precision 0.8317, recall 0.8330, auc 0.8322
epoch 501, loss 0.3102, train acc 83.45%, f1 0.8346, precision 0.8341, recall 0.8350, auc 0.8345
epoch 601, loss 0.3947, train acc 83.41%, f1 0.8342, precision 0.8338, recall 0.8346, auc 0.8341
epoch 701, loss 0.3411, train acc 83.44%, f1 0.8345, precision 0.8344, recall 0.8346, auc 0.8344
epoch 801, loss 0.3595, train acc 83.42%, f1 0.8342, precision 0.8340, recall 0.8344, auc 0.8342
epoch 901, loss 0.2990, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 1001, loss 0.2210, train acc 83.42%, f1 0.8342, precision 0.8341, recall 0.8343, auc 0.8342
epoch 1101, loss 0.3501, train acc 83.46%, f1 0.8346, precision 0.8345, recall 0.8346, auc 0.8346
epoch 1201, loss 0.3670, train acc 83.38%, f1 0.8338, precision 0.8338, recall 0.8338, auc 0.8338
epoch 1301, loss 0.3639, train acc 83.45%, f1 0.8345, precision 0.8345, recall 0.8344, auc 0.8345
epoch 1401, loss 0.4208, train acc 83.44%, f1 0.8344, precision 0.8343, recall 0.8345, auc 0.8344
epoch 1501, loss 0.3664, train acc 83.42%, f1 0.8342, precision 0.8342, recall 0.8342, auc 0.8342
epoch 1601, loss 0.5329, train acc 83.46%, f1 0.8347, precision 0.8345, recall 0.8349, auc 0.8346
epoch 1701, loss 0.2533, train acc 83.48%, f1 0.8348, precision 0.8347, recall 0.8349, auc 0.8348
epoch 1801, loss 0.2692, train acc 83.50%, f1 0.8351, precision 0.8350, recall 0.8351, auc 0.8350
epoch 1901, loss 0.3336, train acc 83.46%, f1 0.8346, precision 0.8347, recall 0.8345, auc 0.8346
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_1
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.5957407407407407

the Fscore is 0.5698924731182795

the precision is 0.4015151515151515

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_True/record_1/MLP_concat_notMirror_True_1
----------------------



epoch 1, loss 0.6936, train acc 54.50%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
Validation loss decreased (inf --> 0.691098).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 21, loss 0.6909, train acc 67.00%, f1 0.6916, precision 0.6016, recall 0.8132, auc 0.6818



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_True
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_True/record_1/MLP_concat_notMirror_True_1
./test_pima/result_MLP_concat_notMirror_True_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.525

the Fscore is 0.5320197044334976

the precision is 0.3624161073825503

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_1
----------------------



epoch 1, loss 0.6938, train acc 50.13%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5815, train acc 78.38%, f1 0.7739, precision 0.8086, recall 0.7422, auc 0.7837
epoch 201, loss 0.3454, train acc 81.66%, f1 0.8151, precision 0.8197, recall 0.8105, auc 0.8166
epoch 301, loss 0.3657, train acc 82.81%, f1 0.8282, precision 0.8257, recall 0.8306, auc 0.8281
epoch 401, loss 0.3289, train acc 83.20%, f1 0.8326, precision 0.8276, recall 0.8376, auc 0.8320
epoch 501, loss 0.4721, train acc 83.35%, f1 0.8340, precision 0.8294, recall 0.8387, auc 0.8335
epoch 601, loss 0.3805, train acc 83.34%, f1 0.8340, precision 0.8290, recall 0.8391, auc 0.8334
epoch 701, loss 0.3070, train acc 83.44%, f1 0.8344, precision 0.8323, recall 0.8365, auc 0.8344
epoch 801, loss 0.4988, train acc 83.45%, f1 0.8344, precision 0.8328, recall 0.8361, auc 0.8345
epoch 901, loss 0.3308, train acc 83.48%, f1 0.8348, precision 0.8327, recall 0.8369, auc 0.8348
epoch 1001, loss 0.4042, train acc 83.46%, f1 0.8347, precision 0.8322, recall 0.8373, auc 0.8346
epoch 1101, loss 0.4167, train acc 83.43%, f1 0.8340, precision 0.8336, recall 0.8343, auc 0.8343
epoch 1201, loss 0.3493, train acc 83.45%, f1 0.8349, precision 0.8305, recall 0.8393, auc 0.8345
epoch 1301, loss 0.3094, train acc 83.41%, f1 0.8342, precision 0.8317, recall 0.8369, auc 0.8342
epoch 1401, loss 0.2982, train acc 83.37%, f1 0.8334, precision 0.8326, recall 0.8342, auc 0.8337
epoch 1501, loss 0.4114, train acc 83.45%, f1 0.8344, precision 0.8326, recall 0.8361, auc 0.8345
epoch 1601, loss 0.2732, train acc 83.44%, f1 0.8344, precision 0.8322, recall 0.8367, auc 0.8344
epoch 1701, loss 0.3220, train acc 83.46%, f1 0.8348, precision 0.8318, recall 0.8377, auc 0.8346
epoch 1801, loss 0.3617, train acc 83.45%, f1 0.8342, precision 0.8335, recall 0.8350, auc 0.8345
epoch 1901, loss 0.3353, train acc 83.50%, f1 0.8346, precision 0.8345, recall 0.8348, auc 0.8350
epoch 2001, loss 0.4447, train acc 83.46%, f1 0.8348, precision 0.8315, recall 0.8381, auc 0.8346
epoch 2101, loss 0.3093, train acc 83.49%, f1 0.8344, precision 0.8347, recall 0.8341, auc 0.8349
epoch 2201, loss 0.4100, train acc 83.54%, f1 0.8347, precision 0.8360, recall 0.8334, auc 0.8354
epoch 2301, loss 0.4632, train acc 83.48%, f1 0.8349, precision 0.8321, recall 0.8378, auc 0.8348
epoch 2401, loss 0.2598, train acc 83.44%, f1 0.8348, precision 0.8307, recall 0.8388, auc 0.8344
epoch 2501, loss 0.3852, train acc 83.57%, f1 0.8354, precision 0.8346, recall 0.8362, auc 0.8357
epoch 2601, loss 0.4224, train acc 83.56%, f1 0.8354, precision 0.8344, recall 0.8363, auc 0.8356
epoch 2701, loss 0.3981, train acc 83.54%, f1 0.8355, precision 0.8330, recall 0.8380, auc 0.8355
epoch 2801, loss 0.3377, train acc 83.56%, f1 0.8349, precision 0.8364, recall 0.8334, auc 0.8356
epoch 2901, loss 0.3883, train acc 83.57%, f1 0.8358, precision 0.8329, recall 0.8388, auc 0.8357
epoch 3001, loss 0.3601, train acc 83.68%, f1 0.8365, precision 0.8362, recall 0.8368, auc 0.8368
epoch 3101, loss 0.3677, train acc 83.73%, f1 0.8372, precision 0.8356, recall 0.8389, auc 0.8373
epoch 3201, loss 0.3646, train acc 83.80%, f1 0.8374, precision 0.8383, recall 0.8364, auc 0.8380
epoch 3301, loss 0.3540, train acc 83.85%, f1 0.8378, precision 0.8396, recall 0.8359, auc 0.8385
epoch 3401, loss 0.3495, train acc 83.98%, f1 0.8397, precision 0.8383, recall 0.8412, auc 0.8398
epoch 3501, loss 0.3581, train acc 84.08%, f1 0.8405, precision 0.8403, recall 0.8406, auc 0.8408
epoch 3601, loss 0.3668, train acc 84.13%, f1 0.8411, precision 0.8400, recall 0.8422, auc 0.8413
epoch 3701, loss 0.3230, train acc 84.18%, f1 0.8415, precision 0.8412, recall 0.8417, auc 0.8418
epoch 3801, loss 0.3891, train acc 84.32%, f1 0.8430, precision 0.8421, recall 0.8438, auc 0.8432
epoch 3901, loss 0.3490, train acc 84.41%, f1 0.8436, precision 0.8438, recall 0.8435, auc 0.8441
epoch 4001, loss 0.2991, train acc 84.60%, f1 0.8455, precision 0.8460, recall 0.8450, auc 0.8460
epoch 4101, loss 0.3926, train acc 84.66%, f1 0.8464, precision 0.8452, recall 0.8477, auc 0.8466
epoch 4201, loss 0.4199, train acc 84.78%, f1 0.8469, precision 0.8500, recall 0.8437, auc 0.8478
epoch 4301, loss 0.3487, train acc 84.88%, f1 0.8489, precision 0.8459, recall 0.8520, auc 0.8488
epoch 4401, loss 0.3206, train acc 84.98%, f1 0.8499, precision 0.8472, recall 0.8526, auc 0.8498
epoch 4501, loss 0.3157, train acc 85.04%, f1 0.8506, precision 0.8473, recall 0.8540, auc 0.8504
epoch 4601, loss 0.3658, train acc 85.13%, f1 0.8516, precision 0.8477, recall 0.8556, auc 0.8513
epoch 4701, loss 0.3312, train acc 85.30%, f1 0.8529, precision 0.8513, recall 0.8544, auc 0.8530
epoch 4801, loss 0.3511, train acc 85.35%, f1 0.8533, precision 0.8524, recall 0.8542, auc 0.8535
epoch 4901, loss 0.2914, train acc 85.44%, f1 0.8539, precision 0.8544, recall 0.8534, auc 0.8544
epoch 5001, loss 0.2859, train acc 85.56%, f1 0.8549, precision 0.8571, recall 0.8527, auc 0.8556
epoch 5101, loss 0.3404, train acc 85.64%, f1 0.8561, precision 0.8560, recall 0.8561, auc 0.8564
epoch 5201, loss 0.4023, train acc 85.68%, f1 0.8568, precision 0.8545, recall 0.8591, auc 0.8568
epoch 5301, loss 0.4139, train acc 85.77%, f1 0.8572, precision 0.8581, recall 0.8564, auc 0.8577
epoch 5401, loss 0.2523, train acc 85.92%, f1 0.8589, precision 0.8590, recall 0.8588, auc 0.8592
epoch 5501, loss 0.3727, train acc 85.98%, f1 0.8592, precision 0.8610, recall 0.8574, auc 0.8598
epoch 5601, loss 0.2988, train acc 86.01%, f1 0.8598, precision 0.8595, recall 0.8601, auc 0.8601
epoch 5701, loss 0.2947, train acc 86.07%, f1 0.8601, precision 0.8615, recall 0.8586, auc 0.8606
epoch 5801, loss 0.3124, train acc 86.14%, f1 0.8606, precision 0.8634, recall 0.8577, auc 0.8614
epoch 5901, loss 0.3672, train acc 86.17%, f1 0.8611, precision 0.8626, recall 0.8596, auc 0.8617
epoch 6001, loss 0.3604, train acc 86.25%, f1 0.8616, precision 0.8653, recall 0.8579, auc 0.8625
epoch 6101, loss 0.2875, train acc 86.28%, f1 0.8618, precision 0.8658, recall 0.8577, auc 0.8627
epoch 6201, loss 0.2228, train acc 86.29%, f1 0.8626, precision 0.8625, recall 0.8626, auc 0.8629
epoch 6301, loss 0.4521, train acc 86.32%, f1 0.8624, precision 0.8651, recall 0.8598, auc 0.8632
epoch 6401, loss 0.3020, train acc 86.42%, f1 0.8638, precision 0.8643, recall 0.8633, auc 0.8642
epoch 6501, loss 0.2473, train acc 86.48%, f1 0.8647, precision 0.8633, recall 0.8661, auc 0.8648
epoch 6601, loss 0.3611, train acc 86.52%, f1 0.8649, precision 0.8647, recall 0.8650, auc 0.8652
epoch 6701, loss 0.4885, train acc 86.56%, f1 0.8648, precision 0.8677, recall 0.8619, auc 0.8656
epoch 6801, loss 0.2499, train acc 86.60%, f1 0.8658, precision 0.8650, recall 0.8667, auc 0.8660
epoch 6901, loss 0.2429, train acc 86.67%, f1 0.8663, precision 0.8672, recall 0.8654, auc 0.8667
epoch 7001, loss 0.2652, train acc 86.74%, f1 0.8670, precision 0.8671, recall 0.8670, auc 0.8674
epoch 7101, loss 0.2887, train acc 86.78%, f1 0.8674, precision 0.8680, recall 0.8667, auc 0.8678
epoch 7201, loss 0.2563, train acc 86.81%, f1 0.8677, precision 0.8680, recall 0.8674, auc 0.8681
epoch 7301, loss 0.2540, train acc 86.84%, f1 0.8680, precision 0.8683, recall 0.8676, auc 0.8684
epoch 7401, loss 0.3043, train acc 86.89%, f1 0.8691, precision 0.8654, recall 0.8729, auc 0.8689
epoch 7501, loss 0.2645, train acc 86.87%, f1 0.8679, precision 0.8710, recall 0.8649, auc 0.8687
epoch 7601, loss 0.2776, train acc 86.96%, f1 0.8692, precision 0.8694, recall 0.8691, auc 0.8696
epoch 7701, loss 0.3350, train acc 86.98%, f1 0.8694, precision 0.8703, recall 0.8684, auc 0.8698
epoch 7801, loss 0.2261, train acc 86.99%, f1 0.8693, precision 0.8709, recall 0.8677, auc 0.8699
epoch 7901, loss 0.3452, train acc 87.00%, f1 0.8693, precision 0.8714, recall 0.8672, auc 0.8700
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_1
./test_pima/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.7022222222222222

the Fscore is 0.6415094339622641

the precision is 0.4857142857142857

the recall is 0.9444444444444444

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_1
----------------------



epoch 1, loss 0.6930, train acc 50.05%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5632, train acc 78.18%, f1 0.7753, precision 0.7983, recall 0.7536, auc 0.7818
epoch 201, loss 0.3710, train acc 81.17%, f1 0.8108, precision 0.8141, recall 0.8075, auc 0.8117
epoch 301, loss 0.3408, train acc 82.66%, f1 0.8271, precision 0.8238, recall 0.8304, auc 0.8266
epoch 401, loss 0.3370, train acc 83.17%, f1 0.8318, precision 0.8305, recall 0.8331, auc 0.8317
epoch 501, loss 0.3977, train acc 83.38%, f1 0.8343, precision 0.8308, recall 0.8379, auc 0.8338
epoch 601, loss 0.3578, train acc 83.45%, f1 0.8350, precision 0.8319, recall 0.8380, auc 0.8345
epoch 701, loss 0.5008, train acc 83.45%, f1 0.8349, precision 0.8324, recall 0.8374, auc 0.8345
epoch 801, loss 0.3424, train acc 83.44%, f1 0.8348, precision 0.8319, recall 0.8378, auc 0.8344
epoch 901, loss 0.3842, train acc 83.45%, f1 0.8343, precision 0.8343, recall 0.8343, auc 0.8345
epoch 1001, loss 0.3473, train acc 83.48%, f1 0.8346, precision 0.8349, recall 0.8342, auc 0.8348
epoch 1101, loss 0.3036, train acc 83.49%, f1 0.8354, precision 0.8324, recall 0.8383, auc 0.8349
epoch 1201, loss 0.4365, train acc 83.46%, f1 0.8346, precision 0.8337, recall 0.8354, auc 0.8346
epoch 1301, loss 0.3560, train acc 83.47%, f1 0.8344, precision 0.8352, recall 0.8335, auc 0.8347
epoch 1401, loss 0.2672, train acc 83.53%, f1 0.8357, precision 0.8325, recall 0.8390, auc 0.8353
epoch 1501, loss 0.4024, train acc 83.51%, f1 0.8355, precision 0.8327, recall 0.8384, auc 0.8351
epoch 1601, loss 0.5647, train acc 83.48%, f1 0.8343, precision 0.8357, recall 0.8330, auc 0.8348
epoch 1701, loss 0.4471, train acc 83.46%, f1 0.8346, precision 0.8338, recall 0.8355, auc 0.8346
epoch 1801, loss 0.4953, train acc 83.45%, f1 0.8347, precision 0.8328, recall 0.8367, auc 0.8345
epoch 1901, loss 0.4320, train acc 83.48%, f1 0.8351, precision 0.8328, recall 0.8374, auc 0.8348
epoch 2001, loss 0.3674, train acc 83.45%, f1 0.8341, precision 0.8352, recall 0.8330, auc 0.8345
epoch 2101, loss 0.3281, train acc 83.46%, f1 0.8343, precision 0.8351, recall 0.8335, auc 0.8346
epoch 2201, loss 0.4447, train acc 83.46%, f1 0.8338, precision 0.8371, recall 0.8306, auc 0.8346
epoch 2301, loss 0.3918, train acc 83.42%, f1 0.8343, precision 0.8328, recall 0.8358, auc 0.8342
epoch 2401, loss 0.3391, train acc 83.47%, f1 0.8351, precision 0.8323, recall 0.8378, auc 0.8347
epoch 2501, loss 0.3954, train acc 83.45%, f1 0.8345, precision 0.8337, recall 0.8352, auc 0.8345
epoch 2601, loss 0.3060, train acc 83.45%, f1 0.8342, precision 0.8350, recall 0.8334, auc 0.8345
epoch 2701, loss 0.3535, train acc 83.46%, f1 0.8347, precision 0.8331, recall 0.8363, auc 0.8346
epoch 2801, loss 0.4597, train acc 83.57%, f1 0.8347, precision 0.8389, recall 0.8306, auc 0.8357
epoch 2901, loss 0.5156, train acc 83.54%, f1 0.8352, precision 0.8354, recall 0.8349, auc 0.8354
epoch 3001, loss 0.3892, train acc 83.61%, f1 0.8359, precision 0.8358, recall 0.8361, auc 0.8361
epoch 3101, loss 0.3685, train acc 83.69%, f1 0.8360, precision 0.8396, recall 0.8325, auc 0.8369
epoch 3201, loss 0.3303, train acc 83.78%, f1 0.8374, precision 0.8384, recall 0.8365, auc 0.8378
epoch 3301, loss 0.3920, train acc 83.86%, f1 0.8380, precision 0.8403, recall 0.8357, auc 0.8386
epoch 3401, loss 0.4032, train acc 83.93%, f1 0.8388, precision 0.8405, recall 0.8371, auc 0.8393
epoch 3501, loss 0.3641, train acc 84.03%, f1 0.8407, precision 0.8378, recall 0.8437, auc 0.8403
epoch 3601, loss 0.3021, train acc 84.13%, f1 0.8410, precision 0.8419, recall 0.8400, auc 0.8413
epoch 3701, loss 0.3626, train acc 84.24%, f1 0.8424, precision 0.8414, recall 0.8435, auc 0.8424
epoch 3801, loss 0.3505, train acc 84.35%, f1 0.8436, precision 0.8422, recall 0.8449, auc 0.8435
epoch 3901, loss 0.4836, train acc 84.54%, f1 0.8446, precision 0.8483, recall 0.8410, auc 0.8454
epoch 4001, loss 0.2878, train acc 84.64%, f1 0.8460, precision 0.8473, recall 0.8448, auc 0.8464
epoch 4101, loss 0.4257, train acc 84.79%, f1 0.8479, precision 0.8474, recall 0.8484, auc 0.8479
epoch 4201, loss 0.2694, train acc 84.86%, f1 0.8485, precision 0.8483, recall 0.8488, auc 0.8486
epoch 4301, loss 0.2679, train acc 85.00%, f1 0.8496, precision 0.8509, recall 0.8483, auc 0.8500
epoch 4401, loss 0.3397, train acc 85.04%, f1 0.8508, precision 0.8481, recall 0.8535, auc 0.8504
epoch 4501, loss 0.4498, train acc 85.12%, f1 0.8510, precision 0.8513, recall 0.8507, auc 0.8512
epoch 4601, loss 0.4266, train acc 85.26%, f1 0.8520, precision 0.8543, recall 0.8498, auc 0.8526
epoch 4701, loss 0.4299, train acc 85.31%, f1 0.8528, precision 0.8535, recall 0.8522, auc 0.8531
epoch 4801, loss 0.2669, train acc 85.45%, f1 0.8541, precision 0.8556, recall 0.8525, auc 0.8545
epoch 4901, loss 0.3205, train acc 85.47%, f1 0.8545, precision 0.8548, recall 0.8543, auc 0.8547
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_1
./test_pima/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6914814814814814

the Fscore is 0.6341463414634146

the precision is 0.4727272727272727

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_1
----------------------



epoch 1, loss 0.6930, train acc 50.21%, f1 0.0216, precision 0.9128, recall 0.0109, auc 0.5049
epoch 101, loss 0.5389, train acc 78.52%, f1 0.7830, precision 0.7954, recall 0.7710, auc 0.7852
epoch 201, loss 0.4334, train acc 81.65%, f1 0.8179, precision 0.8164, recall 0.8193, auc 0.8165
epoch 301, loss 0.4308, train acc 82.83%, f1 0.8296, precision 0.8280, recall 0.8313, auc 0.8283
epoch 401, loss 0.4172, train acc 83.16%, f1 0.8325, precision 0.8325, recall 0.8326, auc 0.8315
epoch 501, loss 0.4608, train acc 83.40%, f1 0.8351, precision 0.8342, recall 0.8361, auc 0.8340
epoch 601, loss 0.4018, train acc 83.39%, f1 0.8345, precision 0.8360, recall 0.8331, auc 0.8339
epoch 701, loss 0.3682, train acc 83.41%, f1 0.8345, precision 0.8371, recall 0.8319, auc 0.8341
epoch 801, loss 0.3536, train acc 83.43%, f1 0.8352, precision 0.8353, recall 0.8350, auc 0.8343
epoch 901, loss 0.4159, train acc 83.43%, f1 0.8350, precision 0.8360, recall 0.8339, auc 0.8343
epoch 1001, loss 0.3983, train acc 83.47%, f1 0.8363, precision 0.8332, recall 0.8393, auc 0.8347
epoch 1101, loss 0.4070, train acc 83.46%, f1 0.8356, precision 0.8356, recall 0.8356, auc 0.8346
epoch 1201, loss 0.3366, train acc 83.39%, f1 0.8351, precision 0.8342, recall 0.8360, auc 0.8339
epoch 1301, loss 0.3465, train acc 83.39%, f1 0.8346, precision 0.8361, recall 0.8331, auc 0.8340
epoch 1401, loss 0.3426, train acc 83.42%, f1 0.8353, precision 0.8346, recall 0.8360, auc 0.8342
epoch 1501, loss 0.3763, train acc 83.42%, f1 0.8352, precision 0.8348, recall 0.8357, auc 0.8342
epoch 1601, loss 0.3752, train acc 83.46%, f1 0.8349, precision 0.8377, recall 0.8322, auc 0.8346
epoch 1701, loss 0.3555, train acc 83.43%, f1 0.8360, precision 0.8320, recall 0.8401, auc 0.8343
epoch 1801, loss 0.4433, train acc 83.47%, f1 0.8358, precision 0.8347, recall 0.8369, auc 0.8347
epoch 1901, loss 0.3189, train acc 83.45%, f1 0.8348, precision 0.8381, recall 0.8315, auc 0.8345
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_1
./test_pima/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.5864814814814814

the Fscore is 0.5621621621621621

the precision is 0.3969465648854962

the recall is 0.9629629629629629

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_True/record_1/MLP_minus_Mirror_True_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
Validation loss decreased (inf --> 0.693147).  Saving model ...
Validation loss decreased (0.693147 --> 0.693066).  Saving model ...
Validation loss decreased (0.693066 --> 0.692990).  Saving model ...
Validation loss decreased (0.692990 --> 0.692910).  Saving model ...
Validation loss decreased (0.692910 --> 0.692822).  Saving model ...
Validation loss decreased (0.692822 --> 0.692722).  Saving model ...
Validation loss decreased (0.692722 --> 0.692614).  Saving model ...
Validation loss decreased (0.692614 --> 0.692494).  Saving model ...
Validation loss decreased (0.692494 --> 0.692360).  Saving model ...
Validation loss decreased (0.692360 --> 0.692217).  Saving model ...
Validation loss decreased (0.692217 --> 0.692063).  Saving model ...
Validation loss decreased (0.692063 --> 0.691893).  Saving model ...
Validation loss decreased (0.691893 --> 0.691704).  Saving model ...
Validation loss decreased (0.691704 --> 0.691495).  Saving model ...
Validation loss decreased (0.691495 --> 0.691266).  Saving model ...
Validation loss decreased (0.691266 --> 0.691016).  Saving model ...
Validation loss decreased (0.691016 --> 0.690752).  Saving model ...
Validation loss decreased (0.690752 --> 0.690467).  Saving model ...
Validation loss decreased (0.690467 --> 0.690156).  Saving model ...
Validation loss decreased (0.690156 --> 0.689824).  Saving model ...
Validation loss decreased (0.689824 --> 0.689465).  Saving model ...
Validation loss decreased (0.689465 --> 0.689084).  Saving model ...
Validation loss decreased (0.689084 --> 0.688678).  Saving model ...
Validation loss decreased (0.688678 --> 0.688252).  Saving model ...
Validation loss decreased (0.688252 --> 0.687802).  Saving model ...
Validation loss decreased (0.687802 --> 0.687325).  Saving model ...
Validation loss decreased (0.687325 --> 0.686819).  Saving model ...
Validation loss decreased (0.686819 --> 0.686298).  Saving model ...
Validation loss decreased (0.686298 --> 0.685749).  Saving model ...
Validation loss decreased (0.685749 --> 0.685172).  Saving model ...
Validation loss decreased (0.685172 --> 0.684571).  Saving model ...
Validation loss decreased (0.684571 --> 0.683951).  Saving model ...
Validation loss decreased (0.683951 --> 0.683304).  Saving model ...
Validation loss decreased (0.683304 --> 0.682628).  Saving model ...
Validation loss decreased (0.682628 --> 0.681933).  Saving model ...
Validation loss decreased (0.681933 --> 0.681202).  Saving model ...
Validation loss decreased (0.681202 --> 0.680448).  Saving model ...
Validation loss decreased (0.680448 --> 0.679666).  Saving model ...
Validation loss decreased (0.679666 --> 0.678840).  Saving model ...
Validation loss decreased (0.678840 --> 0.677981).  Saving model ...
Validation loss decreased (0.677981 --> 0.677114).  Saving model ...
Validation loss decreased (0.677114 --> 0.676207).  Saving model ...
Validation loss decreased (0.676207 --> 0.675269).  Saving model ...
Validation loss decreased (0.675269 --> 0.674299).  Saving model ...
Validation loss decreased (0.674299 --> 0.673303).  Saving model ...
Validation loss decreased (0.673303 --> 0.672275).  Saving model ...
Validation loss decreased (0.672275 --> 0.671201).  Saving model ...
Validation loss decreased (0.671201 --> 0.670092).  Saving model ...
Validation loss decreased (0.670092 --> 0.668938).  Saving model ...
Validation loss decreased (0.668938 --> 0.667784).  Saving model ...
Validation loss decreased (0.667784 --> 0.666603).  Saving model ...
Validation loss decreased (0.666603 --> 0.665413).  Saving model ...
Validation loss decreased (0.665413 --> 0.664162).  Saving model ...
Validation loss decreased (0.664162 --> 0.662856).  Saving model ...
Validation loss decreased (0.662856 --> 0.661552).  Saving model ...
Validation loss decreased (0.661552 --> 0.660215).  Saving model ...
Validation loss decreased (0.660215 --> 0.658824).  Saving model ...
Validation loss decreased (0.658824 --> 0.657411).  Saving model ...
Validation loss decreased (0.657411 --> 0.655968).  Saving model ...
Validation loss decreased (0.655968 --> 0.654479).  Saving model ...
Validation loss decreased (0.654479 --> 0.652952).  Saving model ...
Validation loss decreased (0.652952 --> 0.651441).  Saving model ...
Validation loss decreased (0.651441 --> 0.649883).  Saving model ...
Validation loss decreased (0.649883 --> 0.648291).  Saving model ...
Validation loss decreased (0.648291 --> 0.646697).  Saving model ...
Validation loss decreased (0.646697 --> 0.645101).  Saving model ...
Validation loss decreased (0.645101 --> 0.643520).  Saving model ...
Validation loss decreased (0.643520 --> 0.641930).  Saving model ...
Validation loss decreased (0.641930 --> 0.640326).  Saving model ...
Validation loss decreased (0.640326 --> 0.638748).  Saving model ...
Validation loss decreased (0.638748 --> 0.637170).  Saving model ...
Validation loss decreased (0.637170 --> 0.635562).  Saving model ...
Validation loss decreased (0.635562 --> 0.633942).  Saving model ...
Validation loss decreased (0.633942 --> 0.632322).  Saving model ...
Validation loss decreased (0.632322 --> 0.630648).  Saving model ...
Validation loss decreased (0.630648 --> 0.628959).  Saving model ...
Validation loss decreased (0.628959 --> 0.627298).  Saving model ...
Validation loss decreased (0.627298 --> 0.625618).  Saving model ...
Validation loss decreased (0.625618 --> 0.623893).  Saving model ...
Validation loss decreased (0.623893 --> 0.622171).  Saving model ...
Validation loss decreased (0.622171 --> 0.620409).  Saving model ...
Validation loss decreased (0.620409 --> 0.618642).  Saving model ...
Validation loss decreased (0.618642 --> 0.616869).  Saving model ...
Validation loss decreased (0.616869 --> 0.615080).  Saving model ...
Validation loss decreased (0.615080 --> 0.613262).  Saving model ...
Validation loss decreased (0.613262 --> 0.611428).  Saving model ...
Validation loss decreased (0.611428 --> 0.609558).  Saving model ...
Validation loss decreased (0.609558 --> 0.607697).  Saving model ...
Validation loss decreased (0.607697 --> 0.605901).  Saving model ...
Validation loss decreased (0.605901 --> 0.604067).  Saving model ...
Validation loss decreased (0.604067 --> 0.602240).  Saving model ...
Validation loss decreased (0.602240 --> 0.600365).  Saving model ...
Validation loss decreased (0.600365 --> 0.598493).  Saving model ...
Validation loss decreased (0.598493 --> 0.596566).  Saving model ...
Validation loss decreased (0.596566 --> 0.594617).  Saving model ...
Validation loss decreased (0.594617 --> 0.592651).  Saving model ...
Validation loss decreased (0.592651 --> 0.590728).  Saving model ...
Validation loss decreased (0.590728 --> 0.588829).  Saving model ...
Validation loss decreased (0.588829 --> 0.586931).  Saving model ...
Validation loss decreased (0.586931 --> 0.585102).  Saving model ...
epoch 101, loss 0.6170, train acc 80.00%, f1 0.8030, precision 0.7913, recall 0.8150, auc 0.8000
Validation loss decreased (0.585102 --> 0.583319).  Saving model ...
Validation loss decreased (0.583319 --> 0.581505).  Saving model ...
Validation loss decreased (0.581505 --> 0.579741).  Saving model ...
Validation loss decreased (0.579741 --> 0.577976).  Saving model ...
Validation loss decreased (0.577976 --> 0.576157).  Saving model ...
Validation loss decreased (0.576157 --> 0.574349).  Saving model ...
Validation loss decreased (0.574349 --> 0.572527).  Saving model ...
Validation loss decreased (0.572527 --> 0.570734).  Saving model ...
Validation loss decreased (0.570734 --> 0.568968).  Saving model ...
Validation loss decreased (0.568968 --> 0.567217).  Saving model ...
Validation loss decreased (0.567217 --> 0.565429).  Saving model ...
Validation loss decreased (0.565429 --> 0.563639).  Saving model ...
Validation loss decreased (0.563639 --> 0.561800).  Saving model ...
Validation loss decreased (0.561800 --> 0.559960).  Saving model ...
Validation loss decreased (0.559960 --> 0.558144).  Saving model ...
Validation loss decreased (0.558144 --> 0.556334).  Saving model ...
Validation loss decreased (0.556334 --> 0.554513).  Saving model ...
Validation loss decreased (0.554513 --> 0.552666).  Saving model ...
Validation loss decreased (0.552666 --> 0.550824).  Saving model ...
Validation loss decreased (0.550824 --> 0.548997).  Saving model ...
Validation loss decreased (0.548997 --> 0.547221).  Saving model ...
Validation loss decreased (0.547221 --> 0.545472).  Saving model ...
Validation loss decreased (0.545472 --> 0.543688).  Saving model ...
Validation loss decreased (0.543688 --> 0.541964).  Saving model ...
Validation loss decreased (0.541964 --> 0.540268).  Saving model ...
Validation loss decreased (0.540268 --> 0.538617).  Saving model ...
Validation loss decreased (0.538617 --> 0.536905).  Saving model ...
Validation loss decreased (0.536905 --> 0.535198).  Saving model ...
Validation loss decreased (0.535198 --> 0.533504).  Saving model ...
Validation loss decreased (0.533504 --> 0.531808).  Saving model ...
Validation loss decreased (0.531808 --> 0.530136).  Saving model ...
Validation loss decreased (0.530136 --> 0.528535).  Saving model ...
Validation loss decreased (0.528535 --> 0.526941).  Saving model ...
Validation loss decreased (0.526941 --> 0.525287).  Saving model ...
Validation loss decreased (0.525287 --> 0.523629).  Saving model ...
Validation loss decreased (0.523629 --> 0.521999).  Saving model ...
Validation loss decreased (0.521999 --> 0.520409).  Saving model ...
Validation loss decreased (0.520409 --> 0.518829).  Saving model ...
Validation loss decreased (0.518829 --> 0.517272).  Saving model ...
Validation loss decreased (0.517272 --> 0.515726).  Saving model ...
Validation loss decreased (0.515726 --> 0.514143).  Saving model ...
Validation loss decreased (0.514143 --> 0.512633).  Saving model ...
Validation loss decreased (0.512633 --> 0.511101).  Saving model ...
Validation loss decreased (0.511101 --> 0.509582).  Saving model ...
Validation loss decreased (0.509582 --> 0.508074).  Saving model ...
Validation loss decreased (0.508074 --> 0.506590).  Saving model ...
Validation loss decreased (0.506590 --> 0.505125).  Saving model ...
Validation loss decreased (0.505125 --> 0.503676).  Saving model ...
Validation loss decreased (0.503676 --> 0.502208).  Saving model ...
Validation loss decreased (0.502208 --> 0.500767).  Saving model ...
Validation loss decreased (0.500767 --> 0.499353).  Saving model ...
Validation loss decreased (0.499353 --> 0.497968).  Saving model ...
Validation loss decreased (0.497968 --> 0.496575).  Saving model ...
Validation loss decreased (0.496575 --> 0.495198).  Saving model ...
Validation loss decreased (0.495198 --> 0.493817).  Saving model ...
Validation loss decreased (0.493817 --> 0.492471).  Saving model ...
Validation loss decreased (0.492471 --> 0.491161).  Saving model ...
Validation loss decreased (0.491161 --> 0.489880).  Saving model ...
Validation loss decreased (0.489880 --> 0.488647).  Saving model ...
Validation loss decreased (0.488647 --> 0.487449).  Saving model ...
Validation loss decreased (0.487449 --> 0.486233).  Saving model ...
Validation loss decreased (0.486233 --> 0.484998).  Saving model ...
Validation loss decreased (0.484998 --> 0.483776).  Saving model ...
Validation loss decreased (0.483776 --> 0.482524).  Saving model ...
Validation loss decreased (0.482524 --> 0.481277).  Saving model ...
Validation loss decreased (0.481277 --> 0.480085).  Saving model ...
Validation loss decreased (0.480085 --> 0.478958).  Saving model ...
Validation loss decreased (0.478958 --> 0.477840).  Saving model ...
Validation loss decreased (0.477840 --> 0.476771).  Saving model ...
Validation loss decreased (0.476771 --> 0.475693).  Saving model ...
Validation loss decreased (0.475693 --> 0.474587).  Saving model ...
Validation loss decreased (0.474587 --> 0.473439).  Saving model ...
Validation loss decreased (0.473439 --> 0.472267).  Saving model ...
Validation loss decreased (0.472267 --> 0.471104).  Saving model ...
Validation loss decreased (0.471104 --> 0.470006).  Saving model ...
Validation loss decreased (0.470006 --> 0.468937).  Saving model ...
Validation loss decreased (0.468937 --> 0.467889).  Saving model ...
Validation loss decreased (0.467889 --> 0.466807).  Saving model ...
Validation loss decreased (0.466807 --> 0.465707).  Saving model ...
Validation loss decreased (0.465707 --> 0.464608).  Saving model ...
Validation loss decreased (0.464608 --> 0.463552).  Saving model ...
Validation loss decreased (0.463552 --> 0.462474).  Saving model ...
Validation loss decreased (0.462474 --> 0.461367).  Saving model ...
Validation loss decreased (0.461367 --> 0.460302).  Saving model ...
Validation loss decreased (0.460302 --> 0.459233).  Saving model ...
Validation loss decreased (0.459233 --> 0.458214).  Saving model ...
Validation loss decreased (0.458214 --> 0.457182).  Saving model ...
Validation loss decreased (0.457182 --> 0.456249).  Saving model ...
Validation loss decreased (0.456249 --> 0.455340).  Saving model ...
Validation loss decreased (0.455340 --> 0.454453).  Saving model ...
Validation loss decreased (0.454453 --> 0.453536).  Saving model ...
Validation loss decreased (0.453536 --> 0.452615).  Saving model ...
Validation loss decreased (0.452615 --> 0.451710).  Saving model ...
Validation loss decreased (0.451710 --> 0.450813).  Saving model ...
Validation loss decreased (0.450813 --> 0.449955).  Saving model ...
Validation loss decreased (0.449955 --> 0.449086).  Saving model ...
Validation loss decreased (0.449086 --> 0.448254).  Saving model ...
Validation loss decreased (0.448254 --> 0.447422).  Saving model ...
Validation loss decreased (0.447422 --> 0.446587).  Saving model ...
Validation loss decreased (0.446587 --> 0.445779).  Saving model ...
epoch 201, loss 0.5876, train acc 81.25%, f1 0.8130, precision 0.8109, recall 0.8150, auc 0.8125
Validation loss decreased (0.445779 --> 0.445060).  Saving model ...
Validation loss decreased (0.445060 --> 0.444316).  Saving model ...
Validation loss decreased (0.444316 --> 0.443574).  Saving model ...
Validation loss decreased (0.443574 --> 0.442848).  Saving model ...
Validation loss decreased (0.442848 --> 0.442140).  Saving model ...
Validation loss decreased (0.442140 --> 0.441452).  Saving model ...
Validation loss decreased (0.441452 --> 0.440762).  Saving model ...
Validation loss decreased (0.440762 --> 0.440104).  Saving model ...
Validation loss decreased (0.440104 --> 0.439429).  Saving model ...
Validation loss decreased (0.439429 --> 0.438741).  Saving model ...
Validation loss decreased (0.438741 --> 0.438065).  Saving model ...
Validation loss decreased (0.438065 --> 0.437351).  Saving model ...
Validation loss decreased (0.437351 --> 0.436657).  Saving model ...
Validation loss decreased (0.436657 --> 0.435948).  Saving model ...
Validation loss decreased (0.435948 --> 0.435263).  Saving model ...
Validation loss decreased (0.435263 --> 0.434588).  Saving model ...
Validation loss decreased (0.434588 --> 0.433945).  Saving model ...
Validation loss decreased (0.433945 --> 0.433314).  Saving model ...
Validation loss decreased (0.433314 --> 0.432705).  Saving model ...
Validation loss decreased (0.432705 --> 0.432133).  Saving model ...
Validation loss decreased (0.432133 --> 0.431572).  Saving model ...
Validation loss decreased (0.431572 --> 0.431018).  Saving model ...
Validation loss decreased (0.431018 --> 0.430464).  Saving model ...
Validation loss decreased (0.430464 --> 0.429929).  Saving model ...
Validation loss decreased (0.429929 --> 0.429409).  Saving model ...
Validation loss decreased (0.429409 --> 0.428895).  Saving model ...
Validation loss decreased (0.428895 --> 0.428329).  Saving model ...
Validation loss decreased (0.428329 --> 0.427763).  Saving model ...
Validation loss decreased (0.427763 --> 0.427194).  Saving model ...
Validation loss decreased (0.427194 --> 0.426631).  Saving model ...
Validation loss decreased (0.426631 --> 0.426100).  Saving model ...
Validation loss decreased (0.426100 --> 0.425521).  Saving model ...
Validation loss decreased (0.425521 --> 0.424983).  Saving model ...
Validation loss decreased (0.424983 --> 0.424470).  Saving model ...
Validation loss decreased (0.424470 --> 0.423954).  Saving model ...
Validation loss decreased (0.423954 --> 0.423459).  Saving model ...
Validation loss decreased (0.423459 --> 0.423003).  Saving model ...
Validation loss decreased (0.423003 --> 0.422594).  Saving model ...
Validation loss decreased (0.422594 --> 0.422200).  Saving model ...
Validation loss decreased (0.422200 --> 0.421756).  Saving model ...
Validation loss decreased (0.421756 --> 0.421335).  Saving model ...
Validation loss decreased (0.421335 --> 0.420948).  Saving model ...
Validation loss decreased (0.420948 --> 0.420544).  Saving model ...
Validation loss decreased (0.420544 --> 0.420164).  Saving model ...
Validation loss decreased (0.420164 --> 0.419787).  Saving model ...
Validation loss decreased (0.419787 --> 0.419380).  Saving model ...
Validation loss decreased (0.419380 --> 0.418984).  Saving model ...
Validation loss decreased (0.418984 --> 0.418548).  Saving model ...
Validation loss decreased (0.418548 --> 0.418081).  Saving model ...
Validation loss decreased (0.418081 --> 0.417647).  Saving model ...
Validation loss decreased (0.417647 --> 0.417212).  Saving model ...
Validation loss decreased (0.417212 --> 0.416814).  Saving model ...
Validation loss decreased (0.416814 --> 0.416397).  Saving model ...
Validation loss decreased (0.416397 --> 0.415978).  Saving model ...
Validation loss decreased (0.415978 --> 0.415558).  Saving model ...
Validation loss decreased (0.415558 --> 0.415181).  Saving model ...
Validation loss decreased (0.415181 --> 0.414791).  Saving model ...
Validation loss decreased (0.414791 --> 0.414386).  Saving model ...
Validation loss decreased (0.414386 --> 0.413974).  Saving model ...
Validation loss decreased (0.413974 --> 0.413554).  Saving model ...
Validation loss decreased (0.413554 --> 0.413155).  Saving model ...
Validation loss decreased (0.413155 --> 0.412763).  Saving model ...
Validation loss decreased (0.412763 --> 0.412368).  Saving model ...
Validation loss decreased (0.412368 --> 0.412016).  Saving model ...
Validation loss decreased (0.412016 --> 0.411655).  Saving model ...
Validation loss decreased (0.411655 --> 0.411281).  Saving model ...
Validation loss decreased (0.411281 --> 0.410905).  Saving model ...
Validation loss decreased (0.410905 --> 0.410560).  Saving model ...
Validation loss decreased (0.410560 --> 0.410302).  Saving model ...
Validation loss decreased (0.410302 --> 0.409982).  Saving model ...
Validation loss decreased (0.409982 --> 0.409654).  Saving model ...
Validation loss decreased (0.409654 --> 0.409272).  Saving model ...
Validation loss decreased (0.409272 --> 0.408853).  Saving model ...
Validation loss decreased (0.408853 --> 0.408447).  Saving model ...
Validation loss decreased (0.408447 --> 0.407993).  Saving model ...
Validation loss decreased (0.407993 --> 0.407599).  Saving model ...
Validation loss decreased (0.407599 --> 0.407165).  Saving model ...
Validation loss decreased (0.407165 --> 0.406739).  Saving model ...
Validation loss decreased (0.406739 --> 0.406348).  Saving model ...
Validation loss decreased (0.406348 --> 0.405949).  Saving model ...
Validation loss decreased (0.405949 --> 0.405542).  Saving model ...
Validation loss decreased (0.405542 --> 0.405120).  Saving model ...
Validation loss decreased (0.405120 --> 0.404677).  Saving model ...
Validation loss decreased (0.404677 --> 0.404193).  Saving model ...
Validation loss decreased (0.404193 --> 0.403695).  Saving model ...
Validation loss decreased (0.403695 --> 0.403153).  Saving model ...
Validation loss decreased (0.403153 --> 0.402640).  Saving model ...
Validation loss decreased (0.402640 --> 0.402127).  Saving model ...
Validation loss decreased (0.402127 --> 0.401618).  Saving model ...
Validation loss decreased (0.401618 --> 0.401165).  Saving model ...
Validation loss decreased (0.401165 --> 0.400755).  Saving model ...
Validation loss decreased (0.400755 --> 0.400398).  Saving model ...
Validation loss decreased (0.400398 --> 0.400013).  Saving model ...
Validation loss decreased (0.400013 --> 0.399676).  Saving model ...
Validation loss decreased (0.399676 --> 0.399354).  Saving model ...
Validation loss decreased (0.399354 --> 0.399028).  Saving model ...
Validation loss decreased (0.399028 --> 0.398701).  Saving model ...
Validation loss decreased (0.398701 --> 0.398422).  Saving model ...
Validation loss decreased (0.398422 --> 0.398059).  Saving model ...
Validation loss decreased (0.398059 --> 0.397725).  Saving model ...
epoch 301, loss 0.3907, train acc 83.00%, f1 0.8300, precision 0.8300, recall 0.8300, auc 0.8300
Validation loss decreased (0.397725 --> 0.397415).  Saving model ...
Validation loss decreased (0.397415 --> 0.397122).  Saving model ...
Validation loss decreased (0.397122 --> 0.396789).  Saving model ...
Validation loss decreased (0.396789 --> 0.396413).  Saving model ...
Validation loss decreased (0.396413 --> 0.396008).  Saving model ...
Validation loss decreased (0.396008 --> 0.395560).  Saving model ...
Validation loss decreased (0.395560 --> 0.395181).  Saving model ...
Validation loss decreased (0.395181 --> 0.394785).  Saving model ...
Validation loss decreased (0.394785 --> 0.394433).  Saving model ...
Validation loss decreased (0.394433 --> 0.394105).  Saving model ...
Validation loss decreased (0.394105 --> 0.393790).  Saving model ...
Validation loss decreased (0.393790 --> 0.393505).  Saving model ...
Validation loss decreased (0.393505 --> 0.393235).  Saving model ...
Validation loss decreased (0.393235 --> 0.392965).  Saving model ...
Validation loss decreased (0.392965 --> 0.392713).  Saving model ...
Validation loss decreased (0.392713 --> 0.392474).  Saving model ...
Validation loss decreased (0.392474 --> 0.392233).  Saving model ...
Validation loss decreased (0.392233 --> 0.391970).  Saving model ...
Validation loss decreased (0.391970 --> 0.391749).  Saving model ...
Validation loss decreased (0.391749 --> 0.391547).  Saving model ...
Validation loss decreased (0.391547 --> 0.391371).  Saving model ...
Validation loss decreased (0.391371 --> 0.391264).  Saving model ...
Validation loss decreased (0.391264 --> 0.391156).  Saving model ...
Validation loss decreased (0.391156 --> 0.391022).  Saving model ...
Validation loss decreased (0.391022 --> 0.390866).  Saving model ...
Validation loss decreased (0.390866 --> 0.390727).  Saving model ...
Validation loss decreased (0.390727 --> 0.390612).  Saving model ...
Validation loss decreased (0.390612 --> 0.390494).  Saving model ...
Validation loss decreased (0.390494 --> 0.390421).  Saving model ...
Validation loss decreased (0.390421 --> 0.390362).  Saving model ...
Validation loss decreased (0.390362 --> 0.390233).  Saving model ...
Validation loss decreased (0.390233 --> 0.390083).  Saving model ...
Validation loss decreased (0.390083 --> 0.389862).  Saving model ...
Validation loss decreased (0.389862 --> 0.389647).  Saving model ...
Validation loss decreased (0.389647 --> 0.389476).  Saving model ...
Validation loss decreased (0.389476 --> 0.389277).  Saving model ...
Validation loss decreased (0.389277 --> 0.388974).  Saving model ...
Validation loss decreased (0.388974 --> 0.388789).  Saving model ...
Validation loss decreased (0.388789 --> 0.388541).  Saving model ...
Validation loss decreased (0.388541 --> 0.388193).  Saving model ...
Validation loss decreased (0.388193 --> 0.387804).  Saving model ...
Validation loss decreased (0.387804 --> 0.387385).  Saving model ...
Validation loss decreased (0.387385 --> 0.386960).  Saving model ...
Validation loss decreased (0.386960 --> 0.386542).  Saving model ...
Validation loss decreased (0.386542 --> 0.386107).  Saving model ...
Validation loss decreased (0.386107 --> 0.385703).  Saving model ...
Validation loss decreased (0.385703 --> 0.385300).  Saving model ...
Validation loss decreased (0.385300 --> 0.384864).  Saving model ...
Validation loss decreased (0.384864 --> 0.384479).  Saving model ...
Validation loss decreased (0.384479 --> 0.384116).  Saving model ...
Validation loss decreased (0.384116 --> 0.383799).  Saving model ...
Validation loss decreased (0.383799 --> 0.383523).  Saving model ...
Validation loss decreased (0.383523 --> 0.383198).  Saving model ...
Validation loss decreased (0.383198 --> 0.382858).  Saving model ...
Validation loss decreased (0.382858 --> 0.382527).  Saving model ...
Validation loss decreased (0.382527 --> 0.382277).  Saving model ...
Validation loss decreased (0.382277 --> 0.382012).  Saving model ...
Validation loss decreased (0.382012 --> 0.381736).  Saving model ...
Validation loss decreased (0.381736 --> 0.381464).  Saving model ...
Validation loss decreased (0.381464 --> 0.381238).  Saving model ...
Validation loss decreased (0.381238 --> 0.381020).  Saving model ...
Validation loss decreased (0.381020 --> 0.380838).  Saving model ...
Validation loss decreased (0.380838 --> 0.380668).  Saving model ...
Validation loss decreased (0.380668 --> 0.380514).  Saving model ...
Validation loss decreased (0.380514 --> 0.380417).  Saving model ...
Validation loss decreased (0.380417 --> 0.380321).  Saving model ...
Validation loss decreased (0.380321 --> 0.380188).  Saving model ...
Validation loss decreased (0.380188 --> 0.380079).  Saving model ...
Validation loss decreased (0.380079 --> 0.380025).  Saving model ...
Validation loss decreased (0.380025 --> 0.379954).  Saving model ...
Validation loss decreased (0.379954 --> 0.379871).  Saving model ...
Validation loss decreased (0.379871 --> 0.379718).  Saving model ...
Validation loss decreased (0.379718 --> 0.379509).  Saving model ...
Validation loss decreased (0.379509 --> 0.379339).  Saving model ...
Validation loss decreased (0.379339 --> 0.379126).  Saving model ...
Validation loss decreased (0.379126 --> 0.378863).  Saving model ...
Validation loss decreased (0.378863 --> 0.378572).  Saving model ...
Validation loss decreased (0.378572 --> 0.378295).  Saving model ...
Validation loss decreased (0.378295 --> 0.378004).  Saving model ...
Validation loss decreased (0.378004 --> 0.377672).  Saving model ...
Validation loss decreased (0.377672 --> 0.377361).  Saving model ...
Validation loss decreased (0.377361 --> 0.377042).  Saving model ...
Validation loss decreased (0.377042 --> 0.376811).  Saving model ...
Validation loss decreased (0.376811 --> 0.376539).  Saving model ...
Validation loss decreased (0.376539 --> 0.376281).  Saving model ...
Validation loss decreased (0.376281 --> 0.376092).  Saving model ...
Validation loss decreased (0.376092 --> 0.375841).  Saving model ...
Validation loss decreased (0.375841 --> 0.375661).  Saving model ...
Validation loss decreased (0.375661 --> 0.375464).  Saving model ...
Validation loss decreased (0.375464 --> 0.375282).  Saving model ...
Validation loss decreased (0.375282 --> 0.375069).  Saving model ...
Validation loss decreased (0.375069 --> 0.374914).  Saving model ...
Validation loss decreased (0.374914 --> 0.374645).  Saving model ...
Validation loss decreased (0.374645 --> 0.374419).  Saving model ...
Validation loss decreased (0.374419 --> 0.374201).  Saving model ...
Validation loss decreased (0.374201 --> 0.373933).  Saving model ...
Validation loss decreased (0.373933 --> 0.373652).  Saving model ...
Validation loss decreased (0.373652 --> 0.373295).  Saving model ...
Validation loss decreased (0.373295 --> 0.372975).  Saving model ...
Validation loss decreased (0.372975 --> 0.372653).  Saving model ...
epoch 401, loss 0.5746, train acc 84.50%, f1 0.8450, precision 0.8450, recall 0.8450, auc 0.8450
Validation loss decreased (0.372653 --> 0.372373).  Saving model ...
Validation loss decreased (0.372373 --> 0.372079).  Saving model ...
Validation loss decreased (0.372079 --> 0.371835).  Saving model ...
Validation loss decreased (0.371835 --> 0.371614).  Saving model ...
Validation loss decreased (0.371614 --> 0.371305).  Saving model ...
Validation loss decreased (0.371305 --> 0.371005).  Saving model ...
Validation loss decreased (0.371005 --> 0.370743).  Saving model ...
Validation loss decreased (0.370743 --> 0.370508).  Saving model ...
Validation loss decreased (0.370508 --> 0.370337).  Saving model ...
Validation loss decreased (0.370337 --> 0.370150).  Saving model ...
Validation loss decreased (0.370150 --> 0.369978).  Saving model ...
Validation loss decreased (0.369978 --> 0.369752).  Saving model ...
Validation loss decreased (0.369752 --> 0.369461).  Saving model ...
Validation loss decreased (0.369461 --> 0.369103).  Saving model ...
Validation loss decreased (0.369103 --> 0.368809).  Saving model ...
Validation loss decreased (0.368809 --> 0.368597).  Saving model ...
Validation loss decreased (0.368597 --> 0.368337).  Saving model ...
Validation loss decreased (0.368337 --> 0.368106).  Saving model ...
Validation loss decreased (0.368106 --> 0.367856).  Saving model ...
Validation loss decreased (0.367856 --> 0.367644).  Saving model ...
Validation loss decreased (0.367644 --> 0.367511).  Saving model ...
Validation loss decreased (0.367511 --> 0.367397).  Saving model ...
Validation loss decreased (0.367397 --> 0.367303).  Saving model ...
Validation loss decreased (0.367303 --> 0.367231).  Saving model ...
Validation loss decreased (0.367231 --> 0.367113).  Saving model ...
Validation loss decreased (0.367113 --> 0.366958).  Saving model ...
Validation loss decreased (0.366958 --> 0.366820).  Saving model ...
Validation loss decreased (0.366820 --> 0.366714).  Saving model ...
Validation loss decreased (0.366714 --> 0.366583).  Saving model ...
Validation loss decreased (0.366583 --> 0.366493).  Saving model ...
Validation loss decreased (0.366493 --> 0.366360).  Saving model ...
Validation loss decreased (0.366360 --> 0.366245).  Saving model ...
Validation loss decreased (0.366245 --> 0.366105).  Saving model ...
Validation loss decreased (0.366105 --> 0.365997).  Saving model ...
Validation loss decreased (0.365997 --> 0.365855).  Saving model ...
Validation loss decreased (0.365855 --> 0.365733).  Saving model ...
Validation loss decreased (0.365733 --> 0.365558).  Saving model ...
Validation loss decreased (0.365558 --> 0.365479).  Saving model ...
Validation loss decreased (0.365479 --> 0.365368).  Saving model ...
Validation loss decreased (0.365368 --> 0.365240).  Saving model ...
Validation loss decreased (0.365240 --> 0.365041).  Saving model ...
Validation loss decreased (0.365041 --> 0.364794).  Saving model ...
Validation loss decreased (0.364794 --> 0.364597).  Saving model ...
Validation loss decreased (0.364597 --> 0.364435).  Saving model ...
Validation loss decreased (0.364435 --> 0.364236).  Saving model ...
Validation loss decreased (0.364236 --> 0.364127).  Saving model ...
Validation loss decreased (0.364127 --> 0.364018).  Saving model ...
Validation loss decreased (0.364018 --> 0.363922).  Saving model ...
Validation loss decreased (0.363922 --> 0.363814).  Saving model ...
Validation loss decreased (0.363814 --> 0.363686).  Saving model ...
Validation loss decreased (0.363686 --> 0.363558).  Saving model ...
Validation loss decreased (0.363558 --> 0.363400).  Saving model ...
Validation loss decreased (0.363400 --> 0.363244).  Saving model ...
Validation loss decreased (0.363244 --> 0.363087).  Saving model ...
Validation loss decreased (0.363087 --> 0.362940).  Saving model ...
Validation loss decreased (0.362940 --> 0.362761).  Saving model ...
Validation loss decreased (0.362761 --> 0.362543).  Saving model ...
Validation loss decreased (0.362543 --> 0.362368).  Saving model ...
Validation loss decreased (0.362368 --> 0.362190).  Saving model ...
Validation loss decreased (0.362190 --> 0.362080).  Saving model ...
Validation loss decreased (0.362080 --> 0.362006).  Saving model ...
Validation loss decreased (0.362006 --> 0.361976).  Saving model ...
Validation loss decreased (0.361976 --> 0.361941).  Saving model ...
Validation loss decreased (0.361941 --> 0.361855).  Saving model ...
Validation loss decreased (0.361855 --> 0.361681).  Saving model ...
Validation loss decreased (0.361681 --> 0.361508).  Saving model ...
Validation loss decreased (0.361508 --> 0.361296).  Saving model ...
Validation loss decreased (0.361296 --> 0.361075).  Saving model .../home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

Validation loss decreased (0.361075 --> 0.360889).  Saving model ...
Validation loss decreased (0.360889 --> 0.360769).  Saving model ...
Validation loss decreased (0.360769 --> 0.360662).  Saving model ...
Validation loss decreased (0.360662 --> 0.360514).  Saving model ...
Validation loss decreased (0.360514 --> 0.360314).  Saving model ...
Validation loss decreased (0.360314 --> 0.360163).  Saving model ...
Validation loss decreased (0.360163 --> 0.360018).  Saving model ...
Validation loss decreased (0.360018 --> 0.359912).  Saving model ...
Validation loss decreased (0.359912 --> 0.359781).  Saving model ...
Validation loss decreased (0.359781 --> 0.359699).  Saving model ...
Validation loss decreased (0.359699 --> 0.359650).  Saving model ...
EarlyStopping counter: 1 out of 20
Validation loss decreased (0.359650 --> 0.359595).  Saving model ...
Validation loss decreased (0.359595 --> 0.359557).  Saving model ...
Validation loss decreased (0.359557 --> 0.359549).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
epoch 501, loss 0.4421, train acc 86.00%, f1 0.8600, precision 0.8600, recall 0.8600, auc 0.8600
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 503, loss 0.3950, train acc 86.00%, f1 0.8600, precision 0.8600, recall 0.8600, auc 0.8600



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_True
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_True/record_1/MLP_minus_Mirror_True_1
./test_pima/result_MLP_minus_Mirror_True_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.5957407407407407

the Fscore is 0.5698924731182795

the precision is 0.4015151515151515

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5941, train acc 77.99%, f1 0.7821, precision 0.7742, recall 0.7902, auc 0.7799
epoch 201, loss 0.4405, train acc 80.68%, f1 0.8069, precision 0.8065, recall 0.8073, auc 0.8068
epoch 301, loss 0.4364, train acc 82.00%, f1 0.8200, precision 0.8199, recall 0.8201, auc 0.8200
epoch 401, loss 0.3736, train acc 82.76%, f1 0.8275, precision 0.8278, recall 0.8271, auc 0.8276
epoch 501, loss 0.4383, train acc 83.14%, f1 0.8313, precision 0.8317, recall 0.8308, auc 0.8314
epoch 601, loss 0.3748, train acc 83.31%, f1 0.8330, precision 0.8336, recall 0.8324, auc 0.8331
epoch 701, loss 0.4753, train acc 83.45%, f1 0.8345, precision 0.8347, recall 0.8342, auc 0.8345
epoch 801, loss 0.3748, train acc 83.50%, f1 0.8349, precision 0.8352, recall 0.8346, auc 0.8350
epoch 901, loss 0.3444, train acc 83.49%, f1 0.8349, precision 0.8350, recall 0.8348, auc 0.8349
epoch 1001, loss 0.4492, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8347, auc 0.8348
epoch 1101, loss 0.3348, train acc 83.49%, f1 0.8348, precision 0.8350, recall 0.8347, auc 0.8349
epoch 1201, loss 0.4013, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 1301, loss 0.3567, train acc 83.49%, f1 0.8349, precision 0.8350, recall 0.8348, auc 0.8349
epoch 1401, loss 0.4545, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8347, auc 0.8347
epoch 1501, loss 0.3378, train acc 83.45%, f1 0.8345, precision 0.8345, recall 0.8345, auc 0.8345
epoch 1601, loss 0.4526, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8349, auc 0.8349
epoch 1701, loss 0.3770, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8347, auc 0.8348
epoch 1801, loss 0.3255, train acc 83.50%, f1 0.8350, precision 0.8350, recall 0.8350, auc 0.8350
epoch 1901, loss 0.4334, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8352, auc 0.8351
epoch 2001, loss 0.3389, train acc 83.52%, f1 0.8352, precision 0.8352, recall 0.8352, auc 0.8352
epoch 2101, loss 0.4156, train acc 83.49%, f1 0.8349, precision 0.8350, recall 0.8349, auc 0.8349
epoch 2201, loss 0.3175, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8349, auc 0.8348
epoch 2301, loss 0.4342, train acc 83.45%, f1 0.8345, precision 0.8345, recall 0.8345, auc 0.8345
epoch 2401, loss 0.4241, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8348, auc 0.8347
epoch 2501, loss 0.4079, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 2601, loss 0.3553, train acc 83.45%, f1 0.8345, precision 0.8345, recall 0.8345, auc 0.8345
epoch 2701, loss 0.4688, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8348, auc 0.8347
epoch 2801, loss 0.3520, train acc 83.49%, f1 0.8349, precision 0.8348, recall 0.8350, auc 0.8349
epoch 2901, loss 0.2561, train acc 83.50%, f1 0.8350, precision 0.8349, recall 0.8350, auc 0.8350
epoch 3001, loss 0.4440, train acc 83.49%, f1 0.8349, precision 0.8348, recall 0.8350, auc 0.8349
epoch 3101, loss 0.2768, train acc 83.54%, f1 0.8354, precision 0.8354, recall 0.8353, auc 0.8354
epoch 3201, loss 0.3996, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8351, auc 0.8351
epoch 3301, loss 0.4016, train acc 83.52%, f1 0.8352, precision 0.8351, recall 0.8353, auc 0.8352
epoch 3401, loss 0.4075, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8350, auc 0.8349
epoch 3501, loss 0.4611, train acc 83.50%, f1 0.8349, precision 0.8350, recall 0.8349, auc 0.8350
epoch 3601, loss 0.4093, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8347, auc 0.8347
epoch 3701, loss 0.3497, train acc 83.49%, f1 0.8349, precision 0.8350, recall 0.8348, auc 0.8349
epoch 3801, loss 0.3174, train acc 83.50%, f1 0.8350, precision 0.8349, recall 0.8350, auc 0.8350
epoch 3901, loss 0.3445, train acc 83.49%, f1 0.8349, precision 0.8348, recall 0.8350, auc 0.8349
epoch 4001, loss 0.3931, train acc 83.55%, f1 0.8355, precision 0.8356, recall 0.8353, auc 0.8355
epoch 4101, loss 0.2859, train acc 83.62%, f1 0.8362, precision 0.8361, recall 0.8362, auc 0.8362
epoch 4201, loss 0.3414, train acc 83.57%, f1 0.8357, precision 0.8356, recall 0.8359, auc 0.8357
epoch 4301, loss 0.2742, train acc 83.60%, f1 0.8360, precision 0.8360, recall 0.8361, auc 0.8360
epoch 4401, loss 0.5079, train acc 83.59%, f1 0.8359, precision 0.8358, recall 0.8359, auc 0.8359
epoch 4501, loss 0.4597, train acc 83.63%, f1 0.8363, precision 0.8363, recall 0.8363, auc 0.8363
epoch 4601, loss 0.3658, train acc 83.63%, f1 0.8363, precision 0.8362, recall 0.8363, auc 0.8363
epoch 4701, loss 0.3577, train acc 83.65%, f1 0.8365, precision 0.8365, recall 0.8364, auc 0.8365
epoch 4801, loss 0.3835, train acc 83.67%, f1 0.8367, precision 0.8367, recall 0.8366, auc 0.8367
epoch 4901, loss 0.4663, train acc 83.71%, f1 0.8371, precision 0.8371, recall 0.8371, auc 0.8371
epoch 5001, loss 0.3173, train acc 83.71%, f1 0.8371, precision 0.8371, recall 0.8371, auc 0.8371
epoch 5101, loss 0.3329, train acc 83.76%, f1 0.8376, precision 0.8376, recall 0.8377, auc 0.8376
epoch 5201, loss 0.3056, train acc 83.75%, f1 0.8375, precision 0.8375, recall 0.8375, auc 0.8375
epoch 5301, loss 0.3545, train acc 83.64%, f1 0.8364, precision 0.8364, recall 0.8365, auc 0.8364
epoch 5401, loss 0.4477, train acc 83.77%, f1 0.8378, precision 0.8377, recall 0.8378, auc 0.8377
epoch 5501, loss 0.3009, train acc 83.75%, f1 0.8375, precision 0.8375, recall 0.8375, auc 0.8375
epoch 5601, loss 0.3273, train acc 83.84%, f1 0.8384, precision 0.8383, recall 0.8384, auc 0.8384
epoch 5701, loss 0.3648, train acc 83.78%, f1 0.8379, precision 0.8378, recall 0.8379, auc 0.8378
epoch 5801, loss 0.3242, train acc 83.80%, f1 0.8380, precision 0.8380, recall 0.8380, auc 0.8380
epoch 5901, loss 0.4015, train acc 83.88%, f1 0.8388, precision 0.8388, recall 0.8389, auc 0.8388
epoch 6001, loss 0.2581, train acc 83.87%, f1 0.8387, precision 0.8386, recall 0.8387, auc 0.8387
epoch 6101, loss 0.4925, train acc 83.87%, f1 0.8387, precision 0.8386, recall 0.8387, auc 0.8387
epoch 6201, loss 0.3443, train acc 83.93%, f1 0.8393, precision 0.8393, recall 0.8393, auc 0.8393
epoch 6301, loss 0.3423, train acc 83.95%, f1 0.8395, precision 0.8394, recall 0.8395, auc 0.8395
epoch 6401, loss 0.3294, train acc 83.96%, f1 0.8396, precision 0.8396, recall 0.8396, auc 0.8396
epoch 6501, loss 0.3546, train acc 83.99%, f1 0.8399, precision 0.8399, recall 0.8399, auc 0.8399
epoch 6601, loss 0.4031, train acc 83.99%, f1 0.8399, precision 0.8399, recall 0.8400, auc 0.8399
epoch 6701, loss 0.4209, train acc 84.06%, f1 0.8406, precision 0.8405, recall 0.8407, auc 0.8406
epoch 6801, loss 0.3756, train acc 84.10%, f1 0.8410, precision 0.8410, recall 0.8409, auc 0.8410
epoch 6901, loss 0.3404, train acc 84.12%, f1 0.8412, precision 0.8411, recall 0.8412, auc 0.8412
epoch 7001, loss 0.3588, train acc 84.08%, f1 0.8408, precision 0.8408, recall 0.8408, auc 0.8408
epoch 7101, loss 0.4242, train acc 84.14%, f1 0.8414, precision 0.8414, recall 0.8415, auc 0.8414
epoch 7201, loss 0.2872, train acc 84.16%, f1 0.8417, precision 0.8416, recall 0.8417, auc 0.8416
epoch 7301, loss 0.3068, train acc 84.18%, f1 0.8418, precision 0.8418, recall 0.8418, auc 0.8418
epoch 7401, loss 0.3327, train acc 84.17%, f1 0.8418, precision 0.8417, recall 0.8418, auc 0.8417
epoch 7501, loss 0.2604, train acc 84.23%, f1 0.8423, precision 0.8422, recall 0.8423, auc 0.8423
epoch 7601, loss 0.3136, train acc 84.23%, f1 0.8423, precision 0.8422, recall 0.8423, auc 0.8423
epoch 7701, loss 0.3022, train acc 84.27%, f1 0.8427, precision 0.8426, recall 0.8428, auc 0.8427
epoch 7801, loss 0.2747, train acc 84.26%, f1 0.8426, precision 0.8425, recall 0.8426, auc 0.8426
epoch 7901, loss 0.3904, train acc 84.30%, f1 0.8430, precision 0.8430, recall 0.8430, auc 0.8430
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_1
./test_pima/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6557407407407407

the Fscore is 0.6091954022988505

the precision is 0.44166666666666665

the recall is 0.9814814814814815

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_1
----------------------



epoch 1, loss 0.6935, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6225, train acc 78.08%, f1 0.7843, precision 0.7718, recall 0.7972, auc 0.7808
epoch 201, loss 0.4938, train acc 80.40%, f1 0.8043, precision 0.8030, recall 0.8056, auc 0.8040
epoch 301, loss 0.4074, train acc 82.02%, f1 0.8202, precision 0.8205, recall 0.8198, auc 0.8202
epoch 401, loss 0.4351, train acc 82.79%, f1 0.8279, precision 0.8282, recall 0.8276, auc 0.8279
epoch 501, loss 0.4212, train acc 83.15%, f1 0.8313, precision 0.8324, recall 0.8302, auc 0.8315
epoch 601, loss 0.3343, train acc 83.32%, f1 0.8330, precision 0.8339, recall 0.8321, auc 0.8332
epoch 701, loss 0.3517, train acc 83.40%, f1 0.8339, precision 0.8344, recall 0.8334, auc 0.8340
epoch 801, loss 0.4654, train acc 83.48%, f1 0.8346, precision 0.8355, recall 0.8337, auc 0.8348
epoch 901, loss 0.4425, train acc 83.48%, f1 0.8347, precision 0.8352, recall 0.8342, auc 0.8348
epoch 1001, loss 0.3234, train acc 83.50%, f1 0.8349, precision 0.8354, recall 0.8345, auc 0.8350
epoch 1101, loss 0.2910, train acc 83.49%, f1 0.8348, precision 0.8350, recall 0.8347, auc 0.8349
epoch 1201, loss 0.4013, train acc 83.52%, f1 0.8352, precision 0.8352, recall 0.8351, auc 0.8352
epoch 1301, loss 0.3305, train acc 83.51%, f1 0.8351, precision 0.8352, recall 0.8350, auc 0.8351
epoch 1401, loss 0.4739, train acc 83.50%, f1 0.8350, precision 0.8351, recall 0.8349, auc 0.8350
epoch 1501, loss 0.4858, train acc 83.48%, f1 0.8347, precision 0.8349, recall 0.8345, auc 0.8348
epoch 1601, loss 0.5073, train acc 83.49%, f1 0.8349, precision 0.8350, recall 0.8348, auc 0.8349
epoch 1701, loss 0.2800, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 1801, loss 0.2844, train acc 83.47%, f1 0.8347, precision 0.8347, recall 0.8347, auc 0.8347
epoch 1901, loss 0.2904, train acc 83.53%, f1 0.8353, precision 0.8353, recall 0.8352, auc 0.8353
epoch 2001, loss 0.4375, train acc 83.48%, f1 0.8348, precision 0.8348, recall 0.8348, auc 0.8348
epoch 2101, loss 0.3241, train acc 83.50%, f1 0.8350, precision 0.8350, recall 0.8350, auc 0.8350
epoch 2201, loss 0.4170, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8351, auc 0.8351
epoch 2301, loss 0.3093, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8351, auc 0.8351
epoch 2401, loss 0.4228, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8350, auc 0.8349
epoch 2501, loss 0.3714, train acc 83.47%, f1 0.8347, precision 0.8346, recall 0.8348, auc 0.8347
epoch 2601, loss 0.3525, train acc 83.49%, f1 0.8349, precision 0.8349, recall 0.8349, auc 0.8349
epoch 2701, loss 0.4839, train acc 83.49%, f1 0.8349, precision 0.8348, recall 0.8350, auc 0.8349
epoch 2801, loss 0.3692, train acc 83.50%, f1 0.8351, precision 0.8349, recall 0.8352, auc 0.8350
epoch 2901, loss 0.4628, train acc 83.49%, f1 0.8349, precision 0.8348, recall 0.8349, auc 0.8349
epoch 3001, loss 0.3658, train acc 83.52%, f1 0.8352, precision 0.8351, recall 0.8353, auc 0.8352
epoch 3101, loss 0.3884, train acc 83.52%, f1 0.8351, precision 0.8352, recall 0.8351, auc 0.8352
epoch 3201, loss 0.3450, train acc 83.48%, f1 0.8348, precision 0.8347, recall 0.8349, auc 0.8348
epoch 3301, loss 0.3256, train acc 83.52%, f1 0.8352, precision 0.8352, recall 0.8352, auc 0.8352
epoch 3401, loss 0.3840, train acc 83.52%, f1 0.8352, precision 0.8352, recall 0.8352, auc 0.8352
epoch 3501, loss 0.3184, train acc 83.53%, f1 0.8353, precision 0.8352, recall 0.8353, auc 0.8353
epoch 3601, loss 0.3366, train acc 83.53%, f1 0.8352, precision 0.8353, recall 0.8352, auc 0.8353
epoch 3701, loss 0.3914, train acc 83.54%, f1 0.8353, precision 0.8355, recall 0.8352, auc 0.8354
epoch 3801, loss 0.3825, train acc 83.48%, f1 0.8348, precision 0.8347, recall 0.8348, auc 0.8348
epoch 3901, loss 0.4196, train acc 83.59%, f1 0.8359, precision 0.8360, recall 0.8358, auc 0.8359
epoch 4001, loss 0.4129, train acc 83.56%, f1 0.8356, precision 0.8356, recall 0.8356, auc 0.8356
epoch 4101, loss 0.4017, train acc 83.55%, f1 0.8355, precision 0.8354, recall 0.8356, auc 0.8355
epoch 4201, loss 0.3540, train acc 83.63%, f1 0.8363, precision 0.8364, recall 0.8362, auc 0.8363
epoch 4301, loss 0.3377, train acc 83.63%, f1 0.8362, precision 0.8363, recall 0.8362, auc 0.8363
epoch 4401, loss 0.4345, train acc 83.66%, f1 0.8366, precision 0.8364, recall 0.8367, auc 0.8366
epoch 4501, loss 0.3222, train acc 83.70%, f1 0.8369, precision 0.8372, recall 0.8367, auc 0.8370
epoch 4601, loss 0.3676, train acc 83.69%, f1 0.8369, precision 0.8367, recall 0.8370, auc 0.8369
epoch 4701, loss 0.4331, train acc 83.71%, f1 0.8372, precision 0.8370, recall 0.8373, auc 0.8371
epoch 4801, loss 0.3807, train acc 83.75%, f1 0.8375, precision 0.8376, recall 0.8374, auc 0.8375
epoch 4901, loss 0.3453, train acc 83.76%, f1 0.8376, precision 0.8378, recall 0.8374, auc 0.8376
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_1
./test_pima/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.625

the Fscore is 0.5901639344262295

the precision is 0.4186046511627907

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_1
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6054, train acc 78.03%, f1 0.7778, precision 0.7867, recall 0.7690, auc 0.7803
epoch 201, loss 0.4403, train acc 80.48%, f1 0.8043, precision 0.8061, recall 0.8025, auc 0.8048
epoch 301, loss 0.4045, train acc 81.97%, f1 0.8198, precision 0.8196, recall 0.8200, auc 0.8197
epoch 401, loss 0.3429, train acc 82.79%, f1 0.8279, precision 0.8279, recall 0.8279, auc 0.8279
epoch 501, loss 0.3099, train acc 83.13%, f1 0.8314, precision 0.8308, recall 0.8320, auc 0.8313
epoch 601, loss 0.3758, train acc 83.29%, f1 0.8330, precision 0.8323, recall 0.8337, auc 0.8329
epoch 701, loss 0.4766, train acc 83.42%, f1 0.8343, precision 0.8340, recall 0.8345, auc 0.8342
epoch 801, loss 0.3660, train acc 83.40%, f1 0.8340, precision 0.8339, recall 0.8341, auc 0.8340
epoch 901, loss 0.4280, train acc 83.44%, f1 0.8345, precision 0.8341, recall 0.8349, auc 0.8344
epoch 1001, loss 0.3099, train acc 83.52%, f1 0.8352, precision 0.8349, recall 0.8356, auc 0.8352
epoch 1101, loss 0.3687, train acc 83.50%, f1 0.8350, precision 0.8349, recall 0.8350, auc 0.8350
epoch 1201, loss 0.4282, train acc 83.44%, f1 0.8345, precision 0.8343, recall 0.8346, auc 0.8344
epoch 1301, loss 0.3681, train acc 83.50%, f1 0.8350, precision 0.8350, recall 0.8350, auc 0.8350
epoch 1401, loss 0.2618, train acc 83.45%, f1 0.8346, precision 0.8344, recall 0.8347, auc 0.8345
epoch 1501, loss 0.3526, train acc 83.50%, f1 0.8350, precision 0.8350, recall 0.8350, auc 0.8350
epoch 1601, loss 0.3996, train acc 83.46%, f1 0.8346, precision 0.8346, recall 0.8346, auc 0.8346
epoch 1701, loss 0.4063, train acc 83.53%, f1 0.8354, precision 0.8351, recall 0.8356, auc 0.8353
epoch 1801, loss 0.3509, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8352, auc 0.8351
epoch 1901, loss 0.4100, train acc 83.51%, f1 0.8351, precision 0.8351, recall 0.8351, auc 0.8351
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_1
./test_pima/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6007407407407408

the Fscore is 0.572972972972973

the precision is 0.40458015267175573

the recall is 0.9814814814814815

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_True/record_1/MLP_minus_notMirror_True_1
----------------------



epoch 1, loss 0.6930, train acc 46.00%, f1 0.6301, precision 0.4600, recall 1.0000, auc 0.5000
Validation loss decreased (inf --> 0.693763).  Saving model ...
Validation loss decreased (0.693763 --> 0.693370).  Saving model ...
Validation loss decreased (0.693370 --> 0.692959).  Saving model ...
Validation loss decreased (0.692959 --> 0.692575).  Saving model ...
Validation loss decreased (0.692575 --> 0.692201).  Saving model ...
Validation loss decreased (0.692201 --> 0.691873).  Saving model ...
Validation loss decreased (0.691873 --> 0.691633).  Saving model ...
Validation loss decreased (0.691633 --> 0.691442).  Saving model ...
Validation loss decreased (0.691442 --> 0.691273).  Saving model ...
Validation loss decreased (0.691273 --> 0.691100).  Saving model ...
Validation loss decreased (0.691100 --> 0.690921).  Saving model ...
Validation loss decreased (0.690921 --> 0.690784).  Saving model ...
Validation loss decreased (0.690784 --> 0.690632).  Saving model ...
Validation loss decreased (0.690632 --> 0.690540).  Saving model ...
Validation loss decreased (0.690540 --> 0.690422).  Saving model ...
Validation loss decreased (0.690422 --> 0.690292).  Saving model ...
Validation loss decreased (0.690292 --> 0.690157).  Saving model ...
Validation loss decreased (0.690157 --> 0.690026).  Saving model ...
Validation loss decreased (0.690026 --> 0.689883).  Saving model ...
Validation loss decreased (0.689883 --> 0.689750).  Saving model ...
Validation loss decreased (0.689750 --> 0.689656).  Saving model ...
Validation loss decreased (0.689656 --> 0.689516).  Saving model ...
Validation loss decreased (0.689516 --> 0.689357).  Saving model ...
Validation loss decreased (0.689357 --> 0.689186).  Saving model ...
Validation loss decreased (0.689186 --> 0.688955).  Saving model ...
Validation loss decreased (0.688955 --> 0.688673).  Saving model ...
Validation loss decreased (0.688673 --> 0.688414).  Saving model ...
Validation loss decreased (0.688414 --> 0.688110).  Saving model ...
Validation loss decreased (0.688110 --> 0.687795).  Saving model ...
Validation loss decreased (0.687795 --> 0.687454).  Saving model ...
Validation loss decreased (0.687454 --> 0.687067).  Saving model ...
Validation loss decreased (0.687067 --> 0.686651).  Saving model ...
Validation loss decreased (0.686651 --> 0.686187).  Saving model ...
Validation loss decreased (0.686187 --> 0.685707).  Saving model ...
Validation loss decreased (0.685707 --> 0.685219).  Saving model ...
Validation loss decreased (0.685219 --> 0.684695).  Saving model ...
Validation loss decreased (0.684695 --> 0.684147).  Saving model ...
Validation loss decreased (0.684147 --> 0.683556).  Saving model ...
Validation loss decreased (0.683556 --> 0.682958).  Saving model ...
Validation loss decreased (0.682958 --> 0.682333).  Saving model ...
Validation loss decreased (0.682333 --> 0.681689).  Saving model ...
Validation loss decreased (0.681689 --> 0.681025).  Saving model ...
Validation loss decreased (0.681025 --> 0.680335).  Saving model ...
Validation loss decreased (0.680335 --> 0.679646).  Saving model ...
Validation loss decreased (0.679646 --> 0.678939).  Saving model ...
Validation loss decreased (0.678939 --> 0.678202).  Saving model ...
Validation loss decreased (0.678202 --> 0.677424).  Saving model ...
Validation loss decreased (0.677424 --> 0.676641).  Saving model ...
Validation loss decreased (0.676641 --> 0.675809).  Saving model ...
Validation loss decreased (0.675809 --> 0.674946).  Saving model ...
Validation loss decreased (0.674946 --> 0.674057).  Saving model ...
Validation loss decreased (0.674057 --> 0.673141).  Saving model ...
Validation loss decreased (0.673141 --> 0.672189).  Saving model ...
Validation loss decreased (0.672189 --> 0.671215).  Saving model ...
Validation loss decreased (0.671215 --> 0.670230).  Saving model ...
Validation loss decreased (0.670230 --> 0.669214).  Saving model ...
Validation loss decreased (0.669214 --> 0.668192).  Saving model ...
Validation loss decreased (0.668192 --> 0.667133).  Saving model ...
Validation loss decreased (0.667133 --> 0.666079).  Saving model ...
Validation loss decreased (0.666079 --> 0.665009).  Saving model ...
Validation loss decreased (0.665009 --> 0.663890).  Saving model ...
Validation loss decreased (0.663890 --> 0.662791).  Saving model ...
Validation loss decreased (0.662791 --> 0.661705).  Saving model ...
Validation loss decreased (0.661705 --> 0.660617).  Saving model ...
Validation loss decreased (0.660617 --> 0.659484).  Saving model ...
Validation loss decreased (0.659484 --> 0.658299).  Saving model ...
Validation loss decreased (0.658299 --> 0.657088).  Saving model ...
Validation loss decreased (0.657088 --> 0.655869).  Saving model ...
Validation loss decreased (0.655869 --> 0.654652).  Saving model ...
Validation loss decreased (0.654652 --> 0.653420).  Saving model ...
Validation loss decreased (0.653420 --> 0.652178).  Saving model ...
Validation loss decreased (0.652178 --> 0.650880).  Saving model ...
Validation loss decreased (0.650880 --> 0.649582).  Saving model ...
Validation loss decreased (0.649582 --> 0.648299).  Saving model ...
Validation loss decreased (0.648299 --> 0.646995).  Saving model ...
Validation loss decreased (0.646995 --> 0.645676).  Saving model ...
Validation loss decreased (0.645676 --> 0.644339).  Saving model ...
Validation loss decreased (0.644339 --> 0.642975).  Saving model ...
Validation loss decreased (0.642975 --> 0.641587).  Saving model ...
Validation loss decreased (0.641587 --> 0.640221).  Saving model ...
Validation loss decreased (0.640221 --> 0.638841).  Saving model ...
Validation loss decreased (0.638841 --> 0.637414).  Saving model ...
Validation loss decreased (0.637414 --> 0.636006).  Saving model ...
Validation loss decreased (0.636006 --> 0.634546).  Saving model ...
Validation loss decreased (0.634546 --> 0.633063).  Saving model ...
Validation loss decreased (0.633063 --> 0.631570).  Saving model ...
Validation loss decreased (0.631570 --> 0.630045).  Saving model ...
Validation loss decreased (0.630045 --> 0.628515).  Saving model ...
Validation loss decreased (0.628515 --> 0.626923).  Saving model ...
Validation loss decreased (0.626923 --> 0.625341).  Saving model ...
Validation loss decreased (0.625341 --> 0.623760).  Saving model ...
Validation loss decreased (0.623760 --> 0.622175).  Saving model ...
Validation loss decreased (0.622175 --> 0.620578).  Saving model ...
Validation loss decreased (0.620578 --> 0.618992).  Saving model ...
Validation loss decreased (0.618992 --> 0.617420).  Saving model ...
Validation loss decreased (0.617420 --> 0.615839).  Saving model ...
Validation loss decreased (0.615839 --> 0.614266).  Saving model ...
Validation loss decreased (0.614266 --> 0.612666).  Saving model ...
Validation loss decreased (0.612666 --> 0.611045).  Saving model ...
Validation loss decreased (0.611045 --> 0.609381).  Saving model ...
epoch 101, loss 0.6099, train acc 82.00%, f1 0.8043, precision 0.8043, recall 0.8043, auc 0.8188
Validation loss decreased (0.609381 --> 0.607736).  Saving model ...
Validation loss decreased (0.607736 --> 0.606088).  Saving model ...
Validation loss decreased (0.606088 --> 0.604422).  Saving model ...
Validation loss decreased (0.604422 --> 0.602779).  Saving model ...
Validation loss decreased (0.602779 --> 0.601204).  Saving model ...
Validation loss decreased (0.601204 --> 0.599581).  Saving model ...
Validation loss decreased (0.599581 --> 0.597938).  Saving model ...
Validation loss decreased (0.597938 --> 0.596284).  Saving model ...
Validation loss decreased (0.596284 --> 0.594602).  Saving model ...
Validation loss decreased (0.594602 --> 0.592905).  Saving model ...
Validation loss decreased (0.592905 --> 0.591190).  Saving model ...
Validation loss decreased (0.591190 --> 0.589442).  Saving model ...
Validation loss decreased (0.589442 --> 0.587748).  Saving model ...
Validation loss decreased (0.587748 --> 0.586088).  Saving model ...
Validation loss decreased (0.586088 --> 0.584439).  Saving model ...
Validation loss decreased (0.584439 --> 0.582814).  Saving model ...
Validation loss decreased (0.582814 --> 0.581195).  Saving model ...
Validation loss decreased (0.581195 --> 0.579508).  Saving model ...
Validation loss decreased (0.579508 --> 0.577790).  Saving model ...
Validation loss decreased (0.577790 --> 0.576082).  Saving model ...
Validation loss decreased (0.576082 --> 0.574391).  Saving model ...
Validation loss decreased (0.574391 --> 0.572671).  Saving model ...
Validation loss decreased (0.572671 --> 0.570971).  Saving model ...
Validation loss decreased (0.570971 --> 0.569277).  Saving model ...
Validation loss decreased (0.569277 --> 0.567588).  Saving model ...
Validation loss decreased (0.567588 --> 0.565884).  Saving model ...
Validation loss decreased (0.565884 --> 0.564166).  Saving model ...
Validation loss decreased (0.564166 --> 0.562423).  Saving model ...
Validation loss decreased (0.562423 --> 0.560683).  Saving model ...
Validation loss decreased (0.560683 --> 0.558918).  Saving model ...
Validation loss decreased (0.558918 --> 0.557121).  Saving model ...
Validation loss decreased (0.557121 --> 0.555329).  Saving model ...
Validation loss decreased (0.555329 --> 0.553570).  Saving model ...
Validation loss decreased (0.553570 --> 0.551816).  Saving model ...
Validation loss decreased (0.551816 --> 0.550062).  Saving model ...
Validation loss decreased (0.550062 --> 0.548345).  Saving model ...
Validation loss decreased (0.548345 --> 0.546719).  Saving model ...
Validation loss decreased (0.546719 --> 0.545057).  Saving model ...
Validation loss decreased (0.545057 --> 0.543417).  Saving model ...
Validation loss decreased (0.543417 --> 0.541769).  Saving model ...
Validation loss decreased (0.541769 --> 0.540134).  Saving model ...
Validation loss decreased (0.540134 --> 0.538488).  Saving model ...
Validation loss decreased (0.538488 --> 0.536878).  Saving model ...
Validation loss decreased (0.536878 --> 0.535297).  Saving model ...
Validation loss decreased (0.535297 --> 0.533727).  Saving model ...
Validation loss decreased (0.533727 --> 0.532142).  Saving model ...
Validation loss decreased (0.532142 --> 0.530565).  Saving model ...
Validation loss decreased (0.530565 --> 0.529004).  Saving model ...
Validation loss decreased (0.529004 --> 0.527409).  Saving model ...
Validation loss decreased (0.527409 --> 0.525778).  Saving model ...
Validation loss decreased (0.525778 --> 0.524154).  Saving model ...
Validation loss decreased (0.524154 --> 0.522608).  Saving model ...
Validation loss decreased (0.522608 --> 0.521055).  Saving model ...
Validation loss decreased (0.521055 --> 0.519495).  Saving model ...
Validation loss decreased (0.519495 --> 0.517913).  Saving model ...
Validation loss decreased (0.517913 --> 0.516278).  Saving model ...
Validation loss decreased (0.516278 --> 0.514609).  Saving model ...
Validation loss decreased (0.514609 --> 0.512943).  Saving model ...
Validation loss decreased (0.512943 --> 0.511285).  Saving model ...
Validation loss decreased (0.511285 --> 0.509645).  Saving model ...
Validation loss decreased (0.509645 --> 0.507997).  Saving model ...
Validation loss decreased (0.507997 --> 0.506403).  Saving model ...
Validation loss decreased (0.506403 --> 0.504777).  Saving model ...
Validation loss decreased (0.504777 --> 0.503175).  Saving model ...
Validation loss decreased (0.503175 --> 0.501688).  Saving model ...
Validation loss decreased (0.501688 --> 0.500207).  Saving model ...
Validation loss decreased (0.500207 --> 0.498718).  Saving model ...
Validation loss decreased (0.498718 --> 0.497260).  Saving model ...
Validation loss decreased (0.497260 --> 0.495784).  Saving model ...
Validation loss decreased (0.495784 --> 0.494255).  Saving model ...
Validation loss decreased (0.494255 --> 0.492713).  Saving model ...
Validation loss decreased (0.492713 --> 0.491251).  Saving model ...
Validation loss decreased (0.491251 --> 0.489775).  Saving model ...
Validation loss decreased (0.489775 --> 0.488235).  Saving model ...
Validation loss decreased (0.488235 --> 0.486710).  Saving model ...
Validation loss decreased (0.486710 --> 0.485178).  Saving model ...
Validation loss decreased (0.485178 --> 0.483616).  Saving model ...
Validation loss decreased (0.483616 --> 0.482070).  Saving model ...
Validation loss decreased (0.482070 --> 0.480578).  Saving model ...
Validation loss decreased (0.480578 --> 0.479184).  Saving model ...
Validation loss decreased (0.479184 --> 0.477895).  Saving model ...
Validation loss decreased (0.477895 --> 0.476611).  Saving model ...
Validation loss decreased (0.476611 --> 0.475378).  Saving model ...
Validation loss decreased (0.475378 --> 0.474167).  Saving model ...
Validation loss decreased (0.474167 --> 0.472979).  Saving model ...
Validation loss decreased (0.472979 --> 0.471801).  Saving model ...
Validation loss decreased (0.471801 --> 0.470632).  Saving model ...
Validation loss decreased (0.470632 --> 0.469489).  Saving model ...
Validation loss decreased (0.469489 --> 0.468308).  Saving model ...
Validation loss decreased (0.468308 --> 0.467095).  Saving model ...
Validation loss decreased (0.467095 --> 0.465884).  Saving model ...
Validation loss decreased (0.465884 --> 0.464689).  Saving model ...
Validation loss decreased (0.464689 --> 0.463491).  Saving model ...
Validation loss decreased (0.463491 --> 0.462261).  Saving model ...
Validation loss decreased (0.462261 --> 0.460972).  Saving model ...
Validation loss decreased (0.460972 --> 0.459660).  Saving model ...
Validation loss decreased (0.459660 --> 0.458431).  Saving model ...
Validation loss decreased (0.458431 --> 0.457219).  Saving model ...
Validation loss decreased (0.457219 --> 0.455979).  Saving model ...
Validation loss decreased (0.455979 --> 0.454711).  Saving model ...
epoch 201, loss 0.4678, train acc 84.50%, f1 0.8306, precision 0.8352, recall 0.8261, auc 0.8436
Validation loss decreased (0.454711 --> 0.453436).  Saving model ...
Validation loss decreased (0.453436 --> 0.452169).  Saving model ...
Validation loss decreased (0.452169 --> 0.450963).  Saving model ...
Validation loss decreased (0.450963 --> 0.449764).  Saving model ...
Validation loss decreased (0.449764 --> 0.448548).  Saving model ...
Validation loss decreased (0.448548 --> 0.447349).  Saving model ...
Validation loss decreased (0.447349 --> 0.446151).  Saving model ...
Validation loss decreased (0.446151 --> 0.444975).  Saving model ...
Validation loss decreased (0.444975 --> 0.443813).  Saving model ...
Validation loss decreased (0.443813 --> 0.442597).  Saving model ...
Validation loss decreased (0.442597 --> 0.441371).  Saving model ...
Validation loss decreased (0.441371 --> 0.440130).  Saving model ...
Validation loss decreased (0.440130 --> 0.438898).  Saving model ...
Validation loss decreased (0.438898 --> 0.437725).  Saving model ...
Validation loss decreased (0.437725 --> 0.436558).  Saving model ...
Validation loss decreased (0.436558 --> 0.435446).  Saving model ...
Validation loss decreased (0.435446 --> 0.434291).  Saving model ...
Validation loss decreased (0.434291 --> 0.433223).  Saving model ...
Validation loss decreased (0.433223 --> 0.432124).  Saving model ...
Validation loss decreased (0.432124 --> 0.431045).  Saving model ...
Validation loss decreased (0.431045 --> 0.429991).  Saving model ...
Validation loss decreased (0.429991 --> 0.428934).  Saving model ...
Validation loss decreased (0.428934 --> 0.427832).  Saving model ...
Validation loss decreased (0.427832 --> 0.426782).  Saving model ...
Validation loss decreased (0.426782 --> 0.425791).  Saving model ...
Validation loss decreased (0.425791 --> 0.424826).  Saving model ...
Validation loss decreased (0.424826 --> 0.423774).  Saving model ...
Validation loss decreased (0.423774 --> 0.422743).  Saving model ...
Validation loss decreased (0.422743 --> 0.421718).  Saving model ...
Validation loss decreased (0.421718 --> 0.420681).  Saving model ...
Validation loss decreased (0.420681 --> 0.419639).  Saving model ...
Validation loss decreased (0.419639 --> 0.418577).  Saving model ...
Validation loss decreased (0.418577 --> 0.417556).  Saving model ...
Validation loss decreased (0.417556 --> 0.416539).  Saving model ...
Validation loss decreased (0.416539 --> 0.415592).  Saving model ...
Validation loss decreased (0.415592 --> 0.414572).  Saving model ...
Validation loss decreased (0.414572 --> 0.413558).  Saving model ...
Validation loss decreased (0.413558 --> 0.412524).  Saving model ...
Validation loss decreased (0.412524 --> 0.411448).  Saving model ...
Validation loss decreased (0.411448 --> 0.410423).  Saving model ...
Validation loss decreased (0.410423 --> 0.409377).  Saving model ...
Validation loss decreased (0.409377 --> 0.408383).  Saving model ...
Validation loss decreased (0.408383 --> 0.407368).  Saving model ...
Validation loss decreased (0.407368 --> 0.406358).  Saving model ...
Validation loss decreased (0.406358 --> 0.405335).  Saving model ...
Validation loss decreased (0.405335 --> 0.404274).  Saving model ...
Validation loss decreased (0.404274 --> 0.403186).  Saving model ...
Validation loss decreased (0.403186 --> 0.402138).  Saving model ...
Validation loss decreased (0.402138 --> 0.401163).  Saving model ...
Validation loss decreased (0.401163 --> 0.400193).  Saving model ...
Validation loss decreased (0.400193 --> 0.399134).  Saving model ...
Validation loss decreased (0.399134 --> 0.398073).  Saving model ...
Validation loss decreased (0.398073 --> 0.397067).  Saving model ...
Validation loss decreased (0.397067 --> 0.396065).  Saving model ...
Validation loss decreased (0.396065 --> 0.395086).  Saving model ...
Validation loss decreased (0.395086 --> 0.394099).  Saving model ...
Validation loss decreased (0.394099 --> 0.393081).  Saving model ...
Validation loss decreased (0.393081 --> 0.392078).  Saving model ...
Validation loss decreased (0.392078 --> 0.391093).  Saving model ...
Validation loss decreased (0.391093 --> 0.390170).  Saving model ...
Validation loss decreased (0.390170 --> 0.389252).  Saving model ...
Validation loss decreased (0.389252 --> 0.388324).  Saving model ...
Validation loss decreased (0.388324 --> 0.387411).  Saving model ...
Validation loss decreased (0.387411 --> 0.386472).  Saving model ...
Validation loss decreased (0.386472 --> 0.385537).  Saving model ...
Validation loss decreased (0.385537 --> 0.384568).  Saving model ...
Validation loss decreased (0.384568 --> 0.383610).  Saving model ...
Validation loss decreased (0.383610 --> 0.382640).  Saving model ...
Validation loss decreased (0.382640 --> 0.381702).  Saving model ...
Validation loss decreased (0.381702 --> 0.380754).  Saving model ...
Validation loss decreased (0.380754 --> 0.379867).  Saving model ...
Validation loss decreased (0.379867 --> 0.378950).  Saving model ...
Validation loss decreased (0.378950 --> 0.378069).  Saving model ...
Validation loss decreased (0.378069 --> 0.377189).  Saving model ...
Validation loss decreased (0.377189 --> 0.376330).  Saving model ...
Validation loss decreased (0.376330 --> 0.375431).  Saving model ...
Validation loss decreased (0.375431 --> 0.374545).  Saving model ...
Validation loss decreased (0.374545 --> 0.373705).  Saving model ...
Validation loss decreased (0.373705 --> 0.372866).  Saving model ...
Validation loss decreased (0.372866 --> 0.372064).  Saving model ...
Validation loss decreased (0.372064 --> 0.371193).  Saving model ...
Validation loss decreased (0.371193 --> 0.370362).  Saving model ...
Validation loss decreased (0.370362 --> 0.369484).  Saving model ...
Validation loss decreased (0.369484 --> 0.368650).  Saving model ...
Validation loss decreased (0.368650 --> 0.367876).  Saving model ...
Validation loss decreased (0.367876 --> 0.367173).  Saving model ...
Validation loss decreased (0.367173 --> 0.366458).  Saving model ...
Validation loss decreased (0.366458 --> 0.365769).  Saving model ...
Validation loss decreased (0.365769 --> 0.365005).  Saving model ...
Validation loss decreased (0.365005 --> 0.364240).  Saving model ...
Validation loss decreased (0.364240 --> 0.363488).  Saving model ...
Validation loss decreased (0.363488 --> 0.362739).  Saving model ...
Validation loss decreased (0.362739 --> 0.362010).  Saving model ...
Validation loss decreased (0.362010 --> 0.361203).  Saving model ...
Validation loss decreased (0.361203 --> 0.360414).  Saving model ...
Validation loss decreased (0.360414 --> 0.359689).  Saving model ...
Validation loss decreased (0.359689 --> 0.358930).  Saving model ...
Validation loss decreased (0.358930 --> 0.358158).  Saving model ...
Validation loss decreased (0.358158 --> 0.357384).  Saving model ...
Validation loss decreased (0.357384 --> 0.356614).  Saving model ...
epoch 301, loss 0.4648, train acc 90.00%, f1 0.8901, precision 0.9000, recall 0.8804, auc 0.8986
Validation loss decreased (0.356614 --> 0.355856).  Saving model ...
Validation loss decreased (0.355856 --> 0.355070).  Saving model ...
Validation loss decreased (0.355070 --> 0.354239).  Saving model ...
Validation loss decreased (0.354239 --> 0.353390).  Saving model ...
Validation loss decreased (0.353390 --> 0.352528).  Saving model ...
Validation loss decreased (0.352528 --> 0.351705).  Saving model ...
Validation loss decreased (0.351705 --> 0.350902).  Saving model ...
Validation loss decreased (0.350902 --> 0.350139).  Saving model ...
Validation loss decreased (0.350139 --> 0.349414).  Saving model ...
Validation loss decreased (0.349414 --> 0.348764).  Saving model ...
Validation loss decreased (0.348764 --> 0.348067).  Saving model ...
Validation loss decreased (0.348067 --> 0.347402).  Saving model ...
Validation loss decreased (0.347402 --> 0.346792).  Saving model ...
Validation loss decreased (0.346792 --> 0.346177).  Saving model ...
Validation loss decreased (0.346177 --> 0.345558).  Saving model ...
Validation loss decreased (0.345558 --> 0.344895).  Saving model ...
Validation loss decreased (0.344895 --> 0.344222).  Saving model ...
Validation loss decreased (0.344222 --> 0.343590).  Saving model ...
Validation loss decreased (0.343590 --> 0.342977).  Saving model ...
Validation loss decreased (0.342977 --> 0.342376).  Saving model ...
Validation loss decreased (0.342376 --> 0.341758).  Saving model ...
Validation loss decreased (0.341758 --> 0.341152).  Saving model ...
Validation loss decreased (0.341152 --> 0.340490).  Saving model ...
Validation loss decreased (0.340490 --> 0.339893).  Saving model ...
Validation loss decreased (0.339893 --> 0.339315).  Saving model ...
Validation loss decreased (0.339315 --> 0.338718).  Saving model ...
Validation loss decreased (0.338718 --> 0.338129).  Saving model ...
Validation loss decreased (0.338129 --> 0.337469).  Saving model ...
Validation loss decreased (0.337469 --> 0.336820).  Saving model ...
Validation loss decreased (0.336820 --> 0.336215).  Saving model ...
Validation loss decreased (0.336215 --> 0.335573).  Saving model ...
Validation loss decreased (0.335573 --> 0.334925).  Saving model ...
Validation loss decreased (0.334925 --> 0.334289).  Saving model ...
Validation loss decreased (0.334289 --> 0.333691).  Saving model ...
Validation loss decreased (0.333691 --> 0.333108).  Saving model ...
Validation loss decreased (0.333108 --> 0.332565).  Saving model ...
Validation loss decreased (0.332565 --> 0.332017).  Saving model ...
Validation loss decreased (0.332017 --> 0.331483).  Saving model ...
Validation loss decreased (0.331483 --> 0.330947).  Saving model ...
Validation loss decreased (0.330947 --> 0.330432).  Saving model ...
Validation loss decreased (0.330432 --> 0.329971).  Saving model ...
Validation loss decreased (0.329971 --> 0.329524).  Saving model ...
Validation loss decreased (0.329524 --> 0.329050).  Saving model ...
Validation loss decreased (0.329050 --> 0.328612).  Saving model ...
Validation loss decreased (0.328612 --> 0.328160).  Saving model ...
Validation loss decreased (0.328160 --> 0.327661).  Saving model ...
Validation loss decreased (0.327661 --> 0.327202).  Saving model ...
Validation loss decreased (0.327202 --> 0.326737).  Saving model ...
Validation loss decreased (0.326737 --> 0.326297).  Saving model ...
Validation loss decreased (0.326297 --> 0.325868).  Saving model ...
Validation loss decreased (0.325868 --> 0.325412).  Saving model ...
Validation loss decreased (0.325412 --> 0.324966).  Saving model ...
Validation loss decreased (0.324966 --> 0.324516).  Saving model ...
Validation loss decreased (0.324516 --> 0.324102).  Saving model ...
Validation loss decreased (0.324102 --> 0.323721).  Saving model ...
Validation loss decreased (0.323721 --> 0.323304).  Saving model ...
Validation loss decreased (0.323304 --> 0.322921).  Saving model ...
Validation loss decreased (0.322921 --> 0.322504).  Saving model ...
Validation loss decreased (0.322504 --> 0.322080).  Saving model ...
Validation loss decreased (0.322080 --> 0.321606).  Saving model ...
Validation loss decreased (0.321606 --> 0.321173).  Saving model ...
Validation loss decreased (0.321173 --> 0.320720).  Saving model ...
Validation loss decreased (0.320720 --> 0.320359).  Saving model ...
Validation loss decreased (0.320359 --> 0.319933).  Saving model ...
Validation loss decreased (0.319933 --> 0.319485).  Saving model ...
Validation loss decreased (0.319485 --> 0.319059).  Saving model ...
Validation loss decreased (0.319059 --> 0.318611).  Saving model ...
Validation loss decreased (0.318611 --> 0.318096).  Saving model ...
Validation loss decreased (0.318096 --> 0.317612).  Saving model ...
Validation loss decreased (0.317612 --> 0.317158).  Saving model ...
Validation loss decreased (0.317158 --> 0.316699).  Saving model ...
Validation loss decreased (0.316699 --> 0.316270).  Saving model ...
Validation loss decreased (0.316270 --> 0.315842).  Saving model ...
Validation loss decreased (0.315842 --> 0.315371).  Saving model ...
Validation loss decreased (0.315371 --> 0.314852).  Saving model ...
Validation loss decreased (0.314852 --> 0.314343).  Saving model ...
Validation loss decreased (0.314343 --> 0.313833).  Saving model ...
Validation loss decreased (0.313833 --> 0.313351).  Saving model ...
Validation loss decreased (0.313351 --> 0.312894).  Saving model ...
Validation loss decreased (0.312894 --> 0.312447).  Saving model ...
Validation loss decreased (0.312447 --> 0.312022).  Saving model ...
Validation loss decreased (0.312022 --> 0.311584).  Saving model ...
Validation loss decreased (0.311584 --> 0.311129).  Saving model ...
Validation loss decreased (0.311129 --> 0.310683).  Saving model ...
Validation loss decreased (0.310683 --> 0.310301).  Saving model ...
Validation loss decreased (0.310301 --> 0.309919).  Saving model ...
Validation loss decreased (0.309919 --> 0.309585).  Saving model ...
Validation loss decreased (0.309585 --> 0.309244).  Saving model ...
Validation loss decreased (0.309244 --> 0.308926).  Saving model ...
Validation loss decreased (0.308926 --> 0.308596).  Saving model ...
Validation loss decreased (0.308596 --> 0.308236).  Saving model ...
Validation loss decreased (0.308236 --> 0.307856).  Saving model ...
Validation loss decreased (0.307856 --> 0.307437).  Saving model ...
Validation loss decreased (0.307437 --> 0.306940).  Saving model ...
Validation loss decreased (0.306940 --> 0.306475).  Saving model ...
Validation loss decreased (0.306475 --> 0.305995).  Saving model ...
Validation loss decreased (0.305995 --> 0.305560).  Saving model ...
Validation loss decreased (0.305560 --> 0.305171).  Saving model ...
Validation loss decreased (0.305171 --> 0.304806).  Saving model ...
Validation loss decreased (0.304806 --> 0.304437).  Saving model ...
epoch 401, loss 0.3974, train acc 89.50%, f1 0.8840, precision 0.8989, recall 0.8696, auc 0.8931
Validation loss decreased (0.304437 --> 0.304049).  Saving model ...
Validation loss decreased (0.304049 --> 0.303608).  Saving model ...
Validation loss decreased (0.303608 --> 0.303146).  Saving model ...
Validation loss decreased (0.303146 --> 0.302676).  Saving model ...
Validation loss decreased (0.302676 --> 0.302216).  Saving model ...
Validation loss decreased (0.302216 --> 0.301864).  Saving model ...
Validation loss decreased (0.301864 --> 0.301502).  Saving model ...
Validation loss decreased (0.301502 --> 0.301149).  Saving model ...
Validation loss decreased (0.301149 --> 0.300783).  Saving model ...
Validation loss decreased (0.300783 --> 0.300437).  Saving model ...
Validation loss decreased (0.300437 --> 0.300099).  Saving model ...
Validation loss decreased (0.300099 --> 0.299766).  Saving model ...
Validation loss decreased (0.299766 --> 0.299436).  Saving model ...
Validation loss decreased (0.299436 --> 0.299084).  Saving model ...
Validation loss decreased (0.299084 --> 0.298733).  Saving model ...
Validation loss decreased (0.298733 --> 0.298406).  Saving model ...
Validation loss decreased (0.298406 --> 0.298102).  Saving model ...
Validation loss decreased (0.298102 --> 0.297758).  Saving model ...
Validation loss decreased (0.297758 --> 0.297355).  Saving model ...
Validation loss decreased (0.297355 --> 0.296919).  Saving model ...
Validation loss decreased (0.296919 --> 0.296451).  Saving model ...
Validation loss decreased (0.296451 --> 0.295960).  Saving model ...
Validation loss decreased (0.295960 --> 0.295471).  Saving model ...
Validation loss decreased (0.295471 --> 0.294998).  Saving model ...
Validation loss decreased (0.294998 --> 0.294580).  Saving model ...
Validation loss decreased (0.294580 --> 0.294177).  Saving model ...
Validation loss decreased (0.294177 --> 0.293767).  Saving model ...
Validation loss decreased (0.293767 --> 0.293437).  Saving model ...
Validation loss decreased (0.293437 --> 0.293132).  Saving model ...
Validation loss decreased (0.293132 --> 0.292808).  Saving model ...
Validation loss decreased (0.292808 --> 0.292532).  Saving model ...
Validation loss decreased (0.292532 --> 0.292199).  Saving model ...
Validation loss decreased (0.292199 --> 0.291875).  Saving model ...
Validation loss decreased (0.291875 --> 0.291496).  Saving model ...
Validation loss decreased (0.291496 --> 0.291189).  Saving model ...
Validation loss decreased (0.291189 --> 0.290885).  Saving model ...
Validation loss decreased (0.290885 --> 0.290504).  Saving model ...
Validation loss decreased (0.290504 --> 0.290180).  Saving model ...
Validation loss decreased (0.290180 --> 0.289893).  Saving model ...
Validation loss decreased (0.289893 --> 0.289647).  Saving model ...
Validation loss decreased (0.289647 --> 0.289370).  Saving model ...
Validation loss decreased (0.289370 --> 0.289060).  Saving model ...
Validation loss decreased (0.289060 --> 0.288777).  Saving model ...
Validation loss decreased (0.288777 --> 0.288506).  Saving model ...
Validation loss decreased (0.288506 --> 0.288211).  Saving model ...
Validation loss decreased (0.288211 --> 0.287871).  Saving model ...
Validation loss decreased (0.287871 --> 0.287520).  Saving model ...
Validation loss decreased (0.287520 --> 0.287198).  Saving model ...
Validation loss decreased (0.287198 --> 0.286856).  Saving model ...
Validation loss decreased (0.286856 --> 0.286572).  Saving model ...
Validation loss decreased (0.286572 --> 0.286279).  Saving model ...
Validation loss decreased (0.286279 --> 0.286040).  Saving model ...
Validation loss decreased (0.286040 --> 0.285861).  Saving model ...
Validation loss decreased (0.285861 --> 0.285735).  Saving model ...
Validation loss decreased (0.285735 --> 0.285644).  Saving model ...
Validation loss decreased (0.285644 --> 0.285554).  Saving model ...
Validation loss decreased (0.285554 --> 0.285469).  Saving model ...
Validation loss decreased (0.285469 --> 0.285405).  Saving model ...
Validation loss decreased (0.285405 --> 0.285298).  Saving model ...
Validation loss decreased (0.285298 --> 0.285190).  Saving model ...
Validation loss decreased (0.285190 --> 0.285078).  Saving model ...
Validation loss decreased (0.285078 --> 0.284918).  Saving model ...
Validation loss decreased (0.284918 --> 0.284762).  Saving model ...
Validation loss decreased (0.284762 --> 0.284551).  Saving model ...
Validation loss decreased (0.284551 --> 0.284336).  Saving model ...
Validation loss decreased (0.284336 --> 0.284117).  Saving model ...
Validation loss decreased (0.284117 --> 0.283855).  Saving model ...
Validation loss decreased (0.283855 --> 0.283488).  Saving model ...
Validation loss decreased (0.283488 --> 0.283125).  Saving model ...
Validation loss decreased (0.283125 --> 0.282794).  Saving model ...
Validation loss decreased (0.282794 --> 0.282398).  Saving model ...
Validation loss decreased (0.282398 --> 0.282010).  Saving model ...
Validation loss decreased (0.282010 --> 0.281657).  Saving model ...
Validation loss decreased (0.281657 --> 0.281342).  Saving model ...
Validation loss decreased (0.281342 --> 0.281025).  Saving model ...
Validation loss decreased (0.281025 --> 0.280685).  Saving model ...
Validation loss decreased (0.280685 --> 0.280401).  Saving model ...
Validation loss decreased (0.280401 --> 0.280133).  Saving model ...
Validation loss decreased (0.280133 --> 0.279840).  Saving model ...
Validation loss decreased (0.279840 --> 0.279485).  Saving model ...
Validation loss decreased (0.279485 --> 0.279162).  Saving model ...
Validation loss decreased (0.279162 --> 0.278838).  Saving model ...
Validation loss decreased (0.278838 --> 0.278541).  Saving model ...
Validation loss decreased (0.278541 --> 0.278228).  Saving model ...
Validation loss decreased (0.278228 --> 0.277874).  Saving model ...
Validation loss decreased (0.277874 --> 0.277499).  Saving model ...
Validation loss decreased (0.277499 --> 0.277125).  Saving model ...
Validation loss decreased (0.277125 --> 0.276811).  Saving model ...
Validation loss decreased (0.276811 --> 0.276463).  Saving model ...
Validation loss decreased (0.276463 --> 0.276097).  Saving model ...
Validation loss decreased (0.276097 --> 0.275788).  Saving model ...
Validation loss decreased (0.275788 --> 0.275443).  Saving model ...
Validation loss decreased (0.275443 --> 0.275105).  Saving model ...
Validation loss decreased (0.275105 --> 0.274724).  Saving model ...
Validation loss decreased (0.274724 --> 0.274379).  Saving model ...
Validation loss decreased (0.274379 --> 0.274104).  Saving model ...
Validation loss decreased (0.274104 --> 0.273792).  Saving model ...
Validation loss decreased (0.273792 --> 0.273495).  Saving model ...
Validation loss decreased (0.273495 --> 0.273219).  Saving model ...
Validation loss decreased (0.273219 --> 0.272925).  Saving model ...
epoch 501, loss 0.3774, train acc 89.00%, f1 0.8791, precision 0.8889, recall 0.8696, auc 0.8885
Validation loss decreased (0.272925 --> 0.272621).  Saving model ...
Validation loss decreased (0.272621 --> 0.272277).  Saving model ...
Validation loss decreased (0.272277 --> 0.272005).  Saving model ...
Validation loss decreased (0.272005 --> 0.271752).  Saving model ...
Validation loss decreased (0.271752 --> 0.271447).  Saving model ...
Validation loss decreased (0.271447 --> 0.271159).  Saving model ...
Validation loss decreased (0.271159 --> 0.270842).  Saving model ...
Validation loss decreased (0.270842 --> 0.270563).  Saving model ...
Validation loss decreased (0.270563 --> 0.270298).  Saving model ...
Validation loss decreased (0.270298 --> 0.270016).  Saving model ...
Validation loss decreased (0.270016 --> 0.269717).  Saving model ...
Validation loss decreased (0.269717 --> 0.269408).  Saving model ...
Validation loss decreased (0.269408 --> 0.269121).  Saving model ...
Validation loss decreased (0.269121 --> 0.268827).  Saving model ...
Validation loss decreased (0.268827 --> 0.268571).  Saving model ...
Validation loss decreased (0.268571 --> 0.268300).  Saving model ...
Validation loss decreased (0.268300 --> 0.268052).  Saving model ...
Validation loss decreased (0.268052 --> 0.267830).  Saving model ...
Validation loss decreased (0.267830 --> 0.267539).  Saving model ...
Validation loss decreased (0.267539 --> 0.267261).  Saving model ...
Validation loss decreased (0.267261 --> 0.266972).  Saving model ...
Validation loss decreased (0.266972 --> 0.266666).  Saving model ...
Validation loss decreased (0.266666 --> 0.266364).  Saving model ...
Validation loss decreased (0.266364 --> 0.266112).  Saving model ...
Validation loss decreased (0.266112 --> 0.265925).  Saving model ...
Validation loss decreased (0.265925 --> 0.265745).  Saving model ...
Validation loss decreased (0.265745 --> 0.265584).  Saving model ...
Validation loss decreased (0.265584 --> 0.265380).  Saving model ...
Validation loss decreased (0.265380 --> 0.265177).  Saving model ...
Validation loss decreased (0.265177 --> 0.265019).  Saving model ...
Validation loss decreased (0.265019 --> 0.264857).  Saving model ...
Validation loss decreased (0.264857 --> 0.264693).  Saving model ...
Validation loss decreased (0.264693 --> 0.264500).  Saving model ...
Validation loss decreased (0.264500 --> 0.264262).  Saving model ...
Validation loss decreased (0.264262 --> 0.264000).  Saving model ...
Validation loss decreased (0.264000 --> 0.263719).  Saving model ...
Validation loss decreased (0.263719 --> 0.263461).  Saving model ...
Validation loss decreased (0.263461 --> 0.263225).  Saving model ...
Validation loss decreased (0.263225 --> 0.263003).  Saving model ...
Validation loss decreased (0.263003 --> 0.262820).  Saving model ...
Validation loss decreased (0.262820 --> 0.262640).  Saving model ...
Validation loss decreased (0.262640 --> 0.262446).  Saving model ...
Validation loss decreased (0.262446 --> 0.262264).  Saving model ...
Validation loss decreased (0.262264 --> 0.262081).  Saving model ...
Validation loss decreased (0.262081 --> 0.261910).  Saving model ...
Validation loss decreased (0.261910 --> 0.261713).  Saving model ...
Validation loss decreased (0.261713 --> 0.261533).  Saving model ...
Validation loss decreased (0.261533 --> 0.261388).  Saving model ...
Validation loss decreased (0.261388 --> 0.261241).  Saving model ...
Validation loss decreased (0.261241 --> 0.261100).  Saving model ...
Validation loss decreased (0.261100 --> 0.261019).  Saving model ...
Validation loss decreased (0.261019 --> 0.260946).  Saving model ...
Validation loss decreased (0.260946 --> 0.260826).  Saving model ...
Validation loss decreased (0.260826 --> 0.260681).  Saving model ...
Validation loss decreased (0.260681 --> 0.260580).  Saving model ...
Validation loss decreased (0.260580 --> 0.260467).  Saving model ...
Validation loss decreased (0.260467 --> 0.260353).  Saving model ...
Validation loss decreased (0.260353 --> 0.260256).  Saving model ...
Validation loss decreased (0.260256 --> 0.260151).  Saving model ...
Validation loss decreased (0.260151 --> 0.259999).  Saving model ...
Validation loss decreased (0.259999 --> 0.259798).  Saving model ...
Validation loss decreased (0.259798 --> 0.259654).  Saving model ...
Validation loss decreased (0.259654 --> 0.259579).  Saving model ...
Validation loss decreased (0.259579 --> 0.259495).  Saving model ...
Validation loss decreased (0.259495 --> 0.259372).  Saving model ...
Validation loss decreased (0.259372 --> 0.259236).  Saving model ...
Validation loss decreased (0.259236 --> 0.259082).  Saving model ...
Validation loss decreased (0.259082 --> 0.258922).  Saving model ...
Validation loss decreased (0.258922 --> 0.258747).  Saving model ...
Validation loss decreased (0.258747 --> 0.258595).  Saving model ...
Validation loss decreased (0.258595 --> 0.258375).  Saving model ...
Validation loss decreased (0.258375 --> 0.258179).  Saving model ...
Validation loss decreased (0.258179 --> 0.257946).  Saving model ...
Validation loss decreased (0.257946 --> 0.257725).  Saving model ...
Validation loss decreased (0.257725 --> 0.257522).  Saving model ...
Validation loss decreased (0.257522 --> 0.257339).  Saving model ...
Validation loss decreased (0.257339 --> 0.257156).  Saving model ...
Validation loss decreased (0.257156 --> 0.257015).  Saving model ...
Validation loss decreased (0.257015 --> 0.256859).  Saving model ...
Validation loss decreased (0.256859 --> 0.256751).  Saving model ...
Validation loss decreased (0.256751 --> 0.256674).  Saving model ...
Validation loss decreased (0.256674 --> 0.256597).  Saving model ...
Validation loss decreased (0.256597 --> 0.256542).  Saving model ...
Validation loss decreased (0.256542 --> 0.256468).  Saving model ...
Validation loss decreased (0.256468 --> 0.256333).  Saving model ...
Validation loss decreased (0.256333 --> 0.256212).  Saving model ...
Validation loss decreased (0.256212 --> 0.256100).  Saving model ...
Validation loss decreased (0.256100 --> 0.256044).  Saving model ...
Validation loss decreased (0.256044 --> 0.255944).  Saving model ...
Validation loss decreased (0.255944 --> 0.255783).  Saving model ...
Validation loss decreased (0.255783 --> 0.255625).  Saving model ...
Validation loss decreased (0.255625 --> 0.255408).  Saving model ...
Validation loss decreased (0.255408 --> 0.255232).  Saving model ...
Validation loss decreased (0.255232 --> 0.255030).  Saving model ...
Validation loss decreased (0.255030 --> 0.254821).  Saving model ...
Validation loss decreased (0.254821 --> 0.254620).  Saving model ...
Validation loss decreased (0.254620 --> 0.254440).  Saving model ...
Validation loss decreased (0.254440 --> 0.254247).  Saving model ...
Validation loss decreased (0.254247 --> 0.254103).  Saving model ...
Validation loss decreased (0.254103 --> 0.253988).  Saving model ...
epoch 601, loss 0.3918, train acc 89.00%, f1 0.8791, precision 0.8889, recall 0.8696, auc 0.8885
Validation loss decreased (0.253988 --> 0.253845).  Saving model ...
Validation loss decreased (0.253845 --> 0.253734).  Saving model ...
Validation loss decreased (0.253734 --> 0.253642).  Saving model ...
Validation loss decreased (0.253642 --> 0.253561).  Saving model ...
Validation loss decreased (0.253561 --> 0.253487).  Saving model ...
Validation loss decreased (0.253487 --> 0.253439).  Saving model ...
Validation loss decreased (0.253439 --> 0.253350).  Saving model ...
Validation loss decreased (0.253350 --> 0.253289).  Saving model ...
Validation loss decreased (0.253289 --> 0.253215).  Saving model ...
Validation loss decreased (0.253215 --> 0.253191).  Saving model ...
Validation loss decreased (0.253191 --> 0.253146).  Saving model ...
Validation loss decreased (0.253146 --> 0.253077).  Saving model ...
Validation loss decreased (0.253077 --> 0.253025).  Saving model ...
Validation loss decreased (0.253025 --> 0.252976).  Saving model ...
Validation loss decreased (0.252976 --> 0.252929).  Saving model ...
Validation loss decreased (0.252929 --> 0.252925).  Saving model ...
Validation loss decreased (0.252925 --> 0.252906).  Saving model ...
Validation loss decreased (0.252906 --> 0.252868).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
Validation loss decreased (0.252868 --> 0.252822).  Saving model ...
Validation loss decreased (0.252822 --> 0.252806).  Saving model ...
Validation loss decreased (0.252806 --> 0.252792).  Saving model ...
Validation loss decreased (0.252792 --> 0.252788).  Saving model ...
Validation loss decreased (0.252788 --> 0.252719).  Saving model ...
Validation loss decreased (0.252719 --> 0.252647).  Saving model ...
Validation loss decreased (0.252647 --> 0.252575).  Saving model ...
Validation loss decreased (0.252575 --> 0.252506).  Saving model ...
Validation loss decreased (0.252506 --> 0.252420).  Saving model ...
Validation loss decreased (0.252420 --> 0.252280).  Saving model ...
Validation loss decreased (0.252280 --> 0.252176).  Saving model ...
Validation loss decreased (0.252176 --> 0.252143).  Saving model ...
Validation loss decreased (0.252143 --> 0.252090).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
Validation loss decreased (0.252090 --> 0.252075).  Saving model ...
Validation loss decreased (0.252075 --> 0.251981).  Saving model ...
Validation loss decreased (0.251981 --> 0.251852).  Saving model ...
Validation loss decreased (0.251852 --> 0.251707).  Saving model ...
Validation loss decreased (0.251707 --> 0.251595).  Saving model ...
Validation loss decreased (0.251595 --> 0.251496).  Saving model ...
Validation loss decreased (0.251496 --> 0.251353).  Saving model ...
Validation loss decreased (0.251353 --> 0.251212).  Saving model ...
Validation loss decreased (0.251212 --> 0.251054).  Saving model ...
Validation loss decreased (0.251054 --> 0.250891).  Saving model ...
Validation loss decreased (0.250891 --> 0.250691).  Saving model ...
Validation loss decreased (0.250691 --> 0.250493).  Saving model ...
Validation loss decreased (0.250493 --> 0.250321).  Saving model ...
Validation loss decreased (0.250321 --> 0.250159).  Saving model ...
Validation loss decreased (0.250159 --> 0.250004).  Saving model ...
Validation loss decreased (0.250004 --> 0.249894).  Saving model ...
Validation loss decreased (0.249894 --> 0.249763).  Saving model ...
Validation loss decreased (0.249763 --> 0.249681).  Saving model ...
Validation loss decreased (0.249681 --> 0.249570).  Saving model ...
Validation loss decreased (0.249570 --> 0.249464).  Saving model ...
Validation loss decreased (0.249464 --> 0.249305).  Saving model ...
Validation loss decreased (0.249305 --> 0.249198).  Saving model ...
Validation loss decreased (0.249198 --> 0.249092).  Saving model ...
Validation loss decreased (0.249092 --> 0.249029).  Saving model ...
Validation loss decreased (0.249029 --> 0.248868).  Saving model ...
Validation loss decreased (0.248868 --> 0.248660).  Saving model ...
Validation loss decreased (0.248660 --> 0.248476).  Saving model ...
Validation loss decreased (0.248476 --> 0.248338).  Saving model ...
Validation loss decreased (0.248338 --> 0.248170).  Saving model ...
Validation loss decreased (0.248170 --> 0.248027).  Saving model ...
Validation loss decreased (0.248027 --> 0.247932).  Saving model ...
Validation loss decreased (0.247932 --> 0.247802).  Saving model ...
Validation loss decreased (0.247802 --> 0.247648).  Saving model ...
Validation loss decreased (0.247648 --> 0.247490).  Saving model ...
Validation loss decreased (0.247490 --> 0.247378).  Saving model ...
Validation loss decreased (0.247378 --> 0.247284).  Saving model ...
Validation loss decreased (0.247284 --> 0.247224).  Saving model ...
Validation loss decreased (0.247224 --> 0.247139).  Saving model ...
Validation loss decreased (0.247139 --> 0.246986).  Saving model ...
Validation loss decreased (0.246986 --> 0.246832).  Saving model ...
Validation loss decreased (0.246832 --> 0.246682).  Saving model ...
Validation loss decreased (0.246682 --> 0.246551).  Saving model ...
Validation loss decreased (0.246551 --> 0.246405).  Saving model ...
Validation loss decreased (0.246405 --> 0.246242).  Saving model ...
Validation loss decreased (0.246242 --> 0.246115).  Saving model ...
Validation loss decreased (0.246115 --> 0.246026).  Saving model ...
Validation loss decreased (0.246026 --> 0.245959).  Saving model ...
Validation loss decreased (0.245959 --> 0.245873).  Saving model ...
Validation loss decreased (0.245873 --> 0.245745).  Saving model ...
Validation loss decreased (0.245745 --> 0.245618).  Saving model ...
Validation loss decreased (0.245618 --> 0.245486).  Saving model ...
Validation loss decreased (0.245486 --> 0.245311).  Saving model ...
Validation loss decreased (0.245311 --> 0.245204).  Saving model ...
Validation loss decreased (0.245204 --> 0.245108).  Saving model ...
Validation loss decreased (0.245108 --> 0.245001).  Saving model ...
Validation loss decreased (0.245001 --> 0.244897).  Saving model ...
Validation loss decreased (0.244897 --> 0.244831).  Saving model ...
epoch 701, loss 0.2942, train acc 89.00%, f1 0.8791, precision 0.8889, recall 0.8696, auc 0.8885
Validation loss decreased (0.244831 --> 0.244749).  Saving model ...
Validation loss decreased (0.244749 --> 0.244707).  Saving model ...
Validation loss decreased (0.244707 --> 0.244673).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 723, loss 0.3667, train acc 89.00%, f1 0.8791, precision 0.8889, recall 0.8696, auc 0.8885



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_True
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_True/record_1/MLP_minus_notMirror_True_1
./test_pima/result_MLP_minus_notMirror_True_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.595

the Fscore is 0.5714285714285715

the precision is 0.4

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_1
----------------------



epoch 1, loss 0.6929, train acc 50.02%, f1 0.6669, precision 0.5002, recall 1.0000, auc 0.5000
epoch 101, loss 0.6031, train acc 77.80%, f1 0.7848, precision 0.7618, recall 0.8093, auc 0.7780
epoch 201, loss 0.4930, train acc 80.28%, f1 0.8042, precision 0.7991, recall 0.8094, auc 0.8028
epoch 301, loss 0.5054, train acc 81.85%, f1 0.8187, precision 0.8182, recall 0.8192, auc 0.8185
epoch 401, loss 0.3928, train acc 82.77%, f1 0.8279, precision 0.8274, recall 0.8285, auc 0.8277
epoch 501, loss 0.4436, train acc 83.21%, f1 0.8317, precision 0.8340, recall 0.8295, auc 0.8321
epoch 601, loss 0.4593, train acc 83.30%, f1 0.8328, precision 0.8342, recall 0.8313, auc 0.8330
epoch 701, loss 0.3897, train acc 83.41%, f1 0.8340, precision 0.8345, recall 0.8335, auc 0.8341
epoch 801, loss 0.3135, train acc 83.43%, f1 0.8342, precision 0.8353, recall 0.8331, auc 0.8343
epoch 901, loss 0.4314, train acc 83.44%, f1 0.8344, precision 0.8346, recall 0.8342, auc 0.8344
epoch 1001, loss 0.3411, train acc 83.49%, f1 0.8353, precision 0.8340, recall 0.8366, auc 0.8349
epoch 1101, loss 0.3746, train acc 83.51%, f1 0.8350, precision 0.8357, recall 0.8342, auc 0.8351
epoch 1201, loss 0.3555, train acc 83.47%, f1 0.8344, precision 0.8364, recall 0.8323, auc 0.8347
epoch 1301, loss 0.2883, train acc 83.50%, f1 0.8349, precision 0.8359, recall 0.8340, auc 0.8350
epoch 1401, loss 0.3521, train acc 83.50%, f1 0.8351, precision 0.8349, recall 0.8353, auc 0.8350
epoch 1501, loss 0.4492, train acc 83.43%, f1 0.8342, precision 0.8352, recall 0.8332, auc 0.8343
epoch 1601, loss 0.3673, train acc 83.48%, f1 0.8348, precision 0.8349, recall 0.8348, auc 0.8348
epoch 1701, loss 0.5016, train acc 83.52%, f1 0.8353, precision 0.8350, recall 0.8356, auc 0.8352
epoch 1801, loss 0.4302, train acc 83.47%, f1 0.8349, precision 0.8344, recall 0.8354, auc 0.8347
epoch 1901, loss 0.4691, train acc 83.51%, f1 0.8358, precision 0.8330, recall 0.8385, auc 0.8351
epoch 2001, loss 0.3606, train acc 83.53%, f1 0.8354, precision 0.8353, recall 0.8356, auc 0.8353
epoch 2101, loss 0.4971, train acc 83.53%, f1 0.8358, precision 0.8340, recall 0.8375, auc 0.8353
epoch 2201, loss 0.4113, train acc 83.51%, f1 0.8356, precision 0.8335, recall 0.8376, auc 0.8351
epoch 2301, loss 0.2807, train acc 83.47%, f1 0.8349, precision 0.8340, recall 0.8359, auc 0.8347
epoch 2401, loss 0.4355, train acc 83.45%, f1 0.8348, precision 0.8337, recall 0.8359, auc 0.8345
epoch 2501, loss 0.4073, train acc 83.51%, f1 0.8354, precision 0.8345, recall 0.8363, auc 0.8351
epoch 2601, loss 0.5136, train acc 83.47%, f1 0.8350, precision 0.8340, recall 0.8360, auc 0.8347
epoch 2701, loss 0.4931, train acc 83.46%, f1 0.8350, precision 0.8335, recall 0.8365, auc 0.8346
epoch 2801, loss 0.3980, train acc 83.46%, f1 0.8351, precision 0.8333, recall 0.8368, auc 0.8346
epoch 2901, loss 0.4797, train acc 83.49%, f1 0.8354, precision 0.8333, recall 0.8375, auc 0.8349
epoch 3001, loss 0.4076, train acc 83.54%, f1 0.8355, precision 0.8352, recall 0.8358, auc 0.8354
epoch 3101, loss 0.4119, train acc 83.50%, f1 0.8353, precision 0.8342, recall 0.8365, auc 0.8350
epoch 3201, loss 0.4476, train acc 83.53%, f1 0.8358, precision 0.8336, recall 0.8379, auc 0.8353
epoch 3301, loss 0.3432, train acc 83.53%, f1 0.8361, precision 0.8326, recall 0.8397, auc 0.8353
epoch 3401, loss 0.4659, train acc 83.53%, f1 0.8361, precision 0.8325, recall 0.8398, auc 0.8353
epoch 3501, loss 0.3556, train acc 83.49%, f1 0.8348, precision 0.8359, recall 0.8336, auc 0.8349
epoch 3601, loss 0.4105, train acc 83.53%, f1 0.8355, precision 0.8346, recall 0.8365, auc 0.8353
epoch 3701, loss 0.2783, train acc 83.52%, f1 0.8353, precision 0.8350, recall 0.8357, auc 0.8352
epoch 3801, loss 0.2552, train acc 83.49%, f1 0.8356, precision 0.8326, recall 0.8386, auc 0.8349
epoch 3901, loss 0.3282, train acc 83.54%, f1 0.8354, precision 0.8356, recall 0.8352, auc 0.8354
epoch 4001, loss 0.3457, train acc 83.53%, f1 0.8357, precision 0.8341, recall 0.8373, auc 0.8353
epoch 4101, loss 0.3292, train acc 83.54%, f1 0.8359, precision 0.8338, recall 0.8380, auc 0.8354
epoch 4201, loss 0.3564, train acc 83.54%, f1 0.8358, precision 0.8339, recall 0.8378, auc 0.8354
epoch 4301, loss 0.3059, train acc 83.59%, f1 0.8369, precision 0.8322, recall 0.8416, auc 0.8358
epoch 4401, loss 0.3492, train acc 83.60%, f1 0.8366, precision 0.8339, recall 0.8394, auc 0.8360
epoch 4501, loss 0.3655, train acc 83.60%, f1 0.8371, precision 0.8321, recall 0.8421, auc 0.8360
epoch 4601, loss 0.3573, train acc 83.61%, f1 0.8365, precision 0.8346, recall 0.8385, auc 0.8361
epoch 4701, loss 0.3330, train acc 83.66%, f1 0.8371, precision 0.8349, recall 0.8392, auc 0.8366
epoch 4801, loss 0.3486, train acc 83.64%, f1 0.8365, precision 0.8361, recall 0.8369, auc 0.8364
epoch 4901, loss 0.3523, train acc 83.69%, f1 0.8377, precision 0.8337, recall 0.8418, auc 0.8369
epoch 5001, loss 0.5251, train acc 83.70%, f1 0.8375, precision 0.8350, recall 0.8401, auc 0.8370
epoch 5101, loss 0.3813, train acc 83.69%, f1 0.8371, precision 0.8365, recall 0.8377, auc 0.8369
epoch 5201, loss 0.3623, train acc 83.71%, f1 0.8371, precision 0.8379, recall 0.8362, auc 0.8371
epoch 5301, loss 0.3569, train acc 83.73%, f1 0.8374, precision 0.8376, recall 0.8371, auc 0.8373
epoch 5401, loss 0.3069, train acc 83.75%, f1 0.8379, precision 0.8363, recall 0.8394, auc 0.8375
epoch 5501, loss 0.4183, train acc 83.84%, f1 0.8392, precision 0.8357, recall 0.8426, auc 0.8384
epoch 5601, loss 0.4124, train acc 83.80%, f1 0.8385, precision 0.8363, recall 0.8407, auc 0.8380
epoch 5701, loss 0.3629, train acc 83.79%, f1 0.8382, precision 0.8370, recall 0.8395, auc 0.8379
epoch 5801, loss 0.2458, train acc 83.90%, f1 0.8395, precision 0.8373, recall 0.8416, auc 0.8390
epoch 5901, loss 0.3354, train acc 83.85%, f1 0.8385, precision 0.8390, recall 0.8380, auc 0.8385
epoch 6001, loss 0.3339, train acc 83.85%, f1 0.8385, precision 0.8385, recall 0.8386, auc 0.8385
epoch 6101, loss 0.4166, train acc 83.99%, f1 0.8403, precision 0.8389, recall 0.8416, auc 0.8399
epoch 6201, loss 0.4565, train acc 84.00%, f1 0.8405, precision 0.8387, recall 0.8422, auc 0.8400
epoch 6301, loss 0.4592, train acc 84.02%, f1 0.8402, precision 0.8409, recall 0.8395, auc 0.8402
epoch 6401, loss 0.4315, train acc 84.08%, f1 0.8410, precision 0.8404, recall 0.8417, auc 0.8408
epoch 6501, loss 0.3368, train acc 84.10%, f1 0.8409, precision 0.8417, recall 0.8402, auc 0.8410
epoch 6601, loss 0.4000, train acc 84.05%, f1 0.8404, precision 0.8413, recall 0.8395, auc 0.8405
epoch 6701, loss 0.2998, train acc 84.13%, f1 0.8417, precision 0.8402, recall 0.8431, auc 0.8413
epoch 6801, loss 0.2988, train acc 84.13%, f1 0.8414, precision 0.8413, recall 0.8415, auc 0.8413
epoch 6901, loss 0.3613, train acc 84.16%, f1 0.8420, precision 0.8401, recall 0.8440, auc 0.8416
epoch 7001, loss 0.3204, train acc 84.16%, f1 0.8417, precision 0.8418, recall 0.8415, auc 0.8416
epoch 7101, loss 0.2485, train acc 84.20%, f1 0.8421, precision 0.8423, recall 0.8419, auc 0.8420
epoch 7201, loss 0.3143, train acc 84.25%, f1 0.8423, precision 0.8438, recall 0.8407, auc 0.8425
epoch 7301, loss 0.3515, train acc 84.27%, f1 0.8428, precision 0.8424, recall 0.8432, auc 0.8427
epoch 7401, loss 0.3574, train acc 84.28%, f1 0.8431, precision 0.8419, recall 0.8442, auc 0.8428
epoch 7501, loss 0.3794, train acc 84.36%, f1 0.8436, precision 0.8440, recall 0.8432, auc 0.8436
epoch 7601, loss 0.3280, train acc 84.37%, f1 0.8439, precision 0.8428, recall 0.8451, auc 0.8437
epoch 7701, loss 0.4057, train acc 84.39%, f1 0.8445, precision 0.8419, recall 0.8471, auc 0.8439
epoch 7801, loss 0.4358, train acc 84.45%, f1 0.8450, precision 0.8428, recall 0.8472, auc 0.8445
epoch 7901, loss 0.3802, train acc 84.42%, f1 0.8443, precision 0.8438, recall 0.8448, auc 0.8442
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_1
./test_pima/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6264814814814814

the Fscore is 0.5875706214689266

the precision is 0.42276422764227645

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_1
----------------------



epoch 1, loss 0.6932, train acc 49.80%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6002, train acc 77.43%, f1 0.7801, precision 0.7635, recall 0.7975, auc 0.7742
epoch 201, loss 0.5412, train acc 80.32%, f1 0.8054, precision 0.7999, recall 0.8110, auc 0.8032
epoch 301, loss 0.4452, train acc 81.89%, f1 0.8198, precision 0.8191, recall 0.8205, auc 0.8189
epoch 401, loss 0.4005, train acc 82.83%, f1 0.8284, precision 0.8313, recall 0.8256, auc 0.8283
epoch 501, loss 0.4330, train acc 83.16%, f1 0.8319, precision 0.8339, recall 0.8298, auc 0.8316
epoch 601, loss 0.3979, train acc 83.35%, f1 0.8336, precision 0.8363, recall 0.8310, auc 0.8335
epoch 701, loss 0.3075, train acc 83.47%, f1 0.8349, precision 0.8372, recall 0.8327, auc 0.8347
epoch 801, loss 0.4628, train acc 83.44%, f1 0.8346, precision 0.8370, recall 0.8322, auc 0.8344
epoch 901, loss 0.3794, train acc 83.42%, f1 0.8344, precision 0.8370, recall 0.8318, auc 0.8342
epoch 1001, loss 0.3927, train acc 83.46%, f1 0.8353, precision 0.8354, recall 0.8351, auc 0.8346
epoch 1101, loss 0.3886, train acc 83.52%, f1 0.8356, precision 0.8366, recall 0.8347, auc 0.8352
epoch 1201, loss 0.4482, train acc 83.50%, f1 0.8353, precision 0.8372, recall 0.8333, auc 0.8350
epoch 1301, loss 0.4887, train acc 83.48%, f1 0.8352, precision 0.8367, recall 0.8336, auc 0.8348
epoch 1401, loss 0.2667, train acc 83.46%, f1 0.8349, precision 0.8364, recall 0.8335, auc 0.8346
epoch 1501, loss 0.3291, train acc 83.46%, f1 0.8352, precision 0.8357, recall 0.8346, auc 0.8346
epoch 1601, loss 0.4240, train acc 83.47%, f1 0.8351, precision 0.8366, recall 0.8336, auc 0.8347
epoch 1701, loss 0.4154, train acc 83.50%, f1 0.8354, precision 0.8365, recall 0.8343, auc 0.8350
epoch 1801, loss 0.3640, train acc 83.52%, f1 0.8356, precision 0.8369, recall 0.8343, auc 0.8352
epoch 1901, loss 0.2931, train acc 83.48%, f1 0.8355, precision 0.8354, recall 0.8356, auc 0.8348
epoch 2001, loss 0.3537, train acc 83.48%, f1 0.8354, precision 0.8358, recall 0.8349, auc 0.8348
epoch 2101, loss 0.3130, train acc 83.48%, f1 0.8355, precision 0.8353, recall 0.8357, auc 0.8348
epoch 2201, loss 0.3576, train acc 83.50%, f1 0.8357, precision 0.8357, recall 0.8357, auc 0.8350
epoch 2301, loss 0.3539, train acc 83.52%, f1 0.8352, precision 0.8385, recall 0.8319, auc 0.8352
epoch 2401, loss 0.3578, train acc 83.52%, f1 0.8360, precision 0.8351, recall 0.8370, auc 0.8352
epoch 2501, loss 0.4262, train acc 83.49%, f1 0.8353, precision 0.8368, recall 0.8339, auc 0.8349
epoch 2601, loss 0.3532, train acc 83.51%, f1 0.8363, precision 0.8334, recall 0.8392, auc 0.8351
epoch 2701, loss 0.3504, train acc 83.50%, f1 0.8359, precision 0.8343, recall 0.8376, auc 0.8349
epoch 2801, loss 0.3629, train acc 83.51%, f1 0.8359, precision 0.8353, recall 0.8365, auc 0.8351
epoch 2901, loss 0.3483, train acc 83.46%, f1 0.8351, precision 0.8359, recall 0.8343, auc 0.8346
epoch 3001, loss 0.4202, train acc 83.50%, f1 0.8356, precision 0.8361, recall 0.8351, auc 0.8350
epoch 3101, loss 0.2603, train acc 83.49%, f1 0.8358, precision 0.8346, recall 0.8369, auc 0.8349
epoch 3201, loss 0.4174, train acc 83.50%, f1 0.8352, precision 0.8376, recall 0.8327, auc 0.8350
epoch 3301, loss 0.3841, train acc 83.49%, f1 0.8354, precision 0.8362, recall 0.8347, auc 0.8349
epoch 3401, loss 0.3542, train acc 83.51%, f1 0.8361, precision 0.8347, recall 0.8374, auc 0.8351
epoch 3501, loss 0.4002, train acc 83.53%, f1 0.8359, precision 0.8363, recall 0.8355, auc 0.8353
epoch 3601, loss 0.3500, train acc 83.56%, f1 0.8361, precision 0.8366, recall 0.8357, auc 0.8356
epoch 3701, loss 0.3334, train acc 83.57%, f1 0.8362, precision 0.8372, recall 0.8352, auc 0.8357
epoch 3801, loss 0.3419, train acc 83.54%, f1 0.8358, precision 0.8373, recall 0.8343, auc 0.8354
epoch 3901, loss 0.5167, train acc 83.58%, f1 0.8368, precision 0.8350, recall 0.8386, auc 0.8358
epoch 4001, loss 0.3354, train acc 83.59%, f1 0.8368, precision 0.8354, recall 0.8382, auc 0.8359
epoch 4101, loss 0.3291, train acc 83.64%, f1 0.8373, precision 0.8361, recall 0.8385, auc 0.8364
epoch 4201, loss 0.3521, train acc 83.60%, f1 0.8367, precision 0.8363, recall 0.8372, auc 0.8360
epoch 4301, loss 0.3764, train acc 83.71%, f1 0.8377, precision 0.8380, recall 0.8374, auc 0.8371
epoch 4401, loss 0.5130, train acc 83.70%, f1 0.8377, precision 0.8378, recall 0.8376, auc 0.8370
epoch 4501, loss 0.2725, train acc 83.69%, f1 0.8377, precision 0.8369, recall 0.8385, auc 0.8369
epoch 4601, loss 0.3539, train acc 83.67%, f1 0.8372, precision 0.8382, recall 0.8362, auc 0.8367
epoch 4701, loss 0.2888, train acc 83.74%, f1 0.8381, precision 0.8381, recall 0.8382, auc 0.8374
epoch 4801, loss 0.3409, train acc 83.69%, f1 0.8373, precision 0.8387, recall 0.8360, auc 0.8369
epoch 4901, loss 0.3506, train acc 83.69%, f1 0.8370, precision 0.8395, recall 0.8346, auc 0.8369
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_1
./test_pima/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.625

the Fscore is 0.5901639344262295

the precision is 0.4186046511627907

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_1
----------------------



epoch 1, loss 0.6931, train acc 49.98%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6303, train acc 77.02%, f1 0.7911, precision 0.7254, recall 0.8700, auc 0.7702
epoch 201, loss 0.5360, train acc 80.35%, f1 0.8062, precision 0.7956, recall 0.8172, auc 0.8035
epoch 301, loss 0.4511, train acc 81.89%, f1 0.8189, precision 0.8192, recall 0.8186, auc 0.8189
epoch 401, loss 0.4344, train acc 82.71%, f1 0.8264, precision 0.8304, recall 0.8223, auc 0.8271
epoch 501, loss 0.3910, train acc 83.16%, f1 0.8303, precision 0.8371, recall 0.8236, auc 0.8316
epoch 601, loss 0.3556, train acc 83.36%, f1 0.8326, precision 0.8379, recall 0.8273, auc 0.8336
epoch 701, loss 0.4322, train acc 83.40%, f1 0.8333, precision 0.8369, recall 0.8297, auc 0.8340
epoch 801, loss 0.4405, train acc 83.44%, f1 0.8338, precision 0.8372, recall 0.8304, auc 0.8344
epoch 901, loss 0.5036, train acc 83.47%, f1 0.8337, precision 0.8392, recall 0.8282, auc 0.8347
epoch 1001, loss 0.4797, train acc 83.46%, f1 0.8334, precision 0.8396, recall 0.8273, auc 0.8346
epoch 1101, loss 0.3192, train acc 83.51%, f1 0.8345, precision 0.8378, recall 0.8313, auc 0.8351
epoch 1201, loss 0.3484, train acc 83.49%, f1 0.8344, precision 0.8371, recall 0.8318, auc 0.8349
epoch 1301, loss 0.3408, train acc 83.44%, f1 0.8338, precision 0.8373, recall 0.8303, auc 0.8344
epoch 1401, loss 0.4024, train acc 83.40%, f1 0.8332, precision 0.8377, recall 0.8288, auc 0.8340
epoch 1501, loss 0.4934, train acc 83.49%, f1 0.8343, precision 0.8375, recall 0.8312, auc 0.8349
epoch 1601, loss 0.4362, train acc 83.45%, f1 0.8340, precision 0.8371, recall 0.8308, auc 0.8345
epoch 1701, loss 0.4295, train acc 83.45%, f1 0.8336, precision 0.8384, recall 0.8288, auc 0.8345
epoch 1801, loss 0.4416, train acc 83.41%, f1 0.8331, precision 0.8381, recall 0.8282, auc 0.8341
epoch 1901, loss 0.2525, train acc 83.45%, f1 0.8339, precision 0.8373, recall 0.8305, auc 0.8345
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_1
./test_pima/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.5757407407407408

the Fscore is 0.5578947368421052

the precision is 0.3897058823529412

the recall is 0.9814814814814815

Done
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_normal_True/record_1/MLP_normal_True_1
----------------------



epoch 1, loss 0.6908, train acc 65.07%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
Validation loss decreased (inf --> 0.689417).  Saving model ...
Validation loss decreased (0.689417 --> 0.688097).  Saving model ...
Validation loss decreased (0.688097 --> 0.686794).  Saving model ...
Validation loss decreased (0.686794 --> 0.685509).  Saving model ...
Validation loss decreased (0.685509 --> 0.684241).  Saving model ...
Validation loss decreased (0.684241 --> 0.682992).  Saving model ...
Validation loss decreased (0.682992 --> 0.681759).  Saving model ...
Validation loss decreased (0.681759 --> 0.680544).  Saving model ...
Validation loss decreased (0.680544 --> 0.679347).  Saving model ...
Validation loss decreased (0.679347 --> 0.678167).  Saving model ...
Validation loss decreased (0.678167 --> 0.677004).  Saving model ...
Validation loss decreased (0.677004 --> 0.675859).  Saving model ...
Validation loss decreased (0.675859 --> 0.674730).  Saving model ...
Validation loss decreased (0.674730 --> 0.673618).  Saving model ...
Validation loss decreased (0.673618 --> 0.672522).  Saving model ...
Validation loss decreased (0.672522 --> 0.671442).  Saving model ...
Validation loss decreased (0.671442 --> 0.670379).  Saving model ...
Validation loss decreased (0.670379 --> 0.669332).  Saving model ...
Validation loss decreased (0.669332 --> 0.668300).  Saving model ...
Validation loss decreased (0.668300 --> 0.667285).  Saving model ...
Validation loss decreased (0.667285 --> 0.666285).  Saving model ...
Validation loss decreased (0.666285 --> 0.665301).  Saving model ...
Validation loss decreased (0.665301 --> 0.664332).  Saving model ...
Validation loss decreased (0.664332 --> 0.663379).  Saving model ...
Validation loss decreased (0.663379 --> 0.662440).  Saving model ...
Validation loss decreased (0.662440 --> 0.661517).  Saving model ...
Validation loss decreased (0.661517 --> 0.660608).  Saving model ...
Validation loss decreased (0.660608 --> 0.659714).  Saving model ...
Validation loss decreased (0.659714 --> 0.658835).  Saving model ...
Validation loss decreased (0.658835 --> 0.657969).  Saving model ...
Validation loss decreased (0.657969 --> 0.657118).  Saving model ...
Validation loss decreased (0.657118 --> 0.656281).  Saving model ...
Validation loss decreased (0.656281 --> 0.655457).  Saving model ...
Validation loss decreased (0.655457 --> 0.654647).  Saving model ...
Validation loss decreased (0.654647 --> 0.653850).  Saving model ...
Validation loss decreased (0.653850 --> 0.653066).  Saving model ...
Validation loss decreased (0.653066 --> 0.652294).  Saving model ...
Validation loss decreased (0.652294 --> 0.651536).  Saving model ...
Validation loss decreased (0.651536 --> 0.650790).  Saving model ...
Validation loss decreased (0.650790 --> 0.650056).  Saving model ...
Validation loss decreased (0.650056 --> 0.649335).  Saving model ...
Validation loss decreased (0.649335 --> 0.648625).  Saving model ...
Validation loss decreased (0.648625 --> 0.647927).  Saving model ...
Validation loss decreased (0.647927 --> 0.647241).  Saving model ...
Validation loss decreased (0.647241 --> 0.646566).  Saving model ...
Validation loss decreased (0.646566 --> 0.645902).  Saving model ...
Validation loss decreased (0.645902 --> 0.645248).  Saving model ...
Validation loss decreased (0.645248 --> 0.644606).  Saving model ...
Validation loss decreased (0.644606 --> 0.643974).  Saving model ...
Validation loss decreased (0.643974 --> 0.643353).  Saving model ...
Validation loss decreased (0.643353 --> 0.642741).  Saving model ...
Validation loss decreased (0.642741 --> 0.642140).  Saving model ...
Validation loss decreased (0.642140 --> 0.641548).  Saving model ...
Validation loss decreased (0.641548 --> 0.640966).  Saving model ...
Validation loss decreased (0.640966 --> 0.640393).  Saving model ...
Validation loss decreased (0.640393 --> 0.639830).  Saving model ...
Validation loss decreased (0.639830 --> 0.639275).  Saving model ...
Validation loss decreased (0.639275 --> 0.638729).  Saving model ...
Validation loss decreased (0.638729 --> 0.638192).  Saving model ...
Validation loss decreased (0.638192 --> 0.637663).  Saving model ...
Validation loss decreased (0.637663 --> 0.637143).  Saving model ...
Validation loss decreased (0.637143 --> 0.636630).  Saving model ...
Validation loss decreased (0.636630 --> 0.636126).  Saving model ...
Validation loss decreased (0.636126 --> 0.635629).  Saving model ...
Validation loss decreased (0.635629 --> 0.635139).  Saving model ...
Validation loss decreased (0.635139 --> 0.634657).  Saving model ...
Validation loss decreased (0.634657 --> 0.634182).  Saving model ...
Validation loss decreased (0.634182 --> 0.633714).  Saving model ...
Validation loss decreased (0.633714 --> 0.633252).  Saving model ...
Validation loss decreased (0.633252 --> 0.632798).  Saving model ...
Validation loss decreased (0.632798 --> 0.632349).  Saving model ...
Validation loss decreased (0.632349 --> 0.631907).  Saving model ...
Validation loss decreased (0.631907 --> 0.631471).  Saving model ...
Validation loss decreased (0.631471 --> 0.631041).  Saving model ...
Validation loss decreased (0.631041 --> 0.630617).  Saving model ...
Validation loss decreased (0.630617 --> 0.630198).  Saving model ...
Validation loss decreased (0.630198 --> 0.629785).  Saving model ...
Validation loss decreased (0.629785 --> 0.629377).  Saving model ...
Validation loss decreased (0.629377 --> 0.628974).  Saving model ...
Validation loss decreased (0.628974 --> 0.628576).  Saving model ...
Validation loss decreased (0.628576 --> 0.628183).  Saving model ...
Validation loss decreased (0.628183 --> 0.627794).  Saving model ...
Validation loss decreased (0.627794 --> 0.627410).  Saving model ...
Validation loss decreased (0.627410 --> 0.627030).  Saving model ...
Validation loss decreased (0.627030 --> 0.626655).  Saving model ...
Validation loss decreased (0.626655 --> 0.626284).  Saving model ...
Validation loss decreased (0.626284 --> 0.625916).  Saving model ...
Validation loss decreased (0.625916 --> 0.625553).  Saving model ...
Validation loss decreased (0.625553 --> 0.625193).  Saving model ...
Validation loss decreased (0.625193 --> 0.624837).  Saving model ...
Validation loss decreased (0.624837 --> 0.624484).  Saving model ...
Validation loss decreased (0.624484 --> 0.624135).  Saving model ...
Validation loss decreased (0.624135 --> 0.623789).  Saving model ...
Validation loss decreased (0.623789 --> 0.623445).  Saving model ...
Validation loss decreased (0.623445 --> 0.623106).  Saving model ...
Validation loss decreased (0.623106 --> 0.622768).  Saving model ...
Validation loss decreased (0.622768 --> 0.622434).  Saving model ...
Validation loss decreased (0.622434 --> 0.622102).  Saving model ...
Validation loss decreased (0.622102 --> 0.621773).  Saving model ...
Validation loss decreased (0.621773 --> 0.621447).  Saving model ...
epoch 101, loss 0.6214, train acc 65.07%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
Validation loss decreased (0.621447 --> 0.621122).  Saving model ...
Validation loss decreased (0.621122 --> 0.620800).  Saving model ...
Validation loss decreased (0.620800 --> 0.620481).  Saving model ...
Validation loss decreased (0.620481 --> 0.620163).  Saving model ...
Validation loss decreased (0.620163 --> 0.619848).  Saving model ...
Validation loss decreased (0.619848 --> 0.619535).  Saving model ...
Validation loss decreased (0.619535 --> 0.619223).  Saving model ...
Validation loss decreased (0.619223 --> 0.618914).  Saving model ...
Validation loss decreased (0.618914 --> 0.618606).  Saving model ...
Validation loss decreased (0.618606 --> 0.618300).  Saving model ...
Validation loss decreased (0.618300 --> 0.617995).  Saving model ...
Validation loss decreased (0.617995 --> 0.617692).  Saving model ...
Validation loss decreased (0.617692 --> 0.617391).  Saving model ...
Validation loss decreased (0.617391 --> 0.617091).  Saving model ...
Validation loss decreased (0.617091 --> 0.616793).  Saving model ...
Validation loss decreased (0.616793 --> 0.616496).  Saving model ...
Validation loss decreased (0.616496 --> 0.616200).  Saving model ...
Validation loss decreased (0.616200 --> 0.615906).  Saving model ...
Validation loss decreased (0.615906 --> 0.615613).  Saving model ...
Validation loss decreased (0.615613 --> 0.615321).  Saving model ...
Validation loss decreased (0.615321 --> 0.615030).  Saving model ...
Validation loss decreased (0.615030 --> 0.614741).  Saving model ...
Validation loss decreased (0.614741 --> 0.614452).  Saving model ...
Validation loss decreased (0.614452 --> 0.614165).  Saving model ...
Validation loss decreased (0.614165 --> 0.613879).  Saving model ...
Validation loss decreased (0.613879 --> 0.613593).  Saving model ...
Validation loss decreased (0.613593 --> 0.613309).  Saving model ...
Validation loss decreased (0.613309 --> 0.613025).  Saving model ...
Validation loss decreased (0.613025 --> 0.612743).  Saving model ...
Validation loss decreased (0.612743 --> 0.612461).  Saving model ...
Validation loss decreased (0.612461 --> 0.612180).  Saving model ...
Validation loss decreased (0.612180 --> 0.611900).  Saving model ...
Validation loss decreased (0.611900 --> 0.611621).  Saving model ...
Validation loss decreased (0.611621 --> 0.611342).  Saving model ...
Validation loss decreased (0.611342 --> 0.611064).  Saving model ...
Validation loss decreased (0.611064 --> 0.610787).  Saving model ...
Validation loss decreased (0.610787 --> 0.610511).  Saving model ...
Validation loss decreased (0.610511 --> 0.610235).  Saving model ...
Validation loss decreased (0.610235 --> 0.609960).  Saving model ...
Validation loss decreased (0.609960 --> 0.609686).  Saving model ...
Validation loss decreased (0.609686 --> 0.609412).  Saving model ...
Validation loss decreased (0.609412 --> 0.609139).  Saving model ...
Validation loss decreased (0.609139 --> 0.608866).  Saving model ...
Validation loss decreased (0.608866 --> 0.608594).  Saving model ...
Validation loss decreased (0.608594 --> 0.608323).  Saving model ...
Validation loss decreased (0.608323 --> 0.608052).  Saving model ...
Validation loss decreased (0.608052 --> 0.607782).  Saving model ...
Validation loss decreased (0.607782 --> 0.607512).  Saving model ...
Validation loss decreased (0.607512 --> 0.607243).  Saving model ...
Validation loss decreased (0.607243 --> 0.606974).  Saving model ...
Validation loss decreased (0.606974 --> 0.606706).  Saving model ...
Validation loss decreased (0.606706 --> 0.606438).  Saving model ...
Validation loss decreased (0.606438 --> 0.606170).  Saving model ...
Validation loss decreased (0.606170 --> 0.605903).  Saving model ...
Validation loss decreased (0.605903 --> 0.605637).  Saving model ...
Validation loss decreased (0.605637 --> 0.605371).  Saving model ...
Validation loss decreased (0.605371 --> 0.605105).  Saving model ...
Validation loss decreased (0.605105 --> 0.604839).  Saving model ...
Validation loss decreased (0.604839 --> 0.604574).  Saving model ...
Validation loss decreased (0.604574 --> 0.604310).  Saving model ...
Validation loss decreased (0.604310 --> 0.604045).  Saving model ...
Validation loss decreased (0.604045 --> 0.603781).  Saving model ...
Validation loss decreased (0.603781 --> 0.603518).  Saving model ...
Validation loss decreased (0.603518 --> 0.603254).  Saving model ...
Validation loss decreased (0.603254 --> 0.602991).  Saving model ...
Validation loss decreased (0.602991 --> 0.602728).  Saving model ...
Validation loss decreased (0.602728 --> 0.602466).  Saving model ...
Validation loss decreased (0.602466 --> 0.602203).  Saving model ...
Validation loss decreased (0.602203 --> 0.601941).  Saving model ...
Validation loss decreased (0.601941 --> 0.601680).  Saving model ...
Validation loss decreased (0.601680 --> 0.601418).  Saving model ...
Validation loss decreased (0.601418 --> 0.601157).  Saving model ...
Validation loss decreased (0.601157 --> 0.600896).  Saving model ...
Validation loss decreased (0.600896 --> 0.600635).  Saving model ...
Validation loss decreased (0.600635 --> 0.600374).  Saving model ...
Validation loss decreased (0.600374 --> 0.600114).  Saving model ...
Validation loss decreased (0.600114 --> 0.599854).  Saving model ...
Validation loss decreased (0.599854 --> 0.599594).  Saving model ...
Validation loss decreased (0.599594 --> 0.599334).  Saving model ...
Validation loss decreased (0.599334 --> 0.599074).  Saving model ...
Validation loss decreased (0.599074 --> 0.598814).  Saving model ...
Validation loss decreased (0.598814 --> 0.598555).  Saving model ...
Validation loss decreased (0.598555 --> 0.598296).  Saving model ...
Validation loss decreased (0.598296 --> 0.598037).  Saving model ...
Validation loss decreased (0.598037 --> 0.597778).  Saving model ...
Validation loss decreased (0.597778 --> 0.597519).  Saving model ...
Validation loss decreased (0.597519 --> 0.597260).  Saving model ...
Validation loss decreased (0.597260 --> 0.597001).  Saving model ...
Validation loss decreased (0.597001 --> 0.596743).  Saving model ...
Validation loss decreased (0.596743 --> 0.596485).  Saving model ...
Validation loss decreased (0.596485 --> 0.596226).  Saving model ...
Validation loss decreased (0.596226 --> 0.595968).  Saving model ...
Validation loss decreased (0.595968 --> 0.595710).  Saving model ...
Validation loss decreased (0.595710 --> 0.595452).  Saving model ...
Validation loss decreased (0.595452 --> 0.595194).  Saving model ...
Validation loss decreased (0.595194 --> 0.594936).  Saving model ...
Validation loss decreased (0.594936 --> 0.594678).  Saving model ...
Validation loss decreased (0.594678 --> 0.594421).  Saving model ...
Validation loss decreased (0.594421 --> 0.594163).  Saving model ...
Validation loss decreased (0.594163 --> 0.593905).  Saving model ...
epoch 201, loss 0.5939, train acc 65.07%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
Validation loss decreased (0.593905 --> 0.593648).  Saving model ...
Validation loss decreased (0.593648 --> 0.593390).  Saving model ...
Validation loss decreased (0.593390 --> 0.593133).  Saving model ...
Validation loss decreased (0.593133 --> 0.592876).  Saving model ...
Validation loss decreased (0.592876 --> 0.592618).  Saving model ...
Validation loss decreased (0.592618 --> 0.592361).  Saving model ...
Validation loss decreased (0.592361 --> 0.592104).  Saving model ...
Validation loss decreased (0.592104 --> 0.591847).  Saving model ...
Validation loss decreased (0.591847 --> 0.591589).  Saving model ...
Validation loss decreased (0.591589 --> 0.591332).  Saving model ...
Validation loss decreased (0.591332 --> 0.591075).  Saving model ...
Validation loss decreased (0.591075 --> 0.590818).  Saving model ...
Validation loss decreased (0.590818 --> 0.590561).  Saving model ...
Validation loss decreased (0.590561 --> 0.590304).  Saving model ...
Validation loss decreased (0.590304 --> 0.590047).  Saving model ...
Validation loss decreased (0.590047 --> 0.589790).  Saving model ...
Validation loss decreased (0.589790 --> 0.589533).  Saving model ...
Validation loss decreased (0.589533 --> 0.589276).  Saving model ...
Validation loss decreased (0.589276 --> 0.589019).  Saving model ...
Validation loss decreased (0.589019 --> 0.588763).  Saving model ...
Validation loss decreased (0.588763 --> 0.588506).  Saving model ...
Validation loss decreased (0.588506 --> 0.588249).  Saving model ...
Validation loss decreased (0.588249 --> 0.587992).  Saving model ...
Validation loss decreased (0.587992 --> 0.587736).  Saving model ...
Validation loss decreased (0.587736 --> 0.587479).  Saving model ...
Validation loss decreased (0.587479 --> 0.587222).  Saving model ...
Validation loss decreased (0.587222 --> 0.586966).  Saving model ...
Validation loss decreased (0.586966 --> 0.586709).  Saving model ...
Validation loss decreased (0.586709 --> 0.586452).  Saving model ...
Validation loss decreased (0.586452 --> 0.586196).  Saving model ...
Validation loss decreased (0.586196 --> 0.585939).  Saving model ...
Validation loss decreased (0.585939 --> 0.585683).  Saving model ...
Validation loss decreased (0.585683 --> 0.585426).  Saving model ...
Validation loss decreased (0.585426 --> 0.585170).  Saving model ...
Validation loss decreased (0.585170 --> 0.584913).  Saving model ...
Validation loss decreased (0.584913 --> 0.584657).  Saving model ...
Validation loss decreased (0.584657 --> 0.584401).  Saving model ...
Validation loss decreased (0.584401 --> 0.584144).  Saving model ...
Validation loss decreased (0.584144 --> 0.583888).  Saving model ...
Validation loss decreased (0.583888 --> 0.583632).  Saving model ...
Validation loss decreased (0.583632 --> 0.583376).  Saving model ...
Validation loss decreased (0.583376 --> 0.583120).  Saving model ...
Validation loss decreased (0.583120 --> 0.582864).  Saving model ...
Validation loss decreased (0.582864 --> 0.582607).  Saving model ...
Validation loss decreased (0.582607 --> 0.582351).  Saving model ...
Validation loss decreased (0.582351 --> 0.582096).  Saving model ...
Validation loss decreased (0.582096 --> 0.581840).  Saving model ...
Validation loss decreased (0.581840 --> 0.581584).  Saving model ...
Validation loss decreased (0.581584 --> 0.581328).  Saving model ...
Validation loss decreased (0.581328 --> 0.581073).  Saving model ...
Validation loss decreased (0.581073 --> 0.580817).  Saving model ...
Validation loss decreased (0.580817 --> 0.580562).  Saving model ...
Validation loss decreased (0.580562 --> 0.580306).  Saving model ...
Validation loss decreased (0.580306 --> 0.580051).  Saving model ...
Validation loss decreased (0.580051 --> 0.579796).  Saving model ...
Validation loss decreased (0.579796 --> 0.579540).  Saving model ...
Validation loss decreased (0.579540 --> 0.579285).  Saving model ...
Validation loss decreased (0.579285 --> 0.579030).  Saving model ...
Validation loss decreased (0.579030 --> 0.578775).  Saving model ...
Validation loss decreased (0.578775 --> 0.578521).  Saving model ...
Validation loss decreased (0.578521 --> 0.578266).  Saving model ...
Validation loss decreased (0.578266 --> 0.578012).  Saving model ...
Validation loss decreased (0.578012 --> 0.577757).  Saving model ...
Validation loss decreased (0.577757 --> 0.577503).  Saving model ...
Validation loss decreased (0.577503 --> 0.577248).  Saving model ...
Validation loss decreased (0.577248 --> 0.576994).  Saving model ...
Validation loss decreased (0.576994 --> 0.576740).  Saving model ...
Validation loss decreased (0.576740 --> 0.576487).  Saving model ...
Validation loss decreased (0.576487 --> 0.576233).  Saving model ...
Validation loss decreased (0.576233 --> 0.575979).  Saving model ...
Validation loss decreased (0.575979 --> 0.575726).  Saving model ...
Validation loss decreased (0.575726 --> 0.575473).  Saving model ...
Validation loss decreased (0.575473 --> 0.575219).  Saving model ...
Validation loss decreased (0.575219 --> 0.574967).  Saving model ...
Validation loss decreased (0.574967 --> 0.574714).  Saving model ...
Validation loss decreased (0.574714 --> 0.574461).  Saving model ...
Validation loss decreased (0.574461 --> 0.574209).  Saving model ...
Validation loss decreased (0.574209 --> 0.573956).  Saving model ...
Validation loss decreased (0.573956 --> 0.573704).  Saving model ...
Validation loss decreased (0.573704 --> 0.573452).  Saving model ...
Validation loss decreased (0.573452 --> 0.573200).  Saving model ...
Validation loss decreased (0.573200 --> 0.572949).  Saving model ...
Validation loss decreased (0.572949 --> 0.572697).  Saving model ...
Validation loss decreased (0.572697 --> 0.572446).  Saving model ...
Validation loss decreased (0.572446 --> 0.572195).  Saving model ...
Validation loss decreased (0.572195 --> 0.571944).  Saving model ...
Validation loss decreased (0.571944 --> 0.571694).  Saving model ...
Validation loss decreased (0.571694 --> 0.571443).  Saving model ...
Validation loss decreased (0.571443 --> 0.571193).  Saving model ...
Validation loss decreased (0.571193 --> 0.570943).  Saving model ...
Validation loss decreased (0.570943 --> 0.570693).  Saving model ...
Validation loss decreased (0.570693 --> 0.570444).  Saving model ...
Validation loss decreased (0.570444 --> 0.570195).  Saving model ...
Validation loss decreased (0.570195 --> 0.569946).  Saving model ...
Validation loss decreased (0.569946 --> 0.569697).  Saving model ...
Validation loss decreased (0.569697 --> 0.569448).  Saving model ...
Validation loss decreased (0.569448 --> 0.569200).  Saving model ...
Validation loss decreased (0.569200 --> 0.568952).  Saving model ...
Validation loss decreased (0.568952 --> 0.568704).  Saving model ...
Validation loss decreased (0.568704 --> 0.568456).  Saving model ...
epoch 301, loss 0.5685, train acc 65.41%, f1 0.0194, precision 1.0000, recall 0.0098, auc 0.5049
Validation loss decreased (0.568456 --> 0.568209).  Saving model ...
Validation loss decreased (0.568209 --> 0.567962).  Saving model ...
Validation loss decreased (0.567962 --> 0.567715).  Saving model ...
Validation loss decreased (0.567715 --> 0.567469).  Saving model ...
Validation loss decreased (0.567469 --> 0.567222).  Saving model ...
Validation loss decreased (0.567222 --> 0.566977).  Saving model ...
Validation loss decreased (0.566977 --> 0.566731).  Saving model ...
Validation loss decreased (0.566731 --> 0.566485).  Saving model ...
Validation loss decreased (0.566485 --> 0.566240).  Saving model ...
Validation loss decreased (0.566240 --> 0.565995).  Saving model ...
Validation loss decreased (0.565995 --> 0.565751).  Saving model ...
Validation loss decreased (0.565751 --> 0.565507).  Saving model ...
Validation loss decreased (0.565507 --> 0.565263).  Saving model ...
Validation loss decreased (0.565263 --> 0.565019).  Saving model ...
Validation loss decreased (0.565019 --> 0.564776).  Saving model ...
Validation loss decreased (0.564776 --> 0.564533).  Saving model ...
Validation loss decreased (0.564533 --> 0.564290).  Saving model ...
Validation loss decreased (0.564290 --> 0.564048).  Saving model ...
Validation loss decreased (0.564048 --> 0.563806).  Saving model ...
Validation loss decreased (0.563806 --> 0.563564).  Saving model ...
Validation loss decreased (0.563564 --> 0.563323).  Saving model ...
Validation loss decreased (0.563323 --> 0.563082).  Saving model ...
Validation loss decreased (0.563082 --> 0.562841).  Saving model ...
Validation loss decreased (0.562841 --> 0.562601).  Saving model ...
Validation loss decreased (0.562601 --> 0.562361).  Saving model ...
Validation loss decreased (0.562361 --> 0.562121).  Saving model ...
Validation loss decreased (0.562121 --> 0.561881).  Saving model ...
Validation loss decreased (0.561881 --> 0.561642).  Saving model ...
Validation loss decreased (0.561642 --> 0.561404).  Saving model ...
Validation loss decreased (0.561404 --> 0.561165).  Saving model ...
Validation loss decreased (0.561165 --> 0.560927).  Saving model ...
Validation loss decreased (0.560927 --> 0.560690).  Saving model ...
Validation loss decreased (0.560690 --> 0.560453).  Saving model ...
Validation loss decreased (0.560453 --> 0.560216).  Saving model ...
Validation loss decreased (0.560216 --> 0.559979).  Saving model ...
Validation loss decreased (0.559979 --> 0.559743).  Saving model ...
Validation loss decreased (0.559743 --> 0.559507).  Saving model ...
Validation loss decreased (0.559507 --> 0.559272).  Saving model ...
Validation loss decreased (0.559272 --> 0.559037).  Saving model ...
Validation loss decreased (0.559037 --> 0.558802).  Saving model ...
Validation loss decreased (0.558802 --> 0.558568).  Saving model ...
Validation loss decreased (0.558568 --> 0.558334).  Saving model ...
Validation loss decreased (0.558334 --> 0.558100).  Saving model ...
Validation loss decreased (0.558100 --> 0.557867).  Saving model ...
Validation loss decreased (0.557867 --> 0.557635).  Saving model ...
Validation loss decreased (0.557635 --> 0.557402).  Saving model ...
Validation loss decreased (0.557402 --> 0.557170).  Saving model ...
Validation loss decreased (0.557170 --> 0.556939).  Saving model ...
Validation loss decreased (0.556939 --> 0.556708).  Saving model ...
Validation loss decreased (0.556708 --> 0.556477).  Saving model ...
Validation loss decreased (0.556477 --> 0.556246).  Saving model ...
Validation loss decreased (0.556246 --> 0.556016).  Saving model ...
Validation loss decreased (0.556016 --> 0.555787).  Saving model ...
Validation loss decreased (0.555787 --> 0.555558).  Saving model ...
Validation loss decreased (0.555558 --> 0.555329).  Saving model ...
Validation loss decreased (0.555329 --> 0.555101).  Saving model ...
Validation loss decreased (0.555101 --> 0.554873).  Saving model ...
Validation loss decreased (0.554873 --> 0.554645).  Saving model ...
Validation loss decreased (0.554645 --> 0.554418).  Saving model ...
Validation loss decreased (0.554418 --> 0.554191).  Saving model ...
Validation loss decreased (0.554191 --> 0.553965).  Saving model ...
Validation loss decreased (0.553965 --> 0.553739).  Saving model ...
Validation loss decreased (0.553739 --> 0.553513).  Saving model ...
Validation loss decreased (0.553513 --> 0.553288).  Saving model ...
Validation loss decreased (0.553288 --> 0.553064).  Saving model ...
Validation loss decreased (0.553064 --> 0.552840).  Saving model ...
Validation loss decreased (0.552840 --> 0.552616).  Saving model ...
Validation loss decreased (0.552616 --> 0.552392).  Saving model ...
Validation loss decreased (0.552392 --> 0.552170).  Saving model ...
Validation loss decreased (0.552170 --> 0.551947).  Saving model ...
Validation loss decreased (0.551947 --> 0.551725).  Saving model ...
Validation loss decreased (0.551725 --> 0.551503).  Saving model ...
Validation loss decreased (0.551503 --> 0.551282).  Saving model ...
Validation loss decreased (0.551282 --> 0.551061).  Saving model ...
Validation loss decreased (0.551061 --> 0.550841).  Saving model ...
Validation loss decreased (0.550841 --> 0.550621).  Saving model ...
Validation loss decreased (0.550621 --> 0.550402).  Saving model ...
Validation loss decreased (0.550402 --> 0.550183).  Saving model ...
Validation loss decreased (0.550183 --> 0.549964).  Saving model ...
Validation loss decreased (0.549964 --> 0.549746).  Saving model ...
Validation loss decreased (0.549746 --> 0.549528).  Saving model ...
Validation loss decreased (0.549528 --> 0.549311).  Saving model ...
Validation loss decreased (0.549311 --> 0.549094).  Saving model ...
Validation loss decreased (0.549094 --> 0.548877).  Saving model ...
Validation loss decreased (0.548877 --> 0.548661).  Saving model ...
Validation loss decreased (0.548661 --> 0.548446).  Saving model ...
Validation loss decreased (0.548446 --> 0.548231).  Saving model ...
Validation loss decreased (0.548231 --> 0.548016).  Saving model ...
Validation loss decreased (0.548016 --> 0.547802).  Saving model ...
Validation loss decreased (0.547802 --> 0.547588).  Saving model ...
Validation loss decreased (0.547588 --> 0.547375).  Saving model ...
Validation loss decreased (0.547375 --> 0.547162).  Saving model ...
Validation loss decreased (0.547162 --> 0.546950).  Saving model ...
Validation loss decreased (0.546950 --> 0.546738).  Saving model ...
Validation loss decreased (0.546738 --> 0.546526).  Saving model ...
Validation loss decreased (0.546526 --> 0.546315).  Saving model ...
Validation loss decreased (0.546315 --> 0.546105).  Saving model ...
Validation loss decreased (0.546105 --> 0.545894).  Saving model ...
Validation loss decreased (0.545894 --> 0.545685).  Saving model ...
Validation loss decreased (0.545685 --> 0.545475).  Saving model ...
epoch 401, loss 0.5455, train acc 68.15%, f1 0.1842, precision 0.8750, recall 0.1029, auc 0.5475
Validation loss decreased (0.545475 --> 0.545267).  Saving model ...
Validation loss decreased (0.545267 --> 0.545058).  Saving model ...
Validation loss decreased (0.545058 --> 0.544850).  Saving model ...
Validation loss decreased (0.544850 --> 0.544643).  Saving model ...
Validation loss decreased (0.544643 --> 0.544436).  Saving model ...
Validation loss decreased (0.544436 --> 0.544229).  Saving model ...
Validation loss decreased (0.544229 --> 0.544023).  Saving model ...
Validation loss decreased (0.544023 --> 0.543817).  Saving model ...
Validation loss decreased (0.543817 --> 0.543612).  Saving model ...
Validation loss decreased (0.543612 --> 0.543407).  Saving model ...
Validation loss decreased (0.543407 --> 0.543203).  Saving model ...
Validation loss decreased (0.543203 --> 0.542999).  Saving model ...
Validation loss decreased (0.542999 --> 0.542796).  Saving model ...
Validation loss decreased (0.542796 --> 0.542593).  Saving model ...
Validation loss decreased (0.542593 --> 0.542391).  Saving model ...
Validation loss decreased (0.542391 --> 0.542189).  Saving model ...
Validation loss decreased (0.542189 --> 0.541987).  Saving model ...
Validation loss decreased (0.541987 --> 0.541786).  Saving model ...
Validation loss decreased (0.541786 --> 0.541585).  Saving model ...
Validation loss decreased (0.541585 --> 0.541385).  Saving model ...
Validation loss decreased (0.541385 --> 0.541185).  Saving model ...
Validation loss decreased (0.541185 --> 0.540986).  Saving model ...
Validation loss decreased (0.540986 --> 0.540787).  Saving model ...
Validation loss decreased (0.540787 --> 0.540589).  Saving model ...
Validation loss decreased (0.540589 --> 0.540391).  Saving model ...
Validation loss decreased (0.540391 --> 0.540193).  Saving model ...
Validation loss decreased (0.540193 --> 0.539996).  Saving model ...
Validation loss decreased (0.539996 --> 0.539800).  Saving model ...
Validation loss decreased (0.539800 --> 0.539604).  Saving model ...
Validation loss decreased (0.539604 --> 0.539408).  Saving model ...
Validation loss decreased (0.539408 --> 0.539213).  Saving model ...
Validation loss decreased (0.539213 --> 0.539018).  Saving model ...
Validation loss decreased (0.539018 --> 0.538824).  Saving model ...
Validation loss decreased (0.538824 --> 0.538630).  Saving model ...
Validation loss decreased (0.538630 --> 0.538437).  Saving model ...
Validation loss decreased (0.538437 --> 0.538244).  Saving model ...
Validation loss decreased (0.538244 --> 0.538051).  Saving model ...
Validation loss decreased (0.538051 --> 0.537859).  Saving model ...
Validation loss decreased (0.537859 --> 0.537668).  Saving model ...
Validation loss decreased (0.537668 --> 0.537476).  Saving model ...
Validation loss decreased (0.537476 --> 0.537286).  Saving model ...
Validation loss decreased (0.537286 --> 0.537096).  Saving model ...
Validation loss decreased (0.537096 --> 0.536906).  Saving model ...
Validation loss decreased (0.536906 --> 0.536716).  Saving model ...
Validation loss decreased (0.536716 --> 0.536528).  Saving model ...
Validation loss decreased (0.536528 --> 0.536339).  Saving model ...
Validation loss decreased (0.536339 --> 0.536151).  Saving model ...
Validation loss decreased (0.536151 --> 0.535964).  Saving model ...
Validation loss decreased (0.535964 --> 0.535777).  Saving model ...
Validation loss decreased (0.535777 --> 0.535590).  Saving model ...
Validation loss decreased (0.535590 --> 0.535404).  Saving model ...
Validation loss decreased (0.535404 --> 0.535218).  Saving model ...
Validation loss decreased (0.535218 --> 0.535033).  Saving model ...
Validation loss decreased (0.535033 --> 0.534848).  Saving model ...
Validation loss decreased (0.534848 --> 0.534664).  Saving model ...
Validation loss decreased (0.534664 --> 0.534480).  Saving model ...
Validation loss decreased (0.534480 --> 0.534296).  Saving model ...
Validation loss decreased (0.534296 --> 0.534113).  Saving model ...
Validation loss decreased (0.534113 --> 0.533931).  Saving model ...
Validation loss decreased (0.533931 --> 0.533749).  Saving model ...
Validation loss decreased (0.533749 --> 0.533567).  Saving model ...
Validation loss decreased (0.533567 --> 0.533386).  Saving model ...
Validation loss decreased (0.533386 --> 0.533205).  Saving model ...
Validation loss decreased (0.533205 --> 0.533025).  Saving model ...
Validation loss decreased (0.533025 --> 0.532845).  Saving model ...
Validation loss decreased (0.532845 --> 0.532665).  Saving model ...
Validation loss decreased (0.532665 --> 0.532486).  Saving model ...
Validation loss decreased (0.532486 --> 0.532308).  Saving model ...
Validation loss decreased (0.532308 --> 0.532129).  Saving model ...
Validation loss decreased (0.532129 --> 0.531952).  Saving model ...
Validation loss decreased (0.531952 --> 0.531775).  Saving model ...
Validation loss decreased (0.531775 --> 0.531598).  Saving model ...
Validation loss decreased (0.531598 --> 0.531421).  Saving model ...
Validation loss decreased (0.531421 --> 0.531245).  Saving model ...
Validation loss decreased (0.531245 --> 0.531070).  Saving model ...
Validation loss decreased (0.531070 --> 0.530895).  Saving model ...
Validation loss decreased (0.530895 --> 0.530720).  Saving model ...
Validation loss decreased (0.530720 --> 0.530546).  Saving model ...
Validation loss decreased (0.530546 --> 0.530372).  Saving model ...
Validation loss decreased (0.530372 --> 0.530199).  Saving model ...
Validation loss decreased (0.530199 --> 0.530026).  Saving model ...
Validation loss decreased (0.530026 --> 0.529854).  Saving model ...
Validation loss decreased (0.529854 --> 0.529682).  Saving model ...
Validation loss decreased (0.529682 --> 0.529510).  Saving model ...
Validation loss decreased (0.529510 --> 0.529339).  Saving model ...
Validation loss decreased (0.529339 --> 0.529168).  Saving model ...
Validation loss decreased (0.529168 --> 0.528998).  Saving model ...
Validation loss decreased (0.528998 --> 0.528828).  Saving model ...
Validation loss decreased (0.528828 --> 0.528658).  Saving model ...
Validation loss decreased (0.528658 --> 0.528489).  Saving model ...
Validation loss decreased (0.528489 --> 0.528321).  Saving model ...
Validation loss decreased (0.528321 --> 0.528153).  Saving model ...
Validation loss decreased (0.528153 --> 0.527985).  Saving model ...
Validation loss decreased (0.527985 --> 0.527818).  Saving model ...
Validation loss decreased (0.527818 --> 0.527651).  Saving model ...
Validation loss decreased (0.527651 --> 0.527484).  Saving model ...
Validation loss decreased (0.527484 --> 0.527318).  Saving model ...
Validation loss decreased (0.527318 --> 0.527153).  Saving model ...
Validation loss decreased (0.527153 --> 0.526987).  Saving model ...
Validation loss decreased (0.526987 --> 0.526823).  Saving model ...
epoch 501, loss 0.5268, train acc 71.06%, f1 0.3525, precision 0.8070, recall 0.2255, auc 0.5983
Validation loss decreased (0.526823 --> 0.526658).  Saving model ...
Validation loss decreased (0.526658 --> 0.526495).  Saving model ...
Validation loss decreased (0.526495 --> 0.526331).  Saving model ...
Validation loss decreased (0.526331 --> 0.526168).  Saving model ...
Validation loss decreased (0.526168 --> 0.526005).  Saving model ...
Validation loss decreased (0.526005 --> 0.525843).  Saving model ...
Validation loss decreased (0.525843 --> 0.525681).  Saving model ...
Validation loss decreased (0.525681 --> 0.525520).  Saving model ...
Validation loss decreased (0.525520 --> 0.525359).  Saving model ...
Validation loss decreased (0.525359 --> 0.525198).  Saving model ...
Validation loss decreased (0.525198 --> 0.525038).  Saving model ...
Validation loss decreased (0.525038 --> 0.524878).  Saving model ...
Validation loss decreased (0.524878 --> 0.524719).  Saving model ...
Validation loss decreased (0.524719 --> 0.524560).  Saving model ...
Validation loss decreased (0.524560 --> 0.524402).  Saving model ...
Validation loss decreased (0.524402 --> 0.524243).  Saving model ...
Validation loss decreased (0.524243 --> 0.524086).  Saving model ...
Validation loss decreased (0.524086 --> 0.523929).  Saving model ...
Validation loss decreased (0.523929 --> 0.523772).  Saving model ...
Validation loss decreased (0.523772 --> 0.523615).  Saving model ...
Validation loss decreased (0.523615 --> 0.523459).  Saving model ...
Validation loss decreased (0.523459 --> 0.523304).  Saving model ...
Validation loss decreased (0.523304 --> 0.523148).  Saving model ...
Validation loss decreased (0.523148 --> 0.522993).  Saving model ...
Validation loss decreased (0.522993 --> 0.522839).  Saving model ...
Validation loss decreased (0.522839 --> 0.522685).  Saving model ...
Validation loss decreased (0.522685 --> 0.522531).  Saving model ...
Validation loss decreased (0.522531 --> 0.522378).  Saving model ...
Validation loss decreased (0.522378 --> 0.522225).  Saving model ...
Validation loss decreased (0.522225 --> 0.522073).  Saving model ...
Validation loss decreased (0.522073 --> 0.521921).  Saving model ...
Validation loss decreased (0.521921 --> 0.521769).  Saving model ...
Validation loss decreased (0.521769 --> 0.521618).  Saving model ...
Validation loss decreased (0.521618 --> 0.521467).  Saving model ...
Validation loss decreased (0.521467 --> 0.521317).  Saving model ...
Validation loss decreased (0.521317 --> 0.521167).  Saving model ...
Validation loss decreased (0.521167 --> 0.521017).  Saving model ...
Validation loss decreased (0.521017 --> 0.520868).  Saving model ...
Validation loss decreased (0.520868 --> 0.520719).  Saving model ...
Validation loss decreased (0.520719 --> 0.520571).  Saving model ...
Validation loss decreased (0.520571 --> 0.520422).  Saving model ...
Validation loss decreased (0.520422 --> 0.520275).  Saving model ...
Validation loss decreased (0.520275 --> 0.520127).  Saving model ...
Validation loss decreased (0.520127 --> 0.519981).  Saving model ...
Validation loss decreased (0.519981 --> 0.519834).  Saving model ...
Validation loss decreased (0.519834 --> 0.519688).  Saving model ...
Validation loss decreased (0.519688 --> 0.519542).  Saving model ...
Validation loss decreased (0.519542 --> 0.519397).  Saving model ...
Validation loss decreased (0.519397 --> 0.519252).  Saving model ...
Validation loss decreased (0.519252 --> 0.519107).  Saving model ...
Validation loss decreased (0.519107 --> 0.518963).  Saving model ...
Validation loss decreased (0.518963 --> 0.518819).  Saving model ...
Validation loss decreased (0.518819 --> 0.518676).  Saving model ...
Validation loss decreased (0.518676 --> 0.518533).  Saving model ...
Validation loss decreased (0.518533 --> 0.518390).  Saving model ...
Validation loss decreased (0.518390 --> 0.518248).  Saving model ...
Validation loss decreased (0.518248 --> 0.518106).  Saving model ...
Validation loss decreased (0.518106 --> 0.517964).  Saving model ...
Validation loss decreased (0.517964 --> 0.517823).  Saving model ...
Validation loss decreased (0.517823 --> 0.517683).  Saving model ...
Validation loss decreased (0.517683 --> 0.517542).  Saving model ...
Validation loss decreased (0.517542 --> 0.517402).  Saving model ...
Validation loss decreased (0.517402 --> 0.517263).  Saving model ...
Validation loss decreased (0.517263 --> 0.517123).  Saving model ...
Validation loss decreased (0.517123 --> 0.516984).  Saving model ...
Validation loss decreased (0.516984 --> 0.516846).  Saving model ...
Validation loss decreased (0.516846 --> 0.516708).  Saving model ...
Validation loss decreased (0.516708 --> 0.516570).  Saving model ...
Validation loss decreased (0.516570 --> 0.516433).  Saving model ...
Validation loss decreased (0.516433 --> 0.516296).  Saving model ...
Validation loss decreased (0.516296 --> 0.516159).  Saving model ...
Validation loss decreased (0.516159 --> 0.516023).  Saving model ...
Validation loss decreased (0.516023 --> 0.515887).  Saving model ...
Validation loss decreased (0.515887 --> 0.515751).  Saving model ...
Validation loss decreased (0.515751 --> 0.515616).  Saving model ...
Validation loss decreased (0.515616 --> 0.515481).  Saving model ...
Validation loss decreased (0.515481 --> 0.515347).  Saving model ...
Validation loss decreased (0.515347 --> 0.515212).  Saving model ...
Validation loss decreased (0.515212 --> 0.515079).  Saving model ...
Validation loss decreased (0.515079 --> 0.514945).  Saving model ...
Validation loss decreased (0.514945 --> 0.514812).  Saving model ...
Validation loss decreased (0.514812 --> 0.514680).  Saving model ...
Validation loss decreased (0.514680 --> 0.514547).  Saving model ...
Validation loss decreased (0.514547 --> 0.514415).  Saving model ...
Validation loss decreased (0.514415 --> 0.514284).  Saving model ...
Validation loss decreased (0.514284 --> 0.514153).  Saving model ...
Validation loss decreased (0.514153 --> 0.514022).  Saving model ...
Validation loss decreased (0.514022 --> 0.513891).  Saving model ...
Validation loss decreased (0.513891 --> 0.513761).  Saving model ...
Validation loss decreased (0.513761 --> 0.513631).  Saving model ...
Validation loss decreased (0.513631 --> 0.513502).  Saving model ...
Validation loss decreased (0.513502 --> 0.513373).  Saving model ...
Validation loss decreased (0.513373 --> 0.513244).  Saving model ...
Validation loss decreased (0.513244 --> 0.513116).  Saving model ...
Validation loss decreased (0.513116 --> 0.512988).  Saving model ...
Validation loss decreased (0.512988 --> 0.512860).  Saving model ...
Validation loss decreased (0.512860 --> 0.512732).  Saving model ...
Validation loss decreased (0.512732 --> 0.512605).  Saving model ...
Validation loss decreased (0.512605 --> 0.512479).  Saving model ...
Validation loss decreased (0.512479 --> 0.512353).  Saving model ...
epoch 601, loss 0.5124, train acc 74.14%, f1 0.4916, precision 0.7849, recall 0.3578, auc 0.6526
Validation loss decreased (0.512353 --> 0.512227).  Saving model ...
Validation loss decreased (0.512227 --> 0.512101).  Saving model ...
Validation loss decreased (0.512101 --> 0.511976).  Saving model ...
Validation loss decreased (0.511976 --> 0.511851).  Saving model ...
Validation loss decreased (0.511851 --> 0.511726).  Saving model ...
Validation loss decreased (0.511726 --> 0.511602).  Saving model ...
Validation loss decreased (0.511602 --> 0.511478).  Saving model ...
Validation loss decreased (0.511478 --> 0.511354).  Saving model ...
Validation loss decreased (0.511354 --> 0.511231).  Saving model ...
Validation loss decreased (0.511231 --> 0.511108).  Saving model ...
Validation loss decreased (0.511108 --> 0.510986).  Saving model ...
Validation loss decreased (0.510986 --> 0.510864).  Saving model ...
Validation loss decreased (0.510864 --> 0.510742).  Saving model ...
Validation loss decreased (0.510742 --> 0.510620).  Saving model ...
Validation loss decreased (0.510620 --> 0.510499).  Saving model ...
Validation loss decreased (0.510499 --> 0.510378).  Saving model ...
Validation loss decreased (0.510378 --> 0.510257).  Saving model ...
Validation loss decreased (0.510257 --> 0.510137).  Saving model ...
Validation loss decreased (0.510137 --> 0.510017).  Saving model ...
Validation loss decreased (0.510017 --> 0.509898).  Saving model ...
Validation loss decreased (0.509898 --> 0.509779).  Saving model ...
Validation loss decreased (0.509779 --> 0.509660).  Saving model ...
Validation loss decreased (0.509660 --> 0.509541).  Saving model ...
Validation loss decreased (0.509541 --> 0.509423).  Saving model ...
Validation loss decreased (0.509423 --> 0.509305).  Saving model ...
Validation loss decreased (0.509305 --> 0.509188).  Saving model ...
Validation loss decreased (0.509188 --> 0.509070).  Saving model ...
Validation loss decreased (0.509070 --> 0.508953).  Saving model ...
Validation loss decreased (0.508953 --> 0.508837).  Saving model ...
Validation loss decreased (0.508837 --> 0.508721).  Saving model ...
Validation loss decreased (0.508721 --> 0.508605).  Saving model ...
Validation loss decreased (0.508605 --> 0.508489).  Saving model ...
Validation loss decreased (0.508489 --> 0.508374).  Saving model ...
Validation loss decreased (0.508374 --> 0.508259).  Saving model ...
Validation loss decreased (0.508259 --> 0.508144).  Saving model ...
Validation loss decreased (0.508144 --> 0.508030).  Saving model ...
Validation loss decreased (0.508030 --> 0.507916).  Saving model ...
Validation loss decreased (0.507916 --> 0.507802).  Saving model ...
Validation loss decreased (0.507802 --> 0.507689).  Saving model ...
Validation loss decreased (0.507689 --> 0.507576).  Saving model ...
Validation loss decreased (0.507576 --> 0.507463).  Saving model ...
Validation loss decreased (0.507463 --> 0.507350).  Saving model ...
Validation loss decreased (0.507350 --> 0.507238).  Saving model ...
Validation loss decreased (0.507238 --> 0.507126).  Saving model ...
Validation loss decreased (0.507126 --> 0.507015).  Saving model ...
Validation loss decreased (0.507015 --> 0.506904).  Saving model ...
Validation loss decreased (0.506904 --> 0.506793).  Saving model ...
Validation loss decreased (0.506793 --> 0.506682).  Saving model ...
Validation loss decreased (0.506682 --> 0.506572).  Saving model ...
Validation loss decreased (0.506572 --> 0.506462).  Saving model ...
Validation loss decreased (0.506462 --> 0.506353).  Saving model ...
Validation loss decreased (0.506353 --> 0.506243).  Saving model ...
Validation loss decreased (0.506243 --> 0.506134).  Saving model ...
Validation loss decreased (0.506134 --> 0.506025).  Saving model ...
Validation loss decreased (0.506025 --> 0.505917).  Saving model ...
Validation loss decreased (0.505917 --> 0.505809).  Saving model ...
Validation loss decreased (0.505809 --> 0.505701).  Saving model ...
Validation loss decreased (0.505701 --> 0.505594).  Saving model ...
Validation loss decreased (0.505594 --> 0.505486).  Saving model ...
Validation loss decreased (0.505486 --> 0.505380).  Saving model ...
Validation loss decreased (0.505380 --> 0.505273).  Saving model ...
Validation loss decreased (0.505273 --> 0.505167).  Saving model ...
Validation loss decreased (0.505167 --> 0.505061).  Saving model ...
Validation loss decreased (0.505061 --> 0.504955).  Saving model ...
Validation loss decreased (0.504955 --> 0.504850).  Saving model ...
Validation loss decreased (0.504850 --> 0.504744).  Saving model ...
Validation loss decreased (0.504744 --> 0.504640).  Saving model ...
Validation loss decreased (0.504640 --> 0.504535).  Saving model ...
Validation loss decreased (0.504535 --> 0.504431).  Saving model ...
Validation loss decreased (0.504431 --> 0.504327).  Saving model ...
Validation loss decreased (0.504327 --> 0.504224).  Saving model ...
Validation loss decreased (0.504224 --> 0.504120).  Saving model ...
Validation loss decreased (0.504120 --> 0.504017).  Saving model ...
Validation loss decreased (0.504017 --> 0.503915).  Saving model ...
Validation loss decreased (0.503915 --> 0.503812).  Saving model ...
Validation loss decreased (0.503812 --> 0.503710).  Saving model ...
Validation loss decreased (0.503710 --> 0.503608).  Saving model ...
Validation loss decreased (0.503608 --> 0.503506).  Saving model ...
Validation loss decreased (0.503506 --> 0.503405).  Saving model ...
Validation loss decreased (0.503405 --> 0.503304).  Saving model ...
Validation loss decreased (0.503304 --> 0.503204).  Saving model ...
Validation loss decreased (0.503204 --> 0.503103).  Saving model ...
Validation loss decreased (0.503103 --> 0.503003).  Saving model ...
Validation loss decreased (0.503003 --> 0.502903).  Saving model ...
Validation loss decreased (0.502903 --> 0.502804).  Saving model ...
Validation loss decreased (0.502804 --> 0.502704).  Saving model ...
Validation loss decreased (0.502704 --> 0.502605).  Saving model ...
Validation loss decreased (0.502605 --> 0.502507).  Saving model ...
Validation loss decreased (0.502507 --> 0.502408).  Saving model ...
Validation loss decreased (0.502408 --> 0.502310).  Saving model ...
Validation loss decreased (0.502310 --> 0.502212).  Saving model ...
Validation loss decreased (0.502212 --> 0.502115).  Saving model ...
Validation loss decreased (0.502115 --> 0.502017).  Saving model ...
Validation loss decreased (0.502017 --> 0.501920).  Saving model ...
Validation loss decreased (0.501920 --> 0.501824).  Saving model ...
Validation loss decreased (0.501824 --> 0.501727).  Saving model ...
Validation loss decreased (0.501727 --> 0.501631).  Saving model ...
Validation loss decreased (0.501631 --> 0.501535).  Saving model ...
Validation loss decreased (0.501535 --> 0.501439).  Saving model ...
Validation loss decreased (0.501439 --> 0.501344).  Saving model ...
epoch 701, loss 0.5013, train acc 75.34%, f1 0.5610, precision 0.7419, recall 0.4510, auc 0.6834
Validation loss decreased (0.501344 --> 0.501249).  Saving model ...
Validation loss decreased (0.501249 --> 0.501154).  Saving model ...
Validation loss decreased (0.501154 --> 0.501060).  Saving model ...
Validation loss decreased (0.501060 --> 0.500965).  Saving model ...
Validation loss decreased (0.500965 --> 0.500871).  Saving model ...
Validation loss decreased (0.500871 --> 0.500778).  Saving model ...
Validation loss decreased (0.500778 --> 0.500684).  Saving model ...
Validation loss decreased (0.500684 --> 0.500591).  Saving model ...
Validation loss decreased (0.500591 --> 0.500498).  Saving model ...
Validation loss decreased (0.500498 --> 0.500405).  Saving model ...
Validation loss decreased (0.500405 --> 0.500313).  Saving model ...
Validation loss decreased (0.500313 --> 0.500221).  Saving model ...
Validation loss decreased (0.500221 --> 0.500129).  Saving model ...
Validation loss decreased (0.500129 --> 0.500037).  Saving model ...
Validation loss decreased (0.500037 --> 0.499946).  Saving model ...
Validation loss decreased (0.499946 --> 0.499855).  Saving model ...
Validation loss decreased (0.499855 --> 0.499764).  Saving model ...
Validation loss decreased (0.499764 --> 0.499674).  Saving model ...
Validation loss decreased (0.499674 --> 0.499583).  Saving model ...
Validation loss decreased (0.499583 --> 0.499494).  Saving model ...
Validation loss decreased (0.499494 --> 0.499404).  Saving model ...
Validation loss decreased (0.499404 --> 0.499314).  Saving model ...
Validation loss decreased (0.499314 --> 0.499225).  Saving model ...
Validation loss decreased (0.499225 --> 0.499136).  Saving model ...
Validation loss decreased (0.499136 --> 0.499047).  Saving model ...
Validation loss decreased (0.499047 --> 0.498959).  Saving model ...
Validation loss decreased (0.498959 --> 0.498871).  Saving model ...
Validation loss decreased (0.498871 --> 0.498783).  Saving model ...
Validation loss decreased (0.498783 --> 0.498695).  Saving model ...
Validation loss decreased (0.498695 --> 0.498608).  Saving model ...
Validation loss decreased (0.498608 --> 0.498521).  Saving model ...
Validation loss decreased (0.498521 --> 0.498434).  Saving model ...
Validation loss decreased (0.498434 --> 0.498347).  Saving model ...
Validation loss decreased (0.498347 --> 0.498261).  Saving model ...
Validation loss decreased (0.498261 --> 0.498175).  Saving model ...
Validation loss decreased (0.498175 --> 0.498089).  Saving model ...
Validation loss decreased (0.498089 --> 0.498003).  Saving model ...
Validation loss decreased (0.498003 --> 0.497918).  Saving model ...
Validation loss decreased (0.497918 --> 0.497832).  Saving model ...
Validation loss decreased (0.497832 --> 0.497748).  Saving model ...
Validation loss decreased (0.497748 --> 0.497663).  Saving model ...
Validation loss decreased (0.497663 --> 0.497579).  Saving model ...
Validation loss decreased (0.497579 --> 0.497494).  Saving model ...
Validation loss decreased (0.497494 --> 0.497411).  Saving model ...
Validation loss decreased (0.497411 --> 0.497327).  Saving model ...
Validation loss decreased (0.497327 --> 0.497243).  Saving model ...
Validation loss decreased (0.497243 --> 0.497160).  Saving model ...
Validation loss decreased (0.497160 --> 0.497077).  Saving model ...
Validation loss decreased (0.497077 --> 0.496995).  Saving model ...
Validation loss decreased (0.496995 --> 0.496912).  Saving model ...
Validation loss decreased (0.496912 --> 0.496830).  Saving model ...
Validation loss decreased (0.496830 --> 0.496748).  Saving model ...
Validation loss decreased (0.496748 --> 0.496666).  Saving model ...
Validation loss decreased (0.496666 --> 0.496585).  Saving model ...
Validation loss decreased (0.496585 --> 0.496504).  Saving model ...
Validation loss decreased (0.496504 --> 0.496423).  Saving model ...
Validation loss decreased (0.496423 --> 0.496342).  Saving model ...
Validation loss decreased (0.496342 --> 0.496261).  Saving model ...
Validation loss decreased (0.496261 --> 0.496181).  Saving model ...
Validation loss decreased (0.496181 --> 0.496101).  Saving model ...
Validation loss decreased (0.496101 --> 0.496021).  Saving model ...
Validation loss decreased (0.496021 --> 0.495942).  Saving model ...
Validation loss decreased (0.495942 --> 0.495863).  Saving model ...
Validation loss decreased (0.495863 --> 0.495783).  Saving model ...
Validation loss decreased (0.495783 --> 0.495705).  Saving model ...
Validation loss decreased (0.495705 --> 0.495626).  Saving model ...
Validation loss decreased (0.495626 --> 0.495548).  Saving model ...
Validation loss decreased (0.495548 --> 0.495469).  Saving model ...
Validation loss decreased (0.495469 --> 0.495391).  Saving model ...
Validation loss decreased (0.495391 --> 0.495314).  Saving model ...
Validation loss decreased (0.495314 --> 0.495236).  Saving model ...
Validation loss decreased (0.495236 --> 0.495159).  Saving model ...
Validation loss decreased (0.495159 --> 0.495082).  Saving model ...
Validation loss decreased (0.495082 --> 0.495005).  Saving model ...
Validation loss decreased (0.495005 --> 0.494929).  Saving model ...
Validation loss decreased (0.494929 --> 0.494852).  Saving model ...
Validation loss decreased (0.494852 --> 0.494776).  Saving model ...
Validation loss decreased (0.494776 --> 0.494700).  Saving model ...
Validation loss decreased (0.494700 --> 0.494625).  Saving model ...
Validation loss decreased (0.494625 --> 0.494549).  Saving model ...
Validation loss decreased (0.494549 --> 0.494474).  Saving model ...
Validation loss decreased (0.494474 --> 0.494399).  Saving model ...
Validation loss decreased (0.494399 --> 0.494325).  Saving model ...
Validation loss decreased (0.494325 --> 0.494250).  Saving model ...
Validation loss decreased (0.494250 --> 0.494176).  Saving model ...
Validation loss decreased (0.494176 --> 0.494102).  Saving model ...
Validation loss decreased (0.494102 --> 0.494028).  Saving model ...
Validation loss decreased (0.494028 --> 0.493954).  Saving model ...
Validation loss decreased (0.493954 --> 0.493881).  Saving model ...
Validation loss decreased (0.493881 --> 0.493807).  Saving model ...
Validation loss decreased (0.493807 --> 0.493735).  Saving model ...
Validation loss decreased (0.493735 --> 0.493662).  Saving model ...
Validation loss decreased (0.493662 --> 0.493589).  Saving model ...
Validation loss decreased (0.493589 --> 0.493517).  Saving model ...
Validation loss decreased (0.493517 --> 0.493445).  Saving model ...
Validation loss decreased (0.493445 --> 0.493373).  Saving model ...
Validation loss decreased (0.493373 --> 0.493301).  Saving model ...
Validation loss decreased (0.493301 --> 0.493230).  Saving model ...
Validation loss decreased (0.493230 --> 0.493159).  Saving model ...
Validation loss decreased (0.493159 --> 0.493088).  Saving model ...
epoch 801, loss 0.4931, train acc 75.86%, f1 0.5937, precision 0.7203, recall 0.5049, auc 0.6998
Validation loss decreased (0.493088 --> 0.493017).  Saving model ...
Validation loss decreased (0.493017 --> 0.492946).  Saving model ...
Validation loss decreased (0.492946 --> 0.492876).  Saving model ...
Validation loss decreased (0.492876 --> 0.492806).  Saving model ...
Validation loss decreased (0.492806 --> 0.492736).  Saving model ...
Validation loss decreased (0.492736 --> 0.492666).  Saving model ...
Validation loss decreased (0.492666 --> 0.492596).  Saving model ...
Validation loss decreased (0.492596 --> 0.492527).  Saving model ...
Validation loss decreased (0.492527 --> 0.492458).  Saving model ...
Validation loss decreased (0.492458 --> 0.492389).  Saving model ...
Validation loss decreased (0.492389 --> 0.492320).  Saving model ...
Validation loss decreased (0.492320 --> 0.492252).  Saving model ...
Validation loss decreased (0.492252 --> 0.492183).  Saving model ...
Validation loss decreased (0.492183 --> 0.492115).  Saving model ...
Validation loss decreased (0.492115 --> 0.492047).  Saving model ...
Validation loss decreased (0.492047 --> 0.491980).  Saving model ...
Validation loss decreased (0.491980 --> 0.491912).  Saving model ...
Validation loss decreased (0.491912 --> 0.491845).  Saving model ...
Validation loss decreased (0.491845 --> 0.491778).  Saving model ...
Validation loss decreased (0.491778 --> 0.491711).  Saving model ...
Validation loss decreased (0.491711 --> 0.491644).  Saving model ...
Validation loss decreased (0.491644 --> 0.491578).  Saving model ...
Validation loss decreased (0.491578 --> 0.491512).  Saving model ...
Validation loss decreased (0.491512 --> 0.491446).  Saving model ...
Validation loss decreased (0.491446 --> 0.491380).  Saving model ...
Validation loss decreased (0.491380 --> 0.491314).  Saving model ...
Validation loss decreased (0.491314 --> 0.491249).  Saving model ...
Validation loss decreased (0.491249 --> 0.491183).  Saving model ...
Validation loss decreased (0.491183 --> 0.491118).  Saving model ...
Validation loss decreased (0.491118 --> 0.491053).  Saving model ...
Validation loss decreased (0.491053 --> 0.490989).  Saving model ...
Validation loss decreased (0.490989 --> 0.490924).  Saving model ...
Validation loss decreased (0.490924 --> 0.490860).  Saving model ...
Validation loss decreased (0.490860 --> 0.490796).  Saving model ...
Validation loss decreased (0.490796 --> 0.490732).  Saving model ...
Validation loss decreased (0.490732 --> 0.490668).  Saving model ...
Validation loss decreased (0.490668 --> 0.490605).  Saving model ...
Validation loss decreased (0.490605 --> 0.490541).  Saving model ...
Validation loss decreased (0.490541 --> 0.490478).  Saving model ...
Validation loss decreased (0.490478 --> 0.490415).  Saving model ...
Validation loss decreased (0.490415 --> 0.490353).  Saving model ...
Validation loss decreased (0.490353 --> 0.490290).  Saving model ...
Validation loss decreased (0.490290 --> 0.490228).  Saving model ...
Validation loss decreased (0.490228 --> 0.490165).  Saving model ...
Validation loss decreased (0.490165 --> 0.490103).  Saving model ...
Validation loss decreased (0.490103 --> 0.490042).  Saving model ...
Validation loss decreased (0.490042 --> 0.489980).  Saving model ...
Validation loss decreased (0.489980 --> 0.489919).  Saving model ...
Validation loss decreased (0.489919 --> 0.489857).  Saving model ...
Validation loss decreased (0.489857 --> 0.489796).  Saving model ...
Validation loss decreased (0.489796 --> 0.489735).  Saving model ...
Validation loss decreased (0.489735 --> 0.489675).  Saving model ...
Validation loss decreased (0.489675 --> 0.489614).  Saving model ...
Validation loss decreased (0.489614 --> 0.489554).  Saving model ...
Validation loss decreased (0.489554 --> 0.489494).  Saving model ...
Validation loss decreased (0.489494 --> 0.489434).  Saving model ...
Validation loss decreased (0.489434 --> 0.489374).  Saving model ...
Validation loss decreased (0.489374 --> 0.489315).  Saving model ...
Validation loss decreased (0.489315 --> 0.489255).  Saving model ...
Validation loss decreased (0.489255 --> 0.489196).  Saving model ...
Validation loss decreased (0.489196 --> 0.489137).  Saving model ...
Validation loss decreased (0.489137 --> 0.489078).  Saving model ...
Validation loss decreased (0.489078 --> 0.489019).  Saving model ...
Validation loss decreased (0.489019 --> 0.488961).  Saving model ...
Validation loss decreased (0.488961 --> 0.488903).  Saving model ...
Validation loss decreased (0.488903 --> 0.488844).  Saving model ...
Validation loss decreased (0.488844 --> 0.488787).  Saving model ...
Validation loss decreased (0.488787 --> 0.488729).  Saving model ...
Validation loss decreased (0.488729 --> 0.488671).  Saving model ...
Validation loss decreased (0.488671 --> 0.488614).  Saving model ...
Validation loss decreased (0.488614 --> 0.488556).  Saving model ...
Validation loss decreased (0.488556 --> 0.488499).  Saving model ...
Validation loss decreased (0.488499 --> 0.488443).  Saving model ...
Validation loss decreased (0.488443 --> 0.488386).  Saving model ...
Validation loss decreased (0.488386 --> 0.488329).  Saving model ...
Validation loss decreased (0.488329 --> 0.488273).  Saving model ...
Validation loss decreased (0.488273 --> 0.488217).  Saving model ...
Validation loss decreased (0.488217 --> 0.488161).  Saving model ...
Validation loss decreased (0.488161 --> 0.488105).  Saving model ...
Validation loss decreased (0.488105 --> 0.488049).  Saving model ...
Validation loss decreased (0.488049 --> 0.487993).  Saving model ...
Validation loss decreased (0.487993 --> 0.487938).  Saving model ...
Validation loss decreased (0.487938 --> 0.487883).  Saving model ...
Validation loss decreased (0.487883 --> 0.487828).  Saving model ...
Validation loss decreased (0.487828 --> 0.487773).  Saving model ...
Validation loss decreased (0.487773 --> 0.487718).  Saving model ...
Validation loss decreased (0.487718 --> 0.487664).  Saving model ...
Validation loss decreased (0.487664 --> 0.487609).  Saving model ...
Validation loss decreased (0.487609 --> 0.487555).  Saving model ...
Validation loss decreased (0.487555 --> 0.487501).  Saving model ...
Validation loss decreased (0.487501 --> 0.487447).  Saving model ...
Validation loss decreased (0.487447 --> 0.487394).  Saving model ...
Validation loss decreased (0.487394 --> 0.487340).  Saving model ...
Validation loss decreased (0.487340 --> 0.487287).  Saving model ...
Validation loss decreased (0.487287 --> 0.487234).  Saving model ...
Validation loss decreased (0.487234 --> 0.487181).  Saving model ...
Validation loss decreased (0.487181 --> 0.487128).  Saving model ...
Validation loss decreased (0.487128 --> 0.487075).  Saving model ...
Validation loss decreased (0.487075 --> 0.487023).  Saving model ...
Validation loss decreased (0.487023 --> 0.486970).  Saving model ...
epoch 901, loss 0.4870, train acc 77.23%, f1 0.6336, precision 0.7233, recall 0.5637, auc 0.7240
Validation loss decreased (0.486970 --> 0.486918).  Saving model ...
Validation loss decreased (0.486918 --> 0.486866).  Saving model ...
Validation loss decreased (0.486866 --> 0.486814).  Saving model ...
Validation loss decreased (0.486814 --> 0.486762).  Saving model ...
Validation loss decreased (0.486762 --> 0.486711).  Saving model ...
Validation loss decreased (0.486711 --> 0.486659).  Saving model ...
Validation loss decreased (0.486659 --> 0.486608).  Saving model ...
Validation loss decreased (0.486608 --> 0.486557).  Saving model ...
Validation loss decreased (0.486557 --> 0.486506).  Saving model ...
Validation loss decreased (0.486506 --> 0.486455).  Saving model ...
Validation loss decreased (0.486455 --> 0.486404).  Saving model ...
Validation loss decreased (0.486404 --> 0.486354).  Saving model ...
Validation loss decreased (0.486354 --> 0.486303).  Saving model ...
Validation loss decreased (0.486303 --> 0.486253).  Saving model ...
Validation loss decreased (0.486253 --> 0.486203).  Saving model ...
Validation loss decreased (0.486203 --> 0.486153).  Saving model ...
Validation loss decreased (0.486153 --> 0.486104).  Saving model ...
Validation loss decreased (0.486104 --> 0.486054).  Saving model ...
Validation loss decreased (0.486054 --> 0.486005).  Saving model ...
Validation loss decreased (0.486005 --> 0.485955).  Saving model ...
Validation loss decreased (0.485955 --> 0.485906).  Saving model ...
Validation loss decreased (0.485906 --> 0.485857).  Saving model ...
Validation loss decreased (0.485857 --> 0.485808).  Saving model ...
Validation loss decreased (0.485808 --> 0.485760).  Saving model ...
Validation loss decreased (0.485760 --> 0.485711).  Saving model ...
Validation loss decreased (0.485711 --> 0.485663).  Saving model ...
Validation loss decreased (0.485663 --> 0.485614).  Saving model ...
Validation loss decreased (0.485614 --> 0.485566).  Saving model ...
Validation loss decreased (0.485566 --> 0.485518).  Saving model ...
Validation loss decreased (0.485518 --> 0.485471).  Saving model ...
Validation loss decreased (0.485471 --> 0.485423).  Saving model ...
Validation loss decreased (0.485423 --> 0.485375).  Saving model ...
Validation loss decreased (0.485375 --> 0.485328).  Saving model ...
Validation loss decreased (0.485328 --> 0.485281).  Saving model ...
Validation loss decreased (0.485281 --> 0.485234).  Saving model ...
Validation loss decreased (0.485234 --> 0.485187).  Saving model ...
Validation loss decreased (0.485187 --> 0.485140).  Saving model ...
Validation loss decreased (0.485140 --> 0.485093).  Saving model ...
Validation loss decreased (0.485093 --> 0.485047).  Saving model ...
Validation loss decreased (0.485047 --> 0.485001).  Saving model ...
Validation loss decreased (0.485001 --> 0.484954).  Saving model ...
Validation loss decreased (0.484954 --> 0.484908).  Saving model ...
Validation loss decreased (0.484908 --> 0.484862).  Saving model ...
Validation loss decreased (0.484862 --> 0.484817).  Saving model ...
Validation loss decreased (0.484817 --> 0.484771).  Saving model ...
Validation loss decreased (0.484771 --> 0.484725).  Saving model ...
Validation loss decreased (0.484725 --> 0.484680).  Saving model ...
Validation loss decreased (0.484680 --> 0.484635).  Saving model ...
Validation loss decreased (0.484635 --> 0.484590).  Saving model ...
Validation loss decreased (0.484590 --> 0.484545).  Saving model ...
Validation loss decreased (0.484545 --> 0.484500).  Saving model ...
Validation loss decreased (0.484500 --> 0.484455).  Saving model ...
Validation loss decreased (0.484455 --> 0.484410).  Saving model ...
Validation loss decreased (0.484410 --> 0.484366).  Saving model ...
Validation loss decreased (0.484366 --> 0.484322).  Saving model ...
Validation loss decreased (0.484322 --> 0.484278).  Saving model ...
Validation loss decreased (0.484278 --> 0.484233).  Saving model ...
Validation loss decreased (0.484233 --> 0.484190).  Saving model ...
Validation loss decreased (0.484190 --> 0.484146).  Saving model ...
Validation loss decreased (0.484146 --> 0.484102).  Saving model ...
Validation loss decreased (0.484102 --> 0.484059).  Saving model ...
Validation loss decreased (0.484059 --> 0.484015).  Saving model ...
Validation loss decreased (0.484015 --> 0.483972).  Saving model ...
Validation loss decreased (0.483972 --> 0.483929).  Saving model ...
Validation loss decreased (0.483929 --> 0.483886).  Saving model ...
Validation loss decreased (0.483886 --> 0.483843).  Saving model ...
Validation loss decreased (0.483843 --> 0.483800).  Saving model ...
Validation loss decreased (0.483800 --> 0.483758).  Saving model ...
Validation loss decreased (0.483758 --> 0.483715).  Saving model ...
Validation loss decreased (0.483715 --> 0.483673).  Saving model ...
Validation loss decreased (0.483673 --> 0.483631).  Saving model ...
Validation loss decreased (0.483631 --> 0.483589).  Saving model ...
Validation loss decreased (0.483589 --> 0.483547).  Saving model ...
Validation loss decreased (0.483547 --> 0.483505).  Saving model ...
Validation loss decreased (0.483505 --> 0.483463).  Saving model ...
Validation loss decreased (0.483463 --> 0.483421).  Saving model ...
Validation loss decreased (0.483421 --> 0.483380).  Saving model ...
Validation loss decreased (0.483380 --> 0.483339).  Saving model ...
Validation loss decreased (0.483339 --> 0.483297).  Saving model ...
Validation loss decreased (0.483297 --> 0.483256).  Saving model ...
Validation loss decreased (0.483256 --> 0.483215).  Saving model ...
Validation loss decreased (0.483215 --> 0.483174).  Saving model ...
Validation loss decreased (0.483174 --> 0.483134).  Saving model ...
Validation loss decreased (0.483134 --> 0.483093).  Saving model ...
Validation loss decreased (0.483093 --> 0.483053).  Saving model ...
Validation loss decreased (0.483053 --> 0.483012).  Saving model ...
Validation loss decreased (0.483012 --> 0.482972).  Saving model ...
Validation loss decreased (0.482972 --> 0.482932).  Saving model ...
Validation loss decreased (0.482932 --> 0.482892).  Saving model ...
Validation loss decreased (0.482892 --> 0.482852).  Saving model ...
Validation loss decreased (0.482852 --> 0.482812).  Saving model ...
Validation loss decreased (0.482812 --> 0.482772).  Saving model ...
Validation loss decreased (0.482772 --> 0.482733).  Saving model ...
Validation loss decreased (0.482733 --> 0.482693).  Saving model ...
Validation loss decreased (0.482693 --> 0.482654).  Saving model ...
Validation loss decreased (0.482654 --> 0.482615).  Saving model ...
Validation loss decreased (0.482615 --> 0.482576).  Saving model ...
Validation loss decreased (0.482576 --> 0.482537).  Saving model ...
Validation loss decreased (0.482537 --> 0.482498).  Saving model ...
Validation loss decreased (0.482498 --> 0.482459).  Saving model ...
epoch 1001, loss 0.4825, train acc 76.88%, f1 0.6361, precision 0.7066, recall 0.5784, auc 0.7247
Validation loss decreased (0.482459 --> 0.482421).  Saving model ...
Validation loss decreased (0.482421 --> 0.482382).  Saving model ...
Validation loss decreased (0.482382 --> 0.482344).  Saving model ...
Validation loss decreased (0.482344 --> 0.482305).  Saving model ...
Validation loss decreased (0.482305 --> 0.482267).  Saving model ...
Validation loss decreased (0.482267 --> 0.482229).  Saving model ...
Validation loss decreased (0.482229 --> 0.482191).  Saving model ...
Validation loss decreased (0.482191 --> 0.482153).  Saving model ...
Validation loss decreased (0.482153 --> 0.482115).  Saving model ...
Validation loss decreased (0.482115 --> 0.482078).  Saving model ...
Validation loss decreased (0.482078 --> 0.482040).  Saving model ...
Validation loss decreased (0.482040 --> 0.482003).  Saving model ...
Validation loss decreased (0.482003 --> 0.481965).  Saving model ...
Validation loss decreased (0.481965 --> 0.481928).  Saving model ...
Validation loss decreased (0.481928 --> 0.481891).  Saving model ...
Validation loss decreased (0.481891 --> 0.481854).  Saving model ...
Validation loss decreased (0.481854 --> 0.481817).  Saving model ...
Validation loss decreased (0.481817 --> 0.481780).  Saving model ...
Validation loss decreased (0.481780 --> 0.481744).  Saving model ...
Validation loss decreased (0.481744 --> 0.481707).  Saving model ...
Validation loss decreased (0.481707 --> 0.481671).  Saving model ...
Validation loss decreased (0.481671 --> 0.481634).  Saving model ...
Validation loss decreased (0.481634 --> 0.481598).  Saving model ...
Validation loss decreased (0.481598 --> 0.481562).  Saving model ...
Validation loss decreased (0.481562 --> 0.481526).  Saving model ...
Validation loss decreased (0.481526 --> 0.481490).  Saving model ...
Validation loss decreased (0.481490 --> 0.481454).  Saving model ...
Validation loss decreased (0.481454 --> 0.481418).  Saving model ...
Validation loss decreased (0.481418 --> 0.481382).  Saving model ...
Validation loss decreased (0.481382 --> 0.481347).  Saving model ...
Validation loss decreased (0.481347 --> 0.481311).  Saving model ...
Validation loss decreased (0.481311 --> 0.481276).  Saving model ...
Validation loss decreased (0.481276 --> 0.481240).  Saving model ...
Validation loss decreased (0.481240 --> 0.481205).  Saving model ...
Validation loss decreased (0.481205 --> 0.481170).  Saving model ...
Validation loss decreased (0.481170 --> 0.481135).  Saving model ...
Validation loss decreased (0.481135 --> 0.481100).  Saving model ...
Validation loss decreased (0.481100 --> 0.481066).  Saving model ...
Validation loss decreased (0.481066 --> 0.481031).  Saving model ...
Validation loss decreased (0.481031 --> 0.480996).  Saving model ...
Validation loss decreased (0.480996 --> 0.480962).  Saving model ...
Validation loss decreased (0.480962 --> 0.480927).  Saving model ...
Validation loss decreased (0.480927 --> 0.480893).  Saving model ...
Validation loss decreased (0.480893 --> 0.480859).  Saving model ...
Validation loss decreased (0.480859 --> 0.480824).  Saving model ...
Validation loss decreased (0.480824 --> 0.480790).  Saving model ...
Validation loss decreased (0.480790 --> 0.480756).  Saving model ...
Validation loss decreased (0.480756 --> 0.480722).  Saving model ...
Validation loss decreased (0.480722 --> 0.480689).  Saving model ...
Validation loss decreased (0.480689 --> 0.480655).  Saving model ...
Validation loss decreased (0.480655 --> 0.480621).  Saving model ...
Validation loss decreased (0.480621 --> 0.480588).  Saving model ...
Validation loss decreased (0.480588 --> 0.480554).  Saving model ...
Validation loss decreased (0.480554 --> 0.480521).  Saving model ...
Validation loss decreased (0.480521 --> 0.480488).  Saving model ...
Validation loss decreased (0.480488 --> 0.480454).  Saving model ...
Validation loss decreased (0.480454 --> 0.480421).  Saving model ...
Validation loss decreased (0.480421 --> 0.480388).  Saving model ...
Validation loss decreased (0.480388 --> 0.480355).  Saving model ...
Validation loss decreased (0.480355 --> 0.480323).  Saving model ...
Validation loss decreased (0.480323 --> 0.480290).  Saving model ...
Validation loss decreased (0.480290 --> 0.480257).  Saving model ...
Validation loss decreased (0.480257 --> 0.480225).  Saving model ...
Validation loss decreased (0.480225 --> 0.480192).  Saving model ...
Validation loss decreased (0.480192 --> 0.480160).  Saving model ...
Validation loss decreased (0.480160 --> 0.480127).  Saving model ...
Validation loss decreased (0.480127 --> 0.480095).  Saving model ...
Validation loss decreased (0.480095 --> 0.480063).  Saving model ...
Validation loss decreased (0.480063 --> 0.480031).  Saving model ...
Validation loss decreased (0.480031 --> 0.479999).  Saving model ...
Validation loss decreased (0.479999 --> 0.479967).  Saving model ...
Validation loss decreased (0.479967 --> 0.479935).  Saving model ...
Validation loss decreased (0.479935 --> 0.479903).  Saving model ...
Validation loss decreased (0.479903 --> 0.479871).  Saving model ...
Validation loss decreased (0.479871 --> 0.479839).  Saving model ...
Validation loss decreased (0.479839 --> 0.479808).  Saving model ...
Validation loss decreased (0.479808 --> 0.479776).  Saving model ...
Validation loss decreased (0.479776 --> 0.479745).  Saving model ...
Validation loss decreased (0.479745 --> 0.479714).  Saving model ...
Validation loss decreased (0.479714 --> 0.479682).  Saving model ...
Validation loss decreased (0.479682 --> 0.479651).  Saving model ...
Validation loss decreased (0.479651 --> 0.479620).  Saving model ...
Validation loss decreased (0.479620 --> 0.479589).  Saving model ...
Validation loss decreased (0.479589 --> 0.479558).  Saving model ...
Validation loss decreased (0.479558 --> 0.479527).  Saving model ...
Validation loss decreased (0.479527 --> 0.479496).  Saving model ...
Validation loss decreased (0.479496 --> 0.479465).  Saving model ...
Validation loss decreased (0.479465 --> 0.479435).  Saving model ...
Validation loss decreased (0.479435 --> 0.479404).  Saving model ...
Validation loss decreased (0.479404 --> 0.479373).  Saving model ...
Validation loss decreased (0.479373 --> 0.479343).  Saving model ...
Validation loss decreased (0.479343 --> 0.479312).  Saving model ...
Validation loss decreased (0.479312 --> 0.479282).  Saving model ...
Validation loss decreased (0.479282 --> 0.479252).  Saving model ...
Validation loss decreased (0.479252 --> 0.479221).  Saving model ...
Validation loss decreased (0.479221 --> 0.479191).  Saving model ...
Validation loss decreased (0.479191 --> 0.479161).  Saving model ...
Validation loss decreased (0.479161 --> 0.479131).  Saving model ...
Validation loss decreased (0.479131 --> 0.479101).  Saving model ...
Validation loss decreased (0.479101 --> 0.479071).  Saving model ...
epoch 1101, loss 0.4791, train acc 77.40%, f1 0.6508, precision 0.7069, recall 0.6029, auc 0.7344
Validation loss decreased (0.479071 --> 0.479041).  Saving model ...
Validation loss decreased (0.479041 --> 0.479011).  Saving model ...
Validation loss decreased (0.479011 --> 0.478981).  Saving model ...
Validation loss decreased (0.478981 --> 0.478952).  Saving model ...
Validation loss decreased (0.478952 --> 0.478922).  Saving model ...
Validation loss decreased (0.478922 --> 0.478892).  Saving model ...
Validation loss decreased (0.478892 --> 0.478863).  Saving model ...
Validation loss decreased (0.478863 --> 0.478833).  Saving model ...
Validation loss decreased (0.478833 --> 0.478804).  Saving model ...
Validation loss decreased (0.478804 --> 0.478775).  Saving model ...
Validation loss decreased (0.478775 --> 0.478745).  Saving model ...
Validation loss decreased (0.478745 --> 0.478716).  Saving model ...
Validation loss decreased (0.478716 --> 0.478687).  Saving model ...
Validation loss decreased (0.478687 --> 0.478658).  Saving model ...
Validation loss decreased (0.478658 --> 0.478628).  Saving model ...
Validation loss decreased (0.478628 --> 0.478599).  Saving model ...
Validation loss decreased (0.478599 --> 0.478570).  Saving model ...
Validation loss decreased (0.478570 --> 0.478541).  Saving model ...
Validation loss decreased (0.478541 --> 0.478512).  Saving model ...
Validation loss decreased (0.478512 --> 0.478483).  Saving model ...
Validation loss decreased (0.478483 --> 0.478455).  Saving model ...
Validation loss decreased (0.478455 --> 0.478426).  Saving model ...
Validation loss decreased (0.478426 --> 0.478397).  Saving model ...
Validation loss decreased (0.478397 --> 0.478368).  Saving model ...
Validation loss decreased (0.478368 --> 0.478340).  Saving model ...
Validation loss decreased (0.478340 --> 0.478311).  Saving model ...
Validation loss decreased (0.478311 --> 0.478283).  Saving model ...
Validation loss decreased (0.478283 --> 0.478254).  Saving model ...
Validation loss decreased (0.478254 --> 0.478226).  Saving model ...
Validation loss decreased (0.478226 --> 0.478197).  Saving model ...
Validation loss decreased (0.478197 --> 0.478169).  Saving model ...
Validation loss decreased (0.478169 --> 0.478140).  Saving model ...
Validation loss decreased (0.478140 --> 0.478112).  Saving model ...
Validation loss decreased (0.478112 --> 0.478084).  Saving model ...
Validation loss decreased (0.478084 --> 0.478055).  Saving model ...
Validation loss decreased (0.478055 --> 0.478027).  Saving model ...
Validation loss decreased (0.478027 --> 0.477999).  Saving model ...
Validation loss decreased (0.477999 --> 0.477971).  Saving model ...
Validation loss decreased (0.477971 --> 0.477943).  Saving model ...
Validation loss decreased (0.477943 --> 0.477915).  Saving model ...
Validation loss decreased (0.477915 --> 0.477887).  Saving model ...
Validation loss decreased (0.477887 --> 0.477859).  Saving model ...
Validation loss decreased (0.477859 --> 0.477831).  Saving model ...
Validation loss decreased (0.477831 --> 0.477803).  Saving model ...
Validation loss decreased (0.477803 --> 0.477775).  Saving model ...
Validation loss decreased (0.477775 --> 0.477747).  Saving model ...
Validation loss decreased (0.477747 --> 0.477719).  Saving model ...
Validation loss decreased (0.477719 --> 0.477691).  Saving model ...
Validation loss decreased (0.477691 --> 0.477663).  Saving model ...
Validation loss decreased (0.477663 --> 0.477635).  Saving model ...
Validation loss decreased (0.477635 --> 0.477608).  Saving model ...
Validation loss decreased (0.477608 --> 0.477580).  Saving model ...
Validation loss decreased (0.477580 --> 0.477552).  Saving model ...
Validation loss decreased (0.477552 --> 0.477524).  Saving model ...
Validation loss decreased (0.477524 --> 0.477497).  Saving model ...
Validation loss decreased (0.477497 --> 0.477469).  Saving model ...
Validation loss decreased (0.477469 --> 0.477441).  Saving model ...
Validation loss decreased (0.477441 --> 0.477414).  Saving model ...
Validation loss decreased (0.477414 --> 0.477386).  Saving model ...
Validation loss decreased (0.477386 --> 0.477358).  Saving model ...
Validation loss decreased (0.477358 --> 0.477331).  Saving model ...
Validation loss decreased (0.477331 --> 0.477303).  Saving model ...
Validation loss decreased (0.477303 --> 0.477275).  Saving model ...
Validation loss decreased (0.477275 --> 0.477248).  Saving model ...
Validation loss decreased (0.477248 --> 0.477220).  Saving model ...
Validation loss decreased (0.477220 --> 0.477193).  Saving model ...
Validation loss decreased (0.477193 --> 0.477165).  Saving model ...
Validation loss decreased (0.477165 --> 0.477138).  Saving model ...
Validation loss decreased (0.477138 --> 0.477110).  Saving model ...
Validation loss decreased (0.477110 --> 0.477083).  Saving model ...
Validation loss decreased (0.477083 --> 0.477055).  Saving model ...
Validation loss decreased (0.477055 --> 0.477027).  Saving model ...
Validation loss decreased (0.477027 --> 0.477000).  Saving model ...
Validation loss decreased (0.477000 --> 0.476972).  Saving model ...
Validation loss decreased (0.476972 --> 0.476945).  Saving model ...
Validation loss decreased (0.476945 --> 0.476917).  Saving model ...
Validation loss decreased (0.476917 --> 0.476890).  Saving model ...
Validation loss decreased (0.476890 --> 0.476862).  Saving model ...
Validation loss decreased (0.476862 --> 0.476835).  Saving model ...
Validation loss decreased (0.476835 --> 0.476807).  Saving model ...
Validation loss decreased (0.476807 --> 0.476780).  Saving model ...
Validation loss decreased (0.476780 --> 0.476752).  Saving model ...
Validation loss decreased (0.476752 --> 0.476724).  Saving model ...
Validation loss decreased (0.476724 --> 0.476697).  Saving model ...
Validation loss decreased (0.476697 --> 0.476669).  Saving model ...
Validation loss decreased (0.476669 --> 0.476642).  Saving model ...
Validation loss decreased (0.476642 --> 0.476614).  Saving model ...
Validation loss decreased (0.476614 --> 0.476586).  Saving model ...
Validation loss decreased (0.476586 --> 0.476559).  Saving model ...
Validation loss decreased (0.476559 --> 0.476531).  Saving model ...
Validation loss decreased (0.476531 --> 0.476503).  Saving model ...
Validation loss decreased (0.476503 --> 0.476476).  Saving model ...
Validation loss decreased (0.476476 --> 0.476448).  Saving model ...
Validation loss decreased (0.476448 --> 0.476420).  Saving model ...
Validation loss decreased (0.476420 --> 0.476393).  Saving model ...
Validation loss decreased (0.476393 --> 0.476365).  Saving model ...
Validation loss decreased (0.476365 --> 0.476337).  Saving model ...
Validation loss decreased (0.476337 --> 0.476309).  Saving model ...
Validation loss decreased (0.476309 --> 0.476281).  Saving model ...
Validation loss decreased (0.476281 --> 0.476254).  Saving model ...
epoch 1201, loss 0.4763, train acc 77.91%, f1 0.6614, precision 0.7119, recall 0.6176, auc 0.7417
Validation loss decreased (0.476254 --> 0.476226).  Saving model ...
Validation loss decreased (0.476226 --> 0.476198).  Saving model ...
Validation loss decreased (0.476198 --> 0.476170).  Saving model ...
Validation loss decreased (0.476170 --> 0.476142).  Saving model ...
Validation loss decreased (0.476142 --> 0.476114).  Saving model ...
Validation loss decreased (0.476114 --> 0.476086).  Saving model ...
Validation loss decreased (0.476086 --> 0.476058).  Saving model ...
Validation loss decreased (0.476058 --> 0.476030).  Saving model ...
Validation loss decreased (0.476030 --> 0.476002).  Saving model ...
Validation loss decreased (0.476002 --> 0.475974).  Saving model ...
Validation loss decreased (0.475974 --> 0.475945).  Saving model ...
Validation loss decreased (0.475945 --> 0.475917).  Saving model ...
Validation loss decreased (0.475917 --> 0.475889).  Saving model ...
Validation loss decreased (0.475889 --> 0.475861).  Saving model ...
Validation loss decreased (0.475861 --> 0.475832).  Saving model ...
Validation loss decreased (0.475832 --> 0.475804).  Saving model ...
Validation loss decreased (0.475804 --> 0.475775).  Saving model ...
Validation loss decreased (0.475775 --> 0.475747).  Saving model ...
Validation loss decreased (0.475747 --> 0.475718).  Saving model ...
Validation loss decreased (0.475718 --> 0.475690).  Saving model ...
Validation loss decreased (0.475690 --> 0.475661).  Saving model ...
Validation loss decreased (0.475661 --> 0.475633).  Saving model ...
Validation loss decreased (0.475633 --> 0.475604).  Saving model ...
Validation loss decreased (0.475604 --> 0.475575).  Saving model ...
Validation loss decreased (0.475575 --> 0.475547).  Saving model ...
Validation loss decreased (0.475547 --> 0.475518).  Saving model ...
Validation loss decreased (0.475518 --> 0.475489).  Saving model ...
Validation loss decreased (0.475489 --> 0.475460).  Saving model ...
Validation loss decreased (0.475460 --> 0.475431).  Saving model ...
Validation loss decreased (0.475431 --> 0.475402).  Saving model ...
Validation loss decreased (0.475402 --> 0.475373).  Saving model ...
Validation loss decreased (0.475373 --> 0.475344).  Saving model ...
Validation loss decreased (0.475344 --> 0.475315).  Saving model ...
Validation loss decreased (0.475315 --> 0.475286).  Saving model ...
Validation loss decreased (0.475286 --> 0.475257).  Saving model ...
Validation loss decreased (0.475257 --> 0.475227).  Saving model ...
Validation loss decreased (0.475227 --> 0.475198).  Saving model ...
Validation loss decreased (0.475198 --> 0.475169).  Saving model ...
Validation loss decreased (0.475169 --> 0.475139).  Saving model ...
Validation loss decreased (0.475139 --> 0.475110).  Saving model ...
Validation loss decreased (0.475110 --> 0.475080).  Saving model ...
Validation loss decreased (0.475080 --> 0.475050).  Saving model ...
Validation loss decreased (0.475050 --> 0.475021).  Saving model ...
Validation loss decreased (0.475021 --> 0.474991).  Saving model ...
Validation loss decreased (0.474991 --> 0.474961).  Saving model ...
Validation loss decreased (0.474961 --> 0.474932).  Saving model ...
Validation loss decreased (0.474932 --> 0.474902).  Saving model ...
Validation loss decreased (0.474902 --> 0.474872).  Saving model ...
Validation loss decreased (0.474872 --> 0.474842).  Saving model ...
Validation loss decreased (0.474842 --> 0.474812).  Saving model ...
Validation loss decreased (0.474812 --> 0.474782).  Saving model ...
Validation loss decreased (0.474782 --> 0.474751).  Saving model ...
Validation loss decreased (0.474751 --> 0.474721).  Saving model ...
Validation loss decreased (0.474721 --> 0.474691).  Saving model ...
Validation loss decreased (0.474691 --> 0.474661).  Saving model ...
Validation loss decreased (0.474661 --> 0.474630).  Saving model ...
Validation loss decreased (0.474630 --> 0.474600).  Saving model ...
Validation loss decreased (0.474600 --> 0.474569).  Saving model ...
Validation loss decreased (0.474569 --> 0.474539).  Saving model ...
Validation loss decreased (0.474539 --> 0.474508).  Saving model ...
Validation loss decreased (0.474508 --> 0.474477).  Saving model ...
Validation loss decreased (0.474477 --> 0.474447).  Saving model ...
Validation loss decreased (0.474447 --> 0.474416).  Saving model ...
Validation loss decreased (0.474416 --> 0.474385).  Saving model ...
Validation loss decreased (0.474385 --> 0.474354).  Saving model ...
Validation loss decreased (0.474354 --> 0.474323).  Saving model ...
Validation loss decreased (0.474323 --> 0.474292).  Saving model ...
Validation loss decreased (0.474292 --> 0.474261).  Saving model ...
Validation loss decreased (0.474261 --> 0.474229).  Saving model ...
Validation loss decreased (0.474229 --> 0.474198).  Saving model ...
Validation loss decreased (0.474198 --> 0.474167).  Saving model ...
Validation loss decreased (0.474167 --> 0.474135).  Saving model ...
Validation loss decreased (0.474135 --> 0.474104).  Saving model ...
Validation loss decreased (0.474104 --> 0.474072).  Saving model ...
Validation loss decreased (0.474072 --> 0.474041).  Saving model ...
Validation loss decreased (0.474041 --> 0.474009).  Saving model ...
Validation loss decreased (0.474009 --> 0.473977).  Saving model ...
Validation loss decreased (0.473977 --> 0.473946).  Saving model ...
Validation loss decreased (0.473946 --> 0.473914).  Saving model ...
Validation loss decreased (0.473914 --> 0.473882).  Saving model ...
Validation loss decreased (0.473882 --> 0.473850).  Saving model ...
Validation loss decreased (0.473850 --> 0.473818).  Saving model ...
Validation loss decreased (0.473818 --> 0.473786).  Saving model ...
Validation loss decreased (0.473786 --> 0.473754).  Saving model ...
Validation loss decreased (0.473754 --> 0.473721).  Saving model ...
Validation loss decreased (0.473721 --> 0.473689).  Saving model ...
Validation loss decreased (0.473689 --> 0.473657).  Saving model ...
Validation loss decreased (0.473657 --> 0.473624).  Saving model ...
Validation loss decreased (0.473624 --> 0.473592).  Saving model ...
Validation loss decreased (0.473592 --> 0.473559).  Saving model ...
Validation loss decreased (0.473559 --> 0.473526).  Saving model ...
Validation loss decreased (0.473526 --> 0.473494).  Saving model ...
Validation loss decreased (0.473494 --> 0.473461).  Saving model ...
Validation loss decreased (0.473461 --> 0.473428).  Saving model ...
Validation loss decreased (0.473428 --> 0.473395).  Saving model ...
Validation loss decreased (0.473395 --> 0.473362).  Saving model ...
Validation loss decreased (0.473362 --> 0.473329).  Saving model ...
Validation loss decreased (0.473329 --> 0.473296).  Saving model ...
Validation loss decreased (0.473296 --> 0.473263).  Saving model ...
Validation loss decreased (0.473263 --> 0.473229).  Saving model ...
epoch 1301, loss 0.4732, train acc 78.25%, f1 0.6701, precision 0.7127, recall 0.6324, auc 0.7478
Validation loss decreased (0.473229 --> 0.473196).  Saving model ...
Validation loss decreased (0.473196 --> 0.473163).  Saving model ...
Validation loss decreased (0.473163 --> 0.473129).  Saving model ...
Validation loss decreased (0.473129 --> 0.473096).  Saving model ...
Validation loss decreased (0.473096 --> 0.473062).  Saving model ...
Validation loss decreased (0.473062 --> 0.473028).  Saving model ...
Validation loss decreased (0.473028 --> 0.472994).  Saving model ...
Validation loss decreased (0.472994 --> 0.472961).  Saving model ...
Validation loss decreased (0.472961 --> 0.472927).  Saving model ...
Validation loss decreased (0.472927 --> 0.472893).  Saving model ...
Validation loss decreased (0.472893 --> 0.472859).  Saving model ...
Validation loss decreased (0.472859 --> 0.472825).  Saving model ...
Validation loss decreased (0.472825 --> 0.472790).  Saving model ...
Validation loss decreased (0.472790 --> 0.472756).  Saving model ...
Validation loss decreased (0.472756 --> 0.472722).  Saving model ...
Validation loss decreased (0.472722 --> 0.472688).  Saving model ...
Validation loss decreased (0.472688 --> 0.472653).  Saving model ...
Validation loss decreased (0.472653 --> 0.472619).  Saving model ...
Validation loss decreased (0.472619 --> 0.472584).  Saving model ...
Validation loss decreased (0.472584 --> 0.472549).  Saving model ...
Validation loss decreased (0.472549 --> 0.472515).  Saving model ...
Validation loss decreased (0.472515 --> 0.472480).  Saving model ...
Validation loss decreased (0.472480 --> 0.472445).  Saving model ...
Validation loss decreased (0.472445 --> 0.472410).  Saving model ...
Validation loss decreased (0.472410 --> 0.472375).  Saving model ...
Validation loss decreased (0.472375 --> 0.472340).  Saving model ...
Validation loss decreased (0.472340 --> 0.472305).  Saving model ...
Validation loss decreased (0.472305 --> 0.472270).  Saving model ...
Validation loss decreased (0.472270 --> 0.472234).  Saving model ...
Validation loss decreased (0.472234 --> 0.472199).  Saving model ...
Validation loss decreased (0.472199 --> 0.472164).  Saving model ...
Validation loss decreased (0.472164 --> 0.472128).  Saving model ...
Validation loss decreased (0.472128 --> 0.472093).  Saving model ...
Validation loss decreased (0.472093 --> 0.472057).  Saving model ...
Validation loss decreased (0.472057 --> 0.472022).  Saving model ...
Validation loss decreased (0.472022 --> 0.471986).  Saving model ...
Validation loss decreased (0.471986 --> 0.471950).  Saving model ...
Validation loss decreased (0.471950 --> 0.471914).  Saving model ...
Validation loss decreased (0.471914 --> 0.471879).  Saving model ...
Validation loss decreased (0.471879 --> 0.471843).  Saving model ...
Validation loss decreased (0.471843 --> 0.471807).  Saving model ...
Validation loss decreased (0.471807 --> 0.471771).  Saving model ...
Validation loss decreased (0.471771 --> 0.471734).  Saving model ...
Validation loss decreased (0.471734 --> 0.471698).  Saving model ...
Validation loss decreased (0.471698 --> 0.471662).  Saving model ...
Validation loss decreased (0.471662 --> 0.471626).  Saving model ...
Validation loss decreased (0.471626 --> 0.471590).  Saving model ...
Validation loss decreased (0.471590 --> 0.471553).  Saving model ...
Validation loss decreased (0.471553 --> 0.471517).  Saving model ...
Validation loss decreased (0.471517 --> 0.471480).  Saving model ...
Validation loss decreased (0.471480 --> 0.471444).  Saving model ...
Validation loss decreased (0.471444 --> 0.471407).  Saving model ...
Validation loss decreased (0.471407 --> 0.471370).  Saving model ...
Validation loss decreased (0.471370 --> 0.471334).  Saving model ...
Validation loss decreased (0.471334 --> 0.471297).  Saving model ...
Validation loss decreased (0.471297 --> 0.471260).  Saving model ...
Validation loss decreased (0.471260 --> 0.471223).  Saving model ...
Validation loss decreased (0.471223 --> 0.471186).  Saving model ...
Validation loss decreased (0.471186 --> 0.471149).  Saving model ...
Validation loss decreased (0.471149 --> 0.471112).  Saving model ...
Validation loss decreased (0.471112 --> 0.471075).  Saving model ...
Validation loss decreased (0.471075 --> 0.471038).  Saving model ...
Validation loss decreased (0.471038 --> 0.471001).  Saving model ...
Validation loss decreased (0.471001 --> 0.470964).  Saving model ...
Validation loss decreased (0.470964 --> 0.470927).  Saving model ...
Validation loss decreased (0.470927 --> 0.470889).  Saving model ...
Validation loss decreased (0.470889 --> 0.470852).  Saving model ...
Validation loss decreased (0.470852 --> 0.470815).  Saving model ...
Validation loss decreased (0.470815 --> 0.470777).  Saving model ...
Validation loss decreased (0.470777 --> 0.470740).  Saving model ...
Validation loss decreased (0.470740 --> 0.470702).  Saving model ...
Validation loss decreased (0.470702 --> 0.470665).  Saving model ...
Validation loss decreased (0.470665 --> 0.470627).  Saving model ...
Validation loss decreased (0.470627 --> 0.470590).  Saving model ...
Validation loss decreased (0.470590 --> 0.470552).  Saving model ...
Validation loss decreased (0.470552 --> 0.470514).  Saving model ...
Validation loss decreased (0.470514 --> 0.470477).  Saving model ...
Validation loss decreased (0.470477 --> 0.470439).  Saving model ...
Validation loss decreased (0.470439 --> 0.470401).  Saving model ...
Validation loss decreased (0.470401 --> 0.470363).  Saving model ...
Validation loss decreased (0.470363 --> 0.470325).  Saving model ...
Validation loss decreased (0.470325 --> 0.470287).  Saving model ...
Validation loss decreased (0.470287 --> 0.470249).  Saving model ...
Validation loss decreased (0.470249 --> 0.470211).  Saving model ...
Validation loss decreased (0.470211 --> 0.470173).  Saving model ...
Validation loss decreased (0.470173 --> 0.470135).  Saving model ...
Validation loss decreased (0.470135 --> 0.470097).  Saving model ...
Validation loss decreased (0.470097 --> 0.470059).  Saving model ...
Validation loss decreased (0.470059 --> 0.470021).  Saving model ...
Validation loss decreased (0.470021 --> 0.469983).  Saving model ...
Validation loss decreased (0.469983 --> 0.469945).  Saving model ...
Validation loss decreased (0.469945 --> 0.469907).  Saving model ...
Validation loss decreased (0.469907 --> 0.469868).  Saving model ...
Validation loss decreased (0.469868 --> 0.469830).  Saving model ...
Validation loss decreased (0.469830 --> 0.469792).  Saving model ...
Validation loss decreased (0.469792 --> 0.469753).  Saving model ...
Validation loss decreased (0.469753 --> 0.469715).  Saving model ...
Validation loss decreased (0.469715 --> 0.469677).  Saving model ...
Validation loss decreased (0.469677 --> 0.469638).  Saving model ...
Validation loss decreased (0.469638 --> 0.469600).  Saving model ...
epoch 1401, loss 0.4696, train acc 78.77%, f1 0.6754, precision 0.7247, recall 0.6324, auc 0.7517
Validation loss decreased (0.469600 --> 0.469562).  Saving model ...
Validation loss decreased (0.469562 --> 0.469523).  Saving model ...
Validation loss decreased (0.469523 --> 0.469485).  Saving model ...
Validation loss decreased (0.469485 --> 0.469446).  Saving model ...
Validation loss decreased (0.469446 --> 0.469408).  Saving model ...
Validation loss decreased (0.469408 --> 0.469369).  Saving model ...
Validation loss decreased (0.469369 --> 0.469331).  Saving model ...
Validation loss decreased (0.469331 --> 0.469292).  Saving model ...
Validation loss decreased (0.469292 --> 0.469254).  Saving model ...
Validation loss decreased (0.469254 --> 0.469215).  Saving model ...
Validation loss decreased (0.469215 --> 0.469176).  Saving model ...
Validation loss decreased (0.469176 --> 0.469138).  Saving model ...
Validation loss decreased (0.469138 --> 0.469099).  Saving model ...
Validation loss decreased (0.469099 --> 0.469060).  Saving model ...
Validation loss decreased (0.469060 --> 0.469022).  Saving model ...
Validation loss decreased (0.469022 --> 0.468983).  Saving model ...
Validation loss decreased (0.468983 --> 0.468944).  Saving model ...
Validation loss decreased (0.468944 --> 0.468906).  Saving model ...
Validation loss decreased (0.468906 --> 0.468867).  Saving model ...
Validation loss decreased (0.468867 --> 0.468828).  Saving model ...
Validation loss decreased (0.468828 --> 0.468790).  Saving model ...
Validation loss decreased (0.468790 --> 0.468751).  Saving model ...
Validation loss decreased (0.468751 --> 0.468712).  Saving model ...
Validation loss decreased (0.468712 --> 0.468674).  Saving model ...
Validation loss decreased (0.468674 --> 0.468635).  Saving model ...
Validation loss decreased (0.468635 --> 0.468596).  Saving model ...
Validation loss decreased (0.468596 --> 0.468557).  Saving model ...
Validation loss decreased (0.468557 --> 0.468519).  Saving model ...
Validation loss decreased (0.468519 --> 0.468480).  Saving model ...
Validation loss decreased (0.468480 --> 0.468441).  Saving model ...
Validation loss decreased (0.468441 --> 0.468403).  Saving model ...
Validation loss decreased (0.468403 --> 0.468364).  Saving model ...
Validation loss decreased (0.468364 --> 0.468325).  Saving model ...
Validation loss decreased (0.468325 --> 0.468286).  Saving model ...
Validation loss decreased (0.468286 --> 0.468248).  Saving model ...
Validation loss decreased (0.468248 --> 0.468209).  Saving model ...
Validation loss decreased (0.468209 --> 0.468170).  Saving model ...
Validation loss decreased (0.468170 --> 0.468131).  Saving model ...
Validation loss decreased (0.468131 --> 0.468093).  Saving model ...
Validation loss decreased (0.468093 --> 0.468054).  Saving model ...
Validation loss decreased (0.468054 --> 0.468015).  Saving model ...
Validation loss decreased (0.468015 --> 0.467977).  Saving model ...
Validation loss decreased (0.467977 --> 0.467938).  Saving model ...
Validation loss decreased (0.467938 --> 0.467899).  Saving model ...
Validation loss decreased (0.467899 --> 0.467860).  Saving model ...
Validation loss decreased (0.467860 --> 0.467822).  Saving model ...
Validation loss decreased (0.467822 --> 0.467783).  Saving model ...
Validation loss decreased (0.467783 --> 0.467744).  Saving model ...
Validation loss decreased (0.467744 --> 0.467706).  Saving model ...
Validation loss decreased (0.467706 --> 0.467667).  Saving model ...
Validation loss decreased (0.467667 --> 0.467628).  Saving model ...
Validation loss decreased (0.467628 --> 0.467590).  Saving model ...
Validation loss decreased (0.467590 --> 0.467551).  Saving model ...
Validation loss decreased (0.467551 --> 0.467513).  Saving model ...
Validation loss decreased (0.467513 --> 0.467474).  Saving model ...
Validation loss decreased (0.467474 --> 0.467435).  Saving model ...
Validation loss decreased (0.467435 --> 0.467397).  Saving model ...
Validation loss decreased (0.467397 --> 0.467358).  Saving model ...
Validation loss decreased (0.467358 --> 0.467320).  Saving model ...
Validation loss decreased (0.467320 --> 0.467281).  Saving model ...
Validation loss decreased (0.467281 --> 0.467243).  Saving model ...
Validation loss decreased (0.467243 --> 0.467204).  Saving model ...
Validation loss decreased (0.467204 --> 0.467165).  Saving model ...
Validation loss decreased (0.467165 --> 0.467127).  Saving model ...
Validation loss decreased (0.467127 --> 0.467089).  Saving model ...
Validation loss decreased (0.467089 --> 0.467050).  Saving model ...
Validation loss decreased (0.467050 --> 0.467012).  Saving model ...
Validation loss decreased (0.467012 --> 0.466973).  Saving model ...
Validation loss decreased (0.466973 --> 0.466935).  Saving model ...
Validation loss decreased (0.466935 --> 0.466896).  Saving model ...
Validation loss decreased (0.466896 --> 0.466858).  Saving model ...
Validation loss decreased (0.466858 --> 0.466820).  Saving model ...
Validation loss decreased (0.466820 --> 0.466781).  Saving model ...
Validation loss decreased (0.466781 --> 0.466743).  Saving model ...
Validation loss decreased (0.466743 --> 0.466705).  Saving model ...
Validation loss decreased (0.466705 --> 0.466666).  Saving model ...
Validation loss decreased (0.466666 --> 0.466628).  Saving model ...
Validation loss decreased (0.466628 --> 0.466590).  Saving model ...
Validation loss decreased (0.466590 --> 0.466552).  Saving model ...
Validation loss decreased (0.466552 --> 0.466513).  Saving model ...
Validation loss decreased (0.466513 --> 0.466475).  Saving model ...
Validation loss decreased (0.466475 --> 0.466437).  Saving model ...
Validation loss decreased (0.466437 --> 0.466399).  Saving model ...
Validation loss decreased (0.466399 --> 0.466361).  Saving model ...
Validation loss decreased (0.466361 --> 0.466323).  Saving model ...
Validation loss decreased (0.466323 --> 0.466285).  Saving model ...
Validation loss decreased (0.466285 --> 0.466246).  Saving model ...
Validation loss decreased (0.466246 --> 0.466208).  Saving model ...
Validation loss decreased (0.466208 --> 0.466170).  Saving model ...
Validation loss decreased (0.466170 --> 0.466132).  Saving model ...
Validation loss decreased (0.466132 --> 0.466094).  Saving model ...
Validation loss decreased (0.466094 --> 0.466056).  Saving model ...
Validation loss decreased (0.466056 --> 0.466018).  Saving model ...
Validation loss decreased (0.466018 --> 0.465980).  Saving model ...
Validation loss decreased (0.465980 --> 0.465943).  Saving model ...
Validation loss decreased (0.465943 --> 0.465905).  Saving model ...
Validation loss decreased (0.465905 --> 0.465867).  Saving model ...
Validation loss decreased (0.465867 --> 0.465829).  Saving model ...
Validation loss decreased (0.465829 --> 0.465791).  Saving model ...
Validation loss decreased (0.465791 --> 0.465753).  Saving model ...
epoch 1501, loss 0.4658, train acc 78.25%, f1 0.6631, precision 0.7225, recall 0.6127, auc 0.7432
Validation loss decreased (0.465753 --> 0.465716).  Saving model ...
Validation loss decreased (0.465716 --> 0.465678).  Saving model ...
Validation loss decreased (0.465678 --> 0.465640).  Saving model ...
Validation loss decreased (0.465640 --> 0.465603).  Saving model ...
Validation loss decreased (0.465603 --> 0.465565).  Saving model ...
Validation loss decreased (0.465565 --> 0.465527).  Saving model ...
Validation loss decreased (0.465527 --> 0.465490).  Saving model ...
Validation loss decreased (0.465490 --> 0.465452).  Saving model ...
Validation loss decreased (0.465452 --> 0.465414).  Saving model ...
Validation loss decreased (0.465414 --> 0.465377).  Saving model ...
Validation loss decreased (0.465377 --> 0.465339).  Saving model ...
Validation loss decreased (0.465339 --> 0.465302).  Saving model ...
Validation loss decreased (0.465302 --> 0.465264).  Saving model ...
Validation loss decreased (0.465264 --> 0.465227).  Saving model ...
Validation loss decreased (0.465227 --> 0.465190).  Saving model ...
Validation loss decreased (0.465190 --> 0.465152).  Saving model ...
Validation loss decreased (0.465152 --> 0.465115).  Saving model ...
Validation loss decreased (0.465115 --> 0.465078).  Saving model ...
Validation loss decreased (0.465078 --> 0.465040).  Saving model ...
Validation loss decreased (0.465040 --> 0.465003).  Saving model ...
Validation loss decreased (0.465003 --> 0.464966).  Saving model ...
Validation loss decreased (0.464966 --> 0.464929).  Saving model ...
Validation loss decreased (0.464929 --> 0.464891).  Saving model ...
Validation loss decreased (0.464891 --> 0.464854).  Saving model ...
Validation loss decreased (0.464854 --> 0.464817).  Saving model ...
Validation loss decreased (0.464817 --> 0.464780).  Saving model ...
Validation loss decreased (0.464780 --> 0.464743).  Saving model ...
Validation loss decreased (0.464743 --> 0.464706).  Saving model ...
Validation loss decreased (0.464706 --> 0.464669).  Saving model ...
Validation loss decreased (0.464669 --> 0.464632).  Saving model ...
Validation loss decreased (0.464632 --> 0.464595).  Saving model ...
Validation loss decreased (0.464595 --> 0.464558).  Saving model ...
Validation loss decreased (0.464558 --> 0.464521).  Saving model ...
Validation loss decreased (0.464521 --> 0.464484).  Saving model ...
Validation loss decreased (0.464484 --> 0.464448).  Saving model ...
Validation loss decreased (0.464448 --> 0.464411).  Saving model ...
Validation loss decreased (0.464411 --> 0.464374).  Saving model ...
Validation loss decreased (0.464374 --> 0.464337).  Saving model ...
Validation loss decreased (0.464337 --> 0.464301).  Saving model ...
Validation loss decreased (0.464301 --> 0.464264).  Saving model ...
Validation loss decreased (0.464264 --> 0.464227).  Saving model ...
Validation loss decreased (0.464227 --> 0.464191).  Saving model ...
Validation loss decreased (0.464191 --> 0.464154).  Saving model ...
Validation loss decreased (0.464154 --> 0.464118).  Saving model ...
Validation loss decreased (0.464118 --> 0.464081).  Saving model ...
Validation loss decreased (0.464081 --> 0.464045).  Saving model ...
Validation loss decreased (0.464045 --> 0.464008).  Saving model ...
Validation loss decreased (0.464008 --> 0.463972).  Saving model ...
Validation loss decreased (0.463972 --> 0.463936).  Saving model ...
Validation loss decreased (0.463936 --> 0.463899).  Saving model ...
Validation loss decreased (0.463899 --> 0.463863).  Saving model ...
Validation loss decreased (0.463863 --> 0.463827).  Saving model ...
Validation loss decreased (0.463827 --> 0.463791).  Saving model ...
Validation loss decreased (0.463791 --> 0.463755).  Saving model ...
Validation loss decreased (0.463755 --> 0.463719).  Saving model ...
Validation loss decreased (0.463719 --> 0.463683).  Saving model ...
Validation loss decreased (0.463683 --> 0.463646).  Saving model ...
Validation loss decreased (0.463646 --> 0.463610).  Saving model ...
Validation loss decreased (0.463610 --> 0.463574).  Saving model ...
Validation loss decreased (0.463574 --> 0.463539).  Saving model ...
Validation loss decreased (0.463539 --> 0.463503).  Saving model ...
Validation loss decreased (0.463503 --> 0.463467).  Saving model ...
Validation loss decreased (0.463467 --> 0.463431).  Saving model ...
Validation loss decreased (0.463431 --> 0.463395).  Saving model ...
Validation loss decreased (0.463395 --> 0.463360).  Saving model ...
Validation loss decreased (0.463360 --> 0.463324).  Saving model ...
Validation loss decreased (0.463324 --> 0.463288).  Saving model ...
Validation loss decreased (0.463288 --> 0.463253).  Saving model ...
Validation loss decreased (0.463253 --> 0.463217).  Saving model ...
Validation loss decreased (0.463217 --> 0.463182).  Saving model ...
Validation loss decreased (0.463182 --> 0.463146).  Saving model ...
Validation loss decreased (0.463146 --> 0.463111).  Saving model ...
Validation loss decreased (0.463111 --> 0.463075).  Saving model ...
Validation loss decreased (0.463075 --> 0.463040).  Saving model ...
Validation loss decreased (0.463040 --> 0.463005).  Saving model ...
Validation loss decreased (0.463005 --> 0.462969).  Saving model ...
Validation loss decreased (0.462969 --> 0.462934).  Saving model ...
Validation loss decreased (0.462934 --> 0.462899).  Saving model ...
Validation loss decreased (0.462899 --> 0.462864).  Saving model ...
Validation loss decreased (0.462864 --> 0.462829).  Saving model ...
Validation loss decreased (0.462829 --> 0.462794).  Saving model ...
Validation loss decreased (0.462794 --> 0.462759).  Saving model ...
Validation loss decreased (0.462759 --> 0.462724).  Saving model ...
Validation loss decreased (0.462724 --> 0.462689).  Saving model ...
Validation loss decreased (0.462689 --> 0.462654).  Saving model ...
Validation loss decreased (0.462654 --> 0.462619).  Saving model ...
Validation loss decreased (0.462619 --> 0.462584).  Saving model ...
Validation loss decreased (0.462584 --> 0.462550).  Saving model ...
Validation loss decreased (0.462550 --> 0.462515).  Saving model ...
Validation loss decreased (0.462515 --> 0.462480).  Saving model ...
Validation loss decreased (0.462480 --> 0.462446).  Saving model ...
Validation loss decreased (0.462446 --> 0.462411).  Saving model ...
Validation loss decreased (0.462411 --> 0.462377).  Saving model ...
Validation loss decreased (0.462377 --> 0.462342).  Saving model ...
Validation loss decreased (0.462342 --> 0.462308).  Saving model ...
Validation loss decreased (0.462308 --> 0.462273).  Saving model ...
Validation loss decreased (0.462273 --> 0.462239).  Saving model ...
Validation loss decreased (0.462239 --> 0.462205).  Saving model ...
Validation loss decreased (0.462205 --> 0.462171).  Saving model ...
Validation loss decreased (0.462171 --> 0.462136).  Saving model ...
epoch 1601, loss 0.4621, train acc 77.91%, f1 0.6596, precision 0.7143, recall 0.6127, auc 0.7406
Validation loss decreased (0.462136 --> 0.462102).  Saving model ...
Validation loss decreased (0.462102 --> 0.462068).  Saving model ...
Validation loss decreased (0.462068 --> 0.462034).  Saving model ...
Validation loss decreased (0.462034 --> 0.462000).  Saving model ...
Validation loss decreased (0.462000 --> 0.461966).  Saving model ...
Validation loss decreased (0.461966 --> 0.461932).  Saving model ...
Validation loss decreased (0.461932 --> 0.461898).  Saving model ...
Validation loss decreased (0.461898 --> 0.461865).  Saving model ...
Validation loss decreased (0.461865 --> 0.461831).  Saving model ...
Validation loss decreased (0.461831 --> 0.461797).  Saving model ...
Validation loss decreased (0.461797 --> 0.461764).  Saving model ...
Validation loss decreased (0.461764 --> 0.461730).  Saving model ...
Validation loss decreased (0.461730 --> 0.461696).  Saving model ...
Validation loss decreased (0.461696 --> 0.461663).  Saving model ...
Validation loss decreased (0.461663 --> 0.461629).  Saving model ...
Validation loss decreased (0.461629 --> 0.461596).  Saving model ...
Validation loss decreased (0.461596 --> 0.461563).  Saving model ...
Validation loss decreased (0.461563 --> 0.461529).  Saving model ...
Validation loss decreased (0.461529 --> 0.461496).  Saving model ...
Validation loss decreased (0.461496 --> 0.461463).  Saving model ...
Validation loss decreased (0.461463 --> 0.461430).  Saving model ...
Validation loss decreased (0.461430 --> 0.461397).  Saving model ...
Validation loss decreased (0.461397 --> 0.461364).  Saving model ...
Validation loss decreased (0.461364 --> 0.461331).  Saving model ...
Validation loss decreased (0.461331 --> 0.461298).  Saving model ...
Validation loss decreased (0.461298 --> 0.461265).  Saving model ...
Validation loss decreased (0.461265 --> 0.461232).  Saving model ...
Validation loss decreased (0.461232 --> 0.461199).  Saving model ...
Validation loss decreased (0.461199 --> 0.461166).  Saving model ...
Validation loss decreased (0.461166 --> 0.461134).  Saving model ...
Validation loss decreased (0.461134 --> 0.461101).  Saving model ...
Validation loss decreased (0.461101 --> 0.461068).  Saving model ...
Validation loss decreased (0.461068 --> 0.461036).  Saving model ...
Validation loss decreased (0.461036 --> 0.461003).  Saving model ...
Validation loss decreased (0.461003 --> 0.460971).  Saving model ...
Validation loss decreased (0.460971 --> 0.460938).  Saving model ...
Validation loss decreased (0.460938 --> 0.460906).  Saving model ...
Validation loss decreased (0.460906 --> 0.460874).  Saving model ...
Validation loss decreased (0.460874 --> 0.460841).  Saving model ...
Validation loss decreased (0.460841 --> 0.460809).  Saving model ...
Validation loss decreased (0.460809 --> 0.460777).  Saving model ...
Validation loss decreased (0.460777 --> 0.460745).  Saving model ...
Validation loss decreased (0.460745 --> 0.460713).  Saving model ...
Validation loss decreased (0.460713 --> 0.460681).  Saving model ...
Validation loss decreased (0.460681 --> 0.460649).  Saving model ...
Validation loss decreased (0.460649 --> 0.460617).  Saving model ...
Validation loss decreased (0.460617 --> 0.460585).  Saving model ...
Validation loss decreased (0.460585 --> 0.460554).  Saving model ...
Validation loss decreased (0.460554 --> 0.460522).  Saving model ...
Validation loss decreased (0.460522 --> 0.460490).  Saving model ...
Validation loss decreased (0.460490 --> 0.460458).  Saving model ...
Validation loss decreased (0.460458 --> 0.460427).  Saving model ...
Validation loss decreased (0.460427 --> 0.460395).  Saving model ...
Validation loss decreased (0.460395 --> 0.460364).  Saving model ...
Validation loss decreased (0.460364 --> 0.460332).  Saving model ...
Validation loss decreased (0.460332 --> 0.460301).  Saving model ...
Validation loss decreased (0.460301 --> 0.460270).  Saving model ...
Validation loss decreased (0.460270 --> 0.460238).  Saving model ...
Validation loss decreased (0.460238 --> 0.460207).  Saving model ...
Validation loss decreased (0.460207 --> 0.460176).  Saving model ...
Validation loss decreased (0.460176 --> 0.460145).  Saving model ...
Validation loss decreased (0.460145 --> 0.460114).  Saving model ...
Validation loss decreased (0.460114 --> 0.460083).  Saving model ...
Validation loss decreased (0.460083 --> 0.460052).  Saving model ...
Validation loss decreased (0.460052 --> 0.460021).  Saving model ...
Validation loss decreased (0.460021 --> 0.459990).  Saving model ...
Validation loss decreased (0.459990 --> 0.459959).  Saving model ...
Validation loss decreased (0.459959 --> 0.459928).  Saving model ...
Validation loss decreased (0.459928 --> 0.459897).  Saving model ...
Validation loss decreased (0.459897 --> 0.459867).  Saving model ...
Validation loss decreased (0.459867 --> 0.459836).  Saving model ...
Validation loss decreased (0.459836 --> 0.459806).  Saving model ...
Validation loss decreased (0.459806 --> 0.459775).  Saving model ...
Validation loss decreased (0.459775 --> 0.459744).  Saving model ...
Validation loss decreased (0.459744 --> 0.459714).  Saving model ...
Validation loss decreased (0.459714 --> 0.459684).  Saving model ...
Validation loss decreased (0.459684 --> 0.459653).  Saving model ...
Validation loss decreased (0.459653 --> 0.459623).  Saving model ...
Validation loss decreased (0.459623 --> 0.459593).  Saving model ...
Validation loss decreased (0.459593 --> 0.459563).  Saving model ...
Validation loss decreased (0.459563 --> 0.459533).  Saving model ...
Validation loss decreased (0.459533 --> 0.459502).  Saving model ...
Validation loss decreased (0.459502 --> 0.459472).  Saving model ...
Validation loss decreased (0.459472 --> 0.459442).  Saving model ...
Validation loss decreased (0.459442 --> 0.459412).  Saving model ...
Validation loss decreased (0.459412 --> 0.459383).  Saving model ...
Validation loss decreased (0.459383 --> 0.459353).  Saving model ...
Validation loss decreased (0.459353 --> 0.459323).  Saving model ...
Validation loss decreased (0.459323 --> 0.459293).  Saving model ...
Validation loss decreased (0.459293 --> 0.459263).  Saving model ...
Validation loss decreased (0.459263 --> 0.459234).  Saving model ...
Validation loss decreased (0.459234 --> 0.459204).  Saving model ...
Validation loss decreased (0.459204 --> 0.459175).  Saving model ...
Validation loss decreased (0.459175 --> 0.459145).  Saving model ...
Validation loss decreased (0.459145 --> 0.459116).  Saving model ...
Validation loss decreased (0.459116 --> 0.459086).  Saving model ...
Validation loss decreased (0.459086 --> 0.459057).  Saving model ...
Validation loss decreased (0.459057 --> 0.459028).  Saving model ...
Validation loss decreased (0.459028 --> 0.458998).  Saving model ...
Validation loss decreased (0.458998 --> 0.458969).  Saving model ...
epoch 1701, loss 0.4590, train acc 77.74%, f1 0.6597, precision 0.7079, recall 0.6176, auc 0.7404
Validation loss decreased (0.458969 --> 0.458940).  Saving model ...
Validation loss decreased (0.458940 --> 0.458911).  Saving model ...
Validation loss decreased (0.458911 --> 0.458882).  Saving model ...
Validation loss decreased (0.458882 --> 0.458853).  Saving model ...
Validation loss decreased (0.458853 --> 0.458824).  Saving model ...
Validation loss decreased (0.458824 --> 0.458795).  Saving model ...
Validation loss decreased (0.458795 --> 0.458766).  Saving model ...
Validation loss decreased (0.458766 --> 0.458737).  Saving model ...
Validation loss decreased (0.458737 --> 0.458708).  Saving model ...
Validation loss decreased (0.458708 --> 0.458680).  Saving model ...
Validation loss decreased (0.458680 --> 0.458651).  Saving model ...
Validation loss decreased (0.458651 --> 0.458622).  Saving model ...
Validation loss decreased (0.458622 --> 0.458594).  Saving model ...
Validation loss decreased (0.458594 --> 0.458565).  Saving model ...
Validation loss decreased (0.458565 --> 0.458537).  Saving model ...
Validation loss decreased (0.458537 --> 0.458508).  Saving model ...
Validation loss decreased (0.458508 --> 0.458480).  Saving model ...
Validation loss decreased (0.458480 --> 0.458451).  Saving model ...
Validation loss decreased (0.458451 --> 0.458423).  Saving model ...
Validation loss decreased (0.458423 --> 0.458395).  Saving model ...
Validation loss decreased (0.458395 --> 0.458367).  Saving model ...
Validation loss decreased (0.458367 --> 0.458338).  Saving model ...
Validation loss decreased (0.458338 --> 0.458310).  Saving model ...
Validation loss decreased (0.458310 --> 0.458282).  Saving model ...
Validation loss decreased (0.458282 --> 0.458254).  Saving model ...
Validation loss decreased (0.458254 --> 0.458226).  Saving model ...
Validation loss decreased (0.458226 --> 0.458198).  Saving model ...
Validation loss decreased (0.458198 --> 0.458170).  Saving model ...
Validation loss decreased (0.458170 --> 0.458142).  Saving model ...
Validation loss decreased (0.458142 --> 0.458115).  Saving model ...
Validation loss decreased (0.458115 --> 0.458087).  Saving model ...
Validation loss decreased (0.458087 --> 0.458059).  Saving model ...
Validation loss decreased (0.458059 --> 0.458031).  Saving model ...
Validation loss decreased (0.458031 --> 0.458004).  Saving model ...
Validation loss decreased (0.458004 --> 0.457976).  Saving model ...
Validation loss decreased (0.457976 --> 0.457949).  Saving model ...
Validation loss decreased (0.457949 --> 0.457921).  Saving model ...
Validation loss decreased (0.457921 --> 0.457894).  Saving model ...
Validation loss decreased (0.457894 --> 0.457866).  Saving model ...
Validation loss decreased (0.457866 --> 0.457839).  Saving model ...
Validation loss decreased (0.457839 --> 0.457811).  Saving model ...
Validation loss decreased (0.457811 --> 0.457784).  Saving model ...
Validation loss decreased (0.457784 --> 0.457757).  Saving model ...
Validation loss decreased (0.457757 --> 0.457730).  Saving model ...
Validation loss decreased (0.457730 --> 0.457702).  Saving model ...
Validation loss decreased (0.457702 --> 0.457675).  Saving model ...
Validation loss decreased (0.457675 --> 0.457648).  Saving model ...
Validation loss decreased (0.457648 --> 0.457621).  Saving model ...
Validation loss decreased (0.457621 --> 0.457594).  Saving model ...
Validation loss decreased (0.457594 --> 0.457567).  Saving model ...
Validation loss decreased (0.457567 --> 0.457540).  Saving model ...
Validation loss decreased (0.457540 --> 0.457513).  Saving model ...
Validation loss decreased (0.457513 --> 0.457486).  Saving model ...
Validation loss decreased (0.457486 --> 0.457460).  Saving model ...
Validation loss decreased (0.457460 --> 0.457433).  Saving model ...
Validation loss decreased (0.457433 --> 0.457406).  Saving model ...
Validation loss decreased (0.457406 --> 0.457379).  Saving model ...
Validation loss decreased (0.457379 --> 0.457353).  Saving model ...
Validation loss decreased (0.457353 --> 0.457326).  Saving model ...
Validation loss decreased (0.457326 --> 0.457300).  Saving model ...
Validation loss decreased (0.457300 --> 0.457273).  Saving model ...
Validation loss decreased (0.457273 --> 0.457247).  Saving model ...
Validation loss decreased (0.457247 --> 0.457220).  Saving model ...
Validation loss decreased (0.457220 --> 0.457194).  Saving model ...
Validation loss decreased (0.457194 --> 0.457167).  Saving model ...
Validation loss decreased (0.457167 --> 0.457141).  Saving model ...
Validation loss decreased (0.457141 --> 0.457115).  Saving model ...
Validation loss decreased (0.457115 --> 0.457089).  Saving model ...
Validation loss decreased (0.457089 --> 0.457062).  Saving model ...
Validation loss decreased (0.457062 --> 0.457036).  Saving model ...
Validation loss decreased (0.457036 --> 0.457010).  Saving model ...
Validation loss decreased (0.457010 --> 0.456984).  Saving model ...
Validation loss decreased (0.456984 --> 0.456958).  Saving model ...
Validation loss decreased (0.456958 --> 0.456932).  Saving model ...
Validation loss decreased (0.456932 --> 0.456906).  Saving model ...
Validation loss decreased (0.456906 --> 0.456880).  Saving model ...
Validation loss decreased (0.456880 --> 0.456854).  Saving model ...
Validation loss decreased (0.456854 --> 0.456828).  Saving model ...
Validation loss decreased (0.456828 --> 0.456802).  Saving model ...
Validation loss decreased (0.456802 --> 0.456777).  Saving model ...
Validation loss decreased (0.456777 --> 0.456751).  Saving model ...
Validation loss decreased (0.456751 --> 0.456725).  Saving model ...
Validation loss decreased (0.456725 --> 0.456700).  Saving model ...
Validation loss decreased (0.456700 --> 0.456674).  Saving model ...
Validation loss decreased (0.456674 --> 0.456648).  Saving model ...
Validation loss decreased (0.456648 --> 0.456623).  Saving model ...
Validation loss decreased (0.456623 --> 0.456597).  Saving model ...
Validation loss decreased (0.456597 --> 0.456572).  Saving model ...
Validation loss decreased (0.456572 --> 0.456546).  Saving model ...
Validation loss decreased (0.456546 --> 0.456521).  Saving model ...
Validation loss decreased (0.456521 --> 0.456495).  Saving model ...
Validation loss decreased (0.456495 --> 0.456470).  Saving model ...
Validation loss decreased (0.456470 --> 0.456445).  Saving model ...
Validation loss decreased (0.456445 --> 0.456419).  Saving model ...
Validation loss decreased (0.456419 --> 0.456394).  Saving model ...
Validation loss decreased (0.456394 --> 0.456369).  Saving model ...
Validation loss decreased (0.456369 --> 0.456344).  Saving model ...
Validation loss decreased (0.456344 --> 0.456319).  Saving model ...
Validation loss decreased (0.456319 --> 0.456294).  Saving model ...
Validation loss decreased (0.456294 --> 0.456269).  Saving model ...
epoch 1801, loss 0.4563, train acc 77.74%, f1 0.6615, precision 0.7056, recall 0.6225, auc 0.7415
Validation loss decreased (0.456269 --> 0.456243).  Saving model ...
Validation loss decreased (0.456243 --> 0.456219).  Saving model ...
Validation loss decreased (0.456219 --> 0.456194).  Saving model ...
Validation loss decreased (0.456194 --> 0.456169).  Saving model ...
Validation loss decreased (0.456169 --> 0.456144).  Saving model ...
Validation loss decreased (0.456144 --> 0.456119).  Saving model ...
Validation loss decreased (0.456119 --> 0.456094).  Saving model ...
Validation loss decreased (0.456094 --> 0.456069).  Saving model ...
Validation loss decreased (0.456069 --> 0.456045).  Saving model ...
Validation loss decreased (0.456045 --> 0.456020).  Saving model ...
Validation loss decreased (0.456020 --> 0.455995).  Saving model ...
Validation loss decreased (0.455995 --> 0.455970).  Saving model ...
Validation loss decreased (0.455970 --> 0.455946).  Saving model ...
Validation loss decreased (0.455946 --> 0.455921).  Saving model ...
Validation loss decreased (0.455921 --> 0.455897).  Saving model ...
Validation loss decreased (0.455897 --> 0.455872).  Saving model ...
Validation loss decreased (0.455872 --> 0.455848).  Saving model ...
Validation loss decreased (0.455848 --> 0.455823).  Saving model ...
Validation loss decreased (0.455823 --> 0.455799).  Saving model ...
Validation loss decreased (0.455799 --> 0.455774).  Saving model ...
Validation loss decreased (0.455774 --> 0.455750).  Saving model ...
Validation loss decreased (0.455750 --> 0.455726).  Saving model ...
Validation loss decreased (0.455726 --> 0.455701).  Saving model ...
Validation loss decreased (0.455701 --> 0.455677).  Saving model ...
Validation loss decreased (0.455677 --> 0.455653).  Saving model ...
Validation loss decreased (0.455653 --> 0.455629).  Saving model ...
Validation loss decreased (0.455629 --> 0.455605).  Saving model ...
Validation loss decreased (0.455605 --> 0.455581).  Saving model ...
Validation loss decreased (0.455581 --> 0.455556).  Saving model ...
Validation loss decreased (0.455556 --> 0.455532).  Saving model ...
Validation loss decreased (0.455532 --> 0.455508).  Saving model ...
Validation loss decreased (0.455508 --> 0.455484).  Saving model ...
Validation loss decreased (0.455484 --> 0.455460).  Saving model ...
Validation loss decreased (0.455460 --> 0.455436).  Saving model ...
Validation loss decreased (0.455436 --> 0.455412).  Saving model ...
Validation loss decreased (0.455412 --> 0.455389).  Saving model ...
Validation loss decreased (0.455389 --> 0.455365).  Saving model ...
Validation loss decreased (0.455365 --> 0.455341).  Saving model ...
Validation loss decreased (0.455341 --> 0.455317).  Saving model ...
Validation loss decreased (0.455317 --> 0.455293).  Saving model ...
Validation loss decreased (0.455293 --> 0.455270).  Saving model ...
Validation loss decreased (0.455270 --> 0.455246).  Saving model ...
Validation loss decreased (0.455246 --> 0.455222).  Saving model ...
Validation loss decreased (0.455222 --> 0.455199).  Saving model ...
Validation loss decreased (0.455199 --> 0.455175).  Saving model ...
Validation loss decreased (0.455175 --> 0.455151).  Saving model ...
Validation loss decreased (0.455151 --> 0.455128).  Saving model ...
Validation loss decreased (0.455128 --> 0.455104).  Saving model ...
Validation loss decreased (0.455104 --> 0.455081).  Saving model ...
Validation loss decreased (0.455081 --> 0.455057).  Saving model ...
Validation loss decreased (0.455057 --> 0.455034).  Saving model ...
Validation loss decreased (0.455034 --> 0.455011).  Saving model ...
Validation loss decreased (0.455011 --> 0.454987).  Saving model ...
Validation loss decreased (0.454987 --> 0.454964).  Saving model ...
Validation loss decreased (0.454964 --> 0.454940).  Saving model ...
Validation loss decreased (0.454940 --> 0.454917).  Saving model ...
Validation loss decreased (0.454917 --> 0.454894).  Saving model ...
Validation loss decreased (0.454894 --> 0.454871).  Saving model ...
Validation loss decreased (0.454871 --> 0.454847).  Saving model ...
Validation loss decreased (0.454847 --> 0.454824).  Saving model ...
Validation loss decreased (0.454824 --> 0.454801).  Saving model ...
Validation loss decreased (0.454801 --> 0.454778).  Saving model ...
Validation loss decreased (0.454778 --> 0.454755).  Saving model ...
Validation loss decreased (0.454755 --> 0.454732).  Saving model ...
Validation loss decreased (0.454732 --> 0.454709).  Saving model ...
Validation loss decreased (0.454709 --> 0.454686).  Saving model ...
Validation loss decreased (0.454686 --> 0.454663).  Saving model ...
Validation loss decreased (0.454663 --> 0.454640).  Saving model ...
Validation loss decreased (0.454640 --> 0.454617).  Saving model ...
Validation loss decreased (0.454617 --> 0.454594).  Saving model ...
Validation loss decreased (0.454594 --> 0.454571).  Saving model ...
Validation loss decreased (0.454571 --> 0.454548).  Saving model ...
Validation loss decreased (0.454548 --> 0.454525).  Saving model ...
Validation loss decreased (0.454525 --> 0.454503).  Saving model ...
Validation loss decreased (0.454503 --> 0.454480).  Saving model ...
Validation loss decreased (0.454480 --> 0.454457).  Saving model ...
Validation loss decreased (0.454457 --> 0.454434).  Saving model ...
Validation loss decreased (0.454434 --> 0.454412).  Saving model ...
Validation loss decreased (0.454412 --> 0.454389).  Saving model ...
Validation loss decreased (0.454389 --> 0.454366).  Saving model ...
Validation loss decreased (0.454366 --> 0.454344).  Saving model ...
Validation loss decreased (0.454344 --> 0.454321).  Saving model ...
Validation loss decreased (0.454321 --> 0.454299).  Saving model ...
Validation loss decreased (0.454299 --> 0.454276).  Saving model ...
Validation loss decreased (0.454276 --> 0.454253).  Saving model ...
Validation loss decreased (0.454253 --> 0.454231).  Saving model ...
Validation loss decreased (0.454231 --> 0.454209).  Saving model ...
Validation loss decreased (0.454209 --> 0.454186).  Saving model ...
Validation loss decreased (0.454186 --> 0.454164).  Saving model ...
Validation loss decreased (0.454164 --> 0.454141).  Saving model ...
Validation loss decreased (0.454141 --> 0.454119).  Saving model ...
Validation loss decreased (0.454119 --> 0.454096).  Saving model ...
Validation loss decreased (0.454096 --> 0.454074).  Saving model ...
Validation loss decreased (0.454074 --> 0.454052).  Saving model ...
Validation loss decreased (0.454052 --> 0.454030).  Saving model ...
Validation loss decreased (0.454030 --> 0.454007).  Saving model ...
Validation loss decreased (0.454007 --> 0.453985).  Saving model ...
Validation loss decreased (0.453985 --> 0.453963).  Saving model ...
Validation loss decreased (0.453963 --> 0.453941).  Saving model ...
Validation loss decreased (0.453941 --> 0.453919).  Saving model ...
epoch 1901, loss 0.4539, train acc 77.57%, f1 0.6615, precision 0.6995, recall 0.6275, auc 0.7414
Validation loss decreased (0.453919 --> 0.453896).  Saving model ...
Validation loss decreased (0.453896 --> 0.453874).  Saving model ...
Validation loss decreased (0.453874 --> 0.453852).  Saving model ...
Validation loss decreased (0.453852 --> 0.453830).  Saving model ...
Validation loss decreased (0.453830 --> 0.453808).  Saving model ...
Validation loss decreased (0.453808 --> 0.453786).  Saving model ...
Validation loss decreased (0.453786 --> 0.453764).  Saving model ...
Validation loss decreased (0.453764 --> 0.453742).  Saving model ...
Validation loss decreased (0.453742 --> 0.453720).  Saving model ...
Validation loss decreased (0.453720 --> 0.453698).  Saving model ...
Validation loss decreased (0.453698 --> 0.453676).  Saving model ...
Validation loss decreased (0.453676 --> 0.453654).  Saving model ...
Validation loss decreased (0.453654 --> 0.453632).  Saving model ...
Validation loss decreased (0.453632 --> 0.453611).  Saving model ...
Validation loss decreased (0.453611 --> 0.453589).  Saving model ...
Validation loss decreased (0.453589 --> 0.453567).  Saving model ...
Validation loss decreased (0.453567 --> 0.453545).  Saving model ...
Validation loss decreased (0.453545 --> 0.453523).  Saving model ...
Validation loss decreased (0.453523 --> 0.453502).  Saving model ...
Validation loss decreased (0.453502 --> 0.453480).  Saving model ...
Validation loss decreased (0.453480 --> 0.453458).  Saving model ...
Validation loss decreased (0.453458 --> 0.453436).  Saving model ...
Validation loss decreased (0.453436 --> 0.453415).  Saving model ...
Validation loss decreased (0.453415 --> 0.453393).  Saving model ...
Validation loss decreased (0.453393 --> 0.453372).  Saving model ...
Validation loss decreased (0.453372 --> 0.453350).  Saving model ...
Validation loss decreased (0.453350 --> 0.453328).  Saving model ...
Validation loss decreased (0.453328 --> 0.453307).  Saving model ...
Validation loss decreased (0.453307 --> 0.453285).  Saving model ...
Validation loss decreased (0.453285 --> 0.453264).  Saving model ...
Validation loss decreased (0.453264 --> 0.453242).  Saving model ...
Validation loss decreased (0.453242 --> 0.453221).  Saving model ...
Validation loss decreased (0.453221 --> 0.453199).  Saving model ...
Validation loss decreased (0.453199 --> 0.453178).  Saving model ...
Validation loss decreased (0.453178 --> 0.453156).  Saving model ...
Validation loss decreased (0.453156 --> 0.453135).  Saving model ...
Validation loss decreased (0.453135 --> 0.453113).  Saving model ...
Validation loss decreased (0.453113 --> 0.453092).  Saving model ...
Validation loss decreased (0.453092 --> 0.453071).  Saving model ...
Validation loss decreased (0.453071 --> 0.453049).  Saving model ...
Validation loss decreased (0.453049 --> 0.453028).  Saving model ...
Validation loss decreased (0.453028 --> 0.453007).  Saving model ...
Validation loss decreased (0.453007 --> 0.452985).  Saving model ...
Validation loss decreased (0.452985 --> 0.452964).  Saving model ...
Validation loss decreased (0.452964 --> 0.452943).  Saving model ...
Validation loss decreased (0.452943 --> 0.452922).  Saving model ...
Validation loss decreased (0.452922 --> 0.452900).  Saving model ...
Validation loss decreased (0.452900 --> 0.452879).  Saving model ...
Validation loss decreased (0.452879 --> 0.452858).  Saving model ...
Validation loss decreased (0.452858 --> 0.452837).  Saving model ...
Validation loss decreased (0.452837 --> 0.452816).  Saving model ...
Validation loss decreased (0.452816 --> 0.452794).  Saving model ...
Validation loss decreased (0.452794 --> 0.452773).  Saving model ...
Validation loss decreased (0.452773 --> 0.452752).  Saving model ...
Validation loss decreased (0.452752 --> 0.452731).  Saving model ...
Validation loss decreased (0.452731 --> 0.452710).  Saving model ...
Validation loss decreased (0.452710 --> 0.452689).  Saving model ...
Validation loss decreased (0.452689 --> 0.452668).  Saving model ...
Validation loss decreased (0.452668 --> 0.452647).  Saving model ...
Validation loss decreased (0.452647 --> 0.452626).  Saving model ...
Validation loss decreased (0.452626 --> 0.452605).  Saving model ...
Validation loss decreased (0.452605 --> 0.452584).  Saving model ...
Validation loss decreased (0.452584 --> 0.452563).  Saving model ...
Validation loss decreased (0.452563 --> 0.452542).  Saving model ...
Validation loss decreased (0.452542 --> 0.452521).  Saving model ...
Validation loss decreased (0.452521 --> 0.452500).  Saving model ...
Validation loss decreased (0.452500 --> 0.452479).  Saving model ...
Validation loss decreased (0.452479 --> 0.452458).  Saving model ...
Validation loss decreased (0.452458 --> 0.452437).  Saving model ...
Validation loss decreased (0.452437 --> 0.452416).  Saving model ...
Validation loss decreased (0.452416 --> 0.452395).  Saving model ...
Validation loss decreased (0.452395 --> 0.452374).  Saving model ...
Validation loss decreased (0.452374 --> 0.452353).  Saving model ...
Validation loss decreased (0.452353 --> 0.452332).  Saving model ...
Validation loss decreased (0.452332 --> 0.452312).  Saving model ...
Validation loss decreased (0.452312 --> 0.452291).  Saving model ...
Validation loss decreased (0.452291 --> 0.452270).  Saving model ...
Validation loss decreased (0.452270 --> 0.452249).  Saving model ...
Validation loss decreased (0.452249 --> 0.452228).  Saving model ...
Validation loss decreased (0.452228 --> 0.452207).  Saving model ...
Validation loss decreased (0.452207 --> 0.452187).  Saving model ...
Validation loss decreased (0.452187 --> 0.452166).  Saving model ...
Validation loss decreased (0.452166 --> 0.452145).  Saving model ...
Validation loss decreased (0.452145 --> 0.452124).  Saving model ...
Validation loss decreased (0.452124 --> 0.452104).  Saving model ...
Validation loss decreased (0.452104 --> 0.452083).  Saving model ...
Validation loss decreased (0.452083 --> 0.452062).  Saving model ...
Validation loss decreased (0.452062 --> 0.452041).  Saving model ...
Validation loss decreased (0.452041 --> 0.452021).  Saving model ...
Validation loss decreased (0.452021 --> 0.452000).  Saving model ...
Validation loss decreased (0.452000 --> 0.451979).  Saving model ...
Validation loss decreased (0.451979 --> 0.451959).  Saving model ...
Validation loss decreased (0.451959 --> 0.451938).  Saving model ...
Validation loss decreased (0.451938 --> 0.451917).  Saving model ...
Validation loss decreased (0.451917 --> 0.451897).  Saving model ...
Validation loss decreased (0.451897 --> 0.451876).  Saving model ...
Validation loss decreased (0.451876 --> 0.451855).  Saving model ...
Validation loss decreased (0.451855 --> 0.451835).  Saving model ...
Validation loss decreased (0.451835 --> 0.451814).  Saving model ...
Validation loss decreased (0.451814 --> 0.451793).  Saving model ...
epoch 2001, loss 0.4518, train acc 77.57%, f1 0.6615, precision 0.6995, recall 0.6275, auc 0.7414
Validation loss decreased (0.451793 --> 0.451773).  Saving model ...
Validation loss decreased (0.451773 --> 0.451752).  Saving model ...
Validation loss decreased (0.451752 --> 0.451732).  Saving model ...
Validation loss decreased (0.451732 --> 0.451711).  Saving model ...
Validation loss decreased (0.451711 --> 0.451690).  Saving model ...
Validation loss decreased (0.451690 --> 0.451670).  Saving model ...
Validation loss decreased (0.451670 --> 0.451649).  Saving model ...
Validation loss decreased (0.451649 --> 0.451629).  Saving model ...
Validation loss decreased (0.451629 --> 0.451608).  Saving model ...
Validation loss decreased (0.451608 --> 0.451587).  Saving model ...
Validation loss decreased (0.451587 --> 0.451567).  Saving model ...
Validation loss decreased (0.451567 --> 0.451546).  Saving model ...
Validation loss decreased (0.451546 --> 0.451526).  Saving model ...
Validation loss decreased (0.451526 --> 0.451505).  Saving model ...
Validation loss decreased (0.451505 --> 0.451485).  Saving model ...
Validation loss decreased (0.451485 --> 0.451464).  Saving model ...
Validation loss decreased (0.451464 --> 0.451444).  Saving model ...
Validation loss decreased (0.451444 --> 0.451423).  Saving model ...
Validation loss decreased (0.451423 --> 0.451402).  Saving model ...
Validation loss decreased (0.451402 --> 0.451382).  Saving model ...
Validation loss decreased (0.451382 --> 0.451361).  Saving model ...
Validation loss decreased (0.451361 --> 0.451341).  Saving model ...
Validation loss decreased (0.451341 --> 0.451320).  Saving model ...
Validation loss decreased (0.451320 --> 0.451300).  Saving model ...
Validation loss decreased (0.451300 --> 0.451279).  Saving model ...
Validation loss decreased (0.451279 --> 0.451259).  Saving model ...
Validation loss decreased (0.451259 --> 0.451238).  Saving model ...
Validation loss decreased (0.451238 --> 0.451218).  Saving model ...
Validation loss decreased (0.451218 --> 0.451197).  Saving model ...
Validation loss decreased (0.451197 --> 0.451177).  Saving model ...
Validation loss decreased (0.451177 --> 0.451156).  Saving model ...
Validation loss decreased (0.451156 --> 0.451136).  Saving model ...
Validation loss decreased (0.451136 --> 0.451115).  Saving model ...
Validation loss decreased (0.451115 --> 0.451094).  Saving model ...
Validation loss decreased (0.451094 --> 0.451074).  Saving model ...
Validation loss decreased (0.451074 --> 0.451053).  Saving model ...
Validation loss decreased (0.451053 --> 0.451033).  Saving model ...
Validation loss decreased (0.451033 --> 0.451012).  Saving model ...
Validation loss decreased (0.451012 --> 0.450992).  Saving model ...
Validation loss decreased (0.450992 --> 0.450971).  Saving model ...
Validation loss decreased (0.450971 --> 0.450951).  Saving model ...
Validation loss decreased (0.450951 --> 0.450930).  Saving model ...
Validation loss decreased (0.450930 --> 0.450909).  Saving model ...
Validation loss decreased (0.450909 --> 0.450889).  Saving model ...
Validation loss decreased (0.450889 --> 0.450868).  Saving model ...
Validation loss decreased (0.450868 --> 0.450848).  Saving model ...
Validation loss decreased (0.450848 --> 0.450827).  Saving model ...
Validation loss decreased (0.450827 --> 0.450807).  Saving model ...
Validation loss decreased (0.450807 --> 0.450786).  Saving model ...
Validation loss decreased (0.450786 --> 0.450765).  Saving model ...
Validation loss decreased (0.450765 --> 0.450745).  Saving model ...
Validation loss decreased (0.450745 --> 0.450724).  Saving model ...
Validation loss decreased (0.450724 --> 0.450704).  Saving model ...
Validation loss decreased (0.450704 --> 0.450683).  Saving model ...
Validation loss decreased (0.450683 --> 0.450662).  Saving model ...
Validation loss decreased (0.450662 --> 0.450642).  Saving model ...
Validation loss decreased (0.450642 --> 0.450621).  Saving model ...
Validation loss decreased (0.450621 --> 0.450600).  Saving model ...
Validation loss decreased (0.450600 --> 0.450580).  Saving model ...
Validation loss decreased (0.450580 --> 0.450559).  Saving model ...
Validation loss decreased (0.450559 --> 0.450538).  Saving model ...
Validation loss decreased (0.450538 --> 0.450518).  Saving model ...
Validation loss decreased (0.450518 --> 0.450497).  Saving model ...
Validation loss decreased (0.450497 --> 0.450476).  Saving model ...
Validation loss decreased (0.450476 --> 0.450456).  Saving model ...
Validation loss decreased (0.450456 --> 0.450435).  Saving model ...
Validation loss decreased (0.450435 --> 0.450414).  Saving model ...
Validation loss decreased (0.450414 --> 0.450393).  Saving model ...
Validation loss decreased (0.450393 --> 0.450373).  Saving model ...
Validation loss decreased (0.450373 --> 0.450352).  Saving model ...
Validation loss decreased (0.450352 --> 0.450331).  Saving model ...
Validation loss decreased (0.450331 --> 0.450310).  Saving model ...
Validation loss decreased (0.450310 --> 0.450290).  Saving model ...
Validation loss decreased (0.450290 --> 0.450269).  Saving model ...
Validation loss decreased (0.450269 --> 0.450248).  Saving model ...
Validation loss decreased (0.450248 --> 0.450227).  Saving model ...
Validation loss decreased (0.450227 --> 0.450206).  Saving model ...
Validation loss decreased (0.450206 --> 0.450185).  Saving model ...
Validation loss decreased (0.450185 --> 0.450165).  Saving model ...
Validation loss decreased (0.450165 --> 0.450144).  Saving model ...
Validation loss decreased (0.450144 --> 0.450123).  Saving model ...
Validation loss decreased (0.450123 --> 0.450102).  Saving model ...
Validation loss decreased (0.450102 --> 0.450081).  Saving model ...
Validation loss decreased (0.450081 --> 0.450060).  Saving model ...
Validation loss decreased (0.450060 --> 0.450039).  Saving model ...
Validation loss decreased (0.450039 --> 0.450018).  Saving model ...
Validation loss decreased (0.450018 --> 0.449997).  Saving model ...
Validation loss decreased (0.449997 --> 0.449976).  Saving model ...
Validation loss decreased (0.449976 --> 0.449955).  Saving model ...
Validation loss decreased (0.449955 --> 0.449934).  Saving model ...
Validation loss decreased (0.449934 --> 0.449913).  Saving model ...
Validation loss decreased (0.449913 --> 0.449892).  Saving model ...
Validation loss decreased (0.449892 --> 0.449871).  Saving model ...
Validation loss decreased (0.449871 --> 0.449850).  Saving model ...
Validation loss decreased (0.449850 --> 0.449829).  Saving model ...
Validation loss decreased (0.449829 --> 0.449807).  Saving model ...
Validation loss decreased (0.449807 --> 0.449786).  Saving model ...
Validation loss decreased (0.449786 --> 0.449765).  Saving model ...
Validation loss decreased (0.449765 --> 0.449744).  Saving model ...
Validation loss decreased (0.449744 --> 0.449723).  Saving model ...
epoch 2101, loss 0.4497, train acc 77.91%, f1 0.6632, precision 0.7095, recall 0.6225, auc 0.7429
Validation loss decreased (0.449723 --> 0.449701).  Saving model ...
Validation loss decreased (0.449701 --> 0.449680).  Saving model ...
Validation loss decreased (0.449680 --> 0.449659).  Saving model ...
Validation loss decreased (0.449659 --> 0.449637).  Saving model ...
Validation loss decreased (0.449637 --> 0.449616).  Saving model ...
Validation loss decreased (0.449616 --> 0.449595).  Saving model ...
Validation loss decreased (0.449595 --> 0.449573).  Saving model ...
Validation loss decreased (0.449573 --> 0.449552).  Saving model ...
Validation loss decreased (0.449552 --> 0.449531).  Saving model ...
Validation loss decreased (0.449531 --> 0.449509).  Saving model ...
Validation loss decreased (0.449509 --> 0.449488).  Saving model ...
Validation loss decreased (0.449488 --> 0.449466).  Saving model ...
Validation loss decreased (0.449466 --> 0.449445).  Saving model ...
Validation loss decreased (0.449445 --> 0.449423).  Saving model ...
Validation loss decreased (0.449423 --> 0.449401).  Saving model ...
Validation loss decreased (0.449401 --> 0.449380).  Saving model ...
Validation loss decreased (0.449380 --> 0.449358).  Saving model ...
Validation loss decreased (0.449358 --> 0.449337).  Saving model ...
Validation loss decreased (0.449337 --> 0.449315).  Saving model ...
Validation loss decreased (0.449315 --> 0.449293).  Saving model ...
Validation loss decreased (0.449293 --> 0.449271).  Saving model ...
Validation loss decreased (0.449271 --> 0.449250).  Saving model ...
Validation loss decreased (0.449250 --> 0.449228).  Saving model ...
Validation loss decreased (0.449228 --> 0.449206).  Saving model ...
Validation loss decreased (0.449206 --> 0.449184).  Saving model ...
Validation loss decreased (0.449184 --> 0.449162).  Saving model ...
Validation loss decreased (0.449162 --> 0.449140).  Saving model ...
Validation loss decreased (0.449140 --> 0.449118).  Saving model ...
Validation loss decreased (0.449118 --> 0.449096).  Saving model ...
Validation loss decreased (0.449096 --> 0.449074).  Saving model ...
Validation loss decreased (0.449074 --> 0.449052).  Saving model ...
Validation loss decreased (0.449052 --> 0.449030).  Saving model ...
Validation loss decreased (0.449030 --> 0.449008).  Saving model ...
Validation loss decreased (0.449008 --> 0.448986).  Saving model ...
Validation loss decreased (0.448986 --> 0.448964).  Saving model ...
Validation loss decreased (0.448964 --> 0.448942).  Saving model ...
Validation loss decreased (0.448942 --> 0.448920).  Saving model ...
Validation loss decreased (0.448920 --> 0.448897).  Saving model ...
Validation loss decreased (0.448897 --> 0.448875).  Saving model ...
Validation loss decreased (0.448875 --> 0.448853).  Saving model ...
Validation loss decreased (0.448853 --> 0.448830).  Saving model ...
Validation loss decreased (0.448830 --> 0.448808).  Saving model ...
Validation loss decreased (0.448808 --> 0.448785).  Saving model ...
Validation loss decreased (0.448785 --> 0.448763).  Saving model ...
Validation loss decreased (0.448763 --> 0.448741).  Saving model ...
Validation loss decreased (0.448741 --> 0.448718).  Saving model ...
Validation loss decreased (0.448718 --> 0.448695).  Saving model ...
Validation loss decreased (0.448695 --> 0.448673).  Saving model ...
Validation loss decreased (0.448673 --> 0.448650).  Saving model ...
Validation loss decreased (0.448650 --> 0.448627).  Saving model ...
Validation loss decreased (0.448627 --> 0.448605).  Saving model ...
Validation loss decreased (0.448605 --> 0.448582).  Saving model ...
Validation loss decreased (0.448582 --> 0.448559).  Saving model ...
Validation loss decreased (0.448559 --> 0.448536).  Saving model ...
Validation loss decreased (0.448536 --> 0.448513).  Saving model ...
Validation loss decreased (0.448513 --> 0.448490).  Saving model ...
Validation loss decreased (0.448490 --> 0.448467).  Saving model ...
Validation loss decreased (0.448467 --> 0.448444).  Saving model ...
Validation loss decreased (0.448444 --> 0.448421).  Saving model ...
Validation loss decreased (0.448421 --> 0.448398).  Saving model ...
Validation loss decreased (0.448398 --> 0.448375).  Saving model ...
Validation loss decreased (0.448375 --> 0.448352).  Saving model ...
Validation loss decreased (0.448352 --> 0.448329).  Saving model ...
Validation loss decreased (0.448329 --> 0.448305).  Saving model ...
Validation loss decreased (0.448305 --> 0.448282).  Saving model ...
Validation loss decreased (0.448282 --> 0.448258).  Saving model ...
Validation loss decreased (0.448258 --> 0.448235).  Saving model ...
Validation loss decreased (0.448235 --> 0.448212).  Saving model ...
Validation loss decreased (0.448212 --> 0.448188).  Saving model ...
Validation loss decreased (0.448188 --> 0.448164).  Saving model ...
Validation loss decreased (0.448164 --> 0.448141).  Saving model ...
Validation loss decreased (0.448141 --> 0.448117).  Saving model ...
Validation loss decreased (0.448117 --> 0.448093).  Saving model ...
Validation loss decreased (0.448093 --> 0.448070).  Saving model ...
Validation loss decreased (0.448070 --> 0.448046).  Saving model ...
Validation loss decreased (0.448046 --> 0.448022).  Saving model ...
Validation loss decreased (0.448022 --> 0.447998).  Saving model ...
Validation loss decreased (0.447998 --> 0.447974).  Saving model ...
Validation loss decreased (0.447974 --> 0.447950).  Saving model ...
Validation loss decreased (0.447950 --> 0.447926).  Saving model ...
Validation loss decreased (0.447926 --> 0.447902).  Saving model ...
Validation loss decreased (0.447902 --> 0.447878).  Saving model ...
Validation loss decreased (0.447878 --> 0.447853).  Saving model ...
Validation loss decreased (0.447853 --> 0.447829).  Saving model ...
Validation loss decreased (0.447829 --> 0.447805).  Saving model ...
Validation loss decreased (0.447805 --> 0.447780).  Saving model ...
Validation loss decreased (0.447780 --> 0.447756).  Saving model ...
Validation loss decreased (0.447756 --> 0.447731).  Saving model ...
Validation loss decreased (0.447731 --> 0.447707).  Saving model ...
Validation loss decreased (0.447707 --> 0.447682).  Saving model ...
Validation loss decreased (0.447682 --> 0.447657).  Saving model ...
Validation loss decreased (0.447657 --> 0.447633).  Saving model ...
Validation loss decreased (0.447633 --> 0.447608).  Saving model ...
Validation loss decreased (0.447608 --> 0.447583).  Saving model ...
Validation loss decreased (0.447583 --> 0.447558).  Saving model ...
Validation loss decreased (0.447558 --> 0.447533).  Saving model ...
Validation loss decreased (0.447533 --> 0.447508).  Saving model ...
Validation loss decreased (0.447508 --> 0.447483).  Saving model ...
Validation loss decreased (0.447483 --> 0.447458).  Saving model ...
Validation loss decreased (0.447458 --> 0.447432).  Saving model ...
epoch 2201, loss 0.4474, train acc 77.74%, f1 0.6597, precision 0.7079, recall 0.6176, auc 0.7404
Validation loss decreased (0.447432 --> 0.447407).  Saving model ...
Validation loss decreased (0.447407 --> 0.447382).  Saving model ...
Validation loss decreased (0.447382 --> 0.447356).  Saving model ...
Validation loss decreased (0.447356 --> 0.447331).  Saving model ...
Validation loss decreased (0.447331 --> 0.447305).  Saving model ...
Validation loss decreased (0.447305 --> 0.447280).  Saving model ...
Validation loss decreased (0.447280 --> 0.447254).  Saving model ...
Validation loss decreased (0.447254 --> 0.447228).  Saving model ...
Validation loss decreased (0.447228 --> 0.447202).  Saving model ...
Validation loss decreased (0.447202 --> 0.447176).  Saving model ...
Validation loss decreased (0.447176 --> 0.447150).  Saving model ...
Validation loss decreased (0.447150 --> 0.447124).  Saving model ...
Validation loss decreased (0.447124 --> 0.447098).  Saving model ...
Validation loss decreased (0.447098 --> 0.447072).  Saving model ...
Validation loss decreased (0.447072 --> 0.447046).  Saving model ...
Validation loss decreased (0.447046 --> 0.447019).  Saving model ...
Validation loss decreased (0.447019 --> 0.446993).  Saving model ...
Validation loss decreased (0.446993 --> 0.446966).  Saving model ...
Validation loss decreased (0.446966 --> 0.446940).  Saving model ...
Validation loss decreased (0.446940 --> 0.446913).  Saving model ...
Validation loss decreased (0.446913 --> 0.446887).  Saving model ...
Validation loss decreased (0.446887 --> 0.446860).  Saving model ...
Validation loss decreased (0.446860 --> 0.446833).  Saving model ...
Validation loss decreased (0.446833 --> 0.446806).  Saving model ...
Validation loss decreased (0.446806 --> 0.446779).  Saving model ...
Validation loss decreased (0.446779 --> 0.446752).  Saving model ...
Validation loss decreased (0.446752 --> 0.446725).  Saving model ...
Validation loss decreased (0.446725 --> 0.446698).  Saving model ...
Validation loss decreased (0.446698 --> 0.446670).  Saving model ...
Validation loss decreased (0.446670 --> 0.446643).  Saving model ...
Validation loss decreased (0.446643 --> 0.446615).  Saving model ...
Validation loss decreased (0.446615 --> 0.446588).  Saving model ...
Validation loss decreased (0.446588 --> 0.446560).  Saving model ...
Validation loss decreased (0.446560 --> 0.446533).  Saving model ...
Validation loss decreased (0.446533 --> 0.446505).  Saving model ...
Validation loss decreased (0.446505 --> 0.446477).  Saving model ...
Validation loss decreased (0.446477 --> 0.446449).  Saving model ...
Validation loss decreased (0.446449 --> 0.446421).  Saving model ...
Validation loss decreased (0.446421 --> 0.446393).  Saving model ...
Validation loss decreased (0.446393 --> 0.446365).  Saving model ...
Validation loss decreased (0.446365 --> 0.446337).  Saving model ...
Validation loss decreased (0.446337 --> 0.446308).  Saving model ...
Validation loss decreased (0.446308 --> 0.446280).  Saving model ...
Validation loss decreased (0.446280 --> 0.446251).  Saving model ...
Validation loss decreased (0.446251 --> 0.446223).  Saving model ...
Validation loss decreased (0.446223 --> 0.446194).  Saving model ...
Validation loss decreased (0.446194 --> 0.446166).  Saving model ...
Validation loss decreased (0.446166 --> 0.446137).  Saving model ...
Validation loss decreased (0.446137 --> 0.446108).  Saving model ...
Validation loss decreased (0.446108 --> 0.446079).  Saving model ...
Validation loss decreased (0.446079 --> 0.446050).  Saving model ...
Validation loss decreased (0.446050 --> 0.446021).  Saving model ...
Validation loss decreased (0.446021 --> 0.445992).  Saving model ...
Validation loss decreased (0.445992 --> 0.445963).  Saving model ...
Validation loss decreased (0.445963 --> 0.445933).  Saving model ...
Validation loss decreased (0.445933 --> 0.445904).  Saving model ...
Validation loss decreased (0.445904 --> 0.445875).  Saving model ...
Validation loss decreased (0.445875 --> 0.445845).  Saving model ...
Validation loss decreased (0.445845 --> 0.445816).  Saving model ...
Validation loss decreased (0.445816 --> 0.445786).  Saving model ...
Validation loss decreased (0.445786 --> 0.445756).  Saving model ...
Validation loss decreased (0.445756 --> 0.445726).  Saving model ...
Validation loss decreased (0.445726 --> 0.445697).  Saving model ...
Validation loss decreased (0.445697 --> 0.445667).  Saving model ...
Validation loss decreased (0.445667 --> 0.445637).  Saving model ...
Validation loss decreased (0.445637 --> 0.445607).  Saving model ...
Validation loss decreased (0.445607 --> 0.445577).  Saving model ...
Validation loss decreased (0.445577 --> 0.445547).  Saving model ...
Validation loss decreased (0.445547 --> 0.445516).  Saving model ...
Validation loss decreased (0.445516 --> 0.445486).  Saving model ...
Validation loss decreased (0.445486 --> 0.445456).  Saving model ...
Validation loss decreased (0.445456 --> 0.445425).  Saving model ...
Validation loss decreased (0.445425 --> 0.445395).  Saving model ...
Validation loss decreased (0.445395 --> 0.445364).  Saving model ...
Validation loss decreased (0.445364 --> 0.445334).  Saving model ...
Validation loss decreased (0.445334 --> 0.445303).  Saving model ...
Validation loss decreased (0.445303 --> 0.445272).  Saving model ...
Validation loss decreased (0.445272 --> 0.445242).  Saving model ...
Validation loss decreased (0.445242 --> 0.445211).  Saving model ...
Validation loss decreased (0.445211 --> 0.445180).  Saving model ...
Validation loss decreased (0.445180 --> 0.445149).  Saving model ...
Validation loss decreased (0.445149 --> 0.445118).  Saving model ...
Validation loss decreased (0.445118 --> 0.445087).  Saving model ...
Validation loss decreased (0.445087 --> 0.445056).  Saving model ...
Validation loss decreased (0.445056 --> 0.445025).  Saving model ...
Validation loss decreased (0.445025 --> 0.444994).  Saving model ...
Validation loss decreased (0.444994 --> 0.444963).  Saving model ...
Validation loss decreased (0.444963 --> 0.444931).  Saving model ...
Validation loss decreased (0.444931 --> 0.444900).  Saving model ...
Validation loss decreased (0.444900 --> 0.444869).  Saving model ...
Validation loss decreased (0.444869 --> 0.444837).  Saving model ...
Validation loss decreased (0.444837 --> 0.444806).  Saving model ...
Validation loss decreased (0.444806 --> 0.444774).  Saving model ...
Validation loss decreased (0.444774 --> 0.444743).  Saving model ...
Validation loss decreased (0.444743 --> 0.444711).  Saving model ...
Validation loss decreased (0.444711 --> 0.444680).  Saving model ...
Validation loss decreased (0.444680 --> 0.444648).  Saving model ...
Validation loss decreased (0.444648 --> 0.444616).  Saving model ...
Validation loss decreased (0.444616 --> 0.444585).  Saving model ...
Validation loss decreased (0.444585 --> 0.444553).  Saving model ...
epoch 2301, loss 0.4446, train acc 77.91%, f1 0.6614, precision 0.7119, recall 0.6176, auc 0.7417
Validation loss decreased (0.444553 --> 0.444521).  Saving model ...
Validation loss decreased (0.444521 --> 0.444489).  Saving model ...
Validation loss decreased (0.444489 --> 0.444457).  Saving model ...
Validation loss decreased (0.444457 --> 0.444426).  Saving model ...
Validation loss decreased (0.444426 --> 0.444394).  Saving model ...
Validation loss decreased (0.444394 --> 0.444362).  Saving model ...
Validation loss decreased (0.444362 --> 0.444330).  Saving model ...
Validation loss decreased (0.444330 --> 0.444298).  Saving model ...
Validation loss decreased (0.444298 --> 0.444265).  Saving model ...
Validation loss decreased (0.444265 --> 0.444233).  Saving model ...
Validation loss decreased (0.444233 --> 0.444201).  Saving model ...
Validation loss decreased (0.444201 --> 0.444169).  Saving model ...
Validation loss decreased (0.444169 --> 0.444137).  Saving model ...
Validation loss decreased (0.444137 --> 0.444105).  Saving model ...
Validation loss decreased (0.444105 --> 0.444073).  Saving model ...
Validation loss decreased (0.444073 --> 0.444040).  Saving model ...
Validation loss decreased (0.444040 --> 0.444008).  Saving model ...
Validation loss decreased (0.444008 --> 0.443976).  Saving model ...
Validation loss decreased (0.443976 --> 0.443943).  Saving model ...
Validation loss decreased (0.443943 --> 0.443911).  Saving model ...
Validation loss decreased (0.443911 --> 0.443878).  Saving model ...
Validation loss decreased (0.443878 --> 0.443846).  Saving model ...
Validation loss decreased (0.443846 --> 0.443813).  Saving model ...
Validation loss decreased (0.443813 --> 0.443781).  Saving model ...
Validation loss decreased (0.443781 --> 0.443748).  Saving model ...
Validation loss decreased (0.443748 --> 0.443716).  Saving model ...
Validation loss decreased (0.443716 --> 0.443683).  Saving model ...
Validation loss decreased (0.443683 --> 0.443651).  Saving model ...
Validation loss decreased (0.443651 --> 0.443618).  Saving model ...
Validation loss decreased (0.443618 --> 0.443586).  Saving model ...
Validation loss decreased (0.443586 --> 0.443553).  Saving model ...
Validation loss decreased (0.443553 --> 0.443520).  Saving model ...
Validation loss decreased (0.443520 --> 0.443488).  Saving model ...
Validation loss decreased (0.443488 --> 0.443455).  Saving model ...
Validation loss decreased (0.443455 --> 0.443422).  Saving model ...
Validation loss decreased (0.443422 --> 0.443389).  Saving model ...
Validation loss decreased (0.443389 --> 0.443357).  Saving model ...
Validation loss decreased (0.443357 --> 0.443324).  Saving model ...
Validation loss decreased (0.443324 --> 0.443291).  Saving model ...
Validation loss decreased (0.443291 --> 0.443258).  Saving model ...
Validation loss decreased (0.443258 --> 0.443225).  Saving model ...
Validation loss decreased (0.443225 --> 0.443192).  Saving model ...
Validation loss decreased (0.443192 --> 0.443159).  Saving model ...
Validation loss decreased (0.443159 --> 0.443126).  Saving model ...
Validation loss decreased (0.443126 --> 0.443094).  Saving model ...
Validation loss decreased (0.443094 --> 0.443061).  Saving model ...
Validation loss decreased (0.443061 --> 0.443028).  Saving model ...
Validation loss decreased (0.443028 --> 0.442995).  Saving model ...
Validation loss decreased (0.442995 --> 0.442961).  Saving model ...
Validation loss decreased (0.442961 --> 0.442928).  Saving model ...
Validation loss decreased (0.442928 --> 0.442895).  Saving model ...
Validation loss decreased (0.442895 --> 0.442862).  Saving model ...
Validation loss decreased (0.442862 --> 0.442829).  Saving model ...
Validation loss decreased (0.442829 --> 0.442796).  Saving model ...
Validation loss decreased (0.442796 --> 0.442763).  Saving model ...
Validation loss decreased (0.442763 --> 0.442730).  Saving model ...
Validation loss decreased (0.442730 --> 0.442697).  Saving model ...
Validation loss decreased (0.442697 --> 0.442664).  Saving model ...
Validation loss decreased (0.442664 --> 0.442630).  Saving model ...
Validation loss decreased (0.442630 --> 0.442597).  Saving model ...
Validation loss decreased (0.442597 --> 0.442564).  Saving model ...
Validation loss decreased (0.442564 --> 0.442531).  Saving model ...
Validation loss decreased (0.442531 --> 0.442497).  Saving model ...
Validation loss decreased (0.442497 --> 0.442464).  Saving model ...
Validation loss decreased (0.442464 --> 0.442431).  Saving model ...
Validation loss decreased (0.442431 --> 0.442398).  Saving model ...
Validation loss decreased (0.442398 --> 0.442364).  Saving model ...
Validation loss decreased (0.442364 --> 0.442331).  Saving model ...
Validation loss decreased (0.442331 --> 0.442298).  Saving model ...
Validation loss decreased (0.442298 --> 0.442264).  Saving model ...
Validation loss decreased (0.442264 --> 0.442231).  Saving model ...
Validation loss decreased (0.442231 --> 0.442198).  Saving model ...
Validation loss decreased (0.442198 --> 0.442164).  Saving model ...
Validation loss decreased (0.442164 --> 0.442131).  Saving model ...
Validation loss decreased (0.442131 --> 0.442097).  Saving model ...
Validation loss decreased (0.442097 --> 0.442064).  Saving model ...
Validation loss decreased (0.442064 --> 0.442031).  Saving model ...
Validation loss decreased (0.442031 --> 0.441997).  Saving model ...
Validation loss decreased (0.441997 --> 0.441964).  Saving model ...
Validation loss decreased (0.441964 --> 0.441930).  Saving model ...
Validation loss decreased (0.441930 --> 0.441897).  Saving model ...
Validation loss decreased (0.441897 --> 0.441863).  Saving model ...
Validation loss decreased (0.441863 --> 0.441830).  Saving model ...
Validation loss decreased (0.441830 --> 0.441796).  Saving model ...
Validation loss decreased (0.441796 --> 0.441763).  Saving model ...
Validation loss decreased (0.441763 --> 0.441729).  Saving model ...
Validation loss decreased (0.441729 --> 0.441696).  Saving model ...
Validation loss decreased (0.441696 --> 0.441662).  Saving model ...
Validation loss decreased (0.441662 --> 0.441629).  Saving model ...
Validation loss decreased (0.441629 --> 0.441595).  Saving model ...
Validation loss decreased (0.441595 --> 0.441562).  Saving model ...
Validation loss decreased (0.441562 --> 0.441528).  Saving model ...
Validation loss decreased (0.441528 --> 0.441495).  Saving model ...
Validation loss decreased (0.441495 --> 0.441461).  Saving model ...
Validation loss decreased (0.441461 --> 0.441428).  Saving model ...
Validation loss decreased (0.441428 --> 0.441394).  Saving model ...
Validation loss decreased (0.441394 --> 0.441361).  Saving model ...
Validation loss decreased (0.441361 --> 0.441327).  Saving model ...
Validation loss decreased (0.441327 --> 0.441294).  Saving model ...
Validation loss decreased (0.441294 --> 0.441260).  Saving model ...
epoch 2401, loss 0.4413, train acc 77.74%, f1 0.6597, precision 0.7079, recall 0.6176, auc 0.7404
Validation loss decreased (0.441260 --> 0.441227).  Saving model ...
Validation loss decreased (0.441227 --> 0.441193).  Saving model ...
Validation loss decreased (0.441193 --> 0.441160).  Saving model ...
Validation loss decreased (0.441160 --> 0.441127).  Saving model ...
Validation loss decreased (0.441127 --> 0.441093).  Saving model ...
Validation loss decreased (0.441093 --> 0.441060).  Saving model ...
Validation loss decreased (0.441060 --> 0.441026).  Saving model ...
Validation loss decreased (0.441026 --> 0.440993).  Saving model ...
Validation loss decreased (0.440993 --> 0.440959).  Saving model ...
Validation loss decreased (0.440959 --> 0.440926).  Saving model ...
Validation loss decreased (0.440926 --> 0.440892).  Saving model ...
Validation loss decreased (0.440892 --> 0.440859).  Saving model ...
Validation loss decreased (0.440859 --> 0.440826).  Saving model ...
Validation loss decreased (0.440826 --> 0.440792).  Saving model ...
Validation loss decreased (0.440792 --> 0.440759).  Saving model ...
Validation loss decreased (0.440759 --> 0.440726).  Saving model ...
Validation loss decreased (0.440726 --> 0.440692).  Saving model ...
Validation loss decreased (0.440692 --> 0.440659).  Saving model ...
Validation loss decreased (0.440659 --> 0.440626).  Saving model ...
Validation loss decreased (0.440626 --> 0.440593).  Saving model ...
Validation loss decreased (0.440593 --> 0.440559).  Saving model ...
Validation loss decreased (0.440559 --> 0.440526).  Saving model ...
Validation loss decreased (0.440526 --> 0.440493).  Saving model ...
Validation loss decreased (0.440493 --> 0.440460).  Saving model ...
Validation loss decreased (0.440460 --> 0.440426).  Saving model ...
Validation loss decreased (0.440426 --> 0.440393).  Saving model ...
Validation loss decreased (0.440393 --> 0.440360).  Saving model ...
Validation loss decreased (0.440360 --> 0.440327).  Saving model ...
Validation loss decreased (0.440327 --> 0.440294).  Saving model ...
Validation loss decreased (0.440294 --> 0.440261).  Saving model ...
Validation loss decreased (0.440261 --> 0.440228).  Saving model ...
Validation loss decreased (0.440228 --> 0.440195).  Saving model ...
Validation loss decreased (0.440195 --> 0.440162).  Saving model ...
Validation loss decreased (0.440162 --> 0.440129).  Saving model ...
Validation loss decreased (0.440129 --> 0.440096).  Saving model ...
Validation loss decreased (0.440096 --> 0.440063).  Saving model ...
Validation loss decreased (0.440063 --> 0.440031).  Saving model ...
Validation loss decreased (0.440031 --> 0.439998).  Saving model ...
Validation loss decreased (0.439998 --> 0.439965).  Saving model ...
Validation loss decreased (0.439965 --> 0.439932).  Saving model ...
Validation loss decreased (0.439932 --> 0.439900).  Saving model ...
Validation loss decreased (0.439900 --> 0.439867).  Saving model ...
Validation loss decreased (0.439867 --> 0.439834).  Saving model ...
Validation loss decreased (0.439834 --> 0.439802).  Saving model ...
Validation loss decreased (0.439802 --> 0.439769).  Saving model ...
Validation loss decreased (0.439769 --> 0.439737).  Saving model ...
Validation loss decreased (0.439737 --> 0.439704).  Saving model ...
Validation loss decreased (0.439704 --> 0.439672).  Saving model ...
Validation loss decreased (0.439672 --> 0.439639).  Saving model ...
Validation loss decreased (0.439639 --> 0.439607).  Saving model ...
Validation loss decreased (0.439607 --> 0.439574).  Saving model ...
Validation loss decreased (0.439574 --> 0.439542).  Saving model ...
Validation loss decreased (0.439542 --> 0.439510).  Saving model ...
Validation loss decreased (0.439510 --> 0.439478).  Saving model ...
Validation loss decreased (0.439478 --> 0.439445).  Saving model ...
Validation loss decreased (0.439445 --> 0.439413).  Saving model ...
Validation loss decreased (0.439413 --> 0.439381).  Saving model ...
Validation loss decreased (0.439381 --> 0.439349).  Saving model ...
Validation loss decreased (0.439349 --> 0.439317).  Saving model ...
Validation loss decreased (0.439317 --> 0.439285).  Saving model ...
Validation loss decreased (0.439285 --> 0.439253).  Saving model ...
Validation loss decreased (0.439253 --> 0.439221).  Saving model ...
Validation loss decreased (0.439221 --> 0.439189).  Saving model ...
Validation loss decreased (0.439189 --> 0.439157).  Saving model ...
Validation loss decreased (0.439157 --> 0.439125).  Saving model ...
Validation loss decreased (0.439125 --> 0.439094).  Saving model ...
Validation loss decreased (0.439094 --> 0.439062).  Saving model ...
Validation loss decreased (0.439062 --> 0.439030).  Saving model ...
Validation loss decreased (0.439030 --> 0.438999).  Saving model ...
Validation loss decreased (0.438999 --> 0.438967).  Saving model ...
Validation loss decreased (0.438967 --> 0.438935).  Saving model ...
Validation loss decreased (0.438935 --> 0.438904).  Saving model ...
Validation loss decreased (0.438904 --> 0.438872).  Saving model ...
Validation loss decreased (0.438872 --> 0.438841).  Saving model ...
Validation loss decreased (0.438841 --> 0.438810).  Saving model ...
Validation loss decreased (0.438810 --> 0.438778).  Saving model ...
Validation loss decreased (0.438778 --> 0.438747).  Saving model ...
Validation loss decreased (0.438747 --> 0.438716).  Saving model ...
Validation loss decreased (0.438716 --> 0.438684).  Saving model ...
Validation loss decreased (0.438684 --> 0.438653).  Saving model ...
Validation loss decreased (0.438653 --> 0.438622).  Saving model ...
Validation loss decreased (0.438622 --> 0.438591).  Saving model ...
Validation loss decreased (0.438591 --> 0.438560).  Saving model ...
Validation loss decreased (0.438560 --> 0.438529).  Saving model ...
Validation loss decreased (0.438529 --> 0.438498).  Saving model ...
Validation loss decreased (0.438498 --> 0.438467).  Saving model ...
Validation loss decreased (0.438467 --> 0.438436).  Saving model ...
Validation loss decreased (0.438436 --> 0.438405).  Saving model ...
Validation loss decreased (0.438405 --> 0.438374).  Saving model ...
Validation loss decreased (0.438374 --> 0.438344).  Saving model ...
Validation loss decreased (0.438344 --> 0.438313).  Saving model ...
Validation loss decreased (0.438313 --> 0.438282).  Saving model ...
Validation loss decreased (0.438282 --> 0.438252).  Saving model ...
Validation loss decreased (0.438252 --> 0.438221).  Saving model ...
Validation loss decreased (0.438221 --> 0.438190).  Saving model ...
Validation loss decreased (0.438190 --> 0.438160).  Saving model ...
Validation loss decreased (0.438160 --> 0.438129).  Saving model ...
Validation loss decreased (0.438129 --> 0.438099).  Saving model ...
Validation loss decreased (0.438099 --> 0.438069).  Saving model ...
Validation loss decreased (0.438069 --> 0.438038).  Saving model ...
epoch 2501, loss 0.4380, train acc 77.40%, f1 0.6545, precision 0.7022, recall 0.6127, auc 0.7366
Validation loss decreased (0.438038 --> 0.438008).  Saving model ...
Validation loss decreased (0.438008 --> 0.437978).  Saving model ...
Validation loss decreased (0.437978 --> 0.437947).  Saving model ...
Validation loss decreased (0.437947 --> 0.437917).  Saving model ...
Validation loss decreased (0.437917 --> 0.437887).  Saving model ...
Validation loss decreased (0.437887 --> 0.437857).  Saving model ...
Validation loss decreased (0.437857 --> 0.437827).  Saving model ...
Validation loss decreased (0.437827 --> 0.437797).  Saving model ...
Validation loss decreased (0.437797 --> 0.437767).  Saving model ...
Validation loss decreased (0.437767 --> 0.437737).  Saving model ...
Validation loss decreased (0.437737 --> 0.437707).  Saving model ...
Validation loss decreased (0.437707 --> 0.437677).  Saving model ...
Validation loss decreased (0.437677 --> 0.437647).  Saving model ...
Validation loss decreased (0.437647 --> 0.437618).  Saving model ...
Validation loss decreased (0.437618 --> 0.437588).  Saving model ...
Validation loss decreased (0.437588 --> 0.437558).  Saving model ...
Validation loss decreased (0.437558 --> 0.437528).  Saving model ...
Validation loss decreased (0.437528 --> 0.437499).  Saving model ...
Validation loss decreased (0.437499 --> 0.437469).  Saving model ...
Validation loss decreased (0.437469 --> 0.437440).  Saving model ...
Validation loss decreased (0.437440 --> 0.437410).  Saving model ...
Validation loss decreased (0.437410 --> 0.437381).  Saving model ...
Validation loss decreased (0.437381 --> 0.437351).  Saving model ...
Validation loss decreased (0.437351 --> 0.437322).  Saving model ...
Validation loss decreased (0.437322 --> 0.437293).  Saving model ...
Validation loss decreased (0.437293 --> 0.437263).  Saving model ...
Validation loss decreased (0.437263 --> 0.437234).  Saving model ...
Validation loss decreased (0.437234 --> 0.437205).  Saving model ...
Validation loss decreased (0.437205 --> 0.437176).  Saving model ...
Validation loss decreased (0.437176 --> 0.437146).  Saving model ...
Validation loss decreased (0.437146 --> 0.437117).  Saving model ...
Validation loss decreased (0.437117 --> 0.437088).  Saving model ...
Validation loss decreased (0.437088 --> 0.437059).  Saving model ...
Validation loss decreased (0.437059 --> 0.437030).  Saving model ...
Validation loss decreased (0.437030 --> 0.437001).  Saving model ...
Validation loss decreased (0.437001 --> 0.436972).  Saving model ...
Validation loss decreased (0.436972 --> 0.436943).  Saving model ...
Validation loss decreased (0.436943 --> 0.436914).  Saving model ...
Validation loss decreased (0.436914 --> 0.436886).  Saving model ...
Validation loss decreased (0.436886 --> 0.436857).  Saving model ...
Validation loss decreased (0.436857 --> 0.436828).  Saving model ...
Validation loss decreased (0.436828 --> 0.436799).  Saving model ...
Validation loss decreased (0.436799 --> 0.436771).  Saving model ...
Validation loss decreased (0.436771 --> 0.436742).  Saving model ...
Validation loss decreased (0.436742 --> 0.436713).  Saving model ...
Validation loss decreased (0.436713 --> 0.436685).  Saving model ...
Validation loss decreased (0.436685 --> 0.436656).  Saving model ...
Validation loss decreased (0.436656 --> 0.436628).  Saving model ...
Validation loss decreased (0.436628 --> 0.436599).  Saving model ...
Validation loss decreased (0.436599 --> 0.436571).  Saving model ...
Validation loss decreased (0.436571 --> 0.436542).  Saving model ...
Validation loss decreased (0.436542 --> 0.436514).  Saving model ...
Validation loss decreased (0.436514 --> 0.436486).  Saving model ...
Validation loss decreased (0.436486 --> 0.436457).  Saving model ...
Validation loss decreased (0.436457 --> 0.436429).  Saving model ...
Validation loss decreased (0.436429 --> 0.436401).  Saving model ...
Validation loss decreased (0.436401 --> 0.436373).  Saving model ...
Validation loss decreased (0.436373 --> 0.436344).  Saving model ...
Validation loss decreased (0.436344 --> 0.436316).  Saving model ...
Validation loss decreased (0.436316 --> 0.436288).  Saving model ...
Validation loss decreased (0.436288 --> 0.436260).  Saving model ...
Validation loss decreased (0.436260 --> 0.436232).  Saving model ...
Validation loss decreased (0.436232 --> 0.436204).  Saving model ...
Validation loss decreased (0.436204 --> 0.436176).  Saving model ...
Validation loss decreased (0.436176 --> 0.436148).  Saving model ...
Validation loss decreased (0.436148 --> 0.436120).  Saving model ...
Validation loss decreased (0.436120 --> 0.436092).  Saving model ...
Validation loss decreased (0.436092 --> 0.436065).  Saving model ...
Validation loss decreased (0.436065 --> 0.436037).  Saving model ...
Validation loss decreased (0.436037 --> 0.436009).  Saving model ...
Validation loss decreased (0.436009 --> 0.435981).  Saving model ...
Validation loss decreased (0.435981 --> 0.435954).  Saving model ...
Validation loss decreased (0.435954 --> 0.435926).  Saving model ...
Validation loss decreased (0.435926 --> 0.435898).  Saving model ...
Validation loss decreased (0.435898 --> 0.435871).  Saving model ...
Validation loss decreased (0.435871 --> 0.435843).  Saving model ...
Validation loss decreased (0.435843 --> 0.435816).  Saving model ...
Validation loss decreased (0.435816 --> 0.435788).  Saving model ...
Validation loss decreased (0.435788 --> 0.435761).  Saving model ...
Validation loss decreased (0.435761 --> 0.435733).  Saving model ...
Validation loss decreased (0.435733 --> 0.435706).  Saving model ...
Validation loss decreased (0.435706 --> 0.435679).  Saving model ...
Validation loss decreased (0.435679 --> 0.435651).  Saving model ...
Validation loss decreased (0.435651 --> 0.435624).  Saving model ...
Validation loss decreased (0.435624 --> 0.435597).  Saving model ...
Validation loss decreased (0.435597 --> 0.435569).  Saving model ...
Validation loss decreased (0.435569 --> 0.435542).  Saving model ...
Validation loss decreased (0.435542 --> 0.435515).  Saving model ...
Validation loss decreased (0.435515 --> 0.435488).  Saving model ...
Validation loss decreased (0.435488 --> 0.435461).  Saving model ...
Validation loss decreased (0.435461 --> 0.435434).  Saving model ...
Validation loss decreased (0.435434 --> 0.435407).  Saving model ...
Validation loss decreased (0.435407 --> 0.435380).  Saving model ...
Validation loss decreased (0.435380 --> 0.435353).  Saving model ...
Validation loss decreased (0.435353 --> 0.435326).  Saving model ...
Validation loss decreased (0.435326 --> 0.435299).  Saving model ...
Validation loss decreased (0.435299 --> 0.435272).  Saving model ...
Validation loss decreased (0.435272 --> 0.435245).  Saving model ...
Validation loss decreased (0.435245 --> 0.435218).  Saving model ...
Validation loss decreased (0.435218 --> 0.435192).  Saving model ...
epoch 2601, loss 0.4352, train acc 77.74%, f1 0.6597, precision 0.7079, recall 0.6176, auc 0.7404
Validation loss decreased (0.435192 --> 0.435165).  Saving model ...
Validation loss decreased (0.435165 --> 0.435138).  Saving model ...
Validation loss decreased (0.435138 --> 0.435111).  Saving model ...
Validation loss decreased (0.435111 --> 0.435085).  Saving model ...
Validation loss decreased (0.435085 --> 0.435058).  Saving model ...
Validation loss decreased (0.435058 --> 0.435032).  Saving model ...
Validation loss decreased (0.435032 --> 0.435005).  Saving model ...
Validation loss decreased (0.435005 --> 0.434978).  Saving model ...
Validation loss decreased (0.434978 --> 0.434952).  Saving model ...
Validation loss decreased (0.434952 --> 0.434925).  Saving model ...
Validation loss decreased (0.434925 --> 0.434899).  Saving model ...
Validation loss decreased (0.434899 --> 0.434873).  Saving model ...
Validation loss decreased (0.434873 --> 0.434846).  Saving model ...
Validation loss decreased (0.434846 --> 0.434820).  Saving model ...
Validation loss decreased (0.434820 --> 0.434794).  Saving model ...
Validation loss decreased (0.434794 --> 0.434767).  Saving model ...
Validation loss decreased (0.434767 --> 0.434741).  Saving model ...
Validation loss decreased (0.434741 --> 0.434715).  Saving model ...
Validation loss decreased (0.434715 --> 0.434688).  Saving model ...
Validation loss decreased (0.434688 --> 0.434662).  Saving model ...
Validation loss decreased (0.434662 --> 0.434636).  Saving model ...
Validation loss decreased (0.434636 --> 0.434610).  Saving model ...
Validation loss decreased (0.434610 --> 0.434584).  Saving model ...
Validation loss decreased (0.434584 --> 0.434558).  Saving model ...
Validation loss decreased (0.434558 --> 0.434532).  Saving model ...
Validation loss decreased (0.434532 --> 0.434506).  Saving model ...
Validation loss decreased (0.434506 --> 0.434480).  Saving model ...
Validation loss decreased (0.434480 --> 0.434454).  Saving model ...
Validation loss decreased (0.434454 --> 0.434428).  Saving model ...
Validation loss decreased (0.434428 --> 0.434402).  Saving model ...
Validation loss decreased (0.434402 --> 0.434376).  Saving model ...
Validation loss decreased (0.434376 --> 0.434351).  Saving model ...
Validation loss decreased (0.434351 --> 0.434325).  Saving model ...
Validation loss decreased (0.434325 --> 0.434299).  Saving model ...
Validation loss decreased (0.434299 --> 0.434273).  Saving model ...
Validation loss decreased (0.434273 --> 0.434248).  Saving model ...
Validation loss decreased (0.434248 --> 0.434222).  Saving model ...
Validation loss decreased (0.434222 --> 0.434196).  Saving model ...
Validation loss decreased (0.434196 --> 0.434171).  Saving model ...
Validation loss decreased (0.434171 --> 0.434145).  Saving model ...
Validation loss decreased (0.434145 --> 0.434119).  Saving model ...
Validation loss decreased (0.434119 --> 0.434094).  Saving model ...
Validation loss decreased (0.434094 --> 0.434068).  Saving model ...
Validation loss decreased (0.434068 --> 0.434043).  Saving model ...
Validation loss decreased (0.434043 --> 0.434018).  Saving model ...
Validation loss decreased (0.434018 --> 0.433992).  Saving model ...
Validation loss decreased (0.433992 --> 0.433967).  Saving model ...
Validation loss decreased (0.433967 --> 0.433941).  Saving model ...
Validation loss decreased (0.433941 --> 0.433916).  Saving model ...
Validation loss decreased (0.433916 --> 0.433891).  Saving model ...
Validation loss decreased (0.433891 --> 0.433865).  Saving model ...
Validation loss decreased (0.433865 --> 0.433840).  Saving model ...
Validation loss decreased (0.433840 --> 0.433815).  Saving model ...
Validation loss decreased (0.433815 --> 0.433790).  Saving model ...
Validation loss decreased (0.433790 --> 0.433765).  Saving model ...
Validation loss decreased (0.433765 --> 0.433739).  Saving model ...
Validation loss decreased (0.433739 --> 0.433714).  Saving model ...
Validation loss decreased (0.433714 --> 0.433689).  Saving model ...
Validation loss decreased (0.433689 --> 0.433664).  Saving model ...
Validation loss decreased (0.433664 --> 0.433639).  Saving model ...
Validation loss decreased (0.433639 --> 0.433614).  Saving model ...
Validation loss decreased (0.433614 --> 0.433589).  Saving model ...
Validation loss decreased (0.433589 --> 0.433564).  Saving model ...
Validation loss decreased (0.433564 --> 0.433539).  Saving model ...
Validation loss decreased (0.433539 --> 0.433514).  Saving model ...
Validation loss decreased (0.433514 --> 0.433490).  Saving model ...
Validation loss decreased (0.433490 --> 0.433465).  Saving model ...
Validation loss decreased (0.433465 --> 0.433440).  Saving model ...
Validation loss decreased (0.433440 --> 0.433415).  Saving model ...
Validation loss decreased (0.433415 --> 0.433390).  Saving model ...
Validation loss decreased (0.433390 --> 0.433366).  Saving model ...
Validation loss decreased (0.433366 --> 0.433341).  Saving model ...
Validation loss decreased (0.433341 --> 0.433316).  Saving model ...
Validation loss decreased (0.433316 --> 0.433291).  Saving model ...
Validation loss decreased (0.433291 --> 0.433267).  Saving model ...
Validation loss decreased (0.433267 --> 0.433242).  Saving model ...
Validation loss decreased (0.433242 --> 0.433218).  Saving model ...
Validation loss decreased (0.433218 --> 0.433193).  Saving model ...
Validation loss decreased (0.433193 --> 0.433169).  Saving model ...
Validation loss decreased (0.433169 --> 0.433144).  Saving model ...
Validation loss decreased (0.433144 --> 0.433120).  Saving model ...
Validation loss decreased (0.433120 --> 0.433095).  Saving model ...
Validation loss decreased (0.433095 --> 0.433071).  Saving model ...
Validation loss decreased (0.433071 --> 0.433046).  Saving model ...
Validation loss decreased (0.433046 --> 0.433022).  Saving model ...
Validation loss decreased (0.433022 --> 0.432998).  Saving model ...
Validation loss decreased (0.432998 --> 0.432973).  Saving model ...
Validation loss decreased (0.432973 --> 0.432949).  Saving model ...
Validation loss decreased (0.432949 --> 0.432925).  Saving model ...
Validation loss decreased (0.432925 --> 0.432900).  Saving model ...
Validation loss decreased (0.432900 --> 0.432876).  Saving model ...
Validation loss decreased (0.432876 --> 0.432852).  Saving model ...
Validation loss decreased (0.432852 --> 0.432828).  Saving model ...
Validation loss decreased (0.432828 --> 0.432804).  Saving model ...
Validation loss decreased (0.432804 --> 0.432780).  Saving model ...
Validation loss decreased (0.432780 --> 0.432755).  Saving model ...
Validation loss decreased (0.432755 --> 0.432731).  Saving model ...
Validation loss decreased (0.432731 --> 0.432707).  Saving model ...
Validation loss decreased (0.432707 --> 0.432683).  Saving model ...
Validation loss decreased (0.432683 --> 0.432659).  Saving model ...
epoch 2701, loss 0.4327, train acc 77.91%, f1 0.6667, precision 0.7049, recall 0.6324, auc 0.7451
Validation loss decreased (0.432659 --> 0.432635).  Saving model ...
Validation loss decreased (0.432635 --> 0.432611).  Saving model ...
Validation loss decreased (0.432611 --> 0.432587).  Saving model ...
Validation loss decreased (0.432587 --> 0.432563).  Saving model ...
Validation loss decreased (0.432563 --> 0.432540).  Saving model ...
Validation loss decreased (0.432540 --> 0.432516).  Saving model ...
Validation loss decreased (0.432516 --> 0.432492).  Saving model ...
Validation loss decreased (0.432492 --> 0.432468).  Saving model ...
Validation loss decreased (0.432468 --> 0.432444).  Saving model ...
Validation loss decreased (0.432444 --> 0.432420).  Saving model ...
Validation loss decreased (0.432420 --> 0.432397).  Saving model ...
Validation loss decreased (0.432397 --> 0.432373).  Saving model ...
Validation loss decreased (0.432373 --> 0.432349).  Saving model ...
Validation loss decreased (0.432349 --> 0.432326).  Saving model ...
Validation loss decreased (0.432326 --> 0.432302).  Saving model ...
Validation loss decreased (0.432302 --> 0.432278).  Saving model ...
Validation loss decreased (0.432278 --> 0.432255).  Saving model ...
Validation loss decreased (0.432255 --> 0.432231).  Saving model ...
Validation loss decreased (0.432231 --> 0.432208).  Saving model ...
Validation loss decreased (0.432208 --> 0.432184).  Saving model ...
Validation loss decreased (0.432184 --> 0.432161).  Saving model ...
Validation loss decreased (0.432161 --> 0.432137).  Saving model ...
Validation loss decreased (0.432137 --> 0.432114).  Saving model ...
Validation loss decreased (0.432114 --> 0.432090).  Saving model ...
Validation loss decreased (0.432090 --> 0.432067).  Saving model ...
Validation loss decreased (0.432067 --> 0.432043).  Saving model ...
Validation loss decreased (0.432043 --> 0.432020).  Saving model ...
Validation loss decreased (0.432020 --> 0.431997).  Saving model ...
Validation loss decreased (0.431997 --> 0.431973).  Saving model ...
Validation loss decreased (0.431973 --> 0.431950).  Saving model ...
Validation loss decreased (0.431950 --> 0.431927).  Saving model ...
Validation loss decreased (0.431927 --> 0.431903).  Saving model ...
Validation loss decreased (0.431903 --> 0.431880).  Saving model ...
Validation loss decreased (0.431880 --> 0.431857).  Saving model ...
Validation loss decreased (0.431857 --> 0.431834).  Saving model ...
Validation loss decreased (0.431834 --> 0.431811).  Saving model ...
Validation loss decreased (0.431811 --> 0.431787).  Saving model ...
Validation loss decreased (0.431787 --> 0.431764).  Saving model ...
Validation loss decreased (0.431764 --> 0.431741).  Saving model ...
Validation loss decreased (0.431741 --> 0.431718).  Saving model ...
Validation loss decreased (0.431718 --> 0.431695).  Saving model ...
Validation loss decreased (0.431695 --> 0.431672).  Saving model ...
Validation loss decreased (0.431672 --> 0.431649).  Saving model ...
Validation loss decreased (0.431649 --> 0.431626).  Saving model ...
Validation loss decreased (0.431626 --> 0.431603).  Saving model ...
Validation loss decreased (0.431603 --> 0.431580).  Saving model ...
Validation loss decreased (0.431580 --> 0.431557).  Saving model ...
Validation loss decreased (0.431557 --> 0.431534).  Saving model ...
Validation loss decreased (0.431534 --> 0.431511).  Saving model ...
Validation loss decreased (0.431511 --> 0.431489).  Saving model ...
Validation loss decreased (0.431489 --> 0.431466).  Saving model ...
Validation loss decreased (0.431466 --> 0.431443).  Saving model ...
Validation loss decreased (0.431443 --> 0.431420).  Saving model ...
Validation loss decreased (0.431420 --> 0.431397).  Saving model ...
Validation loss decreased (0.431397 --> 0.431374).  Saving model ...
Validation loss decreased (0.431374 --> 0.431352).  Saving model ...
Validation loss decreased (0.431352 --> 0.431329).  Saving model ...
Validation loss decreased (0.431329 --> 0.431306).  Saving model ...
Validation loss decreased (0.431306 --> 0.431284).  Saving model ...
Validation loss decreased (0.431284 --> 0.431261).  Saving model ...
Validation loss decreased (0.431261 --> 0.431238).  Saving model ...
Validation loss decreased (0.431238 --> 0.431216).  Saving model ...
Validation loss decreased (0.431216 --> 0.431193).  Saving model ...
Validation loss decreased (0.431193 --> 0.431170).  Saving model ...
Validation loss decreased (0.431170 --> 0.431148).  Saving model ...
Validation loss decreased (0.431148 --> 0.431125).  Saving model ...
Validation loss decreased (0.431125 --> 0.431103).  Saving model ...
Validation loss decreased (0.431103 --> 0.431080).  Saving model ...
Validation loss decreased (0.431080 --> 0.431058).  Saving model ...
Validation loss decreased (0.431058 --> 0.431035).  Saving model ...
Validation loss decreased (0.431035 --> 0.431013).  Saving model ...
Validation loss decreased (0.431013 --> 0.430991).  Saving model ...
Validation loss decreased (0.430991 --> 0.430968).  Saving model ...
Validation loss decreased (0.430968 --> 0.430946).  Saving model ...
Validation loss decreased (0.430946 --> 0.430923).  Saving model ...
Validation loss decreased (0.430923 --> 0.430901).  Saving model ...
Validation loss decreased (0.430901 --> 0.430879).  Saving model ...
Validation loss decreased (0.430879 --> 0.430856).  Saving model ...
Validation loss decreased (0.430856 --> 0.430834).  Saving model ...
Validation loss decreased (0.430834 --> 0.430812).  Saving model ...
Validation loss decreased (0.430812 --> 0.430790).  Saving model ...
Validation loss decreased (0.430790 --> 0.430767).  Saving model ...
Validation loss decreased (0.430767 --> 0.430745).  Saving model ...
Validation loss decreased (0.430745 --> 0.430723).  Saving model ...
Validation loss decreased (0.430723 --> 0.430701).  Saving model ...
Validation loss decreased (0.430701 --> 0.430679).  Saving model ...
Validation loss decreased (0.430679 --> 0.430656).  Saving model ...
Validation loss decreased (0.430656 --> 0.430634).  Saving model ...
Validation loss decreased (0.430634 --> 0.430612).  Saving model ...
Validation loss decreased (0.430612 --> 0.430590).  Saving model ...
Validation loss decreased (0.430590 --> 0.430568).  Saving model ...
Validation loss decreased (0.430568 --> 0.430546).  Saving model ...
Validation loss decreased (0.430546 --> 0.430524).  Saving model ...
Validation loss decreased (0.430524 --> 0.430502).  Saving model ...
Validation loss decreased (0.430502 --> 0.430480).  Saving model ...
Validation loss decreased (0.430480 --> 0.430458).  Saving model ...
Validation loss decreased (0.430458 --> 0.430436).  Saving model ...
Validation loss decreased (0.430436 --> 0.430414).  Saving model ...
Validation loss decreased (0.430414 --> 0.430392).  Saving model ...
Validation loss decreased (0.430392 --> 0.430370).  Saving model ...
epoch 2801, loss 0.4304, train acc 78.42%, f1 0.6753, precision 0.7120, recall 0.6422, auc 0.7513
Validation loss decreased (0.430370 --> 0.430348).  Saving model ...
Validation loss decreased (0.430348 --> 0.430327).  Saving model ...
Validation loss decreased (0.430327 --> 0.430305).  Saving model ...
Validation loss decreased (0.430305 --> 0.430283).  Saving model ...
Validation loss decreased (0.430283 --> 0.430261).  Saving model ...
Validation loss decreased (0.430261 --> 0.430239).  Saving model ...
Validation loss decreased (0.430239 --> 0.430218).  Saving model ...
Validation loss decreased (0.430218 --> 0.430196).  Saving model ...
Validation loss decreased (0.430196 --> 0.430174).  Saving model ...
Validation loss decreased (0.430174 --> 0.430152).  Saving model ...
Validation loss decreased (0.430152 --> 0.430131).  Saving model ...
Validation loss decreased (0.430131 --> 0.430109).  Saving model ...
Validation loss decreased (0.430109 --> 0.430087).  Saving model ...
Validation loss decreased (0.430087 --> 0.430066).  Saving model ...
Validation loss decreased (0.430066 --> 0.430044).  Saving model ...
Validation loss decreased (0.430044 --> 0.430022).  Saving model ...
Validation loss decreased (0.430022 --> 0.430001).  Saving model ...
Validation loss decreased (0.430001 --> 0.429979).  Saving model ...
Validation loss decreased (0.429979 --> 0.429958).  Saving model ...
Validation loss decreased (0.429958 --> 0.429936).  Saving model ...
Validation loss decreased (0.429936 --> 0.429914).  Saving model ...
Validation loss decreased (0.429914 --> 0.429893).  Saving model ...
Validation loss decreased (0.429893 --> 0.429871).  Saving model ...
Validation loss decreased (0.429871 --> 0.429850).  Saving model ...
Validation loss decreased (0.429850 --> 0.429829).  Saving model ...
Validation loss decreased (0.429829 --> 0.429807).  Saving model ...
Validation loss decreased (0.429807 --> 0.429786).  Saving model ...
Validation loss decreased (0.429786 --> 0.429764).  Saving model ...
Validation loss decreased (0.429764 --> 0.429743).  Saving model ...
Validation loss decreased (0.429743 --> 0.429721).  Saving model ...
Validation loss decreased (0.429721 --> 0.429700).  Saving model ...
Validation loss decreased (0.429700 --> 0.429679).  Saving model ...
Validation loss decreased (0.429679 --> 0.429657).  Saving model ...
Validation loss decreased (0.429657 --> 0.429636).  Saving model ...
Validation loss decreased (0.429636 --> 0.429615).  Saving model ...
Validation loss decreased (0.429615 --> 0.429594).  Saving model ...
Validation loss decreased (0.429594 --> 0.429572).  Saving model ...
Validation loss decreased (0.429572 --> 0.429551).  Saving model ...
Validation loss decreased (0.429551 --> 0.429530).  Saving model ...
Validation loss decreased (0.429530 --> 0.429509).  Saving model ...
Validation loss decreased (0.429509 --> 0.429487).  Saving model ...
Validation loss decreased (0.429487 --> 0.429466).  Saving model ...
Validation loss decreased (0.429466 --> 0.429445).  Saving model ...
Validation loss decreased (0.429445 --> 0.429424).  Saving model ...
Validation loss decreased (0.429424 --> 0.429403).  Saving model ...
Validation loss decreased (0.429403 --> 0.429381).  Saving model ...
Validation loss decreased (0.429381 --> 0.429360).  Saving model ...
Validation loss decreased (0.429360 --> 0.429339).  Saving model ...
Validation loss decreased (0.429339 --> 0.429318).  Saving model ...
Validation loss decreased (0.429318 --> 0.429297).  Saving model ...
Validation loss decreased (0.429297 --> 0.429276).  Saving model ...
Validation loss decreased (0.429276 --> 0.429255).  Saving model ...
Validation loss decreased (0.429255 --> 0.429234).  Saving model ...
Validation loss decreased (0.429234 --> 0.429213).  Saving model ...
Validation loss decreased (0.429213 --> 0.429192).  Saving model ...
Validation loss decreased (0.429192 --> 0.429171).  Saving model ...
Validation loss decreased (0.429171 --> 0.429150).  Saving model ...
Validation loss decreased (0.429150 --> 0.429129).  Saving model ...
Validation loss decreased (0.429129 --> 0.429108).  Saving model ...
Validation loss decreased (0.429108 --> 0.429087).  Saving model ...
Validation loss decreased (0.429087 --> 0.429067).  Saving model ...
Validation loss decreased (0.429067 --> 0.429046).  Saving model ...
Validation loss decreased (0.429046 --> 0.429025).  Saving model ...
Validation loss decreased (0.429025 --> 0.429004).  Saving model ...
Validation loss decreased (0.429004 --> 0.428983).  Saving model ...
Validation loss decreased (0.428983 --> 0.428962).  Saving model ...
Validation loss decreased (0.428962 --> 0.428942).  Saving model ...
Validation loss decreased (0.428942 --> 0.428921).  Saving model ...
Validation loss decreased (0.428921 --> 0.428900).  Saving model ...
Validation loss decreased (0.428900 --> 0.428879).  Saving model ...
Validation loss decreased (0.428879 --> 0.428858).  Saving model ...
Validation loss decreased (0.428858 --> 0.428838).  Saving model ...
Validation loss decreased (0.428838 --> 0.428817).  Saving model ...
Validation loss decreased (0.428817 --> 0.428796).  Saving model ...
Validation loss decreased (0.428796 --> 0.428776).  Saving model ...
Validation loss decreased (0.428776 --> 0.428755).  Saving model ...
Validation loss decreased (0.428755 --> 0.428734).  Saving model ...
Validation loss decreased (0.428734 --> 0.428714).  Saving model ...
Validation loss decreased (0.428714 --> 0.428693).  Saving model ...
Validation loss decreased (0.428693 --> 0.428672).  Saving model ...
Validation loss decreased (0.428672 --> 0.428652).  Saving model ...
Validation loss decreased (0.428652 --> 0.428631).  Saving model ...
Validation loss decreased (0.428631 --> 0.428611).  Saving model ...
Validation loss decreased (0.428611 --> 0.428590).  Saving model ...
Validation loss decreased (0.428590 --> 0.428570).  Saving model ...
Validation loss decreased (0.428570 --> 0.428549).  Saving model ...
Validation loss decreased (0.428549 --> 0.428529).  Saving model ...
Validation loss decreased (0.428529 --> 0.428508).  Saving model ...
Validation loss decreased (0.428508 --> 0.428488).  Saving model ...
Validation loss decreased (0.428488 --> 0.428467).  Saving model ...
Validation loss decreased (0.428467 --> 0.428447).  Saving model ...
Validation loss decreased (0.428447 --> 0.428426).  Saving model ...
Validation loss decreased (0.428426 --> 0.428406).  Saving model ...
Validation loss decreased (0.428406 --> 0.428385).  Saving model ...
Validation loss decreased (0.428385 --> 0.428365).  Saving model ...
Validation loss decreased (0.428365 --> 0.428345).  Saving model ...
Validation loss decreased (0.428345 --> 0.428324).  Saving model ...
Validation loss decreased (0.428324 --> 0.428304).  Saving model ...
Validation loss decreased (0.428304 --> 0.428284).  Saving model ...
Validation loss decreased (0.428284 --> 0.428263).  Saving model ...
epoch 2901, loss 0.4283, train acc 78.77%, f1 0.6788, precision 0.7198, recall 0.6422, auc 0.7540
Validation loss decreased (0.428263 --> 0.428243).  Saving model ...
Validation loss decreased (0.428243 --> 0.428223).  Saving model ...
Validation loss decreased (0.428223 --> 0.428202).  Saving model ...
Validation loss decreased (0.428202 --> 0.428182).  Saving model ...
Validation loss decreased (0.428182 --> 0.428162).  Saving model ...
Validation loss decreased (0.428162 --> 0.428142).  Saving model ...
Validation loss decreased (0.428142 --> 0.428121).  Saving model ...
Validation loss decreased (0.428121 --> 0.428101).  Saving model ...
Validation loss decreased (0.428101 --> 0.428081).  Saving model ...
Validation loss decreased (0.428081 --> 0.428061).  Saving model ...
Validation loss decreased (0.428061 --> 0.428041).  Saving model ...
Validation loss decreased (0.428041 --> 0.428020).  Saving model ...
Validation loss decreased (0.428020 --> 0.428000).  Saving model ...
Validation loss decreased (0.428000 --> 0.427980).  Saving model ...
Validation loss decreased (0.427980 --> 0.427960).  Saving model ...
Validation loss decreased (0.427960 --> 0.427940).  Saving model ...
Validation loss decreased (0.427940 --> 0.427920).  Saving model ...
Validation loss decreased (0.427920 --> 0.427900).  Saving model ...
Validation loss decreased (0.427900 --> 0.427880).  Saving model ...
Validation loss decreased (0.427880 --> 0.427860).  Saving model ...
Validation loss decreased (0.427860 --> 0.427840).  Saving model ...
Validation loss decreased (0.427840 --> 0.427820).  Saving model ...
Validation loss decreased (0.427820 --> 0.427799).  Saving model ...
Validation loss decreased (0.427799 --> 0.427779).  Saving model ...
Validation loss decreased (0.427779 --> 0.427759).  Saving model ...
Validation loss decreased (0.427759 --> 0.427740).  Saving model ...
Validation loss decreased (0.427740 --> 0.427720).  Saving model ...
Validation loss decreased (0.427720 --> 0.427700).  Saving model ...
Validation loss decreased (0.427700 --> 0.427680).  Saving model ...
Validation loss decreased (0.427680 --> 0.427660).  Saving model ...
Validation loss decreased (0.427660 --> 0.427640).  Saving model ...
Validation loss decreased (0.427640 --> 0.427620).  Saving model ...
Validation loss decreased (0.427620 --> 0.427600).  Saving model ...
Validation loss decreased (0.427600 --> 0.427580).  Saving model ...
Validation loss decreased (0.427580 --> 0.427560).  Saving model ...
Validation loss decreased (0.427560 --> 0.427540).  Saving model ...
Validation loss decreased (0.427540 --> 0.427521).  Saving model ...
Validation loss decreased (0.427521 --> 0.427501).  Saving model ...
Validation loss decreased (0.427501 --> 0.427481).  Saving model ...
Validation loss decreased (0.427481 --> 0.427461).  Saving model ...
Validation loss decreased (0.427461 --> 0.427441).  Saving model ...
Validation loss decreased (0.427441 --> 0.427422).  Saving model ...
Validation loss decreased (0.427422 --> 0.427402).  Saving model ...
Validation loss decreased (0.427402 --> 0.427382).  Saving model ...
Validation loss decreased (0.427382 --> 0.427362).  Saving model ...
Validation loss decreased (0.427362 --> 0.427343).  Saving model ...
Validation loss decreased (0.427343 --> 0.427323).  Saving model ...
Validation loss decreased (0.427323 --> 0.427303).  Saving model ...
Validation loss decreased (0.427303 --> 0.427284).  Saving model ...
Validation loss decreased (0.427284 --> 0.427264).  Saving model ...
Validation loss decreased (0.427264 --> 0.427244).  Saving model ...
Validation loss decreased (0.427244 --> 0.427225).  Saving model ...
Validation loss decreased (0.427225 --> 0.427205).  Saving model ...
Validation loss decreased (0.427205 --> 0.427185).  Saving model ...
Validation loss decreased (0.427185 --> 0.427166).  Saving model ...
Validation loss decreased (0.427166 --> 0.427146).  Saving model ...
Validation loss decreased (0.427146 --> 0.427127).  Saving model ...
Validation loss decreased (0.427127 --> 0.427107).  Saving model ...
Validation loss decreased (0.427107 --> 0.427087).  Saving model ...
Validation loss decreased (0.427087 --> 0.427068).  Saving model ...
Validation loss decreased (0.427068 --> 0.427048).  Saving model ...
Validation loss decreased (0.427048 --> 0.427029).  Saving model ...
Validation loss decreased (0.427029 --> 0.427009).  Saving model ...
Validation loss decreased (0.427009 --> 0.426990).  Saving model ...
Validation loss decreased (0.426990 --> 0.426970).  Saving model ...
Validation loss decreased (0.426970 --> 0.426951).  Saving model ...
Validation loss decreased (0.426951 --> 0.426931).  Saving model ...
Validation loss decreased (0.426931 --> 0.426912).  Saving model ...
Validation loss decreased (0.426912 --> 0.426892).  Saving model ...
Validation loss decreased (0.426892 --> 0.426873).  Saving model ...
Validation loss decreased (0.426873 --> 0.426854).  Saving model ...
Validation loss decreased (0.426854 --> 0.426834).  Saving model ...
Validation loss decreased (0.426834 --> 0.426815).  Saving model ...
Validation loss decreased (0.426815 --> 0.426796).  Saving model ...
Validation loss decreased (0.426796 --> 0.426776).  Saving model ...
Validation loss decreased (0.426776 --> 0.426757).  Saving model ...
Validation loss decreased (0.426757 --> 0.426737).  Saving model ...
Validation loss decreased (0.426737 --> 0.426718).  Saving model ...
Validation loss decreased (0.426718 --> 0.426699).  Saving model ...
Validation loss decreased (0.426699 --> 0.426679).  Saving model ...
Validation loss decreased (0.426679 --> 0.426660).  Saving model ...
Validation loss decreased (0.426660 --> 0.426641).  Saving model ...
Validation loss decreased (0.426641 --> 0.426622).  Saving model ...
Validation loss decreased (0.426622 --> 0.426602).  Saving model ...
Validation loss decreased (0.426602 --> 0.426583).  Saving model ...
Validation loss decreased (0.426583 --> 0.426564).  Saving model ...
Validation loss decreased (0.426564 --> 0.426545).  Saving model ...
Validation loss decreased (0.426545 --> 0.426525).  Saving model ...
Validation loss decreased (0.426525 --> 0.426506).  Saving model ...
Validation loss decreased (0.426506 --> 0.426487).  Saving model ...
Validation loss decreased (0.426487 --> 0.426468).  Saving model ...
Validation loss decreased (0.426468 --> 0.426449).  Saving model ...
Validation loss decreased (0.426449 --> 0.426430).  Saving model ...
Validation loss decreased (0.426430 --> 0.426410).  Saving model ...
Validation loss decreased (0.426410 --> 0.426391).  Saving model ...
Validation loss decreased (0.426391 --> 0.426372).  Saving model ...
Validation loss decreased (0.426372 --> 0.426353).  Saving model ...
Validation loss decreased (0.426353 --> 0.426334).  Saving model ...
Validation loss decreased (0.426334 --> 0.426315).  Saving model ...
Validation loss decreased (0.426315 --> 0.426296).  Saving model ...
epoch 3001, loss 0.4263, train acc 78.77%, f1 0.6788, precision 0.7198, recall 0.6422, auc 0.7540
Validation loss decreased (0.426296 --> 0.426277).  Saving model ...
Validation loss decreased (0.426277 --> 0.426258).  Saving model ...
Validation loss decreased (0.426258 --> 0.426239).  Saving model ...
Validation loss decreased (0.426239 --> 0.426220).  Saving model ...
Validation loss decreased (0.426220 --> 0.426201).  Saving model ...
Validation loss decreased (0.426201 --> 0.426181).  Saving model ...
Validation loss decreased (0.426181 --> 0.426163).  Saving model ...
Validation loss decreased (0.426163 --> 0.426143).  Saving model ...
Validation loss decreased (0.426143 --> 0.426125).  Saving model ...
Validation loss decreased (0.426125 --> 0.426106).  Saving model ...
Validation loss decreased (0.426106 --> 0.426087).  Saving model ...
Validation loss decreased (0.426087 --> 0.426068).  Saving model ...
Validation loss decreased (0.426068 --> 0.426049).  Saving model ...
Validation loss decreased (0.426049 --> 0.426030).  Saving model ...
Validation loss decreased (0.426030 --> 0.426011).  Saving model ...
Validation loss decreased (0.426011 --> 0.425992).  Saving model ...
Validation loss decreased (0.425992 --> 0.425973).  Saving model ...
Validation loss decreased (0.425973 --> 0.425954).  Saving model ...
Validation loss decreased (0.425954 --> 0.425935).  Saving model ...
Validation loss decreased (0.425935 --> 0.425917).  Saving model ...
Validation loss decreased (0.425917 --> 0.425898).  Saving model ...
Validation loss decreased (0.425898 --> 0.425879).  Saving model ...
Validation loss decreased (0.425879 --> 0.425860).  Saving model ...
Validation loss decreased (0.425860 --> 0.425841).  Saving model ...
Validation loss decreased (0.425841 --> 0.425822).  Saving model ...
Validation loss decreased (0.425822 --> 0.425804).  Saving model ...
Validation loss decreased (0.425804 --> 0.425785).  Saving model ...
Validation loss decreased (0.425785 --> 0.425766).  Saving model ...
Validation loss decreased (0.425766 --> 0.425747).  Saving model ...
Validation loss decreased (0.425747 --> 0.425729).  Saving model ...
Validation loss decreased (0.425729 --> 0.425710).  Saving model ...
Validation loss decreased (0.425710 --> 0.425691).  Saving model ...
Validation loss decreased (0.425691 --> 0.425672).  Saving model ...
Validation loss decreased (0.425672 --> 0.425654).  Saving model ...
Validation loss decreased (0.425654 --> 0.425635).  Saving model ...
Validation loss decreased (0.425635 --> 0.425616).  Saving model ...
Validation loss decreased (0.425616 --> 0.425598).  Saving model ...
Validation loss decreased (0.425598 --> 0.425579).  Saving model ...
Validation loss decreased (0.425579 --> 0.425560).  Saving model ...
Validation loss decreased (0.425560 --> 0.425542).  Saving model ...
Validation loss decreased (0.425542 --> 0.425523).  Saving model ...
Validation loss decreased (0.425523 --> 0.425504).  Saving model ...
Validation loss decreased (0.425504 --> 0.425486).  Saving model ...
Validation loss decreased (0.425486 --> 0.425467).  Saving model ...
Validation loss decreased (0.425467 --> 0.425449).  Saving model ...
Validation loss decreased (0.425449 --> 0.425430).  Saving model ...
Validation loss decreased (0.425430 --> 0.425411).  Saving model ...
Validation loss decreased (0.425411 --> 0.425393).  Saving model ...
Validation loss decreased (0.425393 --> 0.425374).  Saving model ...
Validation loss decreased (0.425374 --> 0.425356).  Saving model ...
Validation loss decreased (0.425356 --> 0.425337).  Saving model ...
Validation loss decreased (0.425337 --> 0.425319).  Saving model ...
Validation loss decreased (0.425319 --> 0.425300).  Saving model ...
Validation loss decreased (0.425300 --> 0.425282).  Saving model ...
Validation loss decreased (0.425282 --> 0.425263).  Saving model ...
Validation loss decreased (0.425263 --> 0.425245).  Saving model ...
Validation loss decreased (0.425245 --> 0.425226).  Saving model ...
Validation loss decreased (0.425226 --> 0.425208).  Saving model ...
Validation loss decreased (0.425208 --> 0.425189).  Saving model ...
Validation loss decreased (0.425189 --> 0.425171).  Saving model ...
Validation loss decreased (0.425171 --> 0.425152).  Saving model ...
Validation loss decreased (0.425152 --> 0.425134).  Saving model ...
Validation loss decreased (0.425134 --> 0.425116).  Saving model ...
Validation loss decreased (0.425116 --> 0.425097).  Saving model ...
Validation loss decreased (0.425097 --> 0.425079).  Saving model ...
Validation loss decreased (0.425079 --> 0.425060).  Saving model ...
Validation loss decreased (0.425060 --> 0.425042).  Saving model ...
Validation loss decreased (0.425042 --> 0.425024).  Saving model ...
Validation loss decreased (0.425024 --> 0.425005).  Saving model ...
Validation loss decreased (0.425005 --> 0.424987).  Saving model ...
Validation loss decreased (0.424987 --> 0.424969).  Saving model ...
Validation loss decreased (0.424969 --> 0.424950).  Saving model ...
Validation loss decreased (0.424950 --> 0.424932).  Saving model ...
Validation loss decreased (0.424932 --> 0.424914).  Saving model ...
Validation loss decreased (0.424914 --> 0.424895).  Saving model ...
Validation loss decreased (0.424895 --> 0.424877).  Saving model ...
Validation loss decreased (0.424877 --> 0.424859).  Saving model ...
Validation loss decreased (0.424859 --> 0.424840).  Saving model ...
Validation loss decreased (0.424840 --> 0.424822).  Saving model ...
Validation loss decreased (0.424822 --> 0.424804).  Saving model ...
Validation loss decreased (0.424804 --> 0.424786).  Saving model ...
Validation loss decreased (0.424786 --> 0.424767).  Saving model ...
Validation loss decreased (0.424767 --> 0.424749).  Saving model ...
Validation loss decreased (0.424749 --> 0.424731).  Saving model ...
Validation loss decreased (0.424731 --> 0.424713).  Saving model ...
Validation loss decreased (0.424713 --> 0.424695).  Saving model ...
Validation loss decreased (0.424695 --> 0.424676).  Saving model ...
Validation loss decreased (0.424676 --> 0.424658).  Saving model ...
Validation loss decreased (0.424658 --> 0.424640).  Saving model ...
Validation loss decreased (0.424640 --> 0.424622).  Saving model ...
Validation loss decreased (0.424622 --> 0.424604).  Saving model ...
Validation loss decreased (0.424604 --> 0.424586).  Saving model ...
Validation loss decreased (0.424586 --> 0.424567).  Saving model ...
Validation loss decreased (0.424567 --> 0.424549).  Saving model ...
Validation loss decreased (0.424549 --> 0.424531).  Saving model ...
Validation loss decreased (0.424531 --> 0.424513).  Saving model ...
Validation loss decreased (0.424513 --> 0.424495).  Saving model ...
Validation loss decreased (0.424495 --> 0.424477).  Saving model ...
Validation loss decreased (0.424477 --> 0.424459).  Saving model ...
Validation loss decreased (0.424459 --> 0.424441).  Saving model ...
epoch 3101, loss 0.4244, train acc 79.28%, f1 0.6857, precision 0.7293, recall 0.6471, auc 0.7591
Validation loss decreased (0.424441 --> 0.424423).  Saving model ...
Validation loss decreased (0.424423 --> 0.424404).  Saving model ...
Validation loss decreased (0.424404 --> 0.424386).  Saving model ...
Validation loss decreased (0.424386 --> 0.424368).  Saving model ...
Validation loss decreased (0.424368 --> 0.424350).  Saving model ...
Validation loss decreased (0.424350 --> 0.424332).  Saving model ...
Validation loss decreased (0.424332 --> 0.424314).  Saving model ...
Validation loss decreased (0.424314 --> 0.424296).  Saving model ...
Validation loss decreased (0.424296 --> 0.424278).  Saving model ...
Validation loss decreased (0.424278 --> 0.424260).  Saving model ...
Validation loss decreased (0.424260 --> 0.424242).  Saving model ...
Validation loss decreased (0.424242 --> 0.424224).  Saving model ...
Validation loss decreased (0.424224 --> 0.424206).  Saving model ...
Validation loss decreased (0.424206 --> 0.424188).  Saving model ...
Validation loss decreased (0.424188 --> 0.424170).  Saving model ...
Validation loss decreased (0.424170 --> 0.424152).  Saving model ...
Validation loss decreased (0.424152 --> 0.424134).  Saving model ...
Validation loss decreased (0.424134 --> 0.424116).  Saving model ...
Validation loss decreased (0.424116 --> 0.424099).  Saving model ...
Validation loss decreased (0.424099 --> 0.424081).  Saving model ...
Validation loss decreased (0.424081 --> 0.424063).  Saving model ...
Validation loss decreased (0.424063 --> 0.424045).  Saving model ...
Validation loss decreased (0.424045 --> 0.424027).  Saving model ...
Validation loss decreased (0.424027 --> 0.424009).  Saving model ...
Validation loss decreased (0.424009 --> 0.423991).  Saving model ...
Validation loss decreased (0.423991 --> 0.423973).  Saving model ...
Validation loss decreased (0.423973 --> 0.423955).  Saving model ...
Validation loss decreased (0.423955 --> 0.423938).  Saving model ...
Validation loss decreased (0.423938 --> 0.423920).  Saving model ...
Validation loss decreased (0.423920 --> 0.423902).  Saving model ...
Validation loss decreased (0.423902 --> 0.423884).  Saving model ...
Validation loss decreased (0.423884 --> 0.423866).  Saving model ...
Validation loss decreased (0.423866 --> 0.423848).  Saving model ...
Validation loss decreased (0.423848 --> 0.423830).  Saving model ...
Validation loss decreased (0.423830 --> 0.423813).  Saving model ...
Validation loss decreased (0.423813 --> 0.423795).  Saving model ...
Validation loss decreased (0.423795 --> 0.423777).  Saving model ...
Validation loss decreased (0.423777 --> 0.423759).  Saving model ...
Validation loss decreased (0.423759 --> 0.423741).  Saving model ...
Validation loss decreased (0.423741 --> 0.423724).  Saving model ...
Validation loss decreased (0.423724 --> 0.423706).  Saving model ...
Validation loss decreased (0.423706 --> 0.423688).  Saving model ...
Validation loss decreased (0.423688 --> 0.423670).  Saving model ...
Validation loss decreased (0.423670 --> 0.423653).  Saving model ...
Validation loss decreased (0.423653 --> 0.423635).  Saving model ...
Validation loss decreased (0.423635 --> 0.423617).  Saving model ...
Validation loss decreased (0.423617 --> 0.423599).  Saving model ...
Validation loss decreased (0.423599 --> 0.423582).  Saving model ...
Validation loss decreased (0.423582 --> 0.423564).  Saving model ...
Validation loss decreased (0.423564 --> 0.423546).  Saving model ...
Validation loss decreased (0.423546 --> 0.423529).  Saving model ...
Validation loss decreased (0.423529 --> 0.423511).  Saving model ...
Validation loss decreased (0.423511 --> 0.423493).  Saving model ...
Validation loss decreased (0.423493 --> 0.423476).  Saving model ...
Validation loss decreased (0.423476 --> 0.423458).  Saving model ...
Validation loss decreased (0.423458 --> 0.423440).  Saving model ...
Validation loss decreased (0.423440 --> 0.423422).  Saving model ...
Validation loss decreased (0.423422 --> 0.423405).  Saving model ...
Validation loss decreased (0.423405 --> 0.423387).  Saving model ...
Validation loss decreased (0.423387 --> 0.423369).  Saving model ...
Validation loss decreased (0.423369 --> 0.423352).  Saving model ...
Validation loss decreased (0.423352 --> 0.423334).  Saving model ...
Validation loss decreased (0.423334 --> 0.423317).  Saving model ...
Validation loss decreased (0.423317 --> 0.423299).  Saving model ...
Validation loss decreased (0.423299 --> 0.423281).  Saving model ...
Validation loss decreased (0.423281 --> 0.423264).  Saving model ...
Validation loss decreased (0.423264 --> 0.423246).  Saving model ...
Validation loss decreased (0.423246 --> 0.423228).  Saving model ...
Validation loss decreased (0.423228 --> 0.423211).  Saving model ...
Validation loss decreased (0.423211 --> 0.423193).  Saving model ...
Validation loss decreased (0.423193 --> 0.423176).  Saving model ...
Validation loss decreased (0.423176 --> 0.423158).  Saving model ...
Validation loss decreased (0.423158 --> 0.423141).  Saving model ...
Validation loss decreased (0.423141 --> 0.423123).  Saving model ...
Validation loss decreased (0.423123 --> 0.423105).  Saving model ...
Validation loss decreased (0.423105 --> 0.423088).  Saving model ...
Validation loss decreased (0.423088 --> 0.423070).  Saving model ...
Validation loss decreased (0.423070 --> 0.423053).  Saving model ...
Validation loss decreased (0.423053 --> 0.423035).  Saving model ...
Validation loss decreased (0.423035 --> 0.423018).  Saving model ...
Validation loss decreased (0.423018 --> 0.423000).  Saving model ...
Validation loss decreased (0.423000 --> 0.422983).  Saving model ...
Validation loss decreased (0.422983 --> 0.422965).  Saving model ...
Validation loss decreased (0.422965 --> 0.422947).  Saving model ...
Validation loss decreased (0.422947 --> 0.422930).  Saving model ...
Validation loss decreased (0.422930 --> 0.422912).  Saving model ...
Validation loss decreased (0.422912 --> 0.422895).  Saving model ...
Validation loss decreased (0.422895 --> 0.422877).  Saving model ...
Validation loss decreased (0.422877 --> 0.422860).  Saving model ...
Validation loss decreased (0.422860 --> 0.422842).  Saving model ...
Validation loss decreased (0.422842 --> 0.422825).  Saving model ...
Validation loss decreased (0.422825 --> 0.422807).  Saving model ...
Validation loss decreased (0.422807 --> 0.422790).  Saving model ...
Validation loss decreased (0.422790 --> 0.422772).  Saving model ...
Validation loss decreased (0.422772 --> 0.422755).  Saving model ...
Validation loss decreased (0.422755 --> 0.422738).  Saving model ...
Validation loss decreased (0.422738 --> 0.422720).  Saving model ...
Validation loss decreased (0.422720 --> 0.422703).  Saving model ...
Validation loss decreased (0.422703 --> 0.422685).  Saving model ...
Validation loss decreased (0.422685 --> 0.422668).  Saving model ...
epoch 3201, loss 0.4227, train acc 79.28%, f1 0.6857, precision 0.7293, recall 0.6471, auc 0.7591
Validation loss decreased (0.422668 --> 0.422650).  Saving model ...
Validation loss decreased (0.422650 --> 0.422633).  Saving model ...
Validation loss decreased (0.422633 --> 0.422615).  Saving model ...
Validation loss decreased (0.422615 --> 0.422598).  Saving model ...
Validation loss decreased (0.422598 --> 0.422581).  Saving model ...
Validation loss decreased (0.422581 --> 0.422563).  Saving model ...
Validation loss decreased (0.422563 --> 0.422546).  Saving model ...
Validation loss decreased (0.422546 --> 0.422528).  Saving model ...
Validation loss decreased (0.422528 --> 0.422511).  Saving model ...
Validation loss decreased (0.422511 --> 0.422493).  Saving model ...
Validation loss decreased (0.422493 --> 0.422476).  Saving model ...
Validation loss decreased (0.422476 --> 0.422459).  Saving model ...
Validation loss decreased (0.422459 --> 0.422441).  Saving model ...
Validation loss decreased (0.422441 --> 0.422424).  Saving model ...
Validation loss decreased (0.422424 --> 0.422406).  Saving model ...
Validation loss decreased (0.422406 --> 0.422389).  Saving model ...
Validation loss decreased (0.422389 --> 0.422372).  Saving model ...
Validation loss decreased (0.422372 --> 0.422354).  Saving model ...
Validation loss decreased (0.422354 --> 0.422337).  Saving model ...
Validation loss decreased (0.422337 --> 0.422320).  Saving model ...
Validation loss decreased (0.422320 --> 0.422302).  Saving model ...
Validation loss decreased (0.422302 --> 0.422285).  Saving model ...
Validation loss decreased (0.422285 --> 0.422267).  Saving model ...
Validation loss decreased (0.422267 --> 0.422250).  Saving model ...
Validation loss decreased (0.422250 --> 0.422233).  Saving model ...
Validation loss decreased (0.422233 --> 0.422215).  Saving model ...
Validation loss decreased (0.422215 --> 0.422198).  Saving model ...
Validation loss decreased (0.422198 --> 0.422181).  Saving model ...
Validation loss decreased (0.422181 --> 0.422163).  Saving model ...
Validation loss decreased (0.422163 --> 0.422146).  Saving model ...
Validation loss decreased (0.422146 --> 0.422129).  Saving model ...
Validation loss decreased (0.422129 --> 0.422111).  Saving model ...
Validation loss decreased (0.422111 --> 0.422094).  Saving model ...
Validation loss decreased (0.422094 --> 0.422076).  Saving model ...
Validation loss decreased (0.422076 --> 0.422059).  Saving model ...
Validation loss decreased (0.422059 --> 0.422042).  Saving model ...
Validation loss decreased (0.422042 --> 0.422025).  Saving model ...
Validation loss decreased (0.422025 --> 0.422007).  Saving model ...
Validation loss decreased (0.422007 --> 0.421990).  Saving model ...
Validation loss decreased (0.421990 --> 0.421973).  Saving model ...
Validation loss decreased (0.421973 --> 0.421955).  Saving model ...
Validation loss decreased (0.421955 --> 0.421938).  Saving model ...
Validation loss decreased (0.421938 --> 0.421921).  Saving model ...
Validation loss decreased (0.421921 --> 0.421903).  Saving model ...
Validation loss decreased (0.421903 --> 0.421886).  Saving model ...
Validation loss decreased (0.421886 --> 0.421869).  Saving model ...
Validation loss decreased (0.421869 --> 0.421851).  Saving model ...
Validation loss decreased (0.421851 --> 0.421834).  Saving model ...
Validation loss decreased (0.421834 --> 0.421817).  Saving model ...
Validation loss decreased (0.421817 --> 0.421799).  Saving model ...
Validation loss decreased (0.421799 --> 0.421782).  Saving model ...
Validation loss decreased (0.421782 --> 0.421765).  Saving model ...
Validation loss decreased (0.421765 --> 0.421748).  Saving model ...
Validation loss decreased (0.421748 --> 0.421730).  Saving model ...
Validation loss decreased (0.421730 --> 0.421713).  Saving model ...
Validation loss decreased (0.421713 --> 0.421696).  Saving model ...
Validation loss decreased (0.421696 --> 0.421678).  Saving model ...
Validation loss decreased (0.421678 --> 0.421661).  Saving model ...
Validation loss decreased (0.421661 --> 0.421644).  Saving model ...
Validation loss decreased (0.421644 --> 0.421627).  Saving model ...
Validation loss decreased (0.421627 --> 0.421609).  Saving model ...
Validation loss decreased (0.421609 --> 0.421592).  Saving model ...
Validation loss decreased (0.421592 --> 0.421575).  Saving model ...
Validation loss decreased (0.421575 --> 0.421557).  Saving model ...
Validation loss decreased (0.421557 --> 0.421540).  Saving model ...
Validation loss decreased (0.421540 --> 0.421523).  Saving model ...
Validation loss decreased (0.421523 --> 0.421506).  Saving model ...
Validation loss decreased (0.421506 --> 0.421488).  Saving model ...
Validation loss decreased (0.421488 --> 0.421471).  Saving model ...
Validation loss decreased (0.421471 --> 0.421454).  Saving model ...
Validation loss decreased (0.421454 --> 0.421437).  Saving model ...
Validation loss decreased (0.421437 --> 0.421419).  Saving model ...
Validation loss decreased (0.421419 --> 0.421402).  Saving model ...
Validation loss decreased (0.421402 --> 0.421385).  Saving model ...
Validation loss decreased (0.421385 --> 0.421367).  Saving model ...
Validation loss decreased (0.421367 --> 0.421350).  Saving model ...
Validation loss decreased (0.421350 --> 0.421333).  Saving model ...
Validation loss decreased (0.421333 --> 0.421316).  Saving model ...
Validation loss decreased (0.421316 --> 0.421298).  Saving model ...
Validation loss decreased (0.421298 --> 0.421281).  Saving model ...
Validation loss decreased (0.421281 --> 0.421264).  Saving model ...
Validation loss decreased (0.421264 --> 0.421247).  Saving model ...
Validation loss decreased (0.421247 --> 0.421229).  Saving model ...
Validation loss decreased (0.421229 --> 0.421212).  Saving model ...
Validation loss decreased (0.421212 --> 0.421195).  Saving model ...
Validation loss decreased (0.421195 --> 0.421178).  Saving model ...
Validation loss decreased (0.421178 --> 0.421160).  Saving model ...
Validation loss decreased (0.421160 --> 0.421143).  Saving model ...
Validation loss decreased (0.421143 --> 0.421126).  Saving model ...
Validation loss decreased (0.421126 --> 0.421109).  Saving model ...
Validation loss decreased (0.421109 --> 0.421091).  Saving model ...
Validation loss decreased (0.421091 --> 0.421074).  Saving model ...
Validation loss decreased (0.421074 --> 0.421057).  Saving model ...
Validation loss decreased (0.421057 --> 0.421039).  Saving model ...
Validation loss decreased (0.421039 --> 0.421022).  Saving model ...
Validation loss decreased (0.421022 --> 0.421005).  Saving model ...
Validation loss decreased (0.421005 --> 0.420988).  Saving model ...
Validation loss decreased (0.420988 --> 0.420970).  Saving model ...
Validation loss decreased (0.420970 --> 0.420953).  Saving model ...
Validation loss decreased (0.420953 --> 0.420936).  Saving model ...
epoch 3301, loss 0.4209, train acc 79.45%, f1 0.6875, precision 0.7333, recall 0.6471, auc 0.7604
Validation loss decreased (0.420936 --> 0.420919).  Saving model ...
Validation loss decreased (0.420919 --> 0.420901).  Saving model ...
Validation loss decreased (0.420901 --> 0.420884).  Saving model ...
Validation loss decreased (0.420884 --> 0.420867).  Saving model ...
Validation loss decreased (0.420867 --> 0.420850).  Saving model ...
Validation loss decreased (0.420850 --> 0.420832).  Saving model ...
Validation loss decreased (0.420832 --> 0.420815).  Saving model ...
Validation loss decreased (0.420815 --> 0.420798).  Saving model ...
Validation loss decreased (0.420798 --> 0.420781).  Saving model ...
Validation loss decreased (0.420781 --> 0.420763).  Saving model ...
Validation loss decreased (0.420763 --> 0.420746).  Saving model ...
Validation loss decreased (0.420746 --> 0.420729).  Saving model ...
Validation loss decreased (0.420729 --> 0.420712).  Saving model ...
Validation loss decreased (0.420712 --> 0.420694).  Saving model ...
Validation loss decreased (0.420694 --> 0.420677).  Saving model ...
Validation loss decreased (0.420677 --> 0.420660).  Saving model ...
Validation loss decreased (0.420660 --> 0.420642).  Saving model ...
Validation loss decreased (0.420642 --> 0.420625).  Saving model ...
Validation loss decreased (0.420625 --> 0.420608).  Saving model ...
Validation loss decreased (0.420608 --> 0.420591).  Saving model ...
Validation loss decreased (0.420591 --> 0.420573).  Saving model ...
Validation loss decreased (0.420573 --> 0.420556).  Saving model ...
Validation loss decreased (0.420556 --> 0.420539).  Saving model ...
Validation loss decreased (0.420539 --> 0.420522).  Saving model ...
Validation loss decreased (0.420522 --> 0.420504).  Saving model ...
Validation loss decreased (0.420504 --> 0.420487).  Saving model ...
Validation loss decreased (0.420487 --> 0.420470).  Saving model ...
Validation loss decreased (0.420470 --> 0.420452).  Saving model ...
Validation loss decreased (0.420452 --> 0.420435).  Saving model ...
Validation loss decreased (0.420435 --> 0.420418).  Saving model ...
Validation loss decreased (0.420418 --> 0.420401).  Saving model ...
Validation loss decreased (0.420401 --> 0.420383).  Saving model ...
Validation loss decreased (0.420383 --> 0.420366).  Saving model ...
Validation loss decreased (0.420366 --> 0.420349).  Saving model ...
Validation loss decreased (0.420349 --> 0.420331).  Saving model ...
Validation loss decreased (0.420331 --> 0.420314).  Saving model ...
Validation loss decreased (0.420314 --> 0.420297).  Saving model ...
Validation loss decreased (0.420297 --> 0.420280).  Saving model ...
Validation loss decreased (0.420280 --> 0.420262).  Saving model ...
Validation loss decreased (0.420262 --> 0.420245).  Saving model ...
Validation loss decreased (0.420245 --> 0.420228).  Saving model ...
Validation loss decreased (0.420228 --> 0.420210).  Saving model ...
Validation loss decreased (0.420210 --> 0.420193).  Saving model ...
Validation loss decreased (0.420193 --> 0.420176).  Saving model ...
Validation loss decreased (0.420176 --> 0.420158).  Saving model ...
Validation loss decreased (0.420158 --> 0.420141).  Saving model ...
Validation loss decreased (0.420141 --> 0.420124).  Saving model ...
Validation loss decreased (0.420124 --> 0.420107).  Saving model ...
Validation loss decreased (0.420107 --> 0.420089).  Saving model ...
Validation loss decreased (0.420089 --> 0.420072).  Saving model ...
Validation loss decreased (0.420072 --> 0.420055).  Saving model ...
Validation loss decreased (0.420055 --> 0.420037).  Saving model ...
Validation loss decreased (0.420037 --> 0.420020).  Saving model ...
Validation loss decreased (0.420020 --> 0.420003).  Saving model ...
Validation loss decreased (0.420003 --> 0.419985).  Saving model ...
Validation loss decreased (0.419985 --> 0.419968).  Saving model ...
Validation loss decreased (0.419968 --> 0.419951).  Saving model ...
Validation loss decreased (0.419951 --> 0.419933).  Saving model ...
Validation loss decreased (0.419933 --> 0.419916).  Saving model ...
Validation loss decreased (0.419916 --> 0.419899).  Saving model ...
Validation loss decreased (0.419899 --> 0.419881).  Saving model ...
Validation loss decreased (0.419881 --> 0.419864).  Saving model ...
Validation loss decreased (0.419864 --> 0.419847).  Saving model ...
Validation loss decreased (0.419847 --> 0.419829).  Saving model ...
Validation loss decreased (0.419829 --> 0.419812).  Saving model ...
Validation loss decreased (0.419812 --> 0.419795).  Saving model ...
Validation loss decreased (0.419795 --> 0.419777).  Saving model ...
Validation loss decreased (0.419777 --> 0.419760).  Saving model ...
Validation loss decreased (0.419760 --> 0.419742).  Saving model ...
Validation loss decreased (0.419742 --> 0.419725).  Saving model ...
Validation loss decreased (0.419725 --> 0.419708).  Saving model ...
Validation loss decreased (0.419708 --> 0.419690).  Saving model ...
Validation loss decreased (0.419690 --> 0.419673).  Saving model ...
Validation loss decreased (0.419673 --> 0.419656).  Saving model ...
Validation loss decreased (0.419656 --> 0.419638).  Saving model ...
Validation loss decreased (0.419638 --> 0.419621).  Saving model ...
Validation loss decreased (0.419621 --> 0.419603).  Saving model ...
Validation loss decreased (0.419603 --> 0.419586).  Saving model ...
Validation loss decreased (0.419586 --> 0.419569).  Saving model ...
Validation loss decreased (0.419569 --> 0.419551).  Saving model ...
Validation loss decreased (0.419551 --> 0.419534).  Saving model ...
Validation loss decreased (0.419534 --> 0.419516).  Saving model ...
Validation loss decreased (0.419516 --> 0.419499).  Saving model ...
Validation loss decreased (0.419499 --> 0.419482).  Saving model ...
Validation loss decreased (0.419482 --> 0.419464).  Saving model ...
Validation loss decreased (0.419464 --> 0.419447).  Saving model ...
Validation loss decreased (0.419447 --> 0.419429).  Saving model ...
Validation loss decreased (0.419429 --> 0.419412).  Saving model ...
Validation loss decreased (0.419412 --> 0.419395).  Saving model ...
Validation loss decreased (0.419395 --> 0.419377).  Saving model ...
Validation loss decreased (0.419377 --> 0.419360).  Saving model ...
Validation loss decreased (0.419360 --> 0.419342).  Saving model ...
Validation loss decreased (0.419342 --> 0.419325).  Saving model ...
Validation loss decreased (0.419325 --> 0.419307).  Saving model ...
Validation loss decreased (0.419307 --> 0.419290).  Saving model ...
Validation loss decreased (0.419290 --> 0.419273).  Saving model ...
Validation loss decreased (0.419273 --> 0.419255).  Saving model ...
Validation loss decreased (0.419255 --> 0.419238).  Saving model ...
Validation loss decreased (0.419238 --> 0.419220).  Saving model ...
Validation loss decreased (0.419220 --> 0.419203).  Saving model ...
epoch 3401, loss 0.4192, train acc 79.62%, f1 0.6893, precision 0.7374, recall 0.6471, auc 0.7617
Validation loss decreased (0.419203 --> 0.419185).  Saving model ...
Validation loss decreased (0.419185 --> 0.419168).  Saving model ...
Validation loss decreased (0.419168 --> 0.419150).  Saving model ...
Validation loss decreased (0.419150 --> 0.419133).  Saving model ...
Validation loss decreased (0.419133 --> 0.419115).  Saving model ...
Validation loss decreased (0.419115 --> 0.419098).  Saving model ...
Validation loss decreased (0.419098 --> 0.419080).  Saving model ...
Validation loss decreased (0.419080 --> 0.419063).  Saving model ...
Validation loss decreased (0.419063 --> 0.419045).  Saving model ...
Validation loss decreased (0.419045 --> 0.419028).  Saving model ...
Validation loss decreased (0.419028 --> 0.419010).  Saving model ...
Validation loss decreased (0.419010 --> 0.418993).  Saving model ...
Validation loss decreased (0.418993 --> 0.418975).  Saving model ...
Validation loss decreased (0.418975 --> 0.418958).  Saving model ...
Validation loss decreased (0.418958 --> 0.418940).  Saving model ...
Validation loss decreased (0.418940 --> 0.418923).  Saving model ...
Validation loss decreased (0.418923 --> 0.418905).  Saving model ...
Validation loss decreased (0.418905 --> 0.418888).  Saving model ...
Validation loss decreased (0.418888 --> 0.418870).  Saving model ...
Validation loss decreased (0.418870 --> 0.418853).  Saving model ...
Validation loss decreased (0.418853 --> 0.418835).  Saving model ...
Validation loss decreased (0.418835 --> 0.418818).  Saving model ...
Validation loss decreased (0.418818 --> 0.418800).  Saving model ...
Validation loss decreased (0.418800 --> 0.418782).  Saving model ...
Validation loss decreased (0.418782 --> 0.418765).  Saving model ...
Validation loss decreased (0.418765 --> 0.418747).  Saving model ...
Validation loss decreased (0.418747 --> 0.418730).  Saving model ...
Validation loss decreased (0.418730 --> 0.418712).  Saving model ...
Validation loss decreased (0.418712 --> 0.418695).  Saving model ...
Validation loss decreased (0.418695 --> 0.418677).  Saving model ...
Validation loss decreased (0.418677 --> 0.418659).  Saving model ...
Validation loss decreased (0.418659 --> 0.418642).  Saving model ...
Validation loss decreased (0.418642 --> 0.418624).  Saving model ...
Validation loss decreased (0.418624 --> 0.418607).  Saving model ...
Validation loss decreased (0.418607 --> 0.418589).  Saving model ...
Validation loss decreased (0.418589 --> 0.418571).  Saving model ...
Validation loss decreased (0.418571 --> 0.418554).  Saving model ...
Validation loss decreased (0.418554 --> 0.418536).  Saving model ...
Validation loss decreased (0.418536 --> 0.418519).  Saving model ...
Validation loss decreased (0.418519 --> 0.418501).  Saving model ...
Validation loss decreased (0.418501 --> 0.418483).  Saving model ...
Validation loss decreased (0.418483 --> 0.418466).  Saving model ...
Validation loss decreased (0.418466 --> 0.418448).  Saving model ...
Validation loss decreased (0.418448 --> 0.418430).  Saving model ...
Validation loss decreased (0.418430 --> 0.418413).  Saving model ...
Validation loss decreased (0.418413 --> 0.418395).  Saving model ...
Validation loss decreased (0.418395 --> 0.418377).  Saving model ...
Validation loss decreased (0.418377 --> 0.418360).  Saving model ...
Validation loss decreased (0.418360 --> 0.418342).  Saving model ...
Validation loss decreased (0.418342 --> 0.418324).  Saving model ...
Validation loss decreased (0.418324 --> 0.418307).  Saving model ...
Validation loss decreased (0.418307 --> 0.418289).  Saving model ...
Validation loss decreased (0.418289 --> 0.418271).  Saving model ...
Validation loss decreased (0.418271 --> 0.418254).  Saving model ...
Validation loss decreased (0.418254 --> 0.418236).  Saving model ...
Validation loss decreased (0.418236 --> 0.418218).  Saving model ...
Validation loss decreased (0.418218 --> 0.418200).  Saving model ...
Validation loss decreased (0.418200 --> 0.418183).  Saving model ...
Validation loss decreased (0.418183 --> 0.418165).  Saving model ...
Validation loss decreased (0.418165 --> 0.418147).  Saving model ...
Validation loss decreased (0.418147 --> 0.418129).  Saving model ...
Validation loss decreased (0.418129 --> 0.418112).  Saving model ...
Validation loss decreased (0.418112 --> 0.418094).  Saving model ...
Validation loss decreased (0.418094 --> 0.418076).  Saving model ...
Validation loss decreased (0.418076 --> 0.418058).  Saving model ...
Validation loss decreased (0.418058 --> 0.418041).  Saving model ...
Validation loss decreased (0.418041 --> 0.418023).  Saving model ...
Validation loss decreased (0.418023 --> 0.418005).  Saving model ...
Validation loss decreased (0.418005 --> 0.417987).  Saving model ...
Validation loss decreased (0.417987 --> 0.417969).  Saving model ...
Validation loss decreased (0.417969 --> 0.417952).  Saving model ...
Validation loss decreased (0.417952 --> 0.417934).  Saving model ...
Validation loss decreased (0.417934 --> 0.417916).  Saving model ...
Validation loss decreased (0.417916 --> 0.417898).  Saving model ...
Validation loss decreased (0.417898 --> 0.417880).  Saving model ...
Validation loss decreased (0.417880 --> 0.417863).  Saving model ...
Validation loss decreased (0.417863 --> 0.417845).  Saving model ...
Validation loss decreased (0.417845 --> 0.417827).  Saving model ...
Validation loss decreased (0.417827 --> 0.417809).  Saving model ...
Validation loss decreased (0.417809 --> 0.417791).  Saving model ...
Validation loss decreased (0.417791 --> 0.417773).  Saving model ...
Validation loss decreased (0.417773 --> 0.417755).  Saving model ...
Validation loss decreased (0.417755 --> 0.417738).  Saving model ...
Validation loss decreased (0.417738 --> 0.417720).  Saving model ...
Validation loss decreased (0.417720 --> 0.417702).  Saving model ...
Validation loss decreased (0.417702 --> 0.417684).  Saving model ...
Validation loss decreased (0.417684 --> 0.417666).  Saving model ...
Validation loss decreased (0.417666 --> 0.417648).  Saving model ...
Validation loss decreased (0.417648 --> 0.417630).  Saving model ...
Validation loss decreased (0.417630 --> 0.417612).  Saving model ...
Validation loss decreased (0.417612 --> 0.417594).  Saving model ...
Validation loss decreased (0.417594 --> 0.417576).  Saving model ...
Validation loss decreased (0.417576 --> 0.417558).  Saving model ...
Validation loss decreased (0.417558 --> 0.417540).  Saving model ...
Validation loss decreased (0.417540 --> 0.417522).  Saving model ...
Validation loss decreased (0.417522 --> 0.417504).  Saving model ...
Validation loss decreased (0.417504 --> 0.417486).  Saving model ...
Validation loss decreased (0.417486 --> 0.417468).  Saving model ...
Validation loss decreased (0.417468 --> 0.417450).  Saving model ...
Validation loss decreased (0.417450 --> 0.417432).  Saving model ...
epoch 3501, loss 0.4174, train acc 79.45%, f1 0.6859, precision 0.7360, recall 0.6422, auc 0.7592
Validation loss decreased (0.417432 --> 0.417414).  Saving model ...
Validation loss decreased (0.417414 --> 0.417396).  Saving model ...
Validation loss decreased (0.417396 --> 0.417378).  Saving model ...
Validation loss decreased (0.417378 --> 0.417360).  Saving model ...
Validation loss decreased (0.417360 --> 0.417342).  Saving model ...
Validation loss decreased (0.417342 --> 0.417324).  Saving model ...
Validation loss decreased (0.417324 --> 0.417306).  Saving model ...
Validation loss decreased (0.417306 --> 0.417288).  Saving model ...
Validation loss decreased (0.417288 --> 0.417270).  Saving model ...
Validation loss decreased (0.417270 --> 0.417252).  Saving model ...
Validation loss decreased (0.417252 --> 0.417234).  Saving model ...
Validation loss decreased (0.417234 --> 0.417216).  Saving model ...
Validation loss decreased (0.417216 --> 0.417197).  Saving model ...
Validation loss decreased (0.417197 --> 0.417179).  Saving model ...
Validation loss decreased (0.417179 --> 0.417161).  Saving model ...
Validation loss decreased (0.417161 --> 0.417143).  Saving model ...
Validation loss decreased (0.417143 --> 0.417125).  Saving model ...
Validation loss decreased (0.417125 --> 0.417107).  Saving model ...
Validation loss decreased (0.417107 --> 0.417088).  Saving model ...
Validation loss decreased (0.417088 --> 0.417070).  Saving model ...
Validation loss decreased (0.417070 --> 0.417052).  Saving model ...
Validation loss decreased (0.417052 --> 0.417034).  Saving model ...
Validation loss decreased (0.417034 --> 0.417015).  Saving model ...
Validation loss decreased (0.417015 --> 0.416997).  Saving model ...
Validation loss decreased (0.416997 --> 0.416979).  Saving model ...
Validation loss decreased (0.416979 --> 0.416961).  Saving model ...
Validation loss decreased (0.416961 --> 0.416942).  Saving model ...
Validation loss decreased (0.416942 --> 0.416924).  Saving model ...
Validation loss decreased (0.416924 --> 0.416906).  Saving model ...
Validation loss decreased (0.416906 --> 0.416888).  Saving model ...
Validation loss decreased (0.416888 --> 0.416869).  Saving model ...
Validation loss decreased (0.416869 --> 0.416851).  Saving model ...
Validation loss decreased (0.416851 --> 0.416832).  Saving model ...
Validation loss decreased (0.416832 --> 0.416814).  Saving model ...
Validation loss decreased (0.416814 --> 0.416796).  Saving model ...
Validation loss decreased (0.416796 --> 0.416777).  Saving model ...
Validation loss decreased (0.416777 --> 0.416759).  Saving model ...
Validation loss decreased (0.416759 --> 0.416741).  Saving model ...
Validation loss decreased (0.416741 --> 0.416722).  Saving model ...
Validation loss decreased (0.416722 --> 0.416704).  Saving model ...
Validation loss decreased (0.416704 --> 0.416685).  Saving model ...
Validation loss decreased (0.416685 --> 0.416667).  Saving model ...
Validation loss decreased (0.416667 --> 0.416648).  Saving model ...
Validation loss decreased (0.416648 --> 0.416630).  Saving model ...
Validation loss decreased (0.416630 --> 0.416611).  Saving model ...
Validation loss decreased (0.416611 --> 0.416593).  Saving model ...
Validation loss decreased (0.416593 --> 0.416574).  Saving model ...
Validation loss decreased (0.416574 --> 0.416556).  Saving model ...
Validation loss decreased (0.416556 --> 0.416537).  Saving model ...
Validation loss decreased (0.416537 --> 0.416519).  Saving model ...
Validation loss decreased (0.416519 --> 0.416500).  Saving model ...
Validation loss decreased (0.416500 --> 0.416482).  Saving model ...
Validation loss decreased (0.416482 --> 0.416463).  Saving model ...
Validation loss decreased (0.416463 --> 0.416445).  Saving model ...
Validation loss decreased (0.416445 --> 0.416426).  Saving model ...
Validation loss decreased (0.416426 --> 0.416408).  Saving model ...
Validation loss decreased (0.416408 --> 0.416389).  Saving model ...
Validation loss decreased (0.416389 --> 0.416370).  Saving model ...
Validation loss decreased (0.416370 --> 0.416352).  Saving model ...
Validation loss decreased (0.416352 --> 0.416333).  Saving model ...
Validation loss decreased (0.416333 --> 0.416314).  Saving model ...
Validation loss decreased (0.416314 --> 0.416296).  Saving model ...
Validation loss decreased (0.416296 --> 0.416277).  Saving model ...
Validation loss decreased (0.416277 --> 0.416259).  Saving model ...
Validation loss decreased (0.416259 --> 0.416240).  Saving model ...
Validation loss decreased (0.416240 --> 0.416221).  Saving model ...
Validation loss decreased (0.416221 --> 0.416202).  Saving model ...
Validation loss decreased (0.416202 --> 0.416184).  Saving model ...
Validation loss decreased (0.416184 --> 0.416165).  Saving model ...
Validation loss decreased (0.416165 --> 0.416146).  Saving model ...
Validation loss decreased (0.416146 --> 0.416128).  Saving model ...
Validation loss decreased (0.416128 --> 0.416109).  Saving model ...
Validation loss decreased (0.416109 --> 0.416090).  Saving model ...
Validation loss decreased (0.416090 --> 0.416072).  Saving model ...
Validation loss decreased (0.416072 --> 0.416053).  Saving model ...
Validation loss decreased (0.416053 --> 0.416034).  Saving model ...
Validation loss decreased (0.416034 --> 0.416016).  Saving model ...
Validation loss decreased (0.416016 --> 0.415997).  Saving model ...
Validation loss decreased (0.415997 --> 0.415978).  Saving model ...
Validation loss decreased (0.415978 --> 0.415959).  Saving model ...
Validation loss decreased (0.415959 --> 0.415941).  Saving model ...
Validation loss decreased (0.415941 --> 0.415922).  Saving model ...
Validation loss decreased (0.415922 --> 0.415903).  Saving model ...
Validation loss decreased (0.415903 --> 0.415884).  Saving model ...
Validation loss decreased (0.415884 --> 0.415866).  Saving model ...
Validation loss decreased (0.415866 --> 0.415847).  Saving model ...
Validation loss decreased (0.415847 --> 0.415828).  Saving model ...
Validation loss decreased (0.415828 --> 0.415810).  Saving model ...
Validation loss decreased (0.415810 --> 0.415791).  Saving model ...
Validation loss decreased (0.415791 --> 0.415772).  Saving model ...
Validation loss decreased (0.415772 --> 0.415753).  Saving model ...
Validation loss decreased (0.415753 --> 0.415735).  Saving model ...
Validation loss decreased (0.415735 --> 0.415716).  Saving model ...
Validation loss decreased (0.415716 --> 0.415697).  Saving model ...
Validation loss decreased (0.415697 --> 0.415679).  Saving model ...
Validation loss decreased (0.415679 --> 0.415660).  Saving model ...
Validation loss decreased (0.415660 --> 0.415641).  Saving model ...
Validation loss decreased (0.415641 --> 0.415622).  Saving model ...
Validation loss decreased (0.415622 --> 0.415604).  Saving model ...
Validation loss decreased (0.415604 --> 0.415585).  Saving model ...
epoch 3601, loss 0.4156, train acc 79.62%, f1 0.6893, precision 0.7374, recall 0.6471, auc 0.7617
Validation loss decreased (0.415585 --> 0.415566).  Saving model ...
Validation loss decreased (0.415566 --> 0.415548).  Saving model ...
Validation loss decreased (0.415548 --> 0.415529).  Saving model ...
Validation loss decreased (0.415529 --> 0.415510).  Saving model ...
Validation loss decreased (0.415510 --> 0.415492).  Saving model ...
Validation loss decreased (0.415492 --> 0.415473).  Saving model ...
Validation loss decreased (0.415473 --> 0.415455).  Saving model ...
Validation loss decreased (0.415455 --> 0.415436).  Saving model ...
Validation loss decreased (0.415436 --> 0.415417).  Saving model ...
Validation loss decreased (0.415417 --> 0.415399).  Saving model ...
Validation loss decreased (0.415399 --> 0.415380).  Saving model ...
Validation loss decreased (0.415380 --> 0.415361).  Saving model ...
Validation loss decreased (0.415361 --> 0.415343).  Saving model ...
Validation loss decreased (0.415343 --> 0.415324).  Saving model ...
Validation loss decreased (0.415324 --> 0.415306).  Saving model ...
Validation loss decreased (0.415306 --> 0.415287).  Saving model ...
Validation loss decreased (0.415287 --> 0.415268).  Saving model ...
Validation loss decreased (0.415268 --> 0.415250).  Saving model ...
Validation loss decreased (0.415250 --> 0.415231).  Saving model ...
Validation loss decreased (0.415231 --> 0.415213).  Saving model ...
Validation loss decreased (0.415213 --> 0.415194).  Saving model ...
Validation loss decreased (0.415194 --> 0.415176).  Saving model ...
Validation loss decreased (0.415176 --> 0.415157).  Saving model ...
Validation loss decreased (0.415157 --> 0.415139).  Saving model ...
Validation loss decreased (0.415139 --> 0.415120).  Saving model ...
Validation loss decreased (0.415120 --> 0.415102).  Saving model ...
Validation loss decreased (0.415102 --> 0.415083).  Saving model ...
Validation loss decreased (0.415083 --> 0.415065).  Saving model ...
Validation loss decreased (0.415065 --> 0.415046).  Saving model ...
Validation loss decreased (0.415046 --> 0.415028).  Saving model ...
Validation loss decreased (0.415028 --> 0.415009).  Saving model ...
Validation loss decreased (0.415009 --> 0.414991).  Saving model ...
Validation loss decreased (0.414991 --> 0.414973).  Saving model ...
Validation loss decreased (0.414973 --> 0.414954).  Saving model ...
Validation loss decreased (0.414954 --> 0.414936).  Saving model ...
Validation loss decreased (0.414936 --> 0.414917).  Saving model ...
Validation loss decreased (0.414917 --> 0.414899).  Saving model ...
Validation loss decreased (0.414899 --> 0.414881).  Saving model ...
Validation loss decreased (0.414881 --> 0.414862).  Saving model ...
Validation loss decreased (0.414862 --> 0.414844).  Saving model ...
Validation loss decreased (0.414844 --> 0.414825).  Saving model ...
Validation loss decreased (0.414825 --> 0.414807).  Saving model ...
Validation loss decreased (0.414807 --> 0.414789).  Saving model ...
Validation loss decreased (0.414789 --> 0.414770).  Saving model ...
Validation loss decreased (0.414770 --> 0.414752).  Saving model ...
Validation loss decreased (0.414752 --> 0.414734).  Saving model ...
Validation loss decreased (0.414734 --> 0.414715).  Saving model ...
Validation loss decreased (0.414715 --> 0.414697).  Saving model ...
Validation loss decreased (0.414697 --> 0.414679).  Saving model ...
Validation loss decreased (0.414679 --> 0.414661).  Saving model ...
Validation loss decreased (0.414661 --> 0.414642).  Saving model ...
Validation loss decreased (0.414642 --> 0.414624).  Saving model ...
Validation loss decreased (0.414624 --> 0.414606).  Saving model ...
Validation loss decreased (0.414606 --> 0.414588).  Saving model ...
Validation loss decreased (0.414588 --> 0.414569).  Saving model ...
Validation loss decreased (0.414569 --> 0.414551).  Saving model ...
Validation loss decreased (0.414551 --> 0.414533).  Saving model ...
Validation loss decreased (0.414533 --> 0.414515).  Saving model ...
Validation loss decreased (0.414515 --> 0.414496).  Saving model ...
Validation loss decreased (0.414496 --> 0.414478).  Saving model ...
Validation loss decreased (0.414478 --> 0.414460).  Saving model ...
Validation loss decreased (0.414460 --> 0.414442).  Saving model ...
Validation loss decreased (0.414442 --> 0.414423).  Saving model ...
Validation loss decreased (0.414423 --> 0.414405).  Saving model ...
Validation loss decreased (0.414405 --> 0.414387).  Saving model ...
Validation loss decreased (0.414387 --> 0.414369).  Saving model ...
Validation loss decreased (0.414369 --> 0.414351).  Saving model ...
Validation loss decreased (0.414351 --> 0.414333).  Saving model ...
Validation loss decreased (0.414333 --> 0.414314).  Saving model ...
Validation loss decreased (0.414314 --> 0.414296).  Saving model ...
Validation loss decreased (0.414296 --> 0.414278).  Saving model ...
Validation loss decreased (0.414278 --> 0.414260).  Saving model ...
Validation loss decreased (0.414260 --> 0.414242).  Saving model ...
Validation loss decreased (0.414242 --> 0.414224).  Saving model ...
Validation loss decreased (0.414224 --> 0.414206).  Saving model ...
Validation loss decreased (0.414206 --> 0.414187).  Saving model ...
Validation loss decreased (0.414187 --> 0.414169).  Saving model ...
Validation loss decreased (0.414169 --> 0.414151).  Saving model ...
Validation loss decreased (0.414151 --> 0.414133).  Saving model ...
Validation loss decreased (0.414133 --> 0.414115).  Saving model ...
Validation loss decreased (0.414115 --> 0.414097).  Saving model ...
Validation loss decreased (0.414097 --> 0.414079).  Saving model ...
Validation loss decreased (0.414079 --> 0.414061).  Saving model ...
Validation loss decreased (0.414061 --> 0.414042).  Saving model ...
Validation loss decreased (0.414042 --> 0.414024).  Saving model ...
Validation loss decreased (0.414024 --> 0.414006).  Saving model ...
Validation loss decreased (0.414006 --> 0.413988).  Saving model ...
Validation loss decreased (0.413988 --> 0.413970).  Saving model ...
Validation loss decreased (0.413970 --> 0.413952).  Saving model ...
Validation loss decreased (0.413952 --> 0.413934).  Saving model ...
Validation loss decreased (0.413934 --> 0.413916).  Saving model ...
Validation loss decreased (0.413916 --> 0.413898).  Saving model ...
Validation loss decreased (0.413898 --> 0.413880).  Saving model ...
Validation loss decreased (0.413880 --> 0.413862).  Saving model ...
Validation loss decreased (0.413862 --> 0.413844).  Saving model ...
Validation loss decreased (0.413844 --> 0.413826).  Saving model ...
Validation loss decreased (0.413826 --> 0.413807).  Saving model ...
Validation loss decreased (0.413807 --> 0.413789).  Saving model ...
Validation loss decreased (0.413789 --> 0.413771).  Saving model ...
Validation loss decreased (0.413771 --> 0.413753).  Saving model ...
epoch 3701, loss 0.4138, train acc 79.79%, f1 0.6943, precision 0.7363, recall 0.6569, auc 0.7653
Validation loss decreased (0.413753 --> 0.413735).  Saving model ...
Validation loss decreased (0.413735 --> 0.413717).  Saving model ...
Validation loss decreased (0.413717 --> 0.413699).  Saving model ...
Validation loss decreased (0.413699 --> 0.413681).  Saving model ...
Validation loss decreased (0.413681 --> 0.413663).  Saving model ...
Validation loss decreased (0.413663 --> 0.413645).  Saving model ...
Validation loss decreased (0.413645 --> 0.413627).  Saving model ...
Validation loss decreased (0.413627 --> 0.413609).  Saving model ...
Validation loss decreased (0.413609 --> 0.413591).  Saving model ...
Validation loss decreased (0.413591 --> 0.413573).  Saving model ...
Validation loss decreased (0.413573 --> 0.413555).  Saving model ...
Validation loss decreased (0.413555 --> 0.413537).  Saving model ...
Validation loss decreased (0.413537 --> 0.413518).  Saving model ...
Validation loss decreased (0.413518 --> 0.413500).  Saving model ...
Validation loss decreased (0.413500 --> 0.413482).  Saving model ...
Validation loss decreased (0.413482 --> 0.413464).  Saving model ...
Validation loss decreased (0.413464 --> 0.413446).  Saving model ...
Validation loss decreased (0.413446 --> 0.413428).  Saving model ...
Validation loss decreased (0.413428 --> 0.413410).  Saving model ...
Validation loss decreased (0.413410 --> 0.413392).  Saving model ...
Validation loss decreased (0.413392 --> 0.413374).  Saving model ...
Validation loss decreased (0.413374 --> 0.413356).  Saving model ...
Validation loss decreased (0.413356 --> 0.413338).  Saving model ...
Validation loss decreased (0.413338 --> 0.413320).  Saving model ...
Validation loss decreased (0.413320 --> 0.413302).  Saving model ...
Validation loss decreased (0.413302 --> 0.413284).  Saving model ...
Validation loss decreased (0.413284 --> 0.413266).  Saving model ...
Validation loss decreased (0.413266 --> 0.413247).  Saving model ...
Validation loss decreased (0.413247 --> 0.413229).  Saving model ...
Validation loss decreased (0.413229 --> 0.413211).  Saving model ...
Validation loss decreased (0.413211 --> 0.413193).  Saving model ...
Validation loss decreased (0.413193 --> 0.413175).  Saving model ...
Validation loss decreased (0.413175 --> 0.413157).  Saving model ...
Validation loss decreased (0.413157 --> 0.413139).  Saving model ...
Validation loss decreased (0.413139 --> 0.413121).  Saving model ...
Validation loss decreased (0.413121 --> 0.413103).  Saving model ...
Validation loss decreased (0.413103 --> 0.413085).  Saving model ...
Validation loss decreased (0.413085 --> 0.413067).  Saving model ...
Validation loss decreased (0.413067 --> 0.413048).  Saving model ...
Validation loss decreased (0.413048 --> 0.413030).  Saving model ...
Validation loss decreased (0.413030 --> 0.413012).  Saving model ...
Validation loss decreased (0.413012 --> 0.412994).  Saving model ...
Validation loss decreased (0.412994 --> 0.412976).  Saving model ...
Validation loss decreased (0.412976 --> 0.412958).  Saving model ...
Validation loss decreased (0.412958 --> 0.412940).  Saving model ...
Validation loss decreased (0.412940 --> 0.412921).  Saving model ...
Validation loss decreased (0.412921 --> 0.412903).  Saving model ...
Validation loss decreased (0.412903 --> 0.412885).  Saving model ...
Validation loss decreased (0.412885 --> 0.412867).  Saving model ...
Validation loss decreased (0.412867 --> 0.412849).  Saving model ...
Validation loss decreased (0.412849 --> 0.412830).  Saving model ...
Validation loss decreased (0.412830 --> 0.412812).  Saving model ...
Validation loss decreased (0.412812 --> 0.412794).  Saving model ...
Validation loss decreased (0.412794 --> 0.412776).  Saving model ...
Validation loss decreased (0.412776 --> 0.412758).  Saving model ...
Validation loss decreased (0.412758 --> 0.412739).  Saving model ...
Validation loss decreased (0.412739 --> 0.412721).  Saving model ...
Validation loss decreased (0.412721 --> 0.412703).  Saving model ...
Validation loss decreased (0.412703 --> 0.412685).  Saving model ...
Validation loss decreased (0.412685 --> 0.412666).  Saving model ...
Validation loss decreased (0.412666 --> 0.412648).  Saving model ...
Validation loss decreased (0.412648 --> 0.412630).  Saving model ...
Validation loss decreased (0.412630 --> 0.412612).  Saving model ...
Validation loss decreased (0.412612 --> 0.412593).  Saving model ...
Validation loss decreased (0.412593 --> 0.412575).  Saving model ...
Validation loss decreased (0.412575 --> 0.412557).  Saving model ...
Validation loss decreased (0.412557 --> 0.412538).  Saving model ...
Validation loss decreased (0.412538 --> 0.412520).  Saving model ...
Validation loss decreased (0.412520 --> 0.412502).  Saving model ...
Validation loss decreased (0.412502 --> 0.412483).  Saving model ...
Validation loss decreased (0.412483 --> 0.412465).  Saving model ...
Validation loss decreased (0.412465 --> 0.412447).  Saving model ...
Validation loss decreased (0.412447 --> 0.412428).  Saving model ...
Validation loss decreased (0.412428 --> 0.412410).  Saving model ...
Validation loss decreased (0.412410 --> 0.412391).  Saving model ...
Validation loss decreased (0.412391 --> 0.412373).  Saving model ...
Validation loss decreased (0.412373 --> 0.412355).  Saving model ...
Validation loss decreased (0.412355 --> 0.412336).  Saving model ...
Validation loss decreased (0.412336 --> 0.412318).  Saving model ...
Validation loss decreased (0.412318 --> 0.412299).  Saving model ...
Validation loss decreased (0.412299 --> 0.412281).  Saving model ...
Validation loss decreased (0.412281 --> 0.412262).  Saving model ...
Validation loss decreased (0.412262 --> 0.412244).  Saving model ...
Validation loss decreased (0.412244 --> 0.412225).  Saving model ...
Validation loss decreased (0.412225 --> 0.412207).  Saving model ...
Validation loss decreased (0.412207 --> 0.412188).  Saving model ...
Validation loss decreased (0.412188 --> 0.412169).  Saving model ...
Validation loss decreased (0.412169 --> 0.412151).  Saving model ...
Validation loss decreased (0.412151 --> 0.412132).  Saving model ...
Validation loss decreased (0.412132 --> 0.412113).  Saving model ...
Validation loss decreased (0.412113 --> 0.412095).  Saving model ...
Validation loss decreased (0.412095 --> 0.412076).  Saving model ...
Validation loss decreased (0.412076 --> 0.412057).  Saving model ...
Validation loss decreased (0.412057 --> 0.412039).  Saving model ...
Validation loss decreased (0.412039 --> 0.412020).  Saving model ...
Validation loss decreased (0.412020 --> 0.412001).  Saving model ...
Validation loss decreased (0.412001 --> 0.411983).  Saving model ...
Validation loss decreased (0.411983 --> 0.411964).  Saving model ...
Validation loss decreased (0.411964 --> 0.411945).  Saving model ...
Validation loss decreased (0.411945 --> 0.411926).  Saving model ...
epoch 3801, loss 0.4119, train acc 79.79%, f1 0.6943, precision 0.7363, recall 0.6569, auc 0.7653
Validation loss decreased (0.411926 --> 0.411907).  Saving model ...
Validation loss decreased (0.411907 --> 0.411888).  Saving model ...
Validation loss decreased (0.411888 --> 0.411870).  Saving model ...
Validation loss decreased (0.411870 --> 0.411851).  Saving model ...
Validation loss decreased (0.411851 --> 0.411832).  Saving model ...
Validation loss decreased (0.411832 --> 0.411813).  Saving model ...
Validation loss decreased (0.411813 --> 0.411794).  Saving model ...
Validation loss decreased (0.411794 --> 0.411775).  Saving model ...
Validation loss decreased (0.411775 --> 0.411756).  Saving model ...
Validation loss decreased (0.411756 --> 0.411737).  Saving model ...
Validation loss decreased (0.411737 --> 0.411718).  Saving model ...
Validation loss decreased (0.411718 --> 0.411699).  Saving model ...
Validation loss decreased (0.411699 --> 0.411680).  Saving model ...
Validation loss decreased (0.411680 --> 0.411660).  Saving model ...
Validation loss decreased (0.411660 --> 0.411641).  Saving model ...
Validation loss decreased (0.411641 --> 0.411622).  Saving model ...
Validation loss decreased (0.411622 --> 0.411603).  Saving model ...
Validation loss decreased (0.411603 --> 0.411584).  Saving model ...
Validation loss decreased (0.411584 --> 0.411564).  Saving model ...
Validation loss decreased (0.411564 --> 0.411545).  Saving model ...
Validation loss decreased (0.411545 --> 0.411526).  Saving model ...
Validation loss decreased (0.411526 --> 0.411506).  Saving model ...
Validation loss decreased (0.411506 --> 0.411487).  Saving model ...
Validation loss decreased (0.411487 --> 0.411468).  Saving model ...
Validation loss decreased (0.411468 --> 0.411448).  Saving model ...
Validation loss decreased (0.411448 --> 0.411429).  Saving model ...
Validation loss decreased (0.411429 --> 0.411409).  Saving model ...
Validation loss decreased (0.411409 --> 0.411390).  Saving model ...
Validation loss decreased (0.411390 --> 0.411370).  Saving model ...
Validation loss decreased (0.411370 --> 0.411351).  Saving model ...
Validation loss decreased (0.411351 --> 0.411331).  Saving model ...
Validation loss decreased (0.411331 --> 0.411311).  Saving model ...
Validation loss decreased (0.411311 --> 0.411292).  Saving model ...
Validation loss decreased (0.411292 --> 0.411272).  Saving model ...
Validation loss decreased (0.411272 --> 0.411252).  Saving model ...
Validation loss decreased (0.411252 --> 0.411232).  Saving model ...
Validation loss decreased (0.411232 --> 0.411213).  Saving model ...
Validation loss decreased (0.411213 --> 0.411193).  Saving model ...
Validation loss decreased (0.411193 --> 0.411173).  Saving model ...
Validation loss decreased (0.411173 --> 0.411153).  Saving model ...
Validation loss decreased (0.411153 --> 0.411133).  Saving model ...
Validation loss decreased (0.411133 --> 0.411113).  Saving model ...
Validation loss decreased (0.411113 --> 0.411093).  Saving model ...
Validation loss decreased (0.411093 --> 0.411073).  Saving model ...
Validation loss decreased (0.411073 --> 0.411053).  Saving model ...
Validation loss decreased (0.411053 --> 0.411033).  Saving model ...
Validation loss decreased (0.411033 --> 0.411013).  Saving model ...
Validation loss decreased (0.411013 --> 0.410993).  Saving model ...
Validation loss decreased (0.410993 --> 0.410972).  Saving model ...
Validation loss decreased (0.410972 --> 0.410952).  Saving model ...
Validation loss decreased (0.410952 --> 0.410932).  Saving model ...
Validation loss decreased (0.410932 --> 0.410912).  Saving model ...
Validation loss decreased (0.410912 --> 0.410891).  Saving model ...
Validation loss decreased (0.410891 --> 0.410871).  Saving model ...
Validation loss decreased (0.410871 --> 0.410850).  Saving model ...
Validation loss decreased (0.410850 --> 0.410830).  Saving model ...
Validation loss decreased (0.410830 --> 0.410809).  Saving model ...
Validation loss decreased (0.410809 --> 0.410789).  Saving model ...
Validation loss decreased (0.410789 --> 0.410768).  Saving model ...
Validation loss decreased (0.410768 --> 0.410748).  Saving model ...
Validation loss decreased (0.410748 --> 0.410727).  Saving model ...
Validation loss decreased (0.410727 --> 0.410706).  Saving model ...
Validation loss decreased (0.410706 --> 0.410686).  Saving model ...
Validation loss decreased (0.410686 --> 0.410665).  Saving model ...
Validation loss decreased (0.410665 --> 0.410644).  Saving model ...
Validation loss decreased (0.410644 --> 0.410623).  Saving model ...
Validation loss decreased (0.410623 --> 0.410602).  Saving model ...
Validation loss decreased (0.410602 --> 0.410581).  Saving model ...
Validation loss decreased (0.410581 --> 0.410560).  Saving model ...
Validation loss decreased (0.410560 --> 0.410540).  Saving model ...
Validation loss decreased (0.410540 --> 0.410518).  Saving model ...
Validation loss decreased (0.410518 --> 0.410497).  Saving model ...
Validation loss decreased (0.410497 --> 0.410476).  Saving model ...
Validation loss decreased (0.410476 --> 0.410455).  Saving model ...
Validation loss decreased (0.410455 --> 0.410434).  Saving model ...
Validation loss decreased (0.410434 --> 0.410413).  Saving model ...
Validation loss decreased (0.410413 --> 0.410392).  Saving model ...
Validation loss decreased (0.410392 --> 0.410370).  Saving model ...
Validation loss decreased (0.410370 --> 0.410349).  Saving model ...
Validation loss decreased (0.410349 --> 0.410328).  Saving model ...
Validation loss decreased (0.410328 --> 0.410307).  Saving model ...
Validation loss decreased (0.410307 --> 0.410285).  Saving model ...
Validation loss decreased (0.410285 --> 0.410264).  Saving model ...
Validation loss decreased (0.410264 --> 0.410242).  Saving model ...
Validation loss decreased (0.410242 --> 0.410221).  Saving model ...
Validation loss decreased (0.410221 --> 0.410199).  Saving model ...
Validation loss decreased (0.410199 --> 0.410178).  Saving model ...
Validation loss decreased (0.410178 --> 0.410156).  Saving model ...
Validation loss decreased (0.410156 --> 0.410135).  Saving model ...
Validation loss decreased (0.410135 --> 0.410113).  Saving model ...
Validation loss decreased (0.410113 --> 0.410091).  Saving model ...
Validation loss decreased (0.410091 --> 0.410070).  Saving model ...
Validation loss decreased (0.410070 --> 0.410048).  Saving model ...
Validation loss decreased (0.410048 --> 0.410026).  Saving model ...
Validation loss decreased (0.410026 --> 0.410004).  Saving model ...
Validation loss decreased (0.410004 --> 0.409983).  Saving model ...
Validation loss decreased (0.409983 --> 0.409961).  Saving model ...
Validation loss decreased (0.409961 --> 0.409939).  Saving model ...
Validation loss decreased (0.409939 --> 0.409917).  Saving model ...
Validation loss decreased (0.409917 --> 0.409895).  Saving model ...
epoch 3901, loss 0.4099, train acc 80.48%, f1 0.7031, precision 0.7500, recall 0.6618, auc 0.7717
Validation loss decreased (0.409895 --> 0.409873).  Saving model ...
Validation loss decreased (0.409873 --> 0.409851).  Saving model ...
Validation loss decreased (0.409851 --> 0.409829).  Saving model ...
Validation loss decreased (0.409829 --> 0.409807).  Saving model ...
Validation loss decreased (0.409807 --> 0.409785).  Saving model ...
Validation loss decreased (0.409785 --> 0.409763).  Saving model ...
Validation loss decreased (0.409763 --> 0.409741).  Saving model ...
Validation loss decreased (0.409741 --> 0.409719).  Saving model ...
Validation loss decreased (0.409719 --> 0.409697).  Saving model ...
Validation loss decreased (0.409697 --> 0.409674).  Saving model ...
Validation loss decreased (0.409674 --> 0.409652).  Saving model ...
Validation loss decreased (0.409652 --> 0.409630).  Saving model ...
Validation loss decreased (0.409630 --> 0.409608).  Saving model ...
Validation loss decreased (0.409608 --> 0.409586).  Saving model ...
Validation loss decreased (0.409586 --> 0.409563).  Saving model ...
Validation loss decreased (0.409563 --> 0.409541).  Saving model ...
Validation loss decreased (0.409541 --> 0.409519).  Saving model ...
Validation loss decreased (0.409519 --> 0.409496).  Saving model ...
Validation loss decreased (0.409496 --> 0.409474).  Saving model ...
Validation loss decreased (0.409474 --> 0.409452).  Saving model ...
Validation loss decreased (0.409452 --> 0.409429).  Saving model ...
Validation loss decreased (0.409429 --> 0.409407).  Saving model ...
Validation loss decreased (0.409407 --> 0.409384).  Saving model ...
Validation loss decreased (0.409384 --> 0.409362).  Saving model ...
Validation loss decreased (0.409362 --> 0.409339).  Saving model ...
Validation loss decreased (0.409339 --> 0.409317).  Saving model ...
Validation loss decreased (0.409317 --> 0.409294).  Saving model ...
Validation loss decreased (0.409294 --> 0.409272).  Saving model ...
Validation loss decreased (0.409272 --> 0.409249).  Saving model ...
Validation loss decreased (0.409249 --> 0.409227).  Saving model ...
Validation loss decreased (0.409227 --> 0.409204).  Saving model ...
Validation loss decreased (0.409204 --> 0.409182).  Saving model ...
Validation loss decreased (0.409182 --> 0.409159).  Saving model ...
Validation loss decreased (0.409159 --> 0.409136).  Saving model ...
Validation loss decreased (0.409136 --> 0.409114).  Saving model ...
Validation loss decreased (0.409114 --> 0.409091).  Saving model ...
Validation loss decreased (0.409091 --> 0.409068).  Saving model ...
Validation loss decreased (0.409068 --> 0.409046).  Saving model ...
Validation loss decreased (0.409046 --> 0.409023).  Saving model ...
Validation loss decreased (0.409023 --> 0.409000).  Saving model ...
Validation loss decreased (0.409000 --> 0.408978).  Saving model ...
Validation loss decreased (0.408978 --> 0.408955).  Saving model ...
Validation loss decreased (0.408955 --> 0.408932).  Saving model ...
Validation loss decreased (0.408932 --> 0.408909).  Saving model ...
Validation loss decreased (0.408909 --> 0.408886).  Saving model ...
Validation loss decreased (0.408886 --> 0.408863).  Saving model ...
Validation loss decreased (0.408863 --> 0.408841).  Saving model ...
Validation loss decreased (0.408841 --> 0.408818).  Saving model ...
Validation loss decreased (0.408818 --> 0.408795).  Saving model ...
Validation loss decreased (0.408795 --> 0.408772).  Saving model ...
Validation loss decreased (0.408772 --> 0.408749).  Saving model ...
Validation loss decreased (0.408749 --> 0.408726).  Saving model ...
Validation loss decreased (0.408726 --> 0.408703).  Saving model ...
Validation loss decreased (0.408703 --> 0.408680).  Saving model ...
Validation loss decreased (0.408680 --> 0.408657).  Saving model ...
Validation loss decreased (0.408657 --> 0.408634).  Saving model ...
Validation loss decreased (0.408634 --> 0.408611).  Saving model ...
Validation loss decreased (0.408611 --> 0.408588).  Saving model ...
Validation loss decreased (0.408588 --> 0.408565).  Saving model ...
Validation loss decreased (0.408565 --> 0.408542).  Saving model ...
Validation loss decreased (0.408542 --> 0.408519).  Saving model ...
Validation loss decreased (0.408519 --> 0.408496).  Saving model ...
Validation loss decreased (0.408496 --> 0.408473).  Saving model ...
Validation loss decreased (0.408473 --> 0.408450).  Saving model ...
Validation loss decreased (0.408450 --> 0.408427).  Saving model ...
Validation loss decreased (0.408427 --> 0.408403).  Saving model ...
Validation loss decreased (0.408403 --> 0.408380).  Saving model ...
Validation loss decreased (0.408380 --> 0.408357).  Saving model ...
Validation loss decreased (0.408357 --> 0.408334).  Saving model ...
Validation loss decreased (0.408334 --> 0.408311).  Saving model ...
Validation loss decreased (0.408311 --> 0.408287).  Saving model ...
Validation loss decreased (0.408287 --> 0.408264).  Saving model ...
Validation loss decreased (0.408264 --> 0.408241).  Saving model ...
Validation loss decreased (0.408241 --> 0.408218).  Saving model ...
Validation loss decreased (0.408218 --> 0.408194).  Saving model ...
Validation loss decreased (0.408194 --> 0.408171).  Saving model ...
Validation loss decreased (0.408171 --> 0.408148).  Saving model ...
Validation loss decreased (0.408148 --> 0.408125).  Saving model ...
Validation loss decreased (0.408125 --> 0.408101).  Saving model ...
Validation loss decreased (0.408101 --> 0.408078).  Saving model ...
Validation loss decreased (0.408078 --> 0.408055).  Saving model ...
Validation loss decreased (0.408055 --> 0.408031).  Saving model ...
Validation loss decreased (0.408031 --> 0.408008).  Saving model ...
Validation loss decreased (0.408008 --> 0.407984).  Saving model ...
Validation loss decreased (0.407984 --> 0.407961).  Saving model ...
Validation loss decreased (0.407961 --> 0.407938).  Saving model ...
Validation loss decreased (0.407938 --> 0.407914).  Saving model ...
Validation loss decreased (0.407914 --> 0.407891).  Saving model ...
Validation loss decreased (0.407891 --> 0.407867).  Saving model ...
Validation loss decreased (0.407867 --> 0.407844).  Saving model ...
Validation loss decreased (0.407844 --> 0.407821).  Saving model ...
Validation loss decreased (0.407821 --> 0.407797).  Saving model ...
Validation loss decreased (0.407797 --> 0.407774).  Saving model ...
Validation loss decreased (0.407774 --> 0.407750).  Saving model ...
Validation loss decreased (0.407750 --> 0.407727).  Saving model ...
Validation loss decreased (0.407727 --> 0.407703).  Saving model ...
Validation loss decreased (0.407703 --> 0.407680).  Saving model ...
Validation loss decreased (0.407680 --> 0.407656).  Saving model ...
Validation loss decreased (0.407656 --> 0.407633).  Saving model ...
Validation loss decreased (0.407633 --> 0.407609).  Saving model ...
epoch 4001, loss 0.4076, train acc 80.82%, f1 0.7083, precision 0.7556, recall 0.6667, auc 0.7754
Validation loss decreased (0.407609 --> 0.407586).  Saving model ...
Validation loss decreased (0.407586 --> 0.407562).  Saving model ...
Validation loss decreased (0.407562 --> 0.407538).  Saving model ...
Validation loss decreased (0.407538 --> 0.407515).  Saving model ...
Validation loss decreased (0.407515 --> 0.407491).  Saving model ...
Validation loss decreased (0.407491 --> 0.407468).  Saving model ...
Validation loss decreased (0.407468 --> 0.407444).  Saving model ...
Validation loss decreased (0.407444 --> 0.407421).  Saving model ...
Validation loss decreased (0.407421 --> 0.407397).  Saving model ...
Validation loss decreased (0.407397 --> 0.407373).  Saving model ...
Validation loss decreased (0.407373 --> 0.407350).  Saving model ...
Validation loss decreased (0.407350 --> 0.407326).  Saving model ...
Validation loss decreased (0.407326 --> 0.407302).  Saving model ...
Validation loss decreased (0.407302 --> 0.407279).  Saving model ...
Validation loss decreased (0.407279 --> 0.407255).  Saving model ...
Validation loss decreased (0.407255 --> 0.407232).  Saving model ...
Validation loss decreased (0.407232 --> 0.407208).  Saving model ...
Validation loss decreased (0.407208 --> 0.407184).  Saving model ...
Validation loss decreased (0.407184 --> 0.407161).  Saving model ...
Validation loss decreased (0.407161 --> 0.407137).  Saving model ...
Validation loss decreased (0.407137 --> 0.407113).  Saving model ...
Validation loss decreased (0.407113 --> 0.407090).  Saving model ...
Validation loss decreased (0.407090 --> 0.407066).  Saving model ...
Validation loss decreased (0.407066 --> 0.407042).  Saving model ...
Validation loss decreased (0.407042 --> 0.407018).  Saving model ...
Validation loss decreased (0.407018 --> 0.406995).  Saving model ...
Validation loss decreased (0.406995 --> 0.406971).  Saving model ...
Validation loss decreased (0.406971 --> 0.406947).  Saving model ...
Validation loss decreased (0.406947 --> 0.406924).  Saving model ...
Validation loss decreased (0.406924 --> 0.406900).  Saving model ...
Validation loss decreased (0.406900 --> 0.406876).  Saving model ...
Validation loss decreased (0.406876 --> 0.406853).  Saving model ...
Validation loss decreased (0.406853 --> 0.406829).  Saving model ...
Validation loss decreased (0.406829 --> 0.406805).  Saving model ...
Validation loss decreased (0.406805 --> 0.406781).  Saving model ...
Validation loss decreased (0.406781 --> 0.406758).  Saving model ...
Validation loss decreased (0.406758 --> 0.406734).  Saving model ...
Validation loss decreased (0.406734 --> 0.406710).  Saving model ...
Validation loss decreased (0.406710 --> 0.406686).  Saving model ...
Validation loss decreased (0.406686 --> 0.406663).  Saving model ...
Validation loss decreased (0.406663 --> 0.406639).  Saving model ...
Validation loss decreased (0.406639 --> 0.406615).  Saving model ...
Validation loss decreased (0.406615 --> 0.406592).  Saving model ...
Validation loss decreased (0.406592 --> 0.406568).  Saving model ...
Validation loss decreased (0.406568 --> 0.406544).  Saving model ...
Validation loss decreased (0.406544 --> 0.406520).  Saving model ...
Validation loss decreased (0.406520 --> 0.406496).  Saving model ...
Validation loss decreased (0.406496 --> 0.406473).  Saving model ...
Validation loss decreased (0.406473 --> 0.406449).  Saving model ...
Validation loss decreased (0.406449 --> 0.406425).  Saving model ...
Validation loss decreased (0.406425 --> 0.406401).  Saving model ...
Validation loss decreased (0.406401 --> 0.406378).  Saving model ...
Validation loss decreased (0.406378 --> 0.406354).  Saving model ...
Validation loss decreased (0.406354 --> 0.406330).  Saving model ...
Validation loss decreased (0.406330 --> 0.406306).  Saving model ...
Validation loss decreased (0.406306 --> 0.406282).  Saving model ...
Validation loss decreased (0.406282 --> 0.406259).  Saving model ...
Validation loss decreased (0.406259 --> 0.406235).  Saving model ...
Validation loss decreased (0.406235 --> 0.406211).  Saving model ...
Validation loss decreased (0.406211 --> 0.406187).  Saving model ...
Validation loss decreased (0.406187 --> 0.406164).  Saving model ...
Validation loss decreased (0.406164 --> 0.406140).  Saving model ...
Validation loss decreased (0.406140 --> 0.406116).  Saving model ...
Validation loss decreased (0.406116 --> 0.406092).  Saving model ...
Validation loss decreased (0.406092 --> 0.406068).  Saving model ...
Validation loss decreased (0.406068 --> 0.406044).  Saving model ...
Validation loss decreased (0.406044 --> 0.406021).  Saving model ...
Validation loss decreased (0.406021 --> 0.405997).  Saving model ...
Validation loss decreased (0.405997 --> 0.405973).  Saving model ...
Validation loss decreased (0.405973 --> 0.405949).  Saving model ...
Validation loss decreased (0.405949 --> 0.405925).  Saving model ...
Validation loss decreased (0.405925 --> 0.405902).  Saving model ...
Validation loss decreased (0.405902 --> 0.405878).  Saving model ...
Validation loss decreased (0.405878 --> 0.405854).  Saving model ...
Validation loss decreased (0.405854 --> 0.405830).  Saving model ...
Validation loss decreased (0.405830 --> 0.405806).  Saving model ...
Validation loss decreased (0.405806 --> 0.405782).  Saving model ...
Validation loss decreased (0.405782 --> 0.405759).  Saving model ...
Validation loss decreased (0.405759 --> 0.405735).  Saving model ...
Validation loss decreased (0.405735 --> 0.405711).  Saving model ...
Validation loss decreased (0.405711 --> 0.405687).  Saving model ...
Validation loss decreased (0.405687 --> 0.405663).  Saving model ...
Validation loss decreased (0.405663 --> 0.405639).  Saving model ...
Validation loss decreased (0.405639 --> 0.405616).  Saving model ...
Validation loss decreased (0.405616 --> 0.405592).  Saving model ...
Validation loss decreased (0.405592 --> 0.405568).  Saving model ...
Validation loss decreased (0.405568 --> 0.405544).  Saving model ...
Validation loss decreased (0.405544 --> 0.405520).  Saving model ...
Validation loss decreased (0.405520 --> 0.405496).  Saving model ...
Validation loss decreased (0.405496 --> 0.405472).  Saving model ...
Validation loss decreased (0.405472 --> 0.405449).  Saving model ...
Validation loss decreased (0.405449 --> 0.405425).  Saving model ...
Validation loss decreased (0.405425 --> 0.405401).  Saving model ...
Validation loss decreased (0.405401 --> 0.405377).  Saving model ...
Validation loss decreased (0.405377 --> 0.405353).  Saving model ...
Validation loss decreased (0.405353 --> 0.405329).  Saving model ...
Validation loss decreased (0.405329 --> 0.405305).  Saving model ...
Validation loss decreased (0.405305 --> 0.405281).  Saving model ...
Validation loss decreased (0.405281 --> 0.405258).  Saving model ...
Validation loss decreased (0.405258 --> 0.405234).  Saving model ...
epoch 4101, loss 0.4052, train acc 80.82%, f1 0.7083, precision 0.7556, recall 0.6667, auc 0.7754
Validation loss decreased (0.405234 --> 0.405210).  Saving model ...
Validation loss decreased (0.405210 --> 0.405186).  Saving model ...
Validation loss decreased (0.405186 --> 0.405162).  Saving model ...
Validation loss decreased (0.405162 --> 0.405138).  Saving model ...
Validation loss decreased (0.405138 --> 0.405114).  Saving model ...
Validation loss decreased (0.405114 --> 0.405090).  Saving model ...
Validation loss decreased (0.405090 --> 0.405066).  Saving model ...
Validation loss decreased (0.405066 --> 0.405043).  Saving model ...
Validation loss decreased (0.405043 --> 0.405019).  Saving model ...
Validation loss decreased (0.405019 --> 0.404995).  Saving model ...
Validation loss decreased (0.404995 --> 0.404971).  Saving model ...
Validation loss decreased (0.404971 --> 0.404947).  Saving model ...
Validation loss decreased (0.404947 --> 0.404923).  Saving model ...
Validation loss decreased (0.404923 --> 0.404899).  Saving model ...
Validation loss decreased (0.404899 --> 0.404875).  Saving model ...
Validation loss decreased (0.404875 --> 0.404851).  Saving model ...
Validation loss decreased (0.404851 --> 0.404827).  Saving model ...
Validation loss decreased (0.404827 --> 0.404803).  Saving model ...
Validation loss decreased (0.404803 --> 0.404780).  Saving model ...
Validation loss decreased (0.404780 --> 0.404756).  Saving model ...
Validation loss decreased (0.404756 --> 0.404732).  Saving model ...
Validation loss decreased (0.404732 --> 0.404708).  Saving model ...
Validation loss decreased (0.404708 --> 0.404684).  Saving model ...
Validation loss decreased (0.404684 --> 0.404660).  Saving model ...
Validation loss decreased (0.404660 --> 0.404636).  Saving model ...
Validation loss decreased (0.404636 --> 0.404612).  Saving model ...
Validation loss decreased (0.404612 --> 0.404588).  Saving model ...
Validation loss decreased (0.404588 --> 0.404564).  Saving model ...
Validation loss decreased (0.404564 --> 0.404540).  Saving model ...
Validation loss decreased (0.404540 --> 0.404516).  Saving model ...
Validation loss decreased (0.404516 --> 0.404492).  Saving model ...
Validation loss decreased (0.404492 --> 0.404468).  Saving model ...
Validation loss decreased (0.404468 --> 0.404444).  Saving model ...
Validation loss decreased (0.404444 --> 0.404420).  Saving model ...
Validation loss decreased (0.404420 --> 0.404397).  Saving model ...
Validation loss decreased (0.404397 --> 0.404373).  Saving model ...
Validation loss decreased (0.404373 --> 0.404349).  Saving model ...
Validation loss decreased (0.404349 --> 0.404325).  Saving model ...
Validation loss decreased (0.404325 --> 0.404301).  Saving model ...
Validation loss decreased (0.404301 --> 0.404277).  Saving model ...
Validation loss decreased (0.404277 --> 0.404253).  Saving model ...
Validation loss decreased (0.404253 --> 0.404229).  Saving model ...
Validation loss decreased (0.404229 --> 0.404205).  Saving model ...
Validation loss decreased (0.404205 --> 0.404181).  Saving model ...
Validation loss decreased (0.404181 --> 0.404157).  Saving model ...
Validation loss decreased (0.404157 --> 0.404133).  Saving model ...
Validation loss decreased (0.404133 --> 0.404109).  Saving model ...
Validation loss decreased (0.404109 --> 0.404085).  Saving model ...
Validation loss decreased (0.404085 --> 0.404061).  Saving model ...
Validation loss decreased (0.404061 --> 0.404037).  Saving model ...
Validation loss decreased (0.404037 --> 0.404013).  Saving model ...
Validation loss decreased (0.404013 --> 0.403989).  Saving model ...
Validation loss decreased (0.403989 --> 0.403965).  Saving model ...
Validation loss decreased (0.403965 --> 0.403941).  Saving model ...
Validation loss decreased (0.403941 --> 0.403917).  Saving model ...
Validation loss decreased (0.403917 --> 0.403893).  Saving model ...
Validation loss decreased (0.403893 --> 0.403869).  Saving model ...
Validation loss decreased (0.403869 --> 0.403845).  Saving model ...
Validation loss decreased (0.403845 --> 0.403821).  Saving model ...
Validation loss decreased (0.403821 --> 0.403797).  Saving model ...
Validation loss decreased (0.403797 --> 0.403773).  Saving model ...
Validation loss decreased (0.403773 --> 0.403749).  Saving model ...
Validation loss decreased (0.403749 --> 0.403725).  Saving model ...
Validation loss decreased (0.403725 --> 0.403701).  Saving model ...
Validation loss decreased (0.403701 --> 0.403677).  Saving model ...
Validation loss decreased (0.403677 --> 0.403653).  Saving model ...
Validation loss decreased (0.403653 --> 0.403629).  Saving model ...
Validation loss decreased (0.403629 --> 0.403605).  Saving model ...
Validation loss decreased (0.403605 --> 0.403581).  Saving model ...
Validation loss decreased (0.403581 --> 0.403557).  Saving model ...
Validation loss decreased (0.403557 --> 0.403533).  Saving model ...
Validation loss decreased (0.403533 --> 0.403509).  Saving model ...
Validation loss decreased (0.403509 --> 0.403485).  Saving model ...
Validation loss decreased (0.403485 --> 0.403461).  Saving model ...
Validation loss decreased (0.403461 --> 0.403437).  Saving model ...
Validation loss decreased (0.403437 --> 0.403413).  Saving model ...
Validation loss decreased (0.403413 --> 0.403388).  Saving model ...
Validation loss decreased (0.403388 --> 0.403364).  Saving model ...
Validation loss decreased (0.403364 --> 0.403340).  Saving model ...
Validation loss decreased (0.403340 --> 0.403316).  Saving model ...
Validation loss decreased (0.403316 --> 0.403292).  Saving model ...
Validation loss decreased (0.403292 --> 0.403268).  Saving model ...
Validation loss decreased (0.403268 --> 0.403244).  Saving model ...
Validation loss decreased (0.403244 --> 0.403220).  Saving model ...
Validation loss decreased (0.403220 --> 0.403196).  Saving model ...
Validation loss decreased (0.403196 --> 0.403172).  Saving model ...
Validation loss decreased (0.403172 --> 0.403148).  Saving model ...
Validation loss decreased (0.403148 --> 0.403124).  Saving model ...
Validation loss decreased (0.403124 --> 0.403100).  Saving model ...
Validation loss decreased (0.403100 --> 0.403076).  Saving model ...
Validation loss decreased (0.403076 --> 0.403052).  Saving model ...
Validation loss decreased (0.403052 --> 0.403028).  Saving model ...
Validation loss decreased (0.403028 --> 0.403004).  Saving model ...
Validation loss decreased (0.403004 --> 0.402980).  Saving model ...
Validation loss decreased (0.402980 --> 0.402956).  Saving model ...
Validation loss decreased (0.402956 --> 0.402932).  Saving model ...
Validation loss decreased (0.402932 --> 0.402907).  Saving model ...
Validation loss decreased (0.402907 --> 0.402883).  Saving model ...
Validation loss decreased (0.402883 --> 0.402859).  Saving model ...
Validation loss decreased (0.402859 --> 0.402835).  Saving model ...
epoch 4201, loss 0.4028, train acc 80.65%, f1 0.7065, precision 0.7514, recall 0.6667, auc 0.7741
Validation loss decreased (0.402835 --> 0.402811).  Saving model ...
Validation loss decreased (0.402811 --> 0.402787).  Saving model ...
Validation loss decreased (0.402787 --> 0.402763).  Saving model ...
Validation loss decreased (0.402763 --> 0.402739).  Saving model ...
Validation loss decreased (0.402739 --> 0.402715).  Saving model ...
Validation loss decreased (0.402715 --> 0.402691).  Saving model ...
Validation loss decreased (0.402691 --> 0.402667).  Saving model ...
Validation loss decreased (0.402667 --> 0.402643).  Saving model ...
Validation loss decreased (0.402643 --> 0.402619).  Saving model ...
Validation loss decreased (0.402619 --> 0.402595).  Saving model ...
Validation loss decreased (0.402595 --> 0.402570).  Saving model ...
Validation loss decreased (0.402570 --> 0.402546).  Saving model ...
Validation loss decreased (0.402546 --> 0.402522).  Saving model ...
Validation loss decreased (0.402522 --> 0.402498).  Saving model ...
Validation loss decreased (0.402498 --> 0.402474).  Saving model ...
Validation loss decreased (0.402474 --> 0.402450).  Saving model ...
Validation loss decreased (0.402450 --> 0.402426).  Saving model ...
Validation loss decreased (0.402426 --> 0.402402).  Saving model ...
Validation loss decreased (0.402402 --> 0.402378).  Saving model ...
Validation loss decreased (0.402378 --> 0.402354).  Saving model ...
Validation loss decreased (0.402354 --> 0.402330).  Saving model ...
Validation loss decreased (0.402330 --> 0.402305).  Saving model ...
Validation loss decreased (0.402305 --> 0.402281).  Saving model ...
Validation loss decreased (0.402281 --> 0.402257).  Saving model ...
Validation loss decreased (0.402257 --> 0.402233).  Saving model ...
Validation loss decreased (0.402233 --> 0.402209).  Saving model ...
Validation loss decreased (0.402209 --> 0.402185).  Saving model ...
Validation loss decreased (0.402185 --> 0.402161).  Saving model ...
Validation loss decreased (0.402161 --> 0.402137).  Saving model ...
Validation loss decreased (0.402137 --> 0.402113).  Saving model ...
Validation loss decreased (0.402113 --> 0.402089).  Saving model ...
Validation loss decreased (0.402089 --> 0.402065).  Saving model ...
Validation loss decreased (0.402065 --> 0.402040).  Saving model ...
Validation loss decreased (0.402040 --> 0.402016).  Saving model ...
Validation loss decreased (0.402016 --> 0.401992).  Saving model ...
Validation loss decreased (0.401992 --> 0.401968).  Saving model ...
Validation loss decreased (0.401968 --> 0.401944).  Saving model ...
Validation loss decreased (0.401944 --> 0.401920).  Saving model ...
Validation loss decreased (0.401920 --> 0.401896).  Saving model ...
Validation loss decreased (0.401896 --> 0.401872).  Saving model ...
Validation loss decreased (0.401872 --> 0.401847).  Saving model ...
Validation loss decreased (0.401847 --> 0.401823).  Saving model ...
Validation loss decreased (0.401823 --> 0.401799).  Saving model ...
Validation loss decreased (0.401799 --> 0.401775).  Saving model ...
Validation loss decreased (0.401775 --> 0.401751).  Saving model ...
Validation loss decreased (0.401751 --> 0.401727).  Saving model ...
Validation loss decreased (0.401727 --> 0.401703).  Saving model ...
Validation loss decreased (0.401703 --> 0.401679).  Saving model ...
Validation loss decreased (0.401679 --> 0.401655).  Saving model ...
Validation loss decreased (0.401655 --> 0.401630).  Saving model ...
Validation loss decreased (0.401630 --> 0.401606).  Saving model ...
Validation loss decreased (0.401606 --> 0.401582).  Saving model ...
Validation loss decreased (0.401582 --> 0.401558).  Saving model ...
Validation loss decreased (0.401558 --> 0.401534).  Saving model ...
Validation loss decreased (0.401534 --> 0.401510).  Saving model ...
Validation loss decreased (0.401510 --> 0.401486).  Saving model ...
Validation loss decreased (0.401486 --> 0.401462).  Saving model ...
Validation loss decreased (0.401462 --> 0.401437).  Saving model ...
Validation loss decreased (0.401437 --> 0.401413).  Saving model ...
Validation loss decreased (0.401413 --> 0.401389).  Saving model ...
Validation loss decreased (0.401389 --> 0.401365).  Saving model ...
Validation loss decreased (0.401365 --> 0.401341).  Saving model ...
Validation loss decreased (0.401341 --> 0.401317).  Saving model ...
Validation loss decreased (0.401317 --> 0.401293).  Saving model ...
Validation loss decreased (0.401293 --> 0.401269).  Saving model ...
Validation loss decreased (0.401269 --> 0.401244).  Saving model ...
Validation loss decreased (0.401244 --> 0.401220).  Saving model ...
Validation loss decreased (0.401220 --> 0.401196).  Saving model ...
Validation loss decreased (0.401196 --> 0.401172).  Saving model ...
Validation loss decreased (0.401172 --> 0.401148).  Saving model ...
Validation loss decreased (0.401148 --> 0.401124).  Saving model ...
Validation loss decreased (0.401124 --> 0.401100).  Saving model ...
Validation loss decreased (0.401100 --> 0.401075).  Saving model ...
Validation loss decreased (0.401075 --> 0.401051).  Saving model ...
Validation loss decreased (0.401051 --> 0.401027).  Saving model ...
Validation loss decreased (0.401027 --> 0.401003).  Saving model ...
Validation loss decreased (0.401003 --> 0.400979).  Saving model ...
Validation loss decreased (0.400979 --> 0.400955).  Saving model ...
Validation loss decreased (0.400955 --> 0.400930).  Saving model ...
Validation loss decreased (0.400930 --> 0.400906).  Saving model ...
Validation loss decreased (0.400906 --> 0.400882).  Saving model ...
Validation loss decreased (0.400882 --> 0.400858).  Saving model ...
Validation loss decreased (0.400858 --> 0.400834).  Saving model ...
Validation loss decreased (0.400834 --> 0.400810).  Saving model ...
Validation loss decreased (0.400810 --> 0.400786).  Saving model ...
Validation loss decreased (0.400786 --> 0.400761).  Saving model ...
Validation loss decreased (0.400761 --> 0.400737).  Saving model ...
Validation loss decreased (0.400737 --> 0.400713).  Saving model ...
Validation loss decreased (0.400713 --> 0.400689).  Saving model ...
Validation loss decreased (0.400689 --> 0.400665).  Saving model ...
Validation loss decreased (0.400665 --> 0.400640).  Saving model ...
Validation loss decreased (0.400640 --> 0.400616).  Saving model ...
Validation loss decreased (0.400616 --> 0.400592).  Saving model ...
Validation loss decreased (0.400592 --> 0.400568).  Saving model ...
Validation loss decreased (0.400568 --> 0.400544).  Saving model ...
Validation loss decreased (0.400544 --> 0.400520).  Saving model ...
Validation loss decreased (0.400520 --> 0.400495).  Saving model ...
Validation loss decreased (0.400495 --> 0.400471).  Saving model ...
Validation loss decreased (0.400471 --> 0.400447).  Saving model ...
Validation loss decreased (0.400447 --> 0.400423).  Saving model ...
epoch 4301, loss 0.4004, train acc 80.82%, f1 0.7113, precision 0.7500, recall 0.6765, auc 0.7777
Validation loss decreased (0.400423 --> 0.400399).  Saving model ...
Validation loss decreased (0.400399 --> 0.400374).  Saving model ...
Validation loss decreased (0.400374 --> 0.400350).  Saving model ...
Validation loss decreased (0.400350 --> 0.400326).  Saving model ...
Validation loss decreased (0.400326 --> 0.400302).  Saving model ...
Validation loss decreased (0.400302 --> 0.400278).  Saving model ...
Validation loss decreased (0.400278 --> 0.400253).  Saving model ...
Validation loss decreased (0.400253 --> 0.400229).  Saving model ...
Validation loss decreased (0.400229 --> 0.400205).  Saving model ...
Validation loss decreased (0.400205 --> 0.400181).  Saving model ...
Validation loss decreased (0.400181 --> 0.400157).  Saving model ...
Validation loss decreased (0.400157 --> 0.400132).  Saving model ...
Validation loss decreased (0.400132 --> 0.400108).  Saving model ...
Validation loss decreased (0.400108 --> 0.400084).  Saving model ...
Validation loss decreased (0.400084 --> 0.400060).  Saving model ...
Validation loss decreased (0.400060 --> 0.400035).  Saving model ...
Validation loss decreased (0.400035 --> 0.400011).  Saving model ...
Validation loss decreased (0.400011 --> 0.399987).  Saving model ...
Validation loss decreased (0.399987 --> 0.399963).  Saving model ...
Validation loss decreased (0.399963 --> 0.399938).  Saving model ...
Validation loss decreased (0.399938 --> 0.399914).  Saving model ...
Validation loss decreased (0.399914 --> 0.399890).  Saving model ...
Validation loss decreased (0.399890 --> 0.399866).  Saving model ...
Validation loss decreased (0.399866 --> 0.399841).  Saving model ...
Validation loss decreased (0.399841 --> 0.399817).  Saving model ...
Validation loss decreased (0.399817 --> 0.399793).  Saving model ...
Validation loss decreased (0.399793 --> 0.399769).  Saving model ...
Validation loss decreased (0.399769 --> 0.399744).  Saving model ...
Validation loss decreased (0.399744 --> 0.399720).  Saving model ...
Validation loss decreased (0.399720 --> 0.399696).  Saving model ...
Validation loss decreased (0.399696 --> 0.399672).  Saving model ...
Validation loss decreased (0.399672 --> 0.399647).  Saving model ...
Validation loss decreased (0.399647 --> 0.399623).  Saving model ...
Validation loss decreased (0.399623 --> 0.399599).  Saving model ...
Validation loss decreased (0.399599 --> 0.399574).  Saving model ...
Validation loss decreased (0.399574 --> 0.399550).  Saving model ...
Validation loss decreased (0.399550 --> 0.399526).  Saving model ...
Validation loss decreased (0.399526 --> 0.399501).  Saving model ...
Validation loss decreased (0.399501 --> 0.399477).  Saving model ...
Validation loss decreased (0.399477 --> 0.399453).  Saving model ...
Validation loss decreased (0.399453 --> 0.399429).  Saving model ...
Validation loss decreased (0.399429 --> 0.399404).  Saving model ...
Validation loss decreased (0.399404 --> 0.399380).  Saving model ...
Validation loss decreased (0.399380 --> 0.399355).  Saving model ...
Validation loss decreased (0.399355 --> 0.399331).  Saving model ...
Validation loss decreased (0.399331 --> 0.399307).  Saving model ...
Validation loss decreased (0.399307 --> 0.399283).  Saving model ...
Validation loss decreased (0.399283 --> 0.399258).  Saving model ...
Validation loss decreased (0.399258 --> 0.399234).  Saving model ...
Validation loss decreased (0.399234 --> 0.399209).  Saving model ...
Validation loss decreased (0.399209 --> 0.399185).  Saving model ...
Validation loss decreased (0.399185 --> 0.399161).  Saving model ...
Validation loss decreased (0.399161 --> 0.399136).  Saving model ...
Validation loss decreased (0.399136 --> 0.399112).  Saving model ...
Validation loss decreased (0.399112 --> 0.399087).  Saving model ...
Validation loss decreased (0.399087 --> 0.399063).  Saving model ...
Validation loss decreased (0.399063 --> 0.399039).  Saving model ...
Validation loss decreased (0.399039 --> 0.399014).  Saving model ...
Validation loss decreased (0.399014 --> 0.398990).  Saving model ...
Validation loss decreased (0.398990 --> 0.398965).  Saving model ...
Validation loss decreased (0.398965 --> 0.398941).  Saving model ...
Validation loss decreased (0.398941 --> 0.398917).  Saving model ...
Validation loss decreased (0.398917 --> 0.398892).  Saving model ...
Validation loss decreased (0.398892 --> 0.398868).  Saving model ...
Validation loss decreased (0.398868 --> 0.398843).  Saving model ...
Validation loss decreased (0.398843 --> 0.398819).  Saving model ...
Validation loss decreased (0.398819 --> 0.398794).  Saving model ...
Validation loss decreased (0.398794 --> 0.398770).  Saving model ...
Validation loss decreased (0.398770 --> 0.398745).  Saving model ...
Validation loss decreased (0.398745 --> 0.398721).  Saving model ...
Validation loss decreased (0.398721 --> 0.398696).  Saving model ...
Validation loss decreased (0.398696 --> 0.398672).  Saving model ...
Validation loss decreased (0.398672 --> 0.398647).  Saving model ...
Validation loss decreased (0.398647 --> 0.398623).  Saving model ...
Validation loss decreased (0.398623 --> 0.398598).  Saving model ...
Validation loss decreased (0.398598 --> 0.398573).  Saving model ...
Validation loss decreased (0.398573 --> 0.398549).  Saving model ...
Validation loss decreased (0.398549 --> 0.398524).  Saving model ...
Validation loss decreased (0.398524 --> 0.398500).  Saving model ...
Validation loss decreased (0.398500 --> 0.398475).  Saving model ...
Validation loss decreased (0.398475 --> 0.398451).  Saving model ...
Validation loss decreased (0.398451 --> 0.398426).  Saving model ...
Validation loss decreased (0.398426 --> 0.398401).  Saving model ...
Validation loss decreased (0.398401 --> 0.398377).  Saving model ...
Validation loss decreased (0.398377 --> 0.398352).  Saving model ...
Validation loss decreased (0.398352 --> 0.398327).  Saving model ...
Validation loss decreased (0.398327 --> 0.398303).  Saving model ...
Validation loss decreased (0.398303 --> 0.398278).  Saving model ...
Validation loss decreased (0.398278 --> 0.398253).  Saving model ...
Validation loss decreased (0.398253 --> 0.398229).  Saving model ...
Validation loss decreased (0.398229 --> 0.398204).  Saving model ...
Validation loss decreased (0.398204 --> 0.398179).  Saving model ...
Validation loss decreased (0.398179 --> 0.398155).  Saving model ...
Validation loss decreased (0.398155 --> 0.398130).  Saving model ...
Validation loss decreased (0.398130 --> 0.398105).  Saving model ...
Validation loss decreased (0.398105 --> 0.398080).  Saving model ...
Validation loss decreased (0.398080 --> 0.398055).  Saving model ...
Validation loss decreased (0.398055 --> 0.398031).  Saving model ...
Validation loss decreased (0.398031 --> 0.398006).  Saving model ...
Validation loss decreased (0.398006 --> 0.397981).  Saving model ...
epoch 4401, loss 0.3980, train acc 80.99%, f1 0.7132, precision 0.7541, recall 0.6765, auc 0.7790
Validation loss decreased (0.397981 --> 0.397956).  Saving model ...
Validation loss decreased (0.397956 --> 0.397931).  Saving model ...
Validation loss decreased (0.397931 --> 0.397907).  Saving model ...
Validation loss decreased (0.397907 --> 0.397882).  Saving model ...
Validation loss decreased (0.397882 --> 0.397857).  Saving model ...
Validation loss decreased (0.397857 --> 0.397832).  Saving model ...
Validation loss decreased (0.397832 --> 0.397807).  Saving model ...
Validation loss decreased (0.397807 --> 0.397782).  Saving model ...
Validation loss decreased (0.397782 --> 0.397757).  Saving model ...
Validation loss decreased (0.397757 --> 0.397732).  Saving model ...
Validation loss decreased (0.397732 --> 0.397707).  Saving model ...
Validation loss decreased (0.397707 --> 0.397682).  Saving model ...
Validation loss decreased (0.397682 --> 0.397657).  Saving model ...
Validation loss decreased (0.397657 --> 0.397632).  Saving model ...
Validation loss decreased (0.397632 --> 0.397607).  Saving model ...
Validation loss decreased (0.397607 --> 0.397582).  Saving model ...
Validation loss decreased (0.397582 --> 0.397557).  Saving model ...
Validation loss decreased (0.397557 --> 0.397532).  Saving model ...
Validation loss decreased (0.397532 --> 0.397507).  Saving model ...
Validation loss decreased (0.397507 --> 0.397481).  Saving model ...
Validation loss decreased (0.397481 --> 0.397456).  Saving model ...
Validation loss decreased (0.397456 --> 0.397431).  Saving model ...
Validation loss decreased (0.397431 --> 0.397406).  Saving model ...
Validation loss decreased (0.397406 --> 0.397381).  Saving model ...
Validation loss decreased (0.397381 --> 0.397355).  Saving model ...
Validation loss decreased (0.397355 --> 0.397330).  Saving model ...
Validation loss decreased (0.397330 --> 0.397305).  Saving model ...
Validation loss decreased (0.397305 --> 0.397279).  Saving model ...
Validation loss decreased (0.397279 --> 0.397254).  Saving model ...
Validation loss decreased (0.397254 --> 0.397229).  Saving model ...
Validation loss decreased (0.397229 --> 0.397203).  Saving model ...
Validation loss decreased (0.397203 --> 0.397178).  Saving model ...
Validation loss decreased (0.397178 --> 0.397152).  Saving model ...
Validation loss decreased (0.397152 --> 0.397127).  Saving model ...
Validation loss decreased (0.397127 --> 0.397102).  Saving model ...
Validation loss decreased (0.397102 --> 0.397076).  Saving model ...
Validation loss decreased (0.397076 --> 0.397051).  Saving model ...
Validation loss decreased (0.397051 --> 0.397025).  Saving model ...
Validation loss decreased (0.397025 --> 0.396999).  Saving model ...
Validation loss decreased (0.396999 --> 0.396974).  Saving model ...
Validation loss decreased (0.396974 --> 0.396948).  Saving model ...
Validation loss decreased (0.396948 --> 0.396922).  Saving model ...
Validation loss decreased (0.396922 --> 0.396897).  Saving model ...
Validation loss decreased (0.396897 --> 0.396871).  Saving model ...
Validation loss decreased (0.396871 --> 0.396845).  Saving model ...
Validation loss decreased (0.396845 --> 0.396820).  Saving model ...
Validation loss decreased (0.396820 --> 0.396794).  Saving model ...
Validation loss decreased (0.396794 --> 0.396768).  Saving model ...
Validation loss decreased (0.396768 --> 0.396742).  Saving model ...
Validation loss decreased (0.396742 --> 0.396716).  Saving model ...
Validation loss decreased (0.396716 --> 0.396690).  Saving model ...
Validation loss decreased (0.396690 --> 0.396664).  Saving model ...
Validation loss decreased (0.396664 --> 0.396638).  Saving model ...
Validation loss decreased (0.396638 --> 0.396612).  Saving model ...
Validation loss decreased (0.396612 --> 0.396586).  Saving model ...
Validation loss decreased (0.396586 --> 0.396560).  Saving model ...
Validation loss decreased (0.396560 --> 0.396534).  Saving model ...
Validation loss decreased (0.396534 --> 0.396508).  Saving model ...
Validation loss decreased (0.396508 --> 0.396481).  Saving model ...
Validation loss decreased (0.396481 --> 0.396455).  Saving model ...
Validation loss decreased (0.396455 --> 0.396429).  Saving model ...
Validation loss decreased (0.396429 --> 0.396403).  Saving model ...
Validation loss decreased (0.396403 --> 0.396376).  Saving model ...
Validation loss decreased (0.396376 --> 0.396350).  Saving model ...
Validation loss decreased (0.396350 --> 0.396323).  Saving model ...
Validation loss decreased (0.396323 --> 0.396297).  Saving model ...
Validation loss decreased (0.396297 --> 0.396270).  Saving model ...
Validation loss decreased (0.396270 --> 0.396244).  Saving model ...
Validation loss decreased (0.396244 --> 0.396217).  Saving model ...
Validation loss decreased (0.396217 --> 0.396191).  Saving model ...
Validation loss decreased (0.396191 --> 0.396164).  Saving model ...
Validation loss decreased (0.396164 --> 0.396137).  Saving model ...
Validation loss decreased (0.396137 --> 0.396110).  Saving model ...
Validation loss decreased (0.396110 --> 0.396083).  Saving model ...
Validation loss decreased (0.396083 --> 0.396056).  Saving model ...
Validation loss decreased (0.396056 --> 0.396029).  Saving model ...
Validation loss decreased (0.396029 --> 0.396002).  Saving model ...
Validation loss decreased (0.396002 --> 0.395975).  Saving model ...
Validation loss decreased (0.395975 --> 0.395948).  Saving model ...
Validation loss decreased (0.395948 --> 0.395921).  Saving model ...
Validation loss decreased (0.395921 --> 0.395894).  Saving model ...
Validation loss decreased (0.395894 --> 0.395867).  Saving model ...
Validation loss decreased (0.395867 --> 0.395839).  Saving model ...
Validation loss decreased (0.395839 --> 0.395812).  Saving model ...
Validation loss decreased (0.395812 --> 0.395784).  Saving model ...
Validation loss decreased (0.395784 --> 0.395757).  Saving model ...
Validation loss decreased (0.395757 --> 0.395729).  Saving model ...
Validation loss decreased (0.395729 --> 0.395702).  Saving model ...
Validation loss decreased (0.395702 --> 0.395674).  Saving model ...
Validation loss decreased (0.395674 --> 0.395646).  Saving model ...
Validation loss decreased (0.395646 --> 0.395619).  Saving model ...
Validation loss decreased (0.395619 --> 0.395591).  Saving model ...
Validation loss decreased (0.395591 --> 0.395563).  Saving model ...
Validation loss decreased (0.395563 --> 0.395535).  Saving model ...
Validation loss decreased (0.395535 --> 0.395507).  Saving model ...
Validation loss decreased (0.395507 --> 0.395479).  Saving model ...
Validation loss decreased (0.395479 --> 0.395450).  Saving model ...
Validation loss decreased (0.395450 --> 0.395422).  Saving model ...
Validation loss decreased (0.395422 --> 0.395394).  Saving model ...
Validation loss decreased (0.395394 --> 0.395365).  Saving model ...
epoch 4501, loss 0.3954, train acc 81.51%, f1 0.7231, precision 0.7581, recall 0.6912, auc 0.7864
Validation loss decreased (0.395365 --> 0.395337).  Saving model ...
Validation loss decreased (0.395337 --> 0.395308).  Saving model ...
Validation loss decreased (0.395308 --> 0.395280).  Saving model ...
Validation loss decreased (0.395280 --> 0.395251).  Saving model ...
Validation loss decreased (0.395251 --> 0.395222).  Saving model ...
Validation loss decreased (0.395222 --> 0.395193).  Saving model ...
Validation loss decreased (0.395193 --> 0.395164).  Saving model ...
Validation loss decreased (0.395164 --> 0.395135).  Saving model ...
Validation loss decreased (0.395135 --> 0.395106).  Saving model ...
Validation loss decreased (0.395106 --> 0.395077).  Saving model ...
Validation loss decreased (0.395077 --> 0.395048).  Saving model ...
Validation loss decreased (0.395048 --> 0.395019).  Saving model ...
Validation loss decreased (0.395019 --> 0.394989).  Saving model ...
Validation loss decreased (0.394989 --> 0.394960).  Saving model ...
Validation loss decreased (0.394960 --> 0.394930).  Saving model ...
Validation loss decreased (0.394930 --> 0.394900).  Saving model ...
Validation loss decreased (0.394900 --> 0.394871).  Saving model ...
Validation loss decreased (0.394871 --> 0.394841).  Saving model ...
Validation loss decreased (0.394841 --> 0.394811).  Saving model ...
Validation loss decreased (0.394811 --> 0.394781).  Saving model ...
Validation loss decreased (0.394781 --> 0.394751).  Saving model ...
Validation loss decreased (0.394751 --> 0.394721).  Saving model ...
Validation loss decreased (0.394721 --> 0.394690).  Saving model ...
Validation loss decreased (0.394690 --> 0.394660).  Saving model ...
Validation loss decreased (0.394660 --> 0.394630).  Saving model ...
Validation loss decreased (0.394630 --> 0.394599).  Saving model ...
Validation loss decreased (0.394599 --> 0.394568).  Saving model ...
Validation loss decreased (0.394568 --> 0.394538).  Saving model ...
Validation loss decreased (0.394538 --> 0.394507).  Saving model ...
Validation loss decreased (0.394507 --> 0.394476).  Saving model ...
Validation loss decreased (0.394476 --> 0.394445).  Saving model ...
Validation loss decreased (0.394445 --> 0.394414).  Saving model ...
Validation loss decreased (0.394414 --> 0.394383).  Saving model ...
Validation loss decreased (0.394383 --> 0.394352).  Saving model ...
Validation loss decreased (0.394352 --> 0.394321).  Saving model ...
Validation loss decreased (0.394321 --> 0.394289).  Saving model ...
Validation loss decreased (0.394289 --> 0.394258).  Saving model ...
Validation loss decreased (0.394258 --> 0.394226).  Saving model ...
Validation loss decreased (0.394226 --> 0.394194).  Saving model ...
Validation loss decreased (0.394194 --> 0.394163).  Saving model ...
Validation loss decreased (0.394163 --> 0.394131).  Saving model ...
Validation loss decreased (0.394131 --> 0.394099).  Saving model ...
Validation loss decreased (0.394099 --> 0.394067).  Saving model ...
Validation loss decreased (0.394067 --> 0.394035).  Saving model ...
Validation loss decreased (0.394035 --> 0.394003).  Saving model ...
Validation loss decreased (0.394003 --> 0.393970).  Saving model ...
Validation loss decreased (0.393970 --> 0.393938).  Saving model ...
Validation loss decreased (0.393938 --> 0.393906).  Saving model ...
Validation loss decreased (0.393906 --> 0.393873).  Saving model ...
Validation loss decreased (0.393873 --> 0.393841).  Saving model ...
Validation loss decreased (0.393841 --> 0.393808).  Saving model ...
Validation loss decreased (0.393808 --> 0.393775).  Saving model ...
Validation loss decreased (0.393775 --> 0.393743).  Saving model ...
Validation loss decreased (0.393743 --> 0.393710).  Saving model ...
Validation loss decreased (0.393710 --> 0.393677).  Saving model ...
Validation loss decreased (0.393677 --> 0.393644).  Saving model ...
Validation loss decreased (0.393644 --> 0.393611).  Saving model ...
Validation loss decreased (0.393611 --> 0.393578).  Saving model ...
Validation loss decreased (0.393578 --> 0.393544).  Saving model ...
Validation loss decreased (0.393544 --> 0.393511).  Saving model ...
Validation loss decreased (0.393511 --> 0.393478).  Saving model ...
Validation loss decreased (0.393478 --> 0.393444).  Saving model ...
Validation loss decreased (0.393444 --> 0.393411).  Saving model ...
Validation loss decreased (0.393411 --> 0.393377).  Saving model ...
Validation loss decreased (0.393377 --> 0.393344).  Saving model ...
Validation loss decreased (0.393344 --> 0.393310).  Saving model ...
Validation loss decreased (0.393310 --> 0.393276).  Saving model ...
Validation loss decreased (0.393276 --> 0.393243).  Saving model ...
Validation loss decreased (0.393243 --> 0.393209).  Saving model ...
Validation loss decreased (0.393209 --> 0.393175).  Saving model ...
Validation loss decreased (0.393175 --> 0.393141).  Saving model ...
Validation loss decreased (0.393141 --> 0.393107).  Saving model ...
Validation loss decreased (0.393107 --> 0.393073).  Saving model ...
Validation loss decreased (0.393073 --> 0.393039).  Saving model ...
Validation loss decreased (0.393039 --> 0.393005).  Saving model ...
Validation loss decreased (0.393005 --> 0.392970).  Saving model ...
Validation loss decreased (0.392970 --> 0.392936).  Saving model ...
Validation loss decreased (0.392936 --> 0.392902).  Saving model ...
Validation loss decreased (0.392902 --> 0.392868).  Saving model ...
Validation loss decreased (0.392868 --> 0.392833).  Saving model ...
Validation loss decreased (0.392833 --> 0.392799).  Saving model ...
Validation loss decreased (0.392799 --> 0.392764).  Saving model ...
Validation loss decreased (0.392764 --> 0.392730).  Saving model ...
Validation loss decreased (0.392730 --> 0.392695).  Saving model ...
Validation loss decreased (0.392695 --> 0.392661).  Saving model ...
Validation loss decreased (0.392661 --> 0.392626).  Saving model ...
Validation loss decreased (0.392626 --> 0.392591).  Saving model ...
Validation loss decreased (0.392591 --> 0.392557).  Saving model ...
Validation loss decreased (0.392557 --> 0.392522).  Saving model ...
Validation loss decreased (0.392522 --> 0.392487).  Saving model ...
Validation loss decreased (0.392487 --> 0.392452).  Saving model ...
Validation loss decreased (0.392452 --> 0.392418).  Saving model ...
Validation loss decreased (0.392418 --> 0.392383).  Saving model ...
Validation loss decreased (0.392383 --> 0.392348).  Saving model ...
Validation loss decreased (0.392348 --> 0.392313).  Saving model ...
Validation loss decreased (0.392313 --> 0.392278).  Saving model ...
Validation loss decreased (0.392278 --> 0.392243).  Saving model ...
Validation loss decreased (0.392243 --> 0.392208).  Saving model ...
Validation loss decreased (0.392208 --> 0.392173).  Saving model ...
Validation loss decreased (0.392173 --> 0.392138).  Saving model ...
epoch 4601, loss 0.3921, train acc 82.02%, f1 0.7287, precision 0.7705, recall 0.6912, auc 0.7903
Validation loss decreased (0.392138 --> 0.392103).  Saving model ...
Validation loss decreased (0.392103 --> 0.392068).  Saving model ...
Validation loss decreased (0.392068 --> 0.392033).  Saving model ...
Validation loss decreased (0.392033 --> 0.391998).  Saving model ...
Validation loss decreased (0.391998 --> 0.391963).  Saving model ...
Validation loss decreased (0.391963 --> 0.391928).  Saving model ...
Validation loss decreased (0.391928 --> 0.391892).  Saving model ...
Validation loss decreased (0.391892 --> 0.391857).  Saving model ...
Validation loss decreased (0.391857 --> 0.391822).  Saving model ...
Validation loss decreased (0.391822 --> 0.391787).  Saving model ...
Validation loss decreased (0.391787 --> 0.391752).  Saving model ...
Validation loss decreased (0.391752 --> 0.391716).  Saving model ...
Validation loss decreased (0.391716 --> 0.391681).  Saving model ...
Validation loss decreased (0.391681 --> 0.391646).  Saving model ...
Validation loss decreased (0.391646 --> 0.391610).  Saving model ...
Validation loss decreased (0.391610 --> 0.391575).  Saving model ...
Validation loss decreased (0.391575 --> 0.391540).  Saving model ...
Validation loss decreased (0.391540 --> 0.391504).  Saving model ...
Validation loss decreased (0.391504 --> 0.391469).  Saving model ...
Validation loss decreased (0.391469 --> 0.391433).  Saving model ...
Validation loss decreased (0.391433 --> 0.391398).  Saving model ...
Validation loss decreased (0.391398 --> 0.391363).  Saving model ...
Validation loss decreased (0.391363 --> 0.391327).  Saving model ...
Validation loss decreased (0.391327 --> 0.391292).  Saving model ...
Validation loss decreased (0.391292 --> 0.391256).  Saving model ...
Validation loss decreased (0.391256 --> 0.391221).  Saving model ...
Validation loss decreased (0.391221 --> 0.391185).  Saving model ...
Validation loss decreased (0.391185 --> 0.391149).  Saving model ...
Validation loss decreased (0.391149 --> 0.391114).  Saving model ...
Validation loss decreased (0.391114 --> 0.391078).  Saving model ...
Validation loss decreased (0.391078 --> 0.391043).  Saving model ...
Validation loss decreased (0.391043 --> 0.391007).  Saving model ...
Validation loss decreased (0.391007 --> 0.390971).  Saving model ...
Validation loss decreased (0.390971 --> 0.390936).  Saving model ...
Validation loss decreased (0.390936 --> 0.390900).  Saving model ...
Validation loss decreased (0.390900 --> 0.390864).  Saving model ...
Validation loss decreased (0.390864 --> 0.390828).  Saving model ...
Validation loss decreased (0.390828 --> 0.390793).  Saving model ...
Validation loss decreased (0.390793 --> 0.390757).  Saving model ...
Validation loss decreased (0.390757 --> 0.390721).  Saving model ...
Validation loss decreased (0.390721 --> 0.390685).  Saving model ...
Validation loss decreased (0.390685 --> 0.390649).  Saving model ...
Validation loss decreased (0.390649 --> 0.390613).  Saving model ...
Validation loss decreased (0.390613 --> 0.390577).  Saving model ...
Validation loss decreased (0.390577 --> 0.390541).  Saving model ...
Validation loss decreased (0.390541 --> 0.390505).  Saving model ...
Validation loss decreased (0.390505 --> 0.390469).  Saving model ...
Validation loss decreased (0.390469 --> 0.390433).  Saving model ...
Validation loss decreased (0.390433 --> 0.390397).  Saving model ...
Validation loss decreased (0.390397 --> 0.390361).  Saving model ...
Validation loss decreased (0.390361 --> 0.390324).  Saving model ...
Validation loss decreased (0.390324 --> 0.390288).  Saving model ...
Validation loss decreased (0.390288 --> 0.390252).  Saving model ...
Validation loss decreased (0.390252 --> 0.390216).  Saving model ...
Validation loss decreased (0.390216 --> 0.390179).  Saving model ...
Validation loss decreased (0.390179 --> 0.390143).  Saving model ...
Validation loss decreased (0.390143 --> 0.390106).  Saving model ...
Validation loss decreased (0.390106 --> 0.390070).  Saving model ...
Validation loss decreased (0.390070 --> 0.390033).  Saving model ...
Validation loss decreased (0.390033 --> 0.389997).  Saving model ...
Validation loss decreased (0.389997 --> 0.389960).  Saving model ...
Validation loss decreased (0.389960 --> 0.389923).  Saving model ...
Validation loss decreased (0.389923 --> 0.389886).  Saving model ...
Validation loss decreased (0.389886 --> 0.389850).  Saving model ...
Validation loss decreased (0.389850 --> 0.389813).  Saving model ...
Validation loss decreased (0.389813 --> 0.389776).  Saving model ...
Validation loss decreased (0.389776 --> 0.389739).  Saving model ...
Validation loss decreased (0.389739 --> 0.389702).  Saving model ...
Validation loss decreased (0.389702 --> 0.389665).  Saving model ...
Validation loss decreased (0.389665 --> 0.389627).  Saving model ...
Validation loss decreased (0.389627 --> 0.389590).  Saving model ...
Validation loss decreased (0.389590 --> 0.389553).  Saving model ...
Validation loss decreased (0.389553 --> 0.389515).  Saving model ...
Validation loss decreased (0.389515 --> 0.389478).  Saving model ...
Validation loss decreased (0.389478 --> 0.389440).  Saving model ...
Validation loss decreased (0.389440 --> 0.389403).  Saving model ...
Validation loss decreased (0.389403 --> 0.389365).  Saving model ...
Validation loss decreased (0.389365 --> 0.389327).  Saving model ...
Validation loss decreased (0.389327 --> 0.389290).  Saving model ...
Validation loss decreased (0.389290 --> 0.389252).  Saving model ...
Validation loss decreased (0.389252 --> 0.389214).  Saving model ...
Validation loss decreased (0.389214 --> 0.389176).  Saving model ...
Validation loss decreased (0.389176 --> 0.389138).  Saving model ...
Validation loss decreased (0.389138 --> 0.389100).  Saving model ...
Validation loss decreased (0.389100 --> 0.389061).  Saving model ...
Validation loss decreased (0.389061 --> 0.389023).  Saving model ...
Validation loss decreased (0.389023 --> 0.388985).  Saving model ...
Validation loss decreased (0.388985 --> 0.388946).  Saving model ...
Validation loss decreased (0.388946 --> 0.388908).  Saving model ...
Validation loss decreased (0.388908 --> 0.388869).  Saving model ...
Validation loss decreased (0.388869 --> 0.388831).  Saving model ...
Validation loss decreased (0.388831 --> 0.388792).  Saving model ...
Validation loss decreased (0.388792 --> 0.388753).  Saving model ...
Validation loss decreased (0.388753 --> 0.388714).  Saving model ...
Validation loss decreased (0.388714 --> 0.388675).  Saving model ...
Validation loss decreased (0.388675 --> 0.388636).  Saving model ...
Validation loss decreased (0.388636 --> 0.388597).  Saving model ...
Validation loss decreased (0.388597 --> 0.388558).  Saving model ...
Validation loss decreased (0.388558 --> 0.388518).  Saving model ...
Validation loss decreased (0.388518 --> 0.388479).  Saving model ...
epoch 4701, loss 0.3885, train acc 82.19%, f1 0.7306, precision 0.7747, recall 0.6912, auc 0.7916
Validation loss decreased (0.388479 --> 0.388440).  Saving model ...
Validation loss decreased (0.388440 --> 0.388400).  Saving model ...
Validation loss decreased (0.388400 --> 0.388360).  Saving model ...
Validation loss decreased (0.388360 --> 0.388321).  Saving model ...
Validation loss decreased (0.388321 --> 0.388281).  Saving model ...
Validation loss decreased (0.388281 --> 0.388241).  Saving model ...
Validation loss decreased (0.388241 --> 0.388201).  Saving model ...
Validation loss decreased (0.388201 --> 0.388161).  Saving model ...
Validation loss decreased (0.388161 --> 0.388121).  Saving model ...
Validation loss decreased (0.388121 --> 0.388081).  Saving model ...
Validation loss decreased (0.388081 --> 0.388041).  Saving model ...
Validation loss decreased (0.388041 --> 0.388000).  Saving model ...
Validation loss decreased (0.388000 --> 0.387960).  Saving model ...
Validation loss decreased (0.387960 --> 0.387919).  Saving model ...
Validation loss decreased (0.387919 --> 0.387879).  Saving model ...
Validation loss decreased (0.387879 --> 0.387838).  Saving model ...
Validation loss decreased (0.387838 --> 0.387797).  Saving model ...
Validation loss decreased (0.387797 --> 0.387756).  Saving model ...
Validation loss decreased (0.387756 --> 0.387715).  Saving model ...
Validation loss decreased (0.387715 --> 0.387674).  Saving model ...
Validation loss decreased (0.387674 --> 0.387633).  Saving model ...
Validation loss decreased (0.387633 --> 0.387591).  Saving model ...
Validation loss decreased (0.387591 --> 0.387550).  Saving model ...
Validation loss decreased (0.387550 --> 0.387509).  Saving model ...
Validation loss decreased (0.387509 --> 0.387467).  Saving model ...
Validation loss decreased (0.387467 --> 0.387425).  Saving model ...
Validation loss decreased (0.387425 --> 0.387383).  Saving model ...
Validation loss decreased (0.387383 --> 0.387341).  Saving model ...
Validation loss decreased (0.387341 --> 0.387299).  Saving model ...
Validation loss decreased (0.387299 --> 0.387257).  Saving model ...
Validation loss decreased (0.387257 --> 0.387215).  Saving model ...
Validation loss decreased (0.387215 --> 0.387172).  Saving model ...
Validation loss decreased (0.387172 --> 0.387130).  Saving model ...
Validation loss decreased (0.387130 --> 0.387087).  Saving model ...
Validation loss decreased (0.387087 --> 0.387044).  Saving model ...
Validation loss decreased (0.387044 --> 0.387002).  Saving model ...
Validation loss decreased (0.387002 --> 0.386959).  Saving model ...
Validation loss decreased (0.386959 --> 0.386916).  Saving model ...
Validation loss decreased (0.386916 --> 0.386872).  Saving model ...
Validation loss decreased (0.386872 --> 0.386829).  Saving model ...
Validation loss decreased (0.386829 --> 0.386786).  Saving model ...
Validation loss decreased (0.386786 --> 0.386742).  Saving model ...
Validation loss decreased (0.386742 --> 0.386698).  Saving model ...
Validation loss decreased (0.386698 --> 0.386655).  Saving model ...
Validation loss decreased (0.386655 --> 0.386611).  Saving model ...
Validation loss decreased (0.386611 --> 0.386567).  Saving model ...
Validation loss decreased (0.386567 --> 0.386523).  Saving model ...
Validation loss decreased (0.386523 --> 0.386478).  Saving model ...
Validation loss decreased (0.386478 --> 0.386434).  Saving model ...
Validation loss decreased (0.386434 --> 0.386390).  Saving model ...
Validation loss decreased (0.386390 --> 0.386345).  Saving model ...
Validation loss decreased (0.386345 --> 0.386301).  Saving model ...
Validation loss decreased (0.386301 --> 0.386256).  Saving model ...
Validation loss decreased (0.386256 --> 0.386212).  Saving model ...
Validation loss decreased (0.386212 --> 0.386167).  Saving model ...
Validation loss decreased (0.386167 --> 0.386122).  Saving model ...
Validation loss decreased (0.386122 --> 0.386077).  Saving model ...
Validation loss decreased (0.386077 --> 0.386032).  Saving model ...
Validation loss decreased (0.386032 --> 0.385987).  Saving model ...
Validation loss decreased (0.385987 --> 0.385942).  Saving model ...
Validation loss decreased (0.385942 --> 0.385897).  Saving model ...
Validation loss decreased (0.385897 --> 0.385852).  Saving model ...
Validation loss decreased (0.385852 --> 0.385807).  Saving model ...
Validation loss decreased (0.385807 --> 0.385762).  Saving model ...
Validation loss decreased (0.385762 --> 0.385716).  Saving model ...
Validation loss decreased (0.385716 --> 0.385671).  Saving model ...
Validation loss decreased (0.385671 --> 0.385626).  Saving model ...
Validation loss decreased (0.385626 --> 0.385581).  Saving model ...
Validation loss decreased (0.385581 --> 0.385536).  Saving model ...
Validation loss decreased (0.385536 --> 0.385491).  Saving model ...
Validation loss decreased (0.385491 --> 0.385446).  Saving model ...
Validation loss decreased (0.385446 --> 0.385401).  Saving model ...
Validation loss decreased (0.385401 --> 0.385356).  Saving model ...
Validation loss decreased (0.385356 --> 0.385311).  Saving model ...
Validation loss decreased (0.385311 --> 0.385266).  Saving model ...
Validation loss decreased (0.385266 --> 0.385221).  Saving model ...
Validation loss decreased (0.385221 --> 0.385176).  Saving model ...
Validation loss decreased (0.385176 --> 0.385132).  Saving model ...
Validation loss decreased (0.385132 --> 0.385087).  Saving model ...
Validation loss decreased (0.385087 --> 0.385042).  Saving model ...
Validation loss decreased (0.385042 --> 0.384998).  Saving model ...
Validation loss decreased (0.384998 --> 0.384954).  Saving model ...
Validation loss decreased (0.384954 --> 0.384909).  Saving model ...
Validation loss decreased (0.384909 --> 0.384865).  Saving model ...
Validation loss decreased (0.384865 --> 0.384821).  Saving model ...
Validation loss decreased (0.384821 --> 0.384777).  Saving model ...
Validation loss decreased (0.384777 --> 0.384733).  Saving model ...
Validation loss decreased (0.384733 --> 0.384689).  Saving model ...
Validation loss decreased (0.384689 --> 0.384645).  Saving model ...
Validation loss decreased (0.384645 --> 0.384602).  Saving model ...
Validation loss decreased (0.384602 --> 0.384558).  Saving model ...
Validation loss decreased (0.384558 --> 0.384515).  Saving model ...
Validation loss decreased (0.384515 --> 0.384471).  Saving model ...
Validation loss decreased (0.384471 --> 0.384428).  Saving model ...
Validation loss decreased (0.384428 --> 0.384385).  Saving model ...
Validation loss decreased (0.384385 --> 0.384342).  Saving model ...
Validation loss decreased (0.384342 --> 0.384299).  Saving model ...
Validation loss decreased (0.384299 --> 0.384256).  Saving model ...
Validation loss decreased (0.384256 --> 0.384213).  Saving model ...
Validation loss decreased (0.384213 --> 0.384171).  Saving model ...
epoch 4801, loss 0.3842, train acc 82.71%, f1 0.7404, precision 0.7784, recall 0.7059, auc 0.7990
Validation loss decreased (0.384171 --> 0.384128).  Saving model ...
Validation loss decreased (0.384128 --> 0.384085).  Saving model ...
Validation loss decreased (0.384085 --> 0.384043).  Saving model ...
Validation loss decreased (0.384043 --> 0.384000).  Saving model ...
Validation loss decreased (0.384000 --> 0.383958).  Saving model ...
Validation loss decreased (0.383958 --> 0.383916).  Saving model ...
Validation loss decreased (0.383916 --> 0.383874).  Saving model ...
Validation loss decreased (0.383874 --> 0.383832).  Saving model ...
Validation loss decreased (0.383832 --> 0.383790).  Saving model ...
Validation loss decreased (0.383790 --> 0.383748).  Saving model ...
Validation loss decreased (0.383748 --> 0.383706).  Saving model ...
Validation loss decreased (0.383706 --> 0.383664).  Saving model ...
Validation loss decreased (0.383664 --> 0.383622).  Saving model ...
Validation loss decreased (0.383622 --> 0.383580).  Saving model ...
Validation loss decreased (0.383580 --> 0.383539).  Saving model ...
Validation loss decreased (0.383539 --> 0.383497).  Saving model ...
Validation loss decreased (0.383497 --> 0.383456).  Saving model ...
Validation loss decreased (0.383456 --> 0.383414).  Saving model ...
Validation loss decreased (0.383414 --> 0.383373).  Saving model ...
Validation loss decreased (0.383373 --> 0.383331).  Saving model ...
Validation loss decreased (0.383331 --> 0.383290).  Saving model ...
Validation loss decreased (0.383290 --> 0.383248).  Saving model ...
Validation loss decreased (0.383248 --> 0.383207).  Saving model ...
Validation loss decreased (0.383207 --> 0.383166).  Saving model ...
Validation loss decreased (0.383166 --> 0.383125).  Saving model ...
Validation loss decreased (0.383125 --> 0.383083).  Saving model ...
Validation loss decreased (0.383083 --> 0.383042).  Saving model ...
Validation loss decreased (0.383042 --> 0.383001).  Saving model ...
Validation loss decreased (0.383001 --> 0.382960).  Saving model ...
Validation loss decreased (0.382960 --> 0.382919).  Saving model ...
Validation loss decreased (0.382919 --> 0.382878).  Saving model ...
Validation loss decreased (0.382878 --> 0.382837).  Saving model ...
Validation loss decreased (0.382837 --> 0.382796).  Saving model ...
Validation loss decreased (0.382796 --> 0.382755).  Saving model ...
Validation loss decreased (0.382755 --> 0.382714).  Saving model ...
Validation loss decreased (0.382714 --> 0.382673).  Saving model ...
Validation loss decreased (0.382673 --> 0.382632).  Saving model ...
Validation loss decreased (0.382632 --> 0.382591).  Saving model ...
Validation loss decreased (0.382591 --> 0.382550).  Saving model ...
Validation loss decreased (0.382550 --> 0.382509).  Saving model ...
Validation loss decreased (0.382509 --> 0.382468).  Saving model ...
Validation loss decreased (0.382468 --> 0.382428).  Saving model ...
Validation loss decreased (0.382428 --> 0.382387).  Saving model ...
Validation loss decreased (0.382387 --> 0.382346).  Saving model ...
Validation loss decreased (0.382346 --> 0.382305).  Saving model ...
Validation loss decreased (0.382305 --> 0.382264).  Saving model ...
Validation loss decreased (0.382264 --> 0.382224).  Saving model ...
Validation loss decreased (0.382224 --> 0.382183).  Saving model ...
Validation loss decreased (0.382183 --> 0.382142).  Saving model ...
Validation loss decreased (0.382142 --> 0.382101).  Saving model ...
Validation loss decreased (0.382101 --> 0.382061).  Saving model ...
Validation loss decreased (0.382061 --> 0.382020).  Saving model ...
Validation loss decreased (0.382020 --> 0.381979).  Saving model ...
Validation loss decreased (0.381979 --> 0.381938).  Saving model ...
Validation loss decreased (0.381938 --> 0.381898).  Saving model ...
Validation loss decreased (0.381898 --> 0.381857).  Saving model ...
Validation loss decreased (0.381857 --> 0.381816).  Saving model ...
Validation loss decreased (0.381816 --> 0.381775).  Saving model ...
Validation loss decreased (0.381775 --> 0.381735).  Saving model ...
Validation loss decreased (0.381735 --> 0.381694).  Saving model ...
Validation loss decreased (0.381694 --> 0.381653).  Saving model ...
Validation loss decreased (0.381653 --> 0.381612).  Saving model ...
Validation loss decreased (0.381612 --> 0.381572).  Saving model ...
Validation loss decreased (0.381572 --> 0.381531).  Saving model ...
Validation loss decreased (0.381531 --> 0.381490).  Saving model ...
Validation loss decreased (0.381490 --> 0.381450).  Saving model ...
Validation loss decreased (0.381450 --> 0.381409).  Saving model ...
Validation loss decreased (0.381409 --> 0.381368).  Saving model ...
Validation loss decreased (0.381368 --> 0.381327).  Saving model ...
Validation loss decreased (0.381327 --> 0.381287).  Saving model ...
Validation loss decreased (0.381287 --> 0.381246).  Saving model ...
Validation loss decreased (0.381246 --> 0.381205).  Saving model ...
Validation loss decreased (0.381205 --> 0.381165).  Saving model ...
Validation loss decreased (0.381165 --> 0.381124).  Saving model ...
Validation loss decreased (0.381124 --> 0.381083).  Saving model ...
Validation loss decreased (0.381083 --> 0.381043).  Saving model ...
Validation loss decreased (0.381043 --> 0.381002).  Saving model ...
Validation loss decreased (0.381002 --> 0.380961).  Saving model ...
Validation loss decreased (0.380961 --> 0.380920).  Saving model ...
Validation loss decreased (0.380920 --> 0.380880).  Saving model ...
Validation loss decreased (0.380880 --> 0.380839).  Saving model ...
Validation loss decreased (0.380839 --> 0.380798).  Saving model ...
Validation loss decreased (0.380798 --> 0.380758).  Saving model ...
Validation loss decreased (0.380758 --> 0.380717).  Saving model ...
Validation loss decreased (0.380717 --> 0.380676).  Saving model ...
Validation loss decreased (0.380676 --> 0.380636).  Saving model ...
Validation loss decreased (0.380636 --> 0.380595).  Saving model ...
Validation loss decreased (0.380595 --> 0.380554).  Saving model ...
Validation loss decreased (0.380554 --> 0.380513).  Saving model ...
Validation loss decreased (0.380513 --> 0.380473).  Saving model ...
Validation loss decreased (0.380473 --> 0.380432).  Saving model ...
Validation loss decreased (0.380432 --> 0.380391).  Saving model ...
Validation loss decreased (0.380391 --> 0.380351).  Saving model ...
Validation loss decreased (0.380351 --> 0.380310).  Saving model ...
Validation loss decreased (0.380310 --> 0.380269).  Saving model ...
Validation loss decreased (0.380269 --> 0.380229).  Saving model ...
Validation loss decreased (0.380229 --> 0.380188).  Saving model ...
Validation loss decreased (0.380188 --> 0.380148).  Saving model ...
Validation loss decreased (0.380148 --> 0.380107).  Saving model ...
Validation loss decreased (0.380107 --> 0.380066).  Saving model ...
epoch 4901, loss 0.3801, train acc 81.85%, f1 0.7268, precision 0.7663, recall 0.6912, auc 0.7890
Validation loss decreased (0.380066 --> 0.380026).  Saving model ...
Validation loss decreased (0.380026 --> 0.379985).  Saving model ...
Validation loss decreased (0.379985 --> 0.379944).  Saving model ...
Validation loss decreased (0.379944 --> 0.379904).  Saving model ...
Validation loss decreased (0.379904 --> 0.379863).  Saving model ...
Validation loss decreased (0.379863 --> 0.379823).  Saving model ...
Validation loss decreased (0.379823 --> 0.379782).  Saving model ...
Validation loss decreased (0.379782 --> 0.379742).  Saving model ...
Validation loss decreased (0.379742 --> 0.379701).  Saving model ...
Validation loss decreased (0.379701 --> 0.379661).  Saving model ...
Validation loss decreased (0.379661 --> 0.379620).  Saving model ...
Validation loss decreased (0.379620 --> 0.379580).  Saving model ...
Validation loss decreased (0.379580 --> 0.379539).  Saving model ...
Validation loss decreased (0.379539 --> 0.379499).  Saving model ...
Validation loss decreased (0.379499 --> 0.379458).  Saving model ...
Validation loss decreased (0.379458 --> 0.379418).  Saving model ...
Validation loss decreased (0.379418 --> 0.379377).  Saving model ...
Validation loss decreased (0.379377 --> 0.379337).  Saving model ...
Validation loss decreased (0.379337 --> 0.379297).  Saving model ...
Validation loss decreased (0.379297 --> 0.379256).  Saving model ...
Validation loss decreased (0.379256 --> 0.379216).  Saving model ...
Validation loss decreased (0.379216 --> 0.379176).  Saving model ...
Validation loss decreased (0.379176 --> 0.379135).  Saving model ...
Validation loss decreased (0.379135 --> 0.379095).  Saving model ...
Validation loss decreased (0.379095 --> 0.379055).  Saving model ...
Validation loss decreased (0.379055 --> 0.379015).  Saving model ...
Validation loss decreased (0.379015 --> 0.378974).  Saving model ...
Validation loss decreased (0.378974 --> 0.378934).  Saving model ...
Validation loss decreased (0.378934 --> 0.378894).  Saving model ...
Validation loss decreased (0.378894 --> 0.378854).  Saving model ...
Validation loss decreased (0.378854 --> 0.378814).  Saving model ...
Validation loss decreased (0.378814 --> 0.378774).  Saving model ...
Validation loss decreased (0.378774 --> 0.378733).  Saving model ...
Validation loss decreased (0.378733 --> 0.378693).  Saving model ...
Validation loss decreased (0.378693 --> 0.378653).  Saving model ...
Validation loss decreased (0.378653 --> 0.378613).  Saving model ...
Validation loss decreased (0.378613 --> 0.378573).  Saving model ...
Validation loss decreased (0.378573 --> 0.378534).  Saving model ...
Validation loss decreased (0.378534 --> 0.378494).  Saving model .../home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Validation loss decreased (0.378494 --> 0.378454).  Saving model ...
Validation loss decreased (0.378454 --> 0.378414).  Saving model ...
Validation loss decreased (0.378414 --> 0.378374).  Saving model ...
Validation loss decreased (0.378374 --> 0.378334).  Saving model ...
Validation loss decreased (0.378334 --> 0.378295).  Saving model ...
Validation loss decreased (0.378295 --> 0.378255).  Saving model ...
Validation loss decreased (0.378255 --> 0.378215).  Saving model ...
Validation loss decreased (0.378215 --> 0.378176).  Saving model ...
Validation loss decreased (0.378176 --> 0.378136).  Saving model ...
Validation loss decreased (0.378136 --> 0.378096).  Saving model ...
Validation loss decreased (0.378096 --> 0.378057).  Saving model ...
Validation loss decreased (0.378057 --> 0.378017).  Saving model ...
Validation loss decreased (0.378017 --> 0.377978).  Saving model ...
Validation loss decreased (0.377978 --> 0.377938).  Saving model ...
Validation loss decreased (0.377938 --> 0.377899).  Saving model ...
Validation loss decreased (0.377899 --> 0.377859).  Saving model ...
Validation loss decreased (0.377859 --> 0.377820).  Saving model ...
Validation loss decreased (0.377820 --> 0.377781).  Saving model ...
Validation loss decreased (0.377781 --> 0.377742).  Saving model ...
Validation loss decreased (0.377742 --> 0.377702).  Saving model ...
Validation loss decreased (0.377702 --> 0.377663).  Saving model ...
Validation loss decreased (0.377663 --> 0.377624).  Saving model ...
Validation loss decreased (0.377624 --> 0.377585).  Saving model ...
Validation loss decreased (0.377585 --> 0.377546).  Saving model ...
Validation loss decreased (0.377546 --> 0.377507).  Saving model ...
Validation loss decreased (0.377507 --> 0.377468).  Saving model ...
Validation loss decreased (0.377468 --> 0.377429).  Saving model ...
Validation loss decreased (0.377429 --> 0.377390).  Saving model ...
Validation loss decreased (0.377390 --> 0.377351).  Saving model ...
Validation loss decreased (0.377351 --> 0.377312).  Saving model ...
Validation loss decreased (0.377312 --> 0.377273).  Saving model ...
Validation loss decreased (0.377273 --> 0.377234).  Saving model ...
Validation loss decreased (0.377234 --> 0.377195).  Saving model ...
Validation loss decreased (0.377195 --> 0.377157).  Saving model ...
Validation loss decreased (0.377157 --> 0.377118).  Saving model ...
Validation loss decreased (0.377118 --> 0.377079).  Saving model ...
Validation loss decreased (0.377079 --> 0.377041).  Saving model ...
Validation loss decreased (0.377041 --> 0.377002).  Saving model ...
Validation loss decreased (0.377002 --> 0.376964).  Saving model ...
Validation loss decreased (0.376964 --> 0.376925).  Saving model ...
Validation loss decreased (0.376925 --> 0.376887).  Saving model ...
Validation loss decreased (0.376887 --> 0.376848).  Saving model ...
Validation loss decreased (0.376848 --> 0.376810).  Saving model ...
Validation loss decreased (0.376810 --> 0.376772).  Saving model ...
Validation loss decreased (0.376772 --> 0.376733).  Saving model ...
Validation loss decreased (0.376733 --> 0.376695).  Saving model ...
Validation loss decreased (0.376695 --> 0.376657).  Saving model ...
Validation loss decreased (0.376657 --> 0.376619).  Saving model ...
Validation loss decreased (0.376619 --> 0.376580).  Saving model ...
Validation loss decreased (0.376580 --> 0.376542).  Saving model ...
Validation loss decreased (0.376542 --> 0.376504).  Saving model ...
Validation loss decreased (0.376504 --> 0.376466).  Saving model ...
Validation loss decreased (0.376466 --> 0.376428).  Saving model ...
Validation loss decreased (0.376428 --> 0.376390).  Saving model ...
Validation loss decreased (0.376390 --> 0.376352).  Saving model ...
Validation loss decreased (0.376352 --> 0.376314).  Saving model ...
Validation loss decreased (0.376314 --> 0.376277).  Saving model ...
Validation loss decreased (0.376277 --> 0.376239).  Saving model ...
Validation loss decreased (0.376239 --> 0.376201).  Saving model ...
Validation loss decreased (0.376201 --> 0.376163).  Saving model ...
Validation loss decreased (0.376163 --> 0.376125).  Saving model ...
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_normal_True
normal_normal
./test_pima/model_MLP_normal_True/record_1/MLP_normal_True_1
./test_pima/result_MLP_normal_True_normal_normal/record_1/
----------------------



Traceback (most recent call last):
  File "./classifier_MLP/test.py", line 193, in <module>
    transform_method, ref_data_type, ref_num_type, ref_times, boundary_type = get_test_info(test_method)
  File "./classifier_MLP/test.py", line 137, in get_test_info
    return transform_method, ref_data_type, ref_num_type, ref_times, boundary_type
UnboundLocalError: local variable 'transform_method' referenced before assignment
