nohup: ignoring input
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_1
----------------------



epoch 1, loss 0.6932, train acc 51.75%, f1 0.0676, precision 1.0000, recall 0.0350, auc 0.5175
Validation loss decreased (inf --> 0.692923).  Saving model ...
Validation loss decreased (0.692923 --> 0.692610).  Saving model ...
Validation loss decreased (0.692610 --> 0.692281).  Saving model ...
Validation loss decreased (0.692281 --> 0.691929).  Saving model ...
Validation loss decreased (0.691929 --> 0.691564).  Saving model ...
Validation loss decreased (0.691564 --> 0.691164).  Saving model ...
Validation loss decreased (0.691164 --> 0.690723).  Saving model ...
Validation loss decreased (0.690723 --> 0.690233).  Saving model ...
Validation loss decreased (0.690233 --> 0.689697).  Saving model ...
Validation loss decreased (0.689697 --> 0.689098).  Saving model ...
Validation loss decreased (0.689098 --> 0.688447).  Saving model ...
Validation loss decreased (0.688447 --> 0.687733).  Saving model ...
Validation loss decreased (0.687733 --> 0.686953).  Saving model ...
Validation loss decreased (0.686953 --> 0.686116).  Saving model ...
Validation loss decreased (0.686116 --> 0.685217).  Saving model ...
Validation loss decreased (0.685217 --> 0.684293).  Saving model ...
Validation loss decreased (0.684293 --> 0.683297).  Saving model ...
Validation loss decreased (0.683297 --> 0.682269).  Saving model ...
Validation loss decreased (0.682269 --> 0.681151).  Saving model ...
Validation loss decreased (0.681151 --> 0.679963).  Saving model ...
Validation loss decreased (0.679963 --> 0.678683).  Saving model ...
Validation loss decreased (0.678683 --> 0.677331).  Saving model ...
Validation loss decreased (0.677331 --> 0.675895).  Saving model ...
Validation loss decreased (0.675895 --> 0.674399).  Saving model ...
Validation loss decreased (0.674399 --> 0.672859).  Saving model ...
Validation loss decreased (0.672859 --> 0.671277).  Saving model ...
Validation loss decreased (0.671277 --> 0.669628).  Saving model ...
Validation loss decreased (0.669628 --> 0.667900).  Saving model ...
Validation loss decreased (0.667900 --> 0.666051).  Saving model ...
Validation loss decreased (0.666051 --> 0.664092).  Saving model ...
Validation loss decreased (0.664092 --> 0.662038).  Saving model ...
Validation loss decreased (0.662038 --> 0.659898).  Saving model ...
Validation loss decreased (0.659898 --> 0.657749).  Saving model ...
Validation loss decreased (0.657749 --> 0.655539).  Saving model ...
Validation loss decreased (0.655539 --> 0.653251).  Saving model ...
Validation loss decreased (0.653251 --> 0.650857).  Saving model ...
Validation loss decreased (0.650857 --> 0.648426).  Saving model ...
Validation loss decreased (0.648426 --> 0.645949).  Saving model ...
Validation loss decreased (0.645949 --> 0.643361).  Saving model ...
Validation loss decreased (0.643361 --> 0.640690).  Saving model ...
Validation loss decreased (0.640690 --> 0.637932).  Saving model ...
Validation loss decreased (0.637932 --> 0.635121).  Saving model ...
Validation loss decreased (0.635121 --> 0.632166).  Saving model ...
Validation loss decreased (0.632166 --> 0.629129).  Saving model ...
Validation loss decreased (0.629129 --> 0.626006).  Saving model ...
Validation loss decreased (0.626006 --> 0.622771).  Saving model ...
Validation loss decreased (0.622771 --> 0.619507).  Saving model ...
Validation loss decreased (0.619507 --> 0.616318).  Saving model ...
Validation loss decreased (0.616318 --> 0.613022).  Saving model ...
Validation loss decreased (0.613022 --> 0.609614).  Saving model ...
Validation loss decreased (0.609614 --> 0.606188).  Saving model ...
Validation loss decreased (0.606188 --> 0.602674).  Saving model ...
Validation loss decreased (0.602674 --> 0.599137).  Saving model ...
Validation loss decreased (0.599137 --> 0.595521).  Saving model ...
Validation loss decreased (0.595521 --> 0.592022).  Saving model ...
Validation loss decreased (0.592022 --> 0.588501).  Saving model ...
Validation loss decreased (0.588501 --> 0.584827).  Saving model ...
Validation loss decreased (0.584827 --> 0.581134).  Saving model ...
Validation loss decreased (0.581134 --> 0.577461).  Saving model ...
Validation loss decreased (0.577461 --> 0.573860).  Saving model ...
Validation loss decreased (0.573860 --> 0.570136).  Saving model ...
Validation loss decreased (0.570136 --> 0.566408).  Saving model ...
Validation loss decreased (0.566408 --> 0.562681).  Saving model ...
Validation loss decreased (0.562681 --> 0.558887).  Saving model ...
Validation loss decreased (0.558887 --> 0.555026).  Saving model ...
Validation loss decreased (0.555026 --> 0.551205).  Saving model ...
Validation loss decreased (0.551205 --> 0.547526).  Saving model ...
Validation loss decreased (0.547526 --> 0.543615).  Saving model ...
Validation loss decreased (0.543615 --> 0.539595).  Saving model ...
Validation loss decreased (0.539595 --> 0.535557).  Saving model ...
Validation loss decreased (0.535557 --> 0.531525).  Saving model ...
Validation loss decreased (0.531525 --> 0.527506).  Saving model ...
Validation loss decreased (0.527506 --> 0.523601).  Saving model ...
Validation loss decreased (0.523601 --> 0.519546).  Saving model ...
Validation loss decreased (0.519546 --> 0.515456).  Saving model ...
Validation loss decreased (0.515456 --> 0.511260).  Saving model ...
Validation loss decreased (0.511260 --> 0.507228).  Saving model ...
Validation loss decreased (0.507228 --> 0.503015).  Saving model ...
Validation loss decreased (0.503015 --> 0.498880).  Saving model ...
Validation loss decreased (0.498880 --> 0.494680).  Saving model ...
Validation loss decreased (0.494680 --> 0.490417).  Saving model ...
Validation loss decreased (0.490417 --> 0.486198).  Saving model ...
Validation loss decreased (0.486198 --> 0.482048).  Saving model ...
Validation loss decreased (0.482048 --> 0.477953).  Saving model ...
Validation loss decreased (0.477953 --> 0.473980).  Saving model ...
Validation loss decreased (0.473980 --> 0.470013).  Saving model ...
Validation loss decreased (0.470013 --> 0.466136).  Saving model ...
Validation loss decreased (0.466136 --> 0.462182).  Saving model ...
Validation loss decreased (0.462182 --> 0.458259).  Saving model ...
Validation loss decreased (0.458259 --> 0.454364).  Saving model ...
Validation loss decreased (0.454364 --> 0.450491).  Saving model ...
Validation loss decreased (0.450491 --> 0.446602).  Saving model ...
Validation loss decreased (0.446602 --> 0.442881).  Saving model ...
Validation loss decreased (0.442881 --> 0.439223).  Saving model ...
Validation loss decreased (0.439223 --> 0.435618).  Saving model ...
Validation loss decreased (0.435618 --> 0.432040).  Saving model ...
Validation loss decreased (0.432040 --> 0.428454).  Saving model ...
Validation loss decreased (0.428454 --> 0.424862).  Saving model ...
Validation loss decreased (0.424862 --> 0.421270).  Saving model ...
Validation loss decreased (0.421270 --> 0.417624).  Saving model ...
epoch 101, loss 0.5750, train acc 96.25%, f1 0.9626, precision 0.9602, recall 0.9650, auc 0.9625
Validation loss decreased (0.417624 --> 0.414072).  Saving model ...
Validation loss decreased (0.414072 --> 0.410462).  Saving model ...
Validation loss decreased (0.410462 --> 0.406895).  Saving model ...
Validation loss decreased (0.406895 --> 0.403415).  Saving model ...
Validation loss decreased (0.403415 --> 0.399993).  Saving model ...
Validation loss decreased (0.399993 --> 0.396489).  Saving model ...
Validation loss decreased (0.396489 --> 0.393026).  Saving model ...
Validation loss decreased (0.393026 --> 0.389737).  Saving model ...
Validation loss decreased (0.389737 --> 0.386520).  Saving model ...
Validation loss decreased (0.386520 --> 0.383304).  Saving model ...
Validation loss decreased (0.383304 --> 0.380131).  Saving model ...
Validation loss decreased (0.380131 --> 0.376998).  Saving model ...
Validation loss decreased (0.376998 --> 0.373823).  Saving model ...
Validation loss decreased (0.373823 --> 0.370615).  Saving model ...
Validation loss decreased (0.370615 --> 0.367340).  Saving model ...
Validation loss decreased (0.367340 --> 0.364212).  Saving model ...
Validation loss decreased (0.364212 --> 0.361217).  Saving model ...
Validation loss decreased (0.361217 --> 0.358178).  Saving model ...
Validation loss decreased (0.358178 --> 0.355165).  Saving model ...
Validation loss decreased (0.355165 --> 0.352161).  Saving model ...
Validation loss decreased (0.352161 --> 0.349184).  Saving model ...
Validation loss decreased (0.349184 --> 0.346312).  Saving model ...
Validation loss decreased (0.346312 --> 0.343599).  Saving model ...
Validation loss decreased (0.343599 --> 0.340881).  Saving model ...
Validation loss decreased (0.340881 --> 0.338155).  Saving model ...
Validation loss decreased (0.338155 --> 0.335431).  Saving model ...
Validation loss decreased (0.335431 --> 0.332745).  Saving model ...
Validation loss decreased (0.332745 --> 0.330075).  Saving model ...
Validation loss decreased (0.330075 --> 0.327366).  Saving model ...
Validation loss decreased (0.327366 --> 0.324743).  Saving model ...
Validation loss decreased (0.324743 --> 0.322138).  Saving model ...
Validation loss decreased (0.322138 --> 0.319571).  Saving model ...
Validation loss decreased (0.319571 --> 0.316978).  Saving model ...
Validation loss decreased (0.316978 --> 0.314462).  Saving model ...
Validation loss decreased (0.314462 --> 0.311948).  Saving model ...
Validation loss decreased (0.311948 --> 0.309439).  Saving model ...
Validation loss decreased (0.309439 --> 0.307100).  Saving model ...
Validation loss decreased (0.307100 --> 0.304836).  Saving model ...
Validation loss decreased (0.304836 --> 0.302570).  Saving model ...
Validation loss decreased (0.302570 --> 0.300380).  Saving model ...
Validation loss decreased (0.300380 --> 0.298172).  Saving model ...
Validation loss decreased (0.298172 --> 0.296135).  Saving model ...
Validation loss decreased (0.296135 --> 0.294017).  Saving model ...
Validation loss decreased (0.294017 --> 0.291942).  Saving model ...
Validation loss decreased (0.291942 --> 0.289854).  Saving model ...
Validation loss decreased (0.289854 --> 0.287713).  Saving model ...
Validation loss decreased (0.287713 --> 0.285593).  Saving model ...
Validation loss decreased (0.285593 --> 0.283507).  Saving model ...
Validation loss decreased (0.283507 --> 0.281353).  Saving model ...
Validation loss decreased (0.281353 --> 0.279274).  Saving model ...
Validation loss decreased (0.279274 --> 0.277265).  Saving model ...
Validation loss decreased (0.277265 --> 0.275244).  Saving model ...
Validation loss decreased (0.275244 --> 0.273247).  Saving model ...
Validation loss decreased (0.273247 --> 0.271300).  Saving model ...
Validation loss decreased (0.271300 --> 0.269388).  Saving model ...
Validation loss decreased (0.269388 --> 0.267504).  Saving model ...
Validation loss decreased (0.267504 --> 0.265548).  Saving model ...
Validation loss decreased (0.265548 --> 0.263643).  Saving model ...
Validation loss decreased (0.263643 --> 0.261754).  Saving model ...
Validation loss decreased (0.261754 --> 0.259953).  Saving model ...
Validation loss decreased (0.259953 --> 0.258107).  Saving model ...
Validation loss decreased (0.258107 --> 0.256357).  Saving model ...
Validation loss decreased (0.256357 --> 0.254730).  Saving model ...
Validation loss decreased (0.254730 --> 0.253126).  Saving model ...
Validation loss decreased (0.253126 --> 0.251602).  Saving model ...
Validation loss decreased (0.251602 --> 0.250059).  Saving model ...
Validation loss decreased (0.250059 --> 0.248526).  Saving model ...
Validation loss decreased (0.248526 --> 0.246991).  Saving model ...
Validation loss decreased (0.246991 --> 0.245545).  Saving model ...
Validation loss decreased (0.245545 --> 0.244103).  Saving model ...
Validation loss decreased (0.244103 --> 0.242601).  Saving model ...
Validation loss decreased (0.242601 --> 0.241121).  Saving model ...
Validation loss decreased (0.241121 --> 0.239708).  Saving model ...
Validation loss decreased (0.239708 --> 0.238275).  Saving model ...
Validation loss decreased (0.238275 --> 0.236819).  Saving model ...
Validation loss decreased (0.236819 --> 0.235403).  Saving model ...
Validation loss decreased (0.235403 --> 0.233977).  Saving model ...
Validation loss decreased (0.233977 --> 0.232687).  Saving model ...
Validation loss decreased (0.232687 --> 0.231342).  Saving model ...
Validation loss decreased (0.231342 --> 0.229920).  Saving model ...
Validation loss decreased (0.229920 --> 0.228588).  Saving model ...
Validation loss decreased (0.228588 --> 0.227341).  Saving model ...
Validation loss decreased (0.227341 --> 0.226200).  Saving model ...
Validation loss decreased (0.226200 --> 0.225038).  Saving model ...
Validation loss decreased (0.225038 --> 0.223850).  Saving model ...
Validation loss decreased (0.223850 --> 0.222656).  Saving model ...
Validation loss decreased (0.222656 --> 0.221410).  Saving model ...
Validation loss decreased (0.221410 --> 0.220185).  Saving model ...
Validation loss decreased (0.220185 --> 0.218969).  Saving model ...
Validation loss decreased (0.218969 --> 0.217883).  Saving model ...
Validation loss decreased (0.217883 --> 0.216851).  Saving model ...
Validation loss decreased (0.216851 --> 0.215856).  Saving model ...
Validation loss decreased (0.215856 --> 0.214780).  Saving model ...
Validation loss decreased (0.214780 --> 0.213673).  Saving model ...
Validation loss decreased (0.213673 --> 0.212672).  Saving model ...
Validation loss decreased (0.212672 --> 0.211676).  Saving model ...
Validation loss decreased (0.211676 --> 0.210658).  Saving model ...
Validation loss decreased (0.210658 --> 0.209668).  Saving model ...
Validation loss decreased (0.209668 --> 0.208635).  Saving model ...
Validation loss decreased (0.208635 --> 0.207642).  Saving model ...
epoch 201, loss 0.4931, train acc 96.50%, f1 0.9650, precision 0.9650, recall 0.9650, auc 0.9650
Validation loss decreased (0.207642 --> 0.206709).  Saving model ...
Validation loss decreased (0.206709 --> 0.205794).  Saving model ...
Validation loss decreased (0.205794 --> 0.204857).  Saving model ...
Validation loss decreased (0.204857 --> 0.203890).  Saving model ...
Validation loss decreased (0.203890 --> 0.202947).  Saving model ...
Validation loss decreased (0.202947 --> 0.201980).  Saving model ...
Validation loss decreased (0.201980 --> 0.201051).  Saving model ...
Validation loss decreased (0.201051 --> 0.200085).  Saving model ...
Validation loss decreased (0.200085 --> 0.199126).  Saving model ...
Validation loss decreased (0.199126 --> 0.198171).  Saving model ...
Validation loss decreased (0.198171 --> 0.197199).  Saving model ...
Validation loss decreased (0.197199 --> 0.196230).  Saving model ...
Validation loss decreased (0.196230 --> 0.195201).  Saving model ...
Validation loss decreased (0.195201 --> 0.194104).  Saving model ...
Validation loss decreased (0.194104 --> 0.193092).  Saving model ...
Validation loss decreased (0.193092 --> 0.192081).  Saving model ...
Validation loss decreased (0.192081 --> 0.191168).  Saving model ...
Validation loss decreased (0.191168 --> 0.190329).  Saving model ...
Validation loss decreased (0.190329 --> 0.189599).  Saving model ...
Validation loss decreased (0.189599 --> 0.188884).  Saving model ...
Validation loss decreased (0.188884 --> 0.188140).  Saving model ...
Validation loss decreased (0.188140 --> 0.187376).  Saving model ...
Validation loss decreased (0.187376 --> 0.186623).  Saving model ...
Validation loss decreased (0.186623 --> 0.185868).  Saving model ...
Validation loss decreased (0.185868 --> 0.185131).  Saving model ...
Validation loss decreased (0.185131 --> 0.184341).  Saving model ...
Validation loss decreased (0.184341 --> 0.183544).  Saving model ...
Validation loss decreased (0.183544 --> 0.182801).  Saving model ...
Validation loss decreased (0.182801 --> 0.181989).  Saving model ...
Validation loss decreased (0.181989 --> 0.181281).  Saving model ...
Validation loss decreased (0.181281 --> 0.180642).  Saving model ...
Validation loss decreased (0.180642 --> 0.180014).  Saving model ...
Validation loss decreased (0.180014 --> 0.179349).  Saving model ...
Validation loss decreased (0.179349 --> 0.178796).  Saving model ...
Validation loss decreased (0.178796 --> 0.178238).  Saving model ...
Validation loss decreased (0.178238 --> 0.177741).  Saving model ...
Validation loss decreased (0.177741 --> 0.177253).  Saving model ...
Validation loss decreased (0.177253 --> 0.176635).  Saving model ...
Validation loss decreased (0.176635 --> 0.176030).  Saving model ...
Validation loss decreased (0.176030 --> 0.175408).  Saving model ...
Validation loss decreased (0.175408 --> 0.174825).  Saving model ...
Validation loss decreased (0.174825 --> 0.174251).  Saving model ...
Validation loss decreased (0.174251 --> 0.173634).  Saving model ...
Validation loss decreased (0.173634 --> 0.173032).  Saving model ...
Validation loss decreased (0.173032 --> 0.172499).  Saving model ...
Validation loss decreased (0.172499 --> 0.171913).  Saving model ...
Validation loss decreased (0.171913 --> 0.171300).  Saving model ...
Validation loss decreased (0.171300 --> 0.170756).  Saving model ...
Validation loss decreased (0.170756 --> 0.170202).  Saving model ...
Validation loss decreased (0.170202 --> 0.169708).  Saving model ...
Validation loss decreased (0.169708 --> 0.169177).  Saving model ...
Validation loss decreased (0.169177 --> 0.168651).  Saving model ...
Validation loss decreased (0.168651 --> 0.168061).  Saving model ...
Validation loss decreased (0.168061 --> 0.167512).  Saving model ...
Validation loss decreased (0.167512 --> 0.166965).  Saving model ...
Validation loss decreased (0.166965 --> 0.166433).  Saving model ...
Validation loss decreased (0.166433 --> 0.165823).  Saving model ...
Validation loss decreased (0.165823 --> 0.165325).  Saving model ...
Validation loss decreased (0.165325 --> 0.164808).  Saving model ...
Validation loss decreased (0.164808 --> 0.164329).  Saving model ...
Validation loss decreased (0.164329 --> 0.163903).  Saving model ...
Validation loss decreased (0.163903 --> 0.163542).  Saving model ...
Validation loss decreased (0.163542 --> 0.163277).  Saving model ...
Validation loss decreased (0.163277 --> 0.162944).  Saving model ...
Validation loss decreased (0.162944 --> 0.162559).  Saving model ...
Validation loss decreased (0.162559 --> 0.162133).  Saving model ...
Validation loss decreased (0.162133 --> 0.161634).  Saving model ...
Validation loss decreased (0.161634 --> 0.161169).  Saving model ...
Validation loss decreased (0.161169 --> 0.160702).  Saving model ...
Validation loss decreased (0.160702 --> 0.160175).  Saving model ...
Validation loss decreased (0.160175 --> 0.159748).  Saving model ...
Validation loss decreased (0.159748 --> 0.159364).  Saving model ...
Validation loss decreased (0.159364 --> 0.159024).  Saving model ...
Validation loss decreased (0.159024 --> 0.158552).  Saving model ...
Validation loss decreased (0.158552 --> 0.158037).  Saving model ...
Validation loss decreased (0.158037 --> 0.157550).  Saving model ...
Validation loss decreased (0.157550 --> 0.157090).  Saving model ...
Validation loss decreased (0.157090 --> 0.156640).  Saving model ...
Validation loss decreased (0.156640 --> 0.156186).  Saving model ...
Validation loss decreased (0.156186 --> 0.155759).  Saving model ...
Validation loss decreased (0.155759 --> 0.155336).  Saving model ...
Validation loss decreased (0.155336 --> 0.154918).  Saving model ...
Validation loss decreased (0.154918 --> 0.154605).  Saving model ...
Validation loss decreased (0.154605 --> 0.154288).  Saving model ...
Validation loss decreased (0.154288 --> 0.153948).  Saving model ...
Validation loss decreased (0.153948 --> 0.153604).  Saving model ...
Validation loss decreased (0.153604 --> 0.153254).  Saving model ...
Validation loss decreased (0.153254 --> 0.152904).  Saving model ...
Validation loss decreased (0.152904 --> 0.152454).  Saving model ...
Validation loss decreased (0.152454 --> 0.152097).  Saving model ...
Validation loss decreased (0.152097 --> 0.151685).  Saving model ...
Validation loss decreased (0.151685 --> 0.151390).  Saving model ...
Validation loss decreased (0.151390 --> 0.151027).  Saving model ...
Validation loss decreased (0.151027 --> 0.150651).  Saving model ...
Validation loss decreased (0.150651 --> 0.150257).  Saving model ...
Validation loss decreased (0.150257 --> 0.149868).  Saving model ...
Validation loss decreased (0.149868 --> 0.149465).  Saving model ...
Validation loss decreased (0.149465 --> 0.149114).  Saving model ...
Validation loss decreased (0.149114 --> 0.148776).  Saving model ...
Validation loss decreased (0.148776 --> 0.148366).  Saving model ...
epoch 301, loss 0.4334, train acc 97.00%, f1 0.9698, precision 0.9747, recall 0.9650, auc 0.9700
Validation loss decreased (0.148366 --> 0.147961).  Saving model ...
Validation loss decreased (0.147961 --> 0.147523).  Saving model ...
Validation loss decreased (0.147523 --> 0.147105).  Saving model ...
Validation loss decreased (0.147105 --> 0.146705).  Saving model ...
Validation loss decreased (0.146705 --> 0.146238).  Saving model ...
Validation loss decreased (0.146238 --> 0.145762).  Saving model ...
Validation loss decreased (0.145762 --> 0.145415).  Saving model ...
Validation loss decreased (0.145415 --> 0.145033).  Saving model ...
Validation loss decreased (0.145033 --> 0.144652).  Saving model ...
Validation loss decreased (0.144652 --> 0.144279).  Saving model ...
Validation loss decreased (0.144279 --> 0.143924).  Saving model ...
Validation loss decreased (0.143924 --> 0.143563).  Saving model ...
Validation loss decreased (0.143563 --> 0.143225).  Saving model ...
Validation loss decreased (0.143225 --> 0.142880).  Saving model ...
Validation loss decreased (0.142880 --> 0.142594).  Saving model ...
Validation loss decreased (0.142594 --> 0.142305).  Saving model ...
Validation loss decreased (0.142305 --> 0.142047).  Saving model ...
Validation loss decreased (0.142047 --> 0.141727).  Saving model ...
Validation loss decreased (0.141727 --> 0.141417).  Saving model ...
Validation loss decreased (0.141417 --> 0.141214).  Saving model ...
Validation loss decreased (0.141214 --> 0.140996).  Saving model ...
Validation loss decreased (0.140996 --> 0.140853).  Saving model ...
Validation loss decreased (0.140853 --> 0.140756).  Saving model ...
Validation loss decreased (0.140756 --> 0.140695).  Saving model ...
Validation loss decreased (0.140695 --> 0.140585).  Saving model ...
Validation loss decreased (0.140585 --> 0.140470).  Saving model ...
Validation loss decreased (0.140470 --> 0.140347).  Saving model ...
Validation loss decreased (0.140347 --> 0.140142).  Saving model ...
Validation loss decreased (0.140142 --> 0.139813).  Saving model ...
Validation loss decreased (0.139813 --> 0.139436).  Saving model ...
Validation loss decreased (0.139436 --> 0.139058).  Saving model ...
Validation loss decreased (0.139058 --> 0.138723).  Saving model ...
Validation loss decreased (0.138723 --> 0.138301).  Saving model ...
Validation loss decreased (0.138301 --> 0.137959).  Saving model ...
Validation loss decreased (0.137959 --> 0.137588).  Saving model ...
Validation loss decreased (0.137588 --> 0.137209).  Saving model ...
Validation loss decreased (0.137209 --> 0.136813).  Saving model ...
Validation loss decreased (0.136813 --> 0.136411).  Saving model ...
Validation loss decreased (0.136411 --> 0.135951).  Saving model ...
Validation loss decreased (0.135951 --> 0.135455).  Saving model ...
Validation loss decreased (0.135455 --> 0.135038).  Saving model ...
Validation loss decreased (0.135038 --> 0.134586).  Saving model ...
Validation loss decreased (0.134586 --> 0.134168).  Saving model ...
Validation loss decreased (0.134168 --> 0.133680).  Saving model ...
Validation loss decreased (0.133680 --> 0.133215).  Saving model ...
Validation loss decreased (0.133215 --> 0.132784).  Saving model ...
Validation loss decreased (0.132784 --> 0.132444).  Saving model ...
Validation loss decreased (0.132444 --> 0.132208).  Saving model ...
Validation loss decreased (0.132208 --> 0.131896).  Saving model ...
Validation loss decreased (0.131896 --> 0.131588).  Saving model ...
Validation loss decreased (0.131588 --> 0.131395).  Saving model ...
Validation loss decreased (0.131395 --> 0.131284).  Saving model ...
Validation loss decreased (0.131284 --> 0.131131).  Saving model ...
Validation loss decreased (0.131131 --> 0.131086).  Saving model ...
Validation loss decreased (0.131086 --> 0.131025).  Saving model ...
Validation loss decreased (0.131025 --> 0.130931).  Saving model ...
Validation loss decreased (0.130931 --> 0.130806).  Saving model ...
Validation loss decreased (0.130806 --> 0.130656).  Saving model ...
Validation loss decreased (0.130656 --> 0.130494).  Saving model ...
Validation loss decreased (0.130494 --> 0.130303).  Saving model ...
Validation loss decreased (0.130303 --> 0.130068).  Saving model ...
Validation loss decreased (0.130068 --> 0.129819).  Saving model ...
Validation loss decreased (0.129819 --> 0.129639).  Saving model ...
Validation loss decreased (0.129639 --> 0.129403).  Saving model ...
Validation loss decreased (0.129403 --> 0.129264).  Saving model ...
Validation loss decreased (0.129264 --> 0.129025).  Saving model ...
Validation loss decreased (0.129025 --> 0.128838).  Saving model ...
Validation loss decreased (0.128838 --> 0.128645).  Saving model ...
Validation loss decreased (0.128645 --> 0.128560).  Saving model ...
Validation loss decreased (0.128560 --> 0.128416).  Saving model ...
Validation loss decreased (0.128416 --> 0.128337).  Saving model ...
Validation loss decreased (0.128337 --> 0.128201).  Saving model ...
Validation loss decreased (0.128201 --> 0.127994).  Saving model ...
Validation loss decreased (0.127994 --> 0.127822).  Saving model ...
Validation loss decreased (0.127822 --> 0.127749).  Saving model ...
Validation loss decreased (0.127749 --> 0.127687).  Saving model ...
Validation loss decreased (0.127687 --> 0.127580).  Saving model ...
Validation loss decreased (0.127580 --> 0.127504).  Saving model ...
Validation loss decreased (0.127504 --> 0.127494).  Saving model ...
Validation loss decreased (0.127494 --> 0.127442).  Saving model ...
Validation loss decreased (0.127442 --> 0.127352).  Saving model ...
Validation loss decreased (0.127352 --> 0.127238).  Saving model ...
Validation loss decreased (0.127238 --> 0.127117).  Saving model ...
Validation loss decreased (0.127117 --> 0.126987).  Saving model ...
Validation loss decreased (0.126987 --> 0.126876).  Saving model ...
Validation loss decreased (0.126876 --> 0.126677).  Saving model ...
Validation loss decreased (0.126677 --> 0.126577).  Saving model ...
Validation loss decreased (0.126577 --> 0.126383).  Saving model ...
Validation loss decreased (0.126383 --> 0.126171).  Saving model ...
Validation loss decreased (0.126171 --> 0.125883).  Saving model ...
Validation loss decreased (0.125883 --> 0.125658).  Saving model ...
Validation loss decreased (0.125658 --> 0.125426).  Saving model ...
Validation loss decreased (0.125426 --> 0.125139).  Saving model ...
Validation loss decreased (0.125139 --> 0.124799).  Saving model ...
Validation loss decreased (0.124799 --> 0.124448).  Saving model ...
Validation loss decreased (0.124448 --> 0.124111).  Saving model ...
Validation loss decreased (0.124111 --> 0.123772).  Saving model ...
Validation loss decreased (0.123772 --> 0.123458).  Saving model ...
Validation loss decreased (0.123458 --> 0.123183).  Saving model ...
Validation loss decreased (0.123183 --> 0.123000).  Saving model ...
epoch 401, loss 0.3460, train acc 97.50%, f1 0.9749, precision 0.9798, recall 0.9700, auc 0.9750
Validation loss decreased (0.123000 --> 0.122744).  Saving model ...
Validation loss decreased (0.122744 --> 0.122480).  Saving model ...
Validation loss decreased (0.122480 --> 0.122206).  Saving model ...
Validation loss decreased (0.122206 --> 0.121965).  Saving model ...
Validation loss decreased (0.121965 --> 0.121749).  Saving model ...
Validation loss decreased (0.121749 --> 0.121579).  Saving model ...
Validation loss decreased (0.121579 --> 0.121485).  Saving model ...
Validation loss decreased (0.121485 --> 0.121414).  Saving model ...
Validation loss decreased (0.121414 --> 0.121264).  Saving model ...
Validation loss decreased (0.121264 --> 0.121048).  Saving model ...
Validation loss decreased (0.121048 --> 0.120824).  Saving model ...
Validation loss decreased (0.120824 --> 0.120627).  Saving model ...
Validation loss decreased (0.120627 --> 0.120372).  Saving model ...
Validation loss decreased (0.120372 --> 0.120142).  Saving model ...
Validation loss decreased (0.120142 --> 0.119968).  Saving model ...
Validation loss decreased (0.119968 --> 0.119822).  Saving model ...
Validation loss decreased (0.119822 --> 0.119635).  Saving model ...
Validation loss decreased (0.119635 --> 0.119372).  Saving model ...
Validation loss decreased (0.119372 --> 0.119072).  Saving model ...
Validation loss decreased (0.119072 --> 0.118722).  Saving model ...
Validation loss decreased (0.118722 --> 0.118348).  Saving model ...
Validation loss decreased (0.118348 --> 0.117994).  Saving model ...
Validation loss decreased (0.117994 --> 0.117685).  Saving model ...
Validation loss decreased (0.117685 --> 0.117427).  Saving model ...
Validation loss decreased (0.117427 --> 0.117221).  Saving model ...
Validation loss decreased (0.117221 --> 0.117009).  Saving model ...
Validation loss decreased (0.117009 --> 0.116882).  Saving model ...
Validation loss decreased (0.116882 --> 0.116788).  Saving model ...
Validation loss decreased (0.116788 --> 0.116652).  Saving model ...
Validation loss decreased (0.116652 --> 0.116479).  Saving model ...
Validation loss decreased (0.116479 --> 0.116318).  Saving model ...
Validation loss decreased (0.116318 --> 0.116207).  Saving model ...
Validation loss decreased (0.116207 --> 0.116074).  Saving model ...
Validation loss decreased (0.116074 --> 0.115875).  Saving model ...
Validation loss decreased (0.115875 --> 0.115679).  Saving model ...
Validation loss decreased (0.115679 --> 0.115495).  Saving model ...
Validation loss decreased (0.115495 --> 0.115337).  Saving model ...
Validation loss decreased (0.115337 --> 0.115220).  Saving model ...
Validation loss decreased (0.115220 --> 0.115121).  Saving model ...
Validation loss decreased (0.115121 --> 0.114992).  Saving model ...
Validation loss decreased (0.114992 --> 0.114854).  Saving model ...
Validation loss decreased (0.114854 --> 0.114687).  Saving model ...
Validation loss decreased (0.114687 --> 0.114511).  Saving model ...
Validation loss decreased (0.114511 --> 0.114415).  Saving model ...
Validation loss decreased (0.114415 --> 0.114269).  Saving model ...
Validation loss decreased (0.114269 --> 0.114200).  Saving model ...
Validation loss decreased (0.114200 --> 0.114113).  Saving model ...
Validation loss decreased (0.114113 --> 0.114097).  Saving model ...
Validation loss decreased (0.114097 --> 0.114096).  Saving model ...
Validation loss decreased (0.114096 --> 0.113999).  Saving model ...
Validation loss decreased (0.113999 --> 0.113872).  Saving model ...
Validation loss decreased (0.113872 --> 0.113769).  Saving model ...
Validation loss decreased (0.113769 --> 0.113651).  Saving model ...
Validation loss decreased (0.113651 --> 0.113570).  Saving model ...
Validation loss decreased (0.113570 --> 0.113497).  Saving model ...
Validation loss decreased (0.113497 --> 0.113388).  Saving model ...
Validation loss decreased (0.113388 --> 0.113239).  Saving model ...
Validation loss decreased (0.113239 --> 0.113140).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
Validation loss decreased (0.113140 --> 0.113097).  Saving model ...
Validation loss decreased (0.113097 --> 0.112979).  Saving model ...
Validation loss decreased (0.112979 --> 0.112836).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 483, loss 0.3916, train acc 97.75%, f1 0.9774, precision 0.9799, recall 0.9750, auc 0.9775



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_True
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_1
./test_pima/result_MLP_concat_Mirror_True_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6014814814814815

the Fscore is 0.5714285714285714

the precision is 0.40625

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_1
----------------------



epoch 1, loss 0.6932, train acc 51.43%, f1 0.6728, precision 0.5072, recall 0.9987, auc 0.5143
epoch 101, loss 0.5686, train acc 78.66%, f1 0.7881, precision 0.7828, recall 0.7933, auc 0.7866
epoch 201, loss 0.4129, train acc 81.43%, f1 0.8141, precision 0.8151, recall 0.8132, auc 0.8143
epoch 301, loss 0.3789, train acc 82.83%, f1 0.8280, precision 0.8293, recall 0.8268, auc 0.8283
epoch 401, loss 0.3976, train acc 83.21%, f1 0.8319, precision 0.8327, recall 0.8311, auc 0.8321
epoch 501, loss 0.4128, train acc 83.24%, f1 0.8321, precision 0.8339, recall 0.8302, auc 0.8324
epoch 601, loss 0.3703, train acc 83.43%, f1 0.8341, precision 0.8353, recall 0.8328, auc 0.8343
epoch 701, loss 0.3634, train acc 83.49%, f1 0.8346, precision 0.8362, recall 0.8330, auc 0.8349
epoch 801, loss 0.3626, train acc 83.49%, f1 0.8346, precision 0.8357, recall 0.8336, auc 0.8349
epoch 901, loss 0.3830, train acc 83.38%, f1 0.8337, precision 0.8345, recall 0.8329, auc 0.8338
epoch 1001, loss 0.3121, train acc 83.41%, f1 0.8340, precision 0.8345, recall 0.8335, auc 0.8341
epoch 1101, loss 0.2859, train acc 83.44%, f1 0.8343, precision 0.8347, recall 0.8339, auc 0.8344
epoch 1201, loss 0.4054, train acc 83.49%, f1 0.8349, precision 0.8351, recall 0.8346, auc 0.8349
epoch 1301, loss 0.2519, train acc 83.48%, f1 0.8348, precision 0.8349, recall 0.8347, auc 0.8348
epoch 1401, loss 0.3757, train acc 83.48%, f1 0.8347, precision 0.8351, recall 0.8343, auc 0.8348
epoch 1501, loss 0.3330, train acc 83.48%, f1 0.8347, precision 0.8351, recall 0.8343, auc 0.8348
epoch 1601, loss 0.3968, train acc 83.46%, f1 0.8346, precision 0.8348, recall 0.8343, auc 0.8346
epoch 1701, loss 0.3722, train acc 83.50%, f1 0.8350, precision 0.8348, recall 0.8352, auc 0.8350
epoch 1801, loss 0.3002, train acc 83.47%, f1 0.8347, precision 0.8349, recall 0.8345, auc 0.8347
epoch 1901, loss 0.3831, train acc 83.41%, f1 0.8341, precision 0.8340, recall 0.8342, auc 0.8341
epoch 2001, loss 0.3411, train acc 83.50%, f1 0.8350, precision 0.8351, recall 0.8350, auc 0.8350
epoch 2101, loss 0.3828, train acc 83.55%, f1 0.8356, precision 0.8352, recall 0.8360, auc 0.8355
epoch 2201, loss 0.4618, train acc 83.60%, f1 0.8360, precision 0.8363, recall 0.8357, auc 0.8360
epoch 2301, loss 0.4627, train acc 83.65%, f1 0.8365, precision 0.8365, recall 0.8365, auc 0.8365
epoch 2401, loss 0.3305, train acc 83.68%, f1 0.8368, precision 0.8366, recall 0.8370, auc 0.8368
epoch 2501, loss 0.3653, train acc 83.78%, f1 0.8378, precision 0.8377, recall 0.8378, auc 0.8378
epoch 2601, loss 0.2314, train acc 83.91%, f1 0.8391, precision 0.8390, recall 0.8392, auc 0.8391
epoch 2701, loss 0.3619, train acc 83.95%, f1 0.8395, precision 0.8394, recall 0.8396, auc 0.8395
epoch 2801, loss 0.3945, train acc 84.09%, f1 0.8410, precision 0.8404, recall 0.8415, auc 0.8409
epoch 2901, loss 0.3302, train acc 84.21%, f1 0.8421, precision 0.8422, recall 0.8420, auc 0.8421
epoch 3001, loss 0.4102, train acc 84.39%, f1 0.8439, precision 0.8437, recall 0.8440, auc 0.8439
epoch 3101, loss 0.3502, train acc 84.34%, f1 0.8433, precision 0.8435, recall 0.8432, auc 0.8434
epoch 3201, loss 0.2781, train acc 84.58%, f1 0.8459, precision 0.8456, recall 0.8462, auc 0.8458
epoch 3301, loss 0.3525, train acc 84.68%, f1 0.8468, precision 0.8467, recall 0.8470, auc 0.8468
epoch 3401, loss 0.2710, train acc 84.81%, f1 0.8481, precision 0.8480, recall 0.8483, auc 0.8481
epoch 3501, loss 0.4104, train acc 84.92%, f1 0.8491, precision 0.8493, recall 0.8490, auc 0.8492
epoch 3601, loss 0.2440, train acc 85.01%, f1 0.8501, precision 0.8503, recall 0.8499, auc 0.8501
epoch 3701, loss 0.3343, train acc 85.12%, f1 0.8512, precision 0.8511, recall 0.8513, auc 0.8512
epoch 3801, loss 0.3137, train acc 85.24%, f1 0.8525, precision 0.8520, recall 0.8529, auc 0.8524
epoch 3901, loss 0.3417, train acc 85.26%, f1 0.8526, precision 0.8528, recall 0.8523, auc 0.8526
epoch 4001, loss 0.3557, train acc 85.36%, f1 0.8537, precision 0.8533, recall 0.8542, auc 0.8536
epoch 4101, loss 0.2915, train acc 85.50%, f1 0.8550, precision 0.8549, recall 0.8550, auc 0.8550
epoch 4201, loss 0.3296, train acc 85.56%, f1 0.8556, precision 0.8557, recall 0.8555, auc 0.8556
epoch 4301, loss 0.3038, train acc 85.67%, f1 0.8567, precision 0.8567, recall 0.8567, auc 0.8567
epoch 4401, loss 0.3736, train acc 85.75%, f1 0.8575, precision 0.8574, recall 0.8575, auc 0.8575
epoch 4501, loss 0.3178, train acc 85.82%, f1 0.8582, precision 0.8581, recall 0.8584, auc 0.8582
epoch 4601, loss 0.3999, train acc 85.90%, f1 0.8589, precision 0.8594, recall 0.8584, auc 0.8590
epoch 4701, loss 0.3040, train acc 85.95%, f1 0.8595, precision 0.8590, recall 0.8600, auc 0.8595
epoch 4801, loss 0.2971, train acc 85.98%, f1 0.8598, precision 0.8597, recall 0.8598, auc 0.8598
epoch 4901, loss 0.2101, train acc 85.97%, f1 0.8598, precision 0.8592, recall 0.8605, auc 0.8597
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_1
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6964814814814815

the Fscore is 0.6380368098159509

the precision is 0.47706422018348627

the recall is 0.9629629629629629

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_1
----------------------



epoch 1, loss 0.6932, train acc 50.29%, f1 0.0192, precision 0.7083, recall 0.0097, auc 0.5029
epoch 101, loss 0.5494, train acc 78.51%, f1 0.7937, precision 0.7632, recall 0.8266, auc 0.7851
epoch 201, loss 0.4380, train acc 81.44%, f1 0.8143, precision 0.8148, recall 0.8138, auc 0.8144
epoch 301, loss 0.3892, train acc 82.77%, f1 0.8269, precision 0.8307, recall 0.8231, auc 0.8277
epoch 401, loss 0.3291, train acc 83.26%, f1 0.8316, precision 0.8364, recall 0.8269, auc 0.8326
epoch 501, loss 0.3819, train acc 83.41%, f1 0.8328, precision 0.8392, recall 0.8265, auc 0.8341
epoch 601, loss 0.2912, train acc 83.43%, f1 0.8334, precision 0.8379, recall 0.8289, auc 0.8343
epoch 701, loss 0.3328, train acc 83.45%, f1 0.8337, precision 0.8378, recall 0.8295, auc 0.8345
epoch 801, loss 0.3648, train acc 83.45%, f1 0.8338, precision 0.8374, recall 0.8302, auc 0.8345
epoch 901, loss 0.3687, train acc 83.51%, f1 0.8346, precision 0.8374, recall 0.8317, auc 0.8351
epoch 1001, loss 0.3751, train acc 83.46%, f1 0.8340, precision 0.8372, recall 0.8308, auc 0.8346
epoch 1101, loss 0.3164, train acc 83.46%, f1 0.8341, precision 0.8365, recall 0.8316, auc 0.8346
epoch 1201, loss 0.3454, train acc 83.41%, f1 0.8336, precision 0.8360, recall 0.8312, auc 0.8341
epoch 1301, loss 0.3161, train acc 83.45%, f1 0.8341, precision 0.8359, recall 0.8323, auc 0.8345
epoch 1401, loss 0.3873, train acc 83.51%, f1 0.8347, precision 0.8368, recall 0.8326, auc 0.8351
epoch 1501, loss 0.2964, train acc 83.49%, f1 0.8346, precision 0.8360, recall 0.8333, auc 0.8349
epoch 1601, loss 0.3110, train acc 83.43%, f1 0.8339, precision 0.8358, recall 0.8320, auc 0.8343
epoch 1701, loss 0.3617, train acc 83.45%, f1 0.8338, precision 0.8373, recall 0.8303, auc 0.8345
epoch 1801, loss 0.3731, train acc 83.40%, f1 0.8338, precision 0.8347, recall 0.8330, auc 0.8340
epoch 1901, loss 0.3697, train acc 83.40%, f1 0.8337, precision 0.8353, recall 0.8322, auc 0.8340
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/standlization_data/pima_std_test_1.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_1
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.595

the Fscore is 0.5714285714285715

the precision is 0.4

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_1.csv
./test_pima/model_MLP_concat_notMirror_True/record_1/MLP_concat_notMirror_True_1
----------------------



Traceback (most recent call last):
  File "./classifier_MLP/train_MLP.py", line 312, in <module>
    transformed_valid_data, transformed_valid_label = transform_data_to_train_form(transform_method, mirror_type, valid_pos_data, valid_neg_data)
  File "./classifier_MLP/train_MLP.py", line 192, in transform_data_to_train_form
    trans_pos_data, trans_pos_label, trans_neg_data, trans_neg_label = handleData_extend_not_mirror(positive_repeat_data, negetive_tile_data)
  File "./classifier_MLP/train_MLP.py", line 154, in handleData_extend_not_mirror
    transfrom_positive_data = transfrom_positive_data[positive_index[0]]
UnboundLocalError: local variable 'transfrom_positive_data' referenced before assignment
