nohup: ignoring input
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_4
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5361, train acc 79.49%, f1 0.7944, precision 0.7963, recall 0.7925, auc 0.7949
epoch 201, loss 0.3941, train acc 83.07%, f1 0.8308, precision 0.8303, recall 0.8312, auc 0.8307
epoch 301, loss 0.4025, train acc 84.60%, f1 0.8460, precision 0.8457, recall 0.8464, auc 0.8460
epoch 401, loss 0.3052, train acc 84.89%, f1 0.8489, precision 0.8485, recall 0.8493, auc 0.8489
epoch 501, loss 0.4109, train acc 84.99%, f1 0.8500, precision 0.8496, recall 0.8503, auc 0.8499
epoch 601, loss 0.2645, train acc 85.05%, f1 0.8506, precision 0.8503, recall 0.8509, auc 0.8505
epoch 701, loss 0.3983, train acc 85.10%, f1 0.8511, precision 0.8508, recall 0.8513, auc 0.8510
epoch 801, loss 0.2071, train acc 85.08%, f1 0.8508, precision 0.8505, recall 0.8511, auc 0.8508
epoch 901, loss 0.2597, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8508, auc 0.8506
epoch 1001, loss 0.3583, train acc 85.04%, f1 0.8504, precision 0.8503, recall 0.8505, auc 0.8504
epoch 1101, loss 0.3153, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 1201, loss 0.2974, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 1301, loss 0.3176, train acc 85.08%, f1 0.8508, precision 0.8508, recall 0.8507, auc 0.8508
epoch 1401, loss 0.4129, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8505, auc 0.8505
epoch 1501, loss 0.3449, train acc 85.08%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8508
epoch 1601, loss 0.2773, train acc 85.11%, f1 0.8511, precision 0.8511, recall 0.8510, auc 0.8511
epoch 1701, loss 0.2689, train acc 85.06%, f1 0.8506, precision 0.8507, recall 0.8505, auc 0.8506
epoch 1801, loss 0.3395, train acc 85.05%, f1 0.8504, precision 0.8506, recall 0.8502, auc 0.8505
epoch 1901, loss 0.2808, train acc 85.12%, f1 0.8512, precision 0.8514, recall 0.8509, auc 0.8512
epoch 2001, loss 0.3182, train acc 85.08%, f1 0.8508, precision 0.8510, recall 0.8506, auc 0.8508
epoch 2101, loss 0.2273, train acc 85.08%, f1 0.8508, precision 0.8510, recall 0.8505, auc 0.8508
epoch 2201, loss 0.2588, train acc 85.07%, f1 0.8507, precision 0.8509, recall 0.8506, auc 0.8507
epoch 2301, loss 0.3030, train acc 85.10%, f1 0.8509, precision 0.8513, recall 0.8506, auc 0.8510
epoch 2401, loss 0.2536, train acc 85.13%, f1 0.8512, precision 0.8517, recall 0.8507, auc 0.8513
epoch 2501, loss 0.4268, train acc 85.13%, f1 0.8513, precision 0.8515, recall 0.8512, auc 0.8513
epoch 2601, loss 0.3010, train acc 85.16%, f1 0.8516, precision 0.8519, recall 0.8512, auc 0.8516
epoch 2701, loss 0.3586, train acc 85.22%, f1 0.8521, precision 0.8524, recall 0.8518, auc 0.8522
epoch 2801, loss 0.3515, train acc 85.25%, f1 0.8524, precision 0.8529, recall 0.8519, auc 0.8525
epoch 2901, loss 0.3088, train acc 85.34%, f1 0.8533, precision 0.8538, recall 0.8528, auc 0.8534
epoch 3001, loss 0.3115, train acc 85.39%, f1 0.8538, precision 0.8543, recall 0.8534, auc 0.8539
epoch 3101, loss 0.5066, train acc 85.45%, f1 0.8544, precision 0.8549, recall 0.8539, auc 0.8545
epoch 3201, loss 0.3356, train acc 85.57%, f1 0.8557, precision 0.8558, recall 0.8555, auc 0.8557
epoch 3301, loss 0.4701, train acc 85.69%, f1 0.8569, precision 0.8569, recall 0.8570, auc 0.8569
epoch 3401, loss 0.2658, train acc 85.82%, f1 0.8583, precision 0.8579, recall 0.8587, auc 0.8582
epoch 3501, loss 0.2906, train acc 85.95%, f1 0.8595, precision 0.8596, recall 0.8593, auc 0.8595
epoch 3601, loss 0.3003, train acc 86.12%, f1 0.8612, precision 0.8610, recall 0.8613, auc 0.8612
epoch 3701, loss 0.4004, train acc 86.23%, f1 0.8622, precision 0.8626, recall 0.8618, auc 0.8623
epoch 3801, loss 0.2822, train acc 86.37%, f1 0.8636, precision 0.8642, recall 0.8631, auc 0.8637
epoch 3901, loss 0.2172, train acc 86.42%, f1 0.8642, precision 0.8642, recall 0.8643, auc 0.8642
epoch 4001, loss 0.3054, train acc 86.47%, f1 0.8647, precision 0.8649, recall 0.8645, auc 0.8647
epoch 4101, loss 0.2672, train acc 86.60%, f1 0.8659, precision 0.8663, recall 0.8655, auc 0.8660
epoch 4201, loss 0.3056, train acc 86.72%, f1 0.8672, precision 0.8673, recall 0.8670, auc 0.8672
epoch 4301, loss 0.2930, train acc 86.73%, f1 0.8673, precision 0.8672, recall 0.8674, auc 0.8673
epoch 4401, loss 0.3864, train acc 86.80%, f1 0.8681, precision 0.8676, recall 0.8687, auc 0.8680
epoch 4501, loss 0.2236, train acc 86.88%, f1 0.8688, precision 0.8690, recall 0.8685, auc 0.8688
epoch 4601, loss 0.4105, train acc 86.92%, f1 0.8692, precision 0.8692, recall 0.8692, auc 0.8692
epoch 4701, loss 0.3040, train acc 86.91%, f1 0.8691, precision 0.8688, recall 0.8694, auc 0.8691
epoch 4801, loss 0.3170, train acc 86.94%, f1 0.8694, precision 0.8696, recall 0.8693, auc 0.8694
epoch 4901, loss 0.2505, train acc 87.04%, f1 0.8705, precision 0.8701, recall 0.8709, auc 0.8704
epoch 5001, loss 0.3195, train acc 87.05%, f1 0.8705, precision 0.8701, recall 0.8710, auc 0.8705
epoch 5101, loss 0.3091, train acc 87.05%, f1 0.8706, precision 0.8701, recall 0.8710, auc 0.8705
epoch 5201, loss 0.2618, train acc 87.14%, f1 0.8714, precision 0.8714, recall 0.8715, auc 0.8714
epoch 5301, loss 0.2812, train acc 87.16%, f1 0.8716, precision 0.8713, recall 0.8718, auc 0.8716
epoch 5401, loss 0.3629, train acc 87.17%, f1 0.8717, precision 0.8715, recall 0.8719, auc 0.8717
epoch 5501, loss 0.2708, train acc 87.21%, f1 0.8721, precision 0.8720, recall 0.8723, auc 0.8721
epoch 5601, loss 0.2234, train acc 87.28%, f1 0.8729, precision 0.8727, recall 0.8731, auc 0.8728
epoch 5701, loss 0.3311, train acc 87.33%, f1 0.8733, precision 0.8730, recall 0.8736, auc 0.8733
epoch 5801, loss 0.3057, train acc 87.34%, f1 0.8734, precision 0.8731, recall 0.8737, auc 0.8734
epoch 5901, loss 0.2945, train acc 87.35%, f1 0.8735, precision 0.8732, recall 0.8737, auc 0.8735
epoch 6001, loss 0.3809, train acc 87.44%, f1 0.8745, precision 0.8740, recall 0.8750, auc 0.8744
epoch 6101, loss 0.3062, train acc 87.50%, f1 0.8751, precision 0.8745, recall 0.8756, auc 0.8750
epoch 6201, loss 0.2306, train acc 87.56%, f1 0.8756, precision 0.8751, recall 0.8762, auc 0.8756
epoch 6301, loss 0.3234, train acc 87.56%, f1 0.8758, precision 0.8750, recall 0.8765, auc 0.8756
epoch 6401, loss 0.2727, train acc 87.61%, f1 0.8761, precision 0.8755, recall 0.8767, auc 0.8761
epoch 6501, loss 0.3303, train acc 87.67%, f1 0.8768, precision 0.8761, recall 0.8775, auc 0.8767
epoch 6601, loss 0.2331, train acc 87.65%, f1 0.8765, precision 0.8761, recall 0.8769, auc 0.8765
epoch 6701, loss 0.2735, train acc 87.71%, f1 0.8772, precision 0.8769, recall 0.8775, auc 0.8771
epoch 6801, loss 0.2748, train acc 87.75%, f1 0.8776, precision 0.8773, recall 0.8779, auc 0.8775
epoch 6901, loss 0.2939, train acc 87.79%, f1 0.8780, precision 0.8774, recall 0.8785, auc 0.8779
epoch 7001, loss 0.4016, train acc 87.85%, f1 0.8786, precision 0.8783, recall 0.8788, auc 0.8785
epoch 7101, loss 0.2564, train acc 87.86%, f1 0.8786, precision 0.8786, recall 0.8786, auc 0.8786
epoch 7201, loss 0.3362, train acc 87.91%, f1 0.8790, precision 0.8792, recall 0.8788, auc 0.8791
epoch 7301, loss 0.3292, train acc 87.95%, f1 0.8795, precision 0.8794, recall 0.8795, auc 0.8795
epoch 7401, loss 0.1907, train acc 87.95%, f1 0.8795, precision 0.8790, recall 0.8801, auc 0.8795
epoch 7501, loss 0.2647, train acc 87.99%, f1 0.8799, precision 0.8798, recall 0.8799, auc 0.8799
epoch 7601, loss 0.3472, train acc 88.05%, f1 0.8805, precision 0.8801, recall 0.8809, auc 0.8805
epoch 7701, loss 0.2353, train acc 88.06%, f1 0.8806, precision 0.8805, recall 0.8806, auc 0.8806
epoch 7801, loss 0.3273, train acc 88.12%, f1 0.8812, precision 0.8812, recall 0.8812, auc 0.8812
epoch 7901, loss 0.2212, train acc 88.16%, f1 0.8816, precision 0.8814, recall 0.8818, auc 0.8816
epoch 8001, loss 0.1573, train acc 88.23%, f1 0.8822, precision 0.8825, recall 0.8820, auc 0.8823
epoch 8101, loss 0.2972, train acc 88.30%, f1 0.8830, precision 0.8831, recall 0.8829, auc 0.8830
epoch 8201, loss 0.2076, train acc 88.27%, f1 0.8827, precision 0.8826, recall 0.8828, auc 0.8827
epoch 8301, loss 0.2671, train acc 88.35%, f1 0.8835, precision 0.8835, recall 0.8836, auc 0.8835
epoch 8401, loss 0.2816, train acc 88.38%, f1 0.8838, precision 0.8837, recall 0.8838, auc 0.8838
epoch 8501, loss 0.2166, train acc 88.45%, f1 0.8844, precision 0.8848, recall 0.8840, auc 0.8845
epoch 8601, loss 0.3097, train acc 88.48%, f1 0.8847, precision 0.8855, recall 0.8840, auc 0.8848
epoch 8701, loss 0.2707, train acc 88.50%, f1 0.8850, precision 0.8850, recall 0.8850, auc 0.8850
epoch 8801, loss 0.1887, train acc 88.54%, f1 0.8854, precision 0.8855, recall 0.8852, auc 0.8854
epoch 8901, loss 0.3470, train acc 88.60%, f1 0.8860, precision 0.8861, recall 0.8858, auc 0.8860
epoch 9001, loss 0.3420, train acc 88.63%, f1 0.8863, precision 0.8865, recall 0.8862, auc 0.8863
epoch 9101, loss 0.2287, train acc 88.66%, f1 0.8865, precision 0.8868, recall 0.8863, auc 0.8866
epoch 9201, loss 0.2252, train acc 88.69%, f1 0.8869, precision 0.8869, recall 0.8870, auc 0.8869
epoch 9301, loss 0.1924, train acc 88.73%, f1 0.8873, precision 0.8873, recall 0.8872, auc 0.8873
epoch 9401, loss 0.3447, train acc 88.79%, f1 0.8878, precision 0.8880, recall 0.8876, auc 0.8879
epoch 9501, loss 0.2450, train acc 88.83%, f1 0.8884, precision 0.8875, recall 0.8894, auc 0.8883
epoch 9601, loss 0.2196, train acc 88.86%, f1 0.8885, precision 0.8888, recall 0.8882, auc 0.8886
epoch 9701, loss 0.3360, train acc 88.88%, f1 0.8889, precision 0.8885, recall 0.8892, auc 0.8888
epoch 9801, loss 0.1659, train acc 88.89%, f1 0.8889, precision 0.8890, recall 0.8888, auc 0.8889
epoch 9901, loss 0.2374, train acc 89.00%, f1 0.8900, precision 0.8903, recall 0.8896, auc 0.8900
epoch 10001, loss 0.2734, train acc 89.04%, f1 0.8904, precision 0.8905, recall 0.8902, auc 0.8904
epoch 10101, loss 0.4402, train acc 89.03%, f1 0.8904, precision 0.8898, recall 0.8909, auc 0.8903
epoch 10201, loss 0.2741, train acc 89.12%, f1 0.8912, precision 0.8908, recall 0.8917, auc 0.8912
epoch 10301, loss 0.1628, train acc 89.11%, f1 0.8912, precision 0.8909, recall 0.8914, auc 0.8911
epoch 10401, loss 0.1685, train acc 89.17%, f1 0.8917, precision 0.8922, recall 0.8912, auc 0.8917
epoch 10501, loss 0.2243, train acc 89.20%, f1 0.8921, precision 0.8912, recall 0.8930, auc 0.8920
epoch 10601, loss 0.3191, train acc 89.23%, f1 0.8924, precision 0.8916, recall 0.8932, auc 0.8923
epoch 10701, loss 0.3012, train acc 89.26%, f1 0.8927, precision 0.8921, recall 0.8934, auc 0.8926
epoch 10801, loss 0.3542, train acc 89.24%, f1 0.8925, precision 0.8922, recall 0.8927, auc 0.8924
epoch 10901, loss 0.2410, train acc 89.34%, f1 0.8935, precision 0.8930, recall 0.8940, auc 0.8934
epoch 11001, loss 0.2557, train acc 89.41%, f1 0.8941, precision 0.8939, recall 0.8943, auc 0.8941
epoch 11101, loss 0.3287, train acc 89.44%, f1 0.8943, precision 0.8948, recall 0.8939, auc 0.8944
epoch 11201, loss 0.2253, train acc 89.49%, f1 0.8949, precision 0.8944, recall 0.8955, auc 0.8949
epoch 11301, loss 0.2202, train acc 89.53%, f1 0.8953, precision 0.8954, recall 0.8952, auc 0.8953
epoch 11401, loss 0.2602, train acc 89.57%, f1 0.8957, precision 0.8957, recall 0.8957, auc 0.8957
epoch 11501, loss 0.2314, train acc 89.59%, f1 0.8960, precision 0.8958, recall 0.8961, auc 0.8959
epoch 11601, loss 0.1559, train acc 89.64%, f1 0.8964, precision 0.8966, recall 0.8962, auc 0.8964
epoch 11701, loss 0.1574, train acc 89.63%, f1 0.8963, precision 0.8966, recall 0.8960, auc 0.8963
epoch 11801, loss 0.2307, train acc 89.67%, f1 0.8966, precision 0.8973, recall 0.8960, auc 0.8967
epoch 11901, loss 0.3530, train acc 89.69%, f1 0.8969, precision 0.8969, recall 0.8969, auc 0.8969
epoch 12001, loss 0.2402, train acc 89.79%, f1 0.8979, precision 0.8978, recall 0.8980, auc 0.8979
epoch 12101, loss 0.3058, train acc 89.79%, f1 0.8981, precision 0.8967, recall 0.8995, auc 0.8979
epoch 12201, loss 0.2283, train acc 89.84%, f1 0.8984, precision 0.8984, recall 0.8984, auc 0.8984
epoch 12301, loss 0.1804, train acc 89.82%, f1 0.8983, precision 0.8975, recall 0.8991, auc 0.8982
epoch 12401, loss 0.2165, train acc 89.88%, f1 0.8988, precision 0.8986, recall 0.8989, auc 0.8988
epoch 12501, loss 0.4375, train acc 89.89%, f1 0.8990, precision 0.8988, recall 0.8991, auc 0.8989
epoch 12601, loss 0.2274, train acc 89.91%, f1 0.8990, precision 0.8998, recall 0.8982, auc 0.8991
epoch 12701, loss 0.2643, train acc 89.98%, f1 0.9000, precision 0.8986, recall 0.9014, auc 0.8998
epoch 12801, loss 0.2440, train acc 89.99%, f1 0.8999, precision 0.8998, recall 0.9000, auc 0.8999
epoch 12901, loss 0.1891, train acc 90.06%, f1 0.9006, precision 0.9009, recall 0.9004, auc 0.9006
epoch 13001, loss 0.3062, train acc 90.04%, f1 0.9005, precision 0.9000, recall 0.9009, auc 0.9004
epoch 13101, loss 0.2944, train acc 90.07%, f1 0.9007, precision 0.9005, recall 0.9008, auc 0.9007
epoch 13201, loss 0.3018, train acc 90.14%, f1 0.9015, precision 0.9012, recall 0.9017, auc 0.9014
epoch 13301, loss 0.1672, train acc 90.15%, f1 0.9015, precision 0.9009, recall 0.9021, auc 0.9015
epoch 13401, loss 0.2198, train acc 90.19%, f1 0.9018, precision 0.9028, recall 0.9008, auc 0.9019
epoch 13501, loss 0.2131, train acc 90.22%, f1 0.9022, precision 0.9020, recall 0.9025, auc 0.9022
epoch 13601, loss 0.2094, train acc 90.25%, f1 0.9026, precision 0.9017, recall 0.9036, auc 0.9025
epoch 13701, loss 0.2396, train acc 90.31%, f1 0.9031, precision 0.9034, recall 0.9028, auc 0.9031
epoch 13801, loss 0.3012, train acc 90.32%, f1 0.9032, precision 0.9029, recall 0.9035, auc 0.9032
epoch 13901, loss 0.2067, train acc 90.35%, f1 0.9035, precision 0.9031, recall 0.9038, auc 0.9035
epoch 14001, loss 0.2204, train acc 90.44%, f1 0.9044, precision 0.9041, recall 0.9048, auc 0.9044
epoch 14101, loss 0.1879, train acc 90.39%, f1 0.9038, precision 0.9048, recall 0.9028, auc 0.9039
epoch 14201, loss 0.2819, train acc 90.48%, f1 0.9047, precision 0.9049, recall 0.9046, auc 0.9048
epoch 14301, loss 0.1912, train acc 90.51%, f1 0.9051, precision 0.9048, recall 0.9055, auc 0.9051
epoch 14401, loss 0.1955, train acc 90.53%, f1 0.9054, precision 0.9049, recall 0.9059, auc 0.9053
epoch 14501, loss 0.3379, train acc 90.56%, f1 0.9056, precision 0.9060, recall 0.9051, auc 0.9056
epoch 14601, loss 0.2188, train acc 90.60%, f1 0.9060, precision 0.9059, recall 0.9061, auc 0.9060
epoch 14701, loss 0.2114, train acc 90.61%, f1 0.9061, precision 0.9063, recall 0.9059, auc 0.9061
epoch 14801, loss 0.1725, train acc 90.67%, f1 0.9068, precision 0.9060, recall 0.9076, auc 0.9067
epoch 14901, loss 0.2075, train acc 90.72%, f1 0.9073, precision 0.9060, recall 0.9086, auc 0.9072
epoch 15001, loss 0.1407, train acc 90.72%, f1 0.9072, precision 0.9071, recall 0.9073, auc 0.9072
epoch 15101, loss 0.1781, train acc 90.77%, f1 0.9078, precision 0.9071, recall 0.9086, auc 0.9077
epoch 15201, loss 0.2303, train acc 90.79%, f1 0.9078, precision 0.9085, recall 0.9072, auc 0.9079
epoch 15301, loss 0.1961, train acc 90.82%, f1 0.9082, precision 0.9076, recall 0.9088, auc 0.9082
epoch 15401, loss 0.2722, train acc 90.81%, f1 0.9081, precision 0.9079, recall 0.9083, auc 0.9081
epoch 15501, loss 0.3630, train acc 90.87%, f1 0.9087, precision 0.9081, recall 0.9094, auc 0.9087
epoch 15601, loss 0.2925, train acc 90.89%, f1 0.9089, precision 0.9087, recall 0.9091, auc 0.9089
epoch 15701, loss 0.1342, train acc 90.90%, f1 0.9090, precision 0.9093, recall 0.9087, auc 0.9090
epoch 15801, loss 0.1591, train acc 90.97%, f1 0.9097, precision 0.9090, recall 0.9104, auc 0.9097
epoch 15901, loss 0.2249, train acc 90.97%, f1 0.9098, precision 0.9094, recall 0.9102, auc 0.9097
epoch 16001, loss 0.1455, train acc 91.01%, f1 0.9101, precision 0.9098, recall 0.9104, auc 0.9101
epoch 16101, loss 0.2962, train acc 91.05%, f1 0.9105, precision 0.9100, recall 0.9110, auc 0.9105
epoch 16201, loss 0.2268, train acc 91.08%, f1 0.9108, precision 0.9106, recall 0.9110, auc 0.9108
epoch 16301, loss 0.2798, train acc 91.05%, f1 0.9106, precision 0.9091, recall 0.9122, auc 0.9105
epoch 16401, loss 0.2309, train acc 91.07%, f1 0.9107, precision 0.9106, recall 0.9109, auc 0.9107
epoch 16501, loss 0.1748, train acc 91.14%, f1 0.9113, precision 0.9122, recall 0.9104, auc 0.9114/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.1810, train acc 91.16%, f1 0.9116, precision 0.9117, recall 0.9116, auc 0.9116
epoch 16701, loss 0.1724, train acc 91.18%, f1 0.9119, precision 0.9114, recall 0.9123, auc 0.9118
epoch 16801, loss 0.2979, train acc 91.19%, f1 0.9120, precision 0.9108, recall 0.9132, auc 0.9119
epoch 16901, loss 0.2828, train acc 91.26%, f1 0.9126, precision 0.9123, recall 0.9129, auc 0.9126
epoch 17001, loss 0.1787, train acc 91.30%, f1 0.9130, precision 0.9131, recall 0.9128, auc 0.9130
epoch 17101, loss 0.2991, train acc 91.32%, f1 0.9132, precision 0.9128, recall 0.9137, auc 0.9132
epoch 17201, loss 0.1269, train acc 91.36%, f1 0.9136, precision 0.9135, recall 0.9136, auc 0.9136
epoch 17301, loss 0.1737, train acc 91.34%, f1 0.9135, precision 0.9124, recall 0.9147, auc 0.9134
epoch 17401, loss 0.2272, train acc 91.36%, f1 0.9137, precision 0.9127, recall 0.9147, auc 0.9136
epoch 17501, loss 0.2838, train acc 91.40%, f1 0.9140, precision 0.9136, recall 0.9145, auc 0.9140
epoch 17601, loss 0.2169, train acc 91.41%, f1 0.9141, precision 0.9146, recall 0.9135, auc 0.9141
epoch 17701, loss 0.2522, train acc 91.44%, f1 0.9144, precision 0.9148, recall 0.9140, auc 0.9144
epoch 17801, loss 0.2494, train acc 91.46%, f1 0.9146, precision 0.9146, recall 0.9147, auc 0.9146
epoch 17901, loss 0.1755, train acc 91.49%, f1 0.9148, precision 0.9155, recall 0.9142, auc 0.9149
epoch 18001, loss 0.2055, train acc 91.54%, f1 0.9154, precision 0.9153, recall 0.9156, auc 0.9154
epoch 18101, loss 0.2131, train acc 91.58%, f1 0.9156, precision 0.9173, recall 0.9139, auc 0.9158
epoch 18201, loss 0.1969, train acc 91.60%, f1 0.9160, precision 0.9151, recall 0.9170, auc 0.9160
epoch 18301, loss 0.1996, train acc 91.67%, f1 0.9166, precision 0.9172, recall 0.9160, auc 0.9167
epoch 18401, loss 0.2258, train acc 91.63%, f1 0.9162, precision 0.9174, recall 0.9149, auc 0.9163
epoch 18501, loss 0.2206, train acc 91.67%, f1 0.9168, precision 0.9155, recall 0.9180, auc 0.9167
epoch 18601, loss 0.1914, train acc 91.68%, f1 0.9167, precision 0.9176, recall 0.9159, auc 0.9168
epoch 18701, loss 0.2710, train acc 91.77%, f1 0.9176, precision 0.9184, recall 0.9168, auc 0.9177
epoch 18801, loss 0.1801, train acc 91.73%, f1 0.9173, precision 0.9171, recall 0.9176, auc 0.9173
epoch 18901, loss 0.1798, train acc 91.81%, f1 0.9179, precision 0.9200, recall 0.9158, auc 0.9181
epoch 19001, loss 0.1751, train acc 91.82%, f1 0.9181, precision 0.9190, recall 0.9171, auc 0.9182
epoch 19101, loss 0.2559, train acc 91.88%, f1 0.9187, precision 0.9194, recall 0.9181, auc 0.9188
epoch 19201, loss 0.1949, train acc 91.85%, f1 0.9185, precision 0.9184, recall 0.9187, auc 0.9185
epoch 19301, loss 0.1544, train acc 91.86%, f1 0.9185, precision 0.9192, recall 0.9179, auc 0.9186
epoch 19401, loss 0.1977, train acc 91.91%, f1 0.9191, precision 0.9193, recall 0.9189, auc 0.9191
epoch 19501, loss 0.1586, train acc 91.98%, f1 0.9197, precision 0.9207, recall 0.9186, auc 0.9198
epoch 19601, loss 0.1322, train acc 91.95%, f1 0.9195, precision 0.9198, recall 0.9192, auc 0.9195
epoch 19701, loss 0.2197, train acc 92.02%, f1 0.9200, precision 0.9220, recall 0.9180, auc 0.9202
epoch 19801, loss 0.2719, train acc 92.05%, f1 0.9205, precision 0.9203, recall 0.9208, auc 0.9205
epoch 19901, loss 0.1451, train acc 92.09%, f1 0.9209, precision 0.9215, recall 0.9203, auc 0.9209
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_4
./test_pima/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6800943396226415

the Fscore is 0.611111111111111

the precision is 0.4835164835164835

the recall is 0.8301886792452831

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_4
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5252, train acc 78.98%, f1 0.7894, precision 0.7912, recall 0.7875, auc 0.7898
epoch 201, loss 0.4001, train acc 82.71%, f1 0.8271, precision 0.8273, recall 0.8270, auc 0.8271
epoch 301, loss 0.4367, train acc 84.48%, f1 0.8448, precision 0.8446, recall 0.8451, auc 0.8448
epoch 401, loss 0.2923, train acc 84.90%, f1 0.8491, precision 0.8485, recall 0.8496, auc 0.8490
epoch 501, loss 0.3408, train acc 85.02%, f1 0.8503, precision 0.8496, recall 0.8509, auc 0.8502
epoch 601, loss 0.3532, train acc 85.03%, f1 0.8504, precision 0.8498, recall 0.8510, auc 0.8503
epoch 701, loss 0.3622, train acc 85.13%, f1 0.8513, precision 0.8509, recall 0.8518, auc 0.8513
epoch 801, loss 0.2879, train acc 85.07%, f1 0.8508, precision 0.8504, recall 0.8512, auc 0.8507
epoch 901, loss 0.2801, train acc 85.09%, f1 0.8510, precision 0.8506, recall 0.8514, auc 0.8509
epoch 1001, loss 0.2724, train acc 85.09%, f1 0.8509, precision 0.8508, recall 0.8510, auc 0.8509
epoch 1101, loss 0.2878, train acc 85.09%, f1 0.8509, precision 0.8507, recall 0.8511, auc 0.8509
epoch 1201, loss 0.3930, train acc 85.10%, f1 0.8510, precision 0.8509, recall 0.8512, auc 0.8510
epoch 1301, loss 0.2488, train acc 85.08%, f1 0.8508, precision 0.8507, recall 0.8509, auc 0.8508
epoch 1401, loss 0.2182, train acc 85.04%, f1 0.8504, precision 0.8503, recall 0.8505, auc 0.8504
epoch 1501, loss 0.3284, train acc 85.09%, f1 0.8509, precision 0.8508, recall 0.8511, auc 0.8509
epoch 1601, loss 0.3202, train acc 85.06%, f1 0.8506, precision 0.8506, recall 0.8506, auc 0.8506
epoch 1701, loss 0.3096, train acc 85.08%, f1 0.8509, precision 0.8505, recall 0.8512, auc 0.8508
epoch 1801, loss 0.3534, train acc 85.09%, f1 0.8509, precision 0.8508, recall 0.8511, auc 0.8509
epoch 1901, loss 0.3323, train acc 85.08%, f1 0.8508, precision 0.8506, recall 0.8510, auc 0.8508
epoch 2001, loss 0.3514, train acc 85.07%, f1 0.8507, precision 0.8505, recall 0.8510, auc 0.8507
epoch 2101, loss 0.3609, train acc 85.10%, f1 0.8510, precision 0.8508, recall 0.8512, auc 0.8510
epoch 2201, loss 0.3341, train acc 85.10%, f1 0.8511, precision 0.8509, recall 0.8512, auc 0.8510
epoch 2301, loss 0.4379, train acc 85.06%, f1 0.8507, precision 0.8504, recall 0.8509, auc 0.8506
epoch 2401, loss 0.3089, train acc 85.15%, f1 0.8516, precision 0.8512, recall 0.8520, auc 0.8515
epoch 2501, loss 0.4708, train acc 85.12%, f1 0.8513, precision 0.8509, recall 0.8517, auc 0.8512
epoch 2601, loss 0.4282, train acc 85.23%, f1 0.8523, precision 0.8520, recall 0.8527, auc 0.8523
epoch 2701, loss 0.3855, train acc 85.24%, f1 0.8524, precision 0.8523, recall 0.8526, auc 0.8524
epoch 2801, loss 0.3511, train acc 85.25%, f1 0.8526, precision 0.8523, recall 0.8529, auc 0.8525
epoch 2901, loss 0.4009, train acc 85.34%, f1 0.8534, precision 0.8532, recall 0.8537, auc 0.8534
epoch 3001, loss 0.3168, train acc 85.36%, f1 0.8536, precision 0.8533, recall 0.8540, auc 0.8536
epoch 3101, loss 0.2905, train acc 85.53%, f1 0.8553, precision 0.8555, recall 0.8551, auc 0.8553
epoch 3201, loss 0.2252, train acc 85.60%, f1 0.8561, precision 0.8560, recall 0.8561, auc 0.8560
epoch 3301, loss 0.3043, train acc 85.73%, f1 0.8573, precision 0.8576, recall 0.8570, auc 0.8573
epoch 3401, loss 0.3587, train acc 85.81%, f1 0.8582, precision 0.8575, recall 0.8589, auc 0.8581
epoch 3501, loss 0.3077, train acc 85.94%, f1 0.8593, precision 0.8601, recall 0.8585, auc 0.8594
epoch 3601, loss 0.3618, train acc 86.04%, f1 0.8603, precision 0.8606, recall 0.8600, auc 0.8604
epoch 3701, loss 0.4482, train acc 86.14%, f1 0.8612, precision 0.8626, recall 0.8598, auc 0.8614
epoch 3801, loss 0.3810, train acc 86.30%, f1 0.8630, precision 0.8629, recall 0.8631, auc 0.8630
epoch 3901, loss 0.3628, train acc 86.31%, f1 0.8631, precision 0.8631, recall 0.8631, auc 0.8631
epoch 4001, loss 0.2584, train acc 86.44%, f1 0.8644, precision 0.8645, recall 0.8643, auc 0.8644
epoch 4101, loss 0.2963, train acc 86.54%, f1 0.8655, precision 0.8648, recall 0.8661, auc 0.8654
epoch 4201, loss 0.1797, train acc 86.63%, f1 0.8664, precision 0.8655, recall 0.8673, auc 0.8663
epoch 4301, loss 0.2525, train acc 86.69%, f1 0.8670, precision 0.8666, recall 0.8673, auc 0.8669
epoch 4401, loss 0.2360, train acc 86.73%, f1 0.8671, precision 0.8686, recall 0.8656, auc 0.8673
epoch 4501, loss 0.2586, train acc 86.74%, f1 0.8675, precision 0.8666, recall 0.8685, auc 0.8674
epoch 4601, loss 0.2147, train acc 86.82%, f1 0.8681, precision 0.8691, recall 0.8670, auc 0.8682
epoch 4701, loss 0.2425, train acc 86.91%, f1 0.8690, precision 0.8693, recall 0.8687, auc 0.8691
epoch 4801, loss 0.2253, train acc 86.92%, f1 0.8693, precision 0.8690, recall 0.8695, auc 0.8692
epoch 4901, loss 0.2413, train acc 86.94%, f1 0.8692, precision 0.8700, recall 0.8685, auc 0.8694
epoch 5001, loss 0.3554, train acc 86.98%, f1 0.8698, precision 0.8700, recall 0.8696, auc 0.8698
epoch 5101, loss 0.4212, train acc 87.00%, f1 0.8699, precision 0.8705, recall 0.8694, auc 0.8700
epoch 5201, loss 0.3240, train acc 87.08%, f1 0.8708, precision 0.8711, recall 0.8705, auc 0.8708
epoch 5301, loss 0.2592, train acc 87.06%, f1 0.8706, precision 0.8708, recall 0.8703, auc 0.8706
epoch 5401, loss 0.3071, train acc 87.11%, f1 0.8710, precision 0.8715, recall 0.8706, auc 0.8711
epoch 5501, loss 0.2280, train acc 87.15%, f1 0.8714, precision 0.8721, recall 0.8706, auc 0.8715
epoch 5601, loss 0.3537, train acc 87.16%, f1 0.8715, precision 0.8721, recall 0.8709, auc 0.8716
epoch 5701, loss 0.3145, train acc 87.18%, f1 0.8719, precision 0.8718, recall 0.8719, auc 0.8718
epoch 5801, loss 0.2947, train acc 87.26%, f1 0.8725, precision 0.8731, recall 0.8720, auc 0.8726
epoch 5901, loss 0.2690, train acc 87.30%, f1 0.8730, precision 0.8733, recall 0.8728, auc 0.8730
epoch 6001, loss 0.2697, train acc 87.33%, f1 0.8733, precision 0.8729, recall 0.8737, auc 0.8733
epoch 6101, loss 0.2487, train acc 87.35%, f1 0.8734, precision 0.8744, recall 0.8723, auc 0.8735
epoch 6201, loss 0.2383, train acc 87.39%, f1 0.8736, precision 0.8754, recall 0.8717, auc 0.8739
epoch 6301, loss 0.1953, train acc 87.44%, f1 0.8744, precision 0.8746, recall 0.8742, auc 0.8744
epoch 6401, loss 0.2378, train acc 87.51%, f1 0.8750, precision 0.8759, recall 0.8740, auc 0.8751
epoch 6501, loss 0.3561, train acc 87.55%, f1 0.8755, precision 0.8759, recall 0.8750, auc 0.8755
epoch 6601, loss 0.2289, train acc 87.61%, f1 0.8760, precision 0.8763, recall 0.8757, auc 0.8761
epoch 6701, loss 0.2117, train acc 87.61%, f1 0.8761, precision 0.8763, recall 0.8758, auc 0.8761
epoch 6801, loss 0.3024, train acc 87.60%, f1 0.8758, precision 0.8773, recall 0.8742, auc 0.8760
epoch 6901, loss 0.2798, train acc 87.70%, f1 0.8771, precision 0.8761, recall 0.8781, auc 0.8770
epoch 7001, loss 0.2401, train acc 87.77%, f1 0.8776, precision 0.8784, recall 0.8767, auc 0.8777
epoch 7101, loss 0.2842, train acc 87.78%, f1 0.8778, precision 0.8779, recall 0.8776, auc 0.8778
epoch 7201, loss 0.2341, train acc 87.79%, f1 0.8779, precision 0.8780, recall 0.8778, auc 0.8779
epoch 7301, loss 0.3991, train acc 87.85%, f1 0.8785, precision 0.8786, recall 0.8784, auc 0.8785
epoch 7401, loss 0.2540, train acc 87.91%, f1 0.8791, precision 0.8788, recall 0.8795, auc 0.8791
epoch 7501, loss 0.2578, train acc 87.90%, f1 0.8790, precision 0.8789, recall 0.8790, auc 0.8790
epoch 7601, loss 0.2784, train acc 87.90%, f1 0.8789, precision 0.8800, recall 0.8778, auc 0.8790
epoch 7701, loss 0.4300, train acc 87.92%, f1 0.8791, precision 0.8797, recall 0.8785, auc 0.8792
epoch 7801, loss 0.2628, train acc 88.06%, f1 0.8807, precision 0.8804, recall 0.8809, auc 0.8806
epoch 7901, loss 0.2761, train acc 88.13%, f1 0.8814, precision 0.8809, recall 0.8818, auc 0.8813
epoch 8001, loss 0.1771, train acc 88.10%, f1 0.8810, precision 0.8808, recall 0.8813, auc 0.8810
epoch 8101, loss 0.2409, train acc 88.13%, f1 0.8812, precision 0.8817, recall 0.8807, auc 0.8813
epoch 8201, loss 0.2485, train acc 88.22%, f1 0.8823, precision 0.8818, recall 0.8829, auc 0.8822/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.1764, train acc 88.25%, f1 0.8825, precision 0.8828, recall 0.8822, auc 0.8825
epoch 8401, loss 0.3608, train acc 88.30%, f1 0.8831, precision 0.8826, recall 0.8836, auc 0.8830
epoch 8501, loss 0.2018, train acc 88.40%, f1 0.8840, precision 0.8842, recall 0.8839, auc 0.8840
epoch 8601, loss 0.2382, train acc 88.42%, f1 0.8842, precision 0.8843, recall 0.8842, auc 0.8842
epoch 8701, loss 0.3052, train acc 88.44%, f1 0.8844, precision 0.8850, recall 0.8837, auc 0.8844
epoch 8801, loss 0.2711, train acc 88.52%, f1 0.8853, precision 0.8847, recall 0.8858, auc 0.8852
epoch 8901, loss 0.2649, train acc 88.52%, f1 0.8851, precision 0.8857, recall 0.8845, auc 0.8852
epoch 9001, loss 0.2107, train acc 88.59%, f1 0.8859, precision 0.8862, recall 0.8855, auc 0.8859
epoch 9101, loss 0.2063, train acc 88.65%, f1 0.8865, precision 0.8859, recall 0.8872, auc 0.8865
epoch 9201, loss 0.2874, train acc 88.59%, f1 0.8859, precision 0.8860, recall 0.8857, auc 0.8859
epoch 9301, loss 0.2275, train acc 88.66%, f1 0.8866, precision 0.8865, recall 0.8868, auc 0.8866
epoch 9401, loss 0.3678, train acc 88.75%, f1 0.8875, precision 0.8875, recall 0.8875, auc 0.8875
epoch 9501, loss 0.1919, train acc 88.80%, f1 0.8880, precision 0.8879, recall 0.8881, auc 0.8880
epoch 9601, loss 0.4051, train acc 88.77%, f1 0.8877, precision 0.8878, recall 0.8876, auc 0.8877
epoch 9701, loss 0.2917, train acc 88.87%, f1 0.8887, precision 0.8887, recall 0.8888, auc 0.8887
epoch 9801, loss 0.2036, train acc 88.87%, f1 0.8888, precision 0.8882, recall 0.8895, auc 0.8887
epoch 9901, loss 0.1835, train acc 88.89%, f1 0.8889, precision 0.8886, recall 0.8893, auc 0.8889
epoch 10001, loss 0.1985, train acc 88.93%, f1 0.8893, precision 0.8893, recall 0.8893, auc 0.8893
epoch 10101, loss 0.3026, train acc 88.96%, f1 0.8897, precision 0.8895, recall 0.8899, auc 0.8896
epoch 10201, loss 0.2097, train acc 89.01%, f1 0.8901, precision 0.8899, recall 0.8904, auc 0.8901
epoch 10301, loss 0.2601, train acc 89.05%, f1 0.8905, precision 0.8903, recall 0.8908, auc 0.8905
epoch 10401, loss 0.3354, train acc 89.07%, f1 0.8907, precision 0.8910, recall 0.8904, auc 0.8907
epoch 10501, loss 0.2361, train acc 89.07%, f1 0.8908, precision 0.8902, recall 0.8914, auc 0.8907
epoch 10601, loss 0.2280, train acc 89.16%, f1 0.8916, precision 0.8917, recall 0.8916, auc 0.8916
epoch 10701, loss 0.2587, train acc 89.20%, f1 0.8920, precision 0.8925, recall 0.8914, auc 0.8920
epoch 10801, loss 0.1805, train acc 89.20%, f1 0.8919, precision 0.8929, recall 0.8910, auc 0.8920
epoch 10901, loss 0.2211, train acc 89.26%, f1 0.8925, precision 0.8927, recall 0.8924, auc 0.8926
epoch 11001, loss 0.2207, train acc 89.26%, f1 0.8925, precision 0.8935, recall 0.8915, auc 0.8926
epoch 11101, loss 0.2834, train acc 89.29%, f1 0.8930, precision 0.8927, recall 0.8933, auc 0.8929
epoch 11201, loss 0.1854, train acc 89.34%, f1 0.8933, precision 0.8939, recall 0.8928, auc 0.8934
epoch 11301, loss 0.2075, train acc 89.35%, f1 0.8935, precision 0.8934, recall 0.8936, auc 0.8935
epoch 11401, loss 0.2690, train acc 89.38%, f1 0.8938, precision 0.8944, recall 0.8932, auc 0.8938
epoch 11501, loss 0.2270, train acc 89.40%, f1 0.8940, precision 0.8943, recall 0.8937, auc 0.8940
epoch 11601, loss 0.2709, train acc 89.52%, f1 0.8951, precision 0.8954, recall 0.8948, auc 0.8952
epoch 11701, loss 0.2200, train acc 89.50%, f1 0.8950, precision 0.8950, recall 0.8950, auc 0.8950
epoch 11801, loss 0.2163, train acc 89.60%, f1 0.8960, precision 0.8958, recall 0.8962, auc 0.8960
epoch 11901, loss 0.2480, train acc 89.61%, f1 0.8960, precision 0.8968, recall 0.8953, auc 0.8961
epoch 12001, loss 0.1791, train acc 89.63%, f1 0.8962, precision 0.8968, recall 0.8956, auc 0.8963
epoch 12101, loss 0.1986, train acc 89.70%, f1 0.8970, precision 0.8966, recall 0.8974, auc 0.8970
epoch 12201, loss 0.2614, train acc 89.70%, f1 0.8970, precision 0.8970, recall 0.8969, auc 0.8970
epoch 12301, loss 0.3425, train acc 89.76%, f1 0.8975, precision 0.8981, recall 0.8969, auc 0.8976
epoch 12401, loss 0.3328, train acc 89.79%, f1 0.8978, precision 0.8989, recall 0.8967, auc 0.8979
epoch 12501, loss 0.2535, train acc 89.81%, f1 0.8980, precision 0.8993, recall 0.8967, auc 0.8981
epoch 12601, loss 0.1894, train acc 89.85%, f1 0.8985, precision 0.8990, recall 0.8980, auc 0.8985
epoch 12701, loss 0.2054, train acc 89.89%, f1 0.8988, precision 0.8993, recall 0.8984, auc 0.8989
epoch 12801, loss 0.3128, train acc 89.90%, f1 0.8990, precision 0.8992, recall 0.8987, auc 0.8990
epoch 12901, loss 0.2007, train acc 89.96%, f1 0.8996, precision 0.8994, recall 0.8997, auc 0.8996
epoch 13001, loss 0.1903, train acc 89.97%, f1 0.8998, precision 0.8986, recall 0.9011, auc 0.8997
epoch 13101, loss 0.2719, train acc 90.02%, f1 0.9001, precision 0.9016, recall 0.8986, auc 0.9002
epoch 13201, loss 0.2608, train acc 90.00%, f1 0.9000, precision 0.9001, recall 0.8998, auc 0.9000
epoch 13301, loss 0.2586, train acc 90.07%, f1 0.9008, precision 0.9006, recall 0.9009, auc 0.9007
epoch 13401, loss 0.2085, train acc 90.11%, f1 0.9011, precision 0.9017, recall 0.9004, auc 0.9011
epoch 13501, loss 0.2563, train acc 90.10%, f1 0.9010, precision 0.9014, recall 0.9006, auc 0.9010
epoch 13601, loss 0.2234, train acc 90.13%, f1 0.9013, precision 0.9018, recall 0.9008, auc 0.9013
epoch 13701, loss 0.2799, train acc 90.17%, f1 0.9016, precision 0.9021, recall 0.9012, auc 0.9017
epoch 13801, loss 0.2292, train acc 90.22%, f1 0.9022, precision 0.9023, recall 0.9021, auc 0.9022
epoch 13901, loss 0.2265, train acc 90.27%, f1 0.9028, precision 0.9018, recall 0.9039, auc 0.9027
epoch 14001, loss 0.2873, train acc 90.27%, f1 0.9027, precision 0.9026, recall 0.9027, auc 0.9027
epoch 14101, loss 0.2297, train acc 90.31%, f1 0.9031, precision 0.9031, recall 0.9031, auc 0.9031
epoch 14201, loss 0.2715, train acc 90.37%, f1 0.9037, precision 0.9036, recall 0.9038, auc 0.9037
epoch 14301, loss 0.1940, train acc 90.39%, f1 0.9039, precision 0.9039, recall 0.9038, auc 0.9039
epoch 14401, loss 0.1761, train acc 90.42%, f1 0.9043, precision 0.9039, recall 0.9047, auc 0.9042
epoch 14501, loss 0.2915, train acc 90.46%, f1 0.9046, precision 0.9046, recall 0.9045, auc 0.9046
epoch 14601, loss 0.1615, train acc 90.49%, f1 0.9049, precision 0.9045, recall 0.9053, auc 0.9049
epoch 14701, loss 0.2274, train acc 90.49%, f1 0.9050, precision 0.9043, recall 0.9057, auc 0.9049
epoch 14801, loss 0.2242, train acc 90.56%, f1 0.9056, precision 0.9052, recall 0.9060, auc 0.9056
epoch 14901, loss 0.2196, train acc 90.58%, f1 0.9058, precision 0.9060, recall 0.9055, auc 0.9058
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_4
./test_pima/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6583962264150944

the Fscore is 0.5987261146496815

the precision is 0.4519230769230769

the recall is 0.8867924528301887

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_4
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5204, train acc 79.22%, f1 0.7915, precision 0.7941, recall 0.7890, auc 0.7922
epoch 201, loss 0.3866, train acc 82.84%, f1 0.8284, precision 0.8284, recall 0.8284, auc 0.8284
epoch 301, loss 0.3516, train acc 84.43%, f1 0.8444, precision 0.8439, recall 0.8448, auc 0.8443
epoch 401, loss 0.3064, train acc 84.81%, f1 0.8482, precision 0.8475, recall 0.8490, auc 0.8481
epoch 501, loss 0.2625, train acc 84.98%, f1 0.8498, precision 0.8494, recall 0.8502, auc 0.8498
epoch 601, loss 0.4146, train acc 84.99%, f1 0.8500, precision 0.8494, recall 0.8506, auc 0.8499
epoch 701, loss 0.3407, train acc 85.10%, f1 0.8511, precision 0.8506, recall 0.8515, auc 0.8510
epoch 801, loss 0.2706, train acc 85.07%, f1 0.8508, precision 0.8504, recall 0.8511, auc 0.8507
epoch 901, loss 0.3218, train acc 85.07%, f1 0.8507, precision 0.8505, recall 0.8509, auc 0.8507
epoch 1001, loss 0.3491, train acc 85.05%, f1 0.8506, precision 0.8502, recall 0.8510, auc 0.8505
epoch 1101, loss 0.3180, train acc 85.09%, f1 0.8510, precision 0.8507, recall 0.8513, auc 0.8509
epoch 1201, loss 0.4076, train acc 85.07%, f1 0.8507, precision 0.8505, recall 0.8509, auc 0.8507
epoch 1301, loss 0.4040, train acc 85.05%, f1 0.8505, precision 0.8501, recall 0.8509, auc 0.8505
epoch 1401, loss 0.2967, train acc 85.05%, f1 0.8506, precision 0.8503, recall 0.8508, auc 0.8505
epoch 1501, loss 0.4076, train acc 85.06%, f1 0.8507, precision 0.8503, recall 0.8511, auc 0.8506
epoch 1601, loss 0.2560, train acc 85.09%, f1 0.8510, precision 0.8507, recall 0.8512, auc 0.8509
epoch 1701, loss 0.4725, train acc 85.06%, f1 0.8507, precision 0.8502, recall 0.8511, auc 0.8506
epoch 1801, loss 0.4299, train acc 85.11%, f1 0.8512, precision 0.8505, recall 0.8519, auc 0.8511
epoch 1901, loss 0.4078, train acc 85.09%, f1 0.8511, precision 0.8504, recall 0.8517, auc 0.8509
epoch 2001, loss 0.3028, train acc 85.10%, f1 0.8512, precision 0.8504, recall 0.8519, auc 0.8510
epoch 2101, loss 0.3759, train acc 85.07%, f1 0.8508, precision 0.8500, recall 0.8516, auc 0.8507
epoch 2201, loss 0.3953, train acc 85.07%, f1 0.8509, precision 0.8499, recall 0.8518, auc 0.8507
epoch 2301, loss 0.3507, train acc 85.15%, f1 0.8516, precision 0.8508, recall 0.8524, auc 0.8515
epoch 2401, loss 0.3744, train acc 85.17%, f1 0.8518, precision 0.8511, recall 0.8525, auc 0.8517
epoch 2501, loss 0.3558, train acc 85.15%, f1 0.8516, precision 0.8508, recall 0.8524, auc 0.8515
epoch 2601, loss 0.2650, train acc 85.19%, f1 0.8521, precision 0.8510, recall 0.8531, auc 0.8519
epoch 2701, loss 0.3341, train acc 85.30%, f1 0.8531, precision 0.8524, recall 0.8538, auc 0.8530
epoch 2801, loss 0.2963, train acc 85.33%, f1 0.8534, precision 0.8528, recall 0.8539, auc 0.8533
epoch 2901, loss 0.1850, train acc 85.42%, f1 0.8543, precision 0.8538, recall 0.8548, auc 0.8542
epoch 3001, loss 0.4129, train acc 85.48%, f1 0.8549, precision 0.8546, recall 0.8551, auc 0.8548
epoch 3101, loss 0.3120, train acc 85.53%, f1 0.8554, precision 0.8548, recall 0.8560, auc 0.8553
epoch 3201, loss 0.4598, train acc 85.69%, f1 0.8570, precision 0.8566, recall 0.8574, auc 0.8569
epoch 3301, loss 0.3114, train acc 85.82%, f1 0.8583, precision 0.8577, recall 0.8588, auc 0.8582
epoch 3401, loss 0.3012, train acc 85.95%, f1 0.8595, precision 0.8591, recall 0.8600, auc 0.8595
epoch 3501, loss 0.3468, train acc 86.11%, f1 0.8611, precision 0.8610, recall 0.8612, auc 0.8611
epoch 3601, loss 0.3104, train acc 86.20%, f1 0.8621, precision 0.8617, recall 0.8625, auc 0.8620
epoch 3701, loss 0.2610, train acc 86.35%, f1 0.8636, precision 0.8632, recall 0.8640, auc 0.8635
epoch 3801, loss 0.3462, train acc 86.44%, f1 0.8644, precision 0.8643, recall 0.8645, auc 0.8644
epoch 3901, loss 0.5422, train acc 86.50%, f1 0.8651, precision 0.8646, recall 0.8656, auc 0.8650
epoch 4001, loss 0.3189, train acc 86.58%, f1 0.8658, precision 0.8658, recall 0.8657, auc 0.8658
epoch 4101, loss 0.3740, train acc 86.61%, f1 0.8661, precision 0.8658, recall 0.8664, auc 0.8661
epoch 4201, loss 0.3726, train acc 86.68%, f1 0.8668, precision 0.8668, recall 0.8668, auc 0.8668
epoch 4301, loss 0.3182, train acc 86.79%, f1 0.8680, precision 0.8678, recall 0.8682, auc 0.8679
epoch 4401, loss 0.2781, train acc 86.82%, f1 0.8683, precision 0.8679, recall 0.8687, auc 0.8682
epoch 4501, loss 0.3578, train acc 86.84%, f1 0.8684, precision 0.8685, recall 0.8683, auc 0.8684
epoch 4601, loss 0.2408, train acc 86.91%, f1 0.8691, precision 0.8690, recall 0.8691, auc 0.8691
epoch 4701, loss 0.3196, train acc 86.95%, f1 0.8695, precision 0.8693, recall 0.8697, auc 0.8695
epoch 4801, loss 0.3938, train acc 87.02%, f1 0.8702, precision 0.8702, recall 0.8701, auc 0.8702
epoch 4901, loss 0.2942, train acc 87.05%, f1 0.8705, precision 0.8700, recall 0.8711, auc 0.8705
epoch 5001, loss 0.3225, train acc 87.07%, f1 0.8707, precision 0.8705, recall 0.8709, auc 0.8707
epoch 5101, loss 0.2245, train acc 87.07%, f1 0.8707, precision 0.8707, recall 0.8708, auc 0.8707
epoch 5201, loss 0.3231, train acc 87.14%, f1 0.8714, precision 0.8713, recall 0.8715, auc 0.8714
epoch 5301, loss 0.2146, train acc 87.15%, f1 0.8715, precision 0.8714, recall 0.8717, auc 0.8715
epoch 5401, loss 0.2686, train acc 87.19%, f1 0.8719, precision 0.8718, recall 0.8720, auc 0.8719
epoch 5501, loss 0.2716, train acc 87.21%, f1 0.8721, precision 0.8719, recall 0.8724, auc 0.8721
epoch 5601, loss 0.2660, train acc 87.25%, f1 0.8726, precision 0.8724, recall 0.8727, auc 0.8725
epoch 5701, loss 0.2074, train acc 87.31%, f1 0.8731, precision 0.8731, recall 0.8731, auc 0.8731
epoch 5801, loss 0.4000, train acc 87.31%, f1 0.8731, precision 0.8731, recall 0.8732, auc 0.8731
epoch 5901, loss 0.3082, train acc 87.39%, f1 0.8739, precision 0.8740, recall 0.8739, auc 0.8739
epoch 6001, loss 0.3100, train acc 87.37%, f1 0.8736, precision 0.8738, recall 0.8735, auc 0.8737
epoch 6101, loss 0.3208, train acc 87.45%, f1 0.8745, precision 0.8744, recall 0.8745, auc 0.8745
epoch 6201, loss 0.2372, train acc 87.50%, f1 0.8749, precision 0.8752, recall 0.8747, auc 0.8750
epoch 6301, loss 0.2358, train acc 87.54%, f1 0.8754, precision 0.8757, recall 0.8751, auc 0.8754
epoch 6401, loss 0.3902, train acc 87.57%, f1 0.8757, precision 0.8757, recall 0.8757, auc 0.8757
epoch 6501, loss 0.3143, train acc 87.62%, f1 0.8761, precision 0.8766, recall 0.8756, auc 0.8762
epoch 6601, loss 0.2258, train acc 87.67%, f1 0.8767, precision 0.8766, recall 0.8767, auc 0.8767
epoch 6701, loss 0.3114, train acc 87.71%, f1 0.8771, precision 0.8774, recall 0.8767, auc 0.8771
epoch 6801, loss 0.2523, train acc 87.74%, f1 0.8773, precision 0.8778, recall 0.8769, auc 0.8774
epoch 6901, loss 0.3505, train acc 87.80%, f1 0.8780, precision 0.8780, recall 0.8779, auc 0.8780
epoch 7001, loss 0.3261, train acc 87.80%, f1 0.8779, precision 0.8790, recall 0.8767, auc 0.8780
epoch 7101, loss 0.2442, train acc 87.85%, f1 0.8785, precision 0.8787, recall 0.8782, auc 0.8785
epoch 7201, loss 0.2847, train acc 87.92%, f1 0.8792, precision 0.8795, recall 0.8788, auc 0.8792
epoch 7301, loss 0.4096, train acc 87.96%, f1 0.8796, precision 0.8794, recall 0.8798, auc 0.8796
epoch 7401, loss 0.2626, train acc 88.06%, f1 0.8806, precision 0.8803, recall 0.8809, auc 0.8806
epoch 7501, loss 0.3085, train acc 88.06%, f1 0.8807, precision 0.8805, recall 0.8808, auc 0.8806
epoch 7601, loss 0.2187, train acc 88.08%, f1 0.8807, precision 0.8808, recall 0.8807, auc 0.8808
epoch 7701, loss 0.2508, train acc 88.16%, f1 0.8815, precision 0.8820, recall 0.8810, auc 0.8816
epoch 7801, loss 0.2182, train acc 88.18%, f1 0.8817, precision 0.8826, recall 0.8808, auc 0.8818
epoch 7901, loss 0.2532, train acc 88.23%, f1 0.8822, precision 0.8831, recall 0.8814, auc 0.8823
epoch 8001, loss 0.1674, train acc 88.31%, f1 0.8830, precision 0.8840, recall 0.8819, auc 0.8831
epoch 8101, loss 0.3163, train acc 88.32%, f1 0.8831, precision 0.8840, recall 0.8822, auc 0.8832
epoch 8201, loss 0.3230, train acc 88.38%, f1 0.8837, precision 0.8843, recall 0.8831, auc 0.8838/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2133, train acc 88.42%, f1 0.8841, precision 0.8850, recall 0.8832, auc 0.8842
epoch 8401, loss 0.2913, train acc 88.41%, f1 0.8840, precision 0.8843, recall 0.8838, auc 0.8841
epoch 8501, loss 0.2411, train acc 88.53%, f1 0.8853, precision 0.8856, recall 0.8850, auc 0.8853
epoch 8601, loss 0.3661, train acc 88.54%, f1 0.8852, precision 0.8863, recall 0.8842, auc 0.8854
epoch 8701, loss 0.3037, train acc 88.60%, f1 0.8859, precision 0.8869, recall 0.8848, auc 0.8860
epoch 8801, loss 0.2774, train acc 88.67%, f1 0.8867, precision 0.8868, recall 0.8866, auc 0.8867
epoch 8901, loss 0.1955, train acc 88.68%, f1 0.8867, precision 0.8874, recall 0.8861, auc 0.8868
epoch 9001, loss 0.2502, train acc 88.72%, f1 0.8871, precision 0.8879, recall 0.8862, auc 0.8872
epoch 9101, loss 0.2540, train acc 88.77%, f1 0.8876, precision 0.8886, recall 0.8865, auc 0.8877
epoch 9201, loss 0.1014, train acc 88.77%, f1 0.8876, precision 0.8887, recall 0.8864, auc 0.8877
epoch 9301, loss 0.2844, train acc 88.89%, f1 0.8888, precision 0.8896, recall 0.8881, auc 0.8889
epoch 9401, loss 0.2816, train acc 88.92%, f1 0.8892, precision 0.8895, recall 0.8890, auc 0.8892
epoch 9501, loss 0.2246, train acc 88.99%, f1 0.8897, precision 0.8909, recall 0.8886, auc 0.8899
epoch 9601, loss 0.2916, train acc 89.04%, f1 0.8903, precision 0.8912, recall 0.8893, auc 0.8904
epoch 9701, loss 0.2318, train acc 89.09%, f1 0.8908, precision 0.8920, recall 0.8895, auc 0.8909
epoch 9801, loss 0.1532, train acc 89.09%, f1 0.8909, precision 0.8910, recall 0.8909, auc 0.8909
epoch 9901, loss 0.2528, train acc 89.13%, f1 0.8912, precision 0.8924, recall 0.8901, auc 0.8913
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_4
./test_pima/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6639622641509434

the Fscore is 0.6013071895424837

the precision is 0.46

the recall is 0.8679245283018868

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_4
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5406, train acc 79.10%, f1 0.7906, precision 0.7922, recall 0.7890, auc 0.7910
epoch 201, loss 0.4246, train acc 82.79%, f1 0.8280, precision 0.8278, recall 0.8282, auc 0.8279
epoch 301, loss 0.3957, train acc 84.32%, f1 0.8432, precision 0.8429, recall 0.8436, auc 0.8432
epoch 401, loss 0.2942, train acc 84.84%, f1 0.8484, precision 0.8482, recall 0.8487, auc 0.8484
epoch 501, loss 0.2497, train acc 84.95%, f1 0.8496, precision 0.8492, recall 0.8501, auc 0.8495
epoch 601, loss 0.3131, train acc 85.11%, f1 0.8511, precision 0.8507, recall 0.8516, auc 0.8511
epoch 701, loss 0.3490, train acc 85.06%, f1 0.8506, precision 0.8504, recall 0.8508, auc 0.8506
epoch 801, loss 0.2934, train acc 85.06%, f1 0.8507, precision 0.8504, recall 0.8509, auc 0.8506
epoch 901, loss 0.3169, train acc 85.08%, f1 0.8509, precision 0.8506, recall 0.8511, auc 0.8508
epoch 1001, loss 0.3334, train acc 85.02%, f1 0.8502, precision 0.8501, recall 0.8502, auc 0.8502
epoch 1101, loss 0.3419, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8507, auc 0.8506
epoch 1201, loss 0.4700, train acc 85.03%, f1 0.8503, precision 0.8502, recall 0.8503, auc 0.8503
epoch 1301, loss 0.3502, train acc 85.08%, f1 0.8508, precision 0.8508, recall 0.8508, auc 0.8508
epoch 1401, loss 0.2337, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 1501, loss 0.3733, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 1601, loss 0.2680, train acc 85.11%, f1 0.8511, precision 0.8511, recall 0.8512, auc 0.8511
epoch 1701, loss 0.4049, train acc 85.09%, f1 0.8509, precision 0.8510, recall 0.8509, auc 0.8509
epoch 1801, loss 0.3421, train acc 85.10%, f1 0.8510, precision 0.8512, recall 0.8508, auc 0.8510
epoch 1901, loss 0.3308, train acc 85.04%, f1 0.8503, precision 0.8505, recall 0.8502, auc 0.8504
epoch 2001, loss 0.2179, train acc 85.07%, f1 0.8506, precision 0.8508, recall 0.8505, auc 0.8507
epoch 2101, loss 0.3475, train acc 85.08%, f1 0.8508, precision 0.8510, recall 0.8506, auc 0.8508
epoch 2201, loss 0.3385, train acc 85.14%, f1 0.8514, precision 0.8514, recall 0.8513, auc 0.8514
epoch 2301, loss 0.2793, train acc 85.07%, f1 0.8507, precision 0.8506, recall 0.8508, auc 0.8507
epoch 2401, loss 0.3935, train acc 85.11%, f1 0.8510, precision 0.8511, recall 0.8509, auc 0.8511
epoch 2501, loss 0.4290, train acc 85.16%, f1 0.8516, precision 0.8515, recall 0.8517, auc 0.8516
epoch 2601, loss 0.2593, train acc 85.20%, f1 0.8520, precision 0.8520, recall 0.8521, auc 0.8520
epoch 2701, loss 0.2640, train acc 85.19%, f1 0.8519, precision 0.8519, recall 0.8518, auc 0.8519
epoch 2801, loss 0.2349, train acc 85.27%, f1 0.8528, precision 0.8526, recall 0.8529, auc 0.8527
epoch 2901, loss 0.2813, train acc 85.33%, f1 0.8533, precision 0.8534, recall 0.8533, auc 0.8533
epoch 3001, loss 0.2557, train acc 85.45%, f1 0.8545, precision 0.8546, recall 0.8544, auc 0.8545
epoch 3101, loss 0.3437, train acc 85.59%, f1 0.8559, precision 0.8559, recall 0.8559, auc 0.8559
epoch 3201, loss 0.2899, train acc 85.64%, f1 0.8564, precision 0.8561, recall 0.8567, auc 0.8564
epoch 3301, loss 0.3151, train acc 85.76%, f1 0.8576, precision 0.8579, recall 0.8573, auc 0.8576
epoch 3401, loss 0.3497, train acc 85.90%, f1 0.8589, precision 0.8593, recall 0.8585, auc 0.8590
epoch 3501, loss 0.2686, train acc 86.03%, f1 0.8603, precision 0.8602, recall 0.8604, auc 0.8603
epoch 3601, loss 0.4506, train acc 86.11%, f1 0.8611, precision 0.8611, recall 0.8611, auc 0.8611
epoch 3701, loss 0.2701, train acc 86.29%, f1 0.8630, precision 0.8625, recall 0.8635, auc 0.8629
epoch 3801, loss 0.2877, train acc 86.41%, f1 0.8641, precision 0.8638, recall 0.8644, auc 0.8641
epoch 3901, loss 0.3638, train acc 86.52%, f1 0.8652, precision 0.8647, recall 0.8657, auc 0.8652
epoch 4001, loss 0.3260, train acc 86.55%, f1 0.8656, precision 0.8653, recall 0.8659, auc 0.8655
epoch 4101, loss 0.2812, train acc 86.65%, f1 0.8664, precision 0.8668, recall 0.8660, auc 0.8665
epoch 4201, loss 0.3467, train acc 86.72%, f1 0.8672, precision 0.8672, recall 0.8672, auc 0.8672
epoch 4301, loss 0.1961, train acc 86.80%, f1 0.8681, precision 0.8678, recall 0.8683, auc 0.8680
epoch 4401, loss 0.3577, train acc 86.76%, f1 0.8676, precision 0.8678, recall 0.8673, auc 0.8676
epoch 4501, loss 0.2015, train acc 86.90%, f1 0.8690, precision 0.8691, recall 0.8688, auc 0.8690
epoch 4601, loss 0.2544, train acc 86.88%, f1 0.8688, precision 0.8688, recall 0.8688, auc 0.8688
epoch 4701, loss 0.2665, train acc 86.99%, f1 0.8699, precision 0.8701, recall 0.8696, auc 0.8699
epoch 4801, loss 0.3966, train acc 86.96%, f1 0.8695, precision 0.8699, recall 0.8692, auc 0.8696
epoch 4901, loss 0.2998, train acc 86.99%, f1 0.8700, precision 0.8697, recall 0.8703, auc 0.8699
epoch 5001, loss 0.3494, train acc 87.06%, f1 0.8705, precision 0.8710, recall 0.8701, auc 0.8706
epoch 5101, loss 0.3004, train acc 87.10%, f1 0.8710, precision 0.8709, recall 0.8711, auc 0.8710
epoch 5201, loss 0.2310, train acc 87.11%, f1 0.8711, precision 0.8710, recall 0.8713, auc 0.8711
epoch 5301, loss 0.3065, train acc 87.15%, f1 0.8715, precision 0.8714, recall 0.8717, auc 0.8715
epoch 5401, loss 0.3143, train acc 87.18%, f1 0.8718, precision 0.8721, recall 0.8715, auc 0.8718
epoch 5501, loss 0.2747, train acc 87.20%, f1 0.8720, precision 0.8720, recall 0.8720, auc 0.8720
epoch 5601, loss 0.3725, train acc 87.18%, f1 0.8718, precision 0.8719, recall 0.8717, auc 0.8718
epoch 5701, loss 0.4168, train acc 87.23%, f1 0.8722, precision 0.8725, recall 0.8720, auc 0.8723
epoch 5801, loss 0.2850, train acc 87.28%, f1 0.8728, precision 0.8725, recall 0.8731, auc 0.8728
epoch 5901, loss 0.2501, train acc 87.30%, f1 0.8730, precision 0.8731, recall 0.8728, auc 0.8730
epoch 6001, loss 0.3397, train acc 87.32%, f1 0.8732, precision 0.8734, recall 0.8730, auc 0.8732
epoch 6101, loss 0.2769, train acc 87.34%, f1 0.8734, precision 0.8734, recall 0.8734, auc 0.8734
epoch 6201, loss 0.2447, train acc 87.39%, f1 0.8740, precision 0.8735, recall 0.8746, auc 0.8739
epoch 6301, loss 0.2305, train acc 87.42%, f1 0.8743, precision 0.8742, recall 0.8743, auc 0.8743
epoch 6401, loss 0.2220, train acc 87.47%, f1 0.8747, precision 0.8747, recall 0.8747, auc 0.8747
epoch 6501, loss 0.3125, train acc 87.49%, f1 0.8748, precision 0.8750, recall 0.8747, auc 0.8749
epoch 6601, loss 0.2507, train acc 87.54%, f1 0.8753, precision 0.8758, recall 0.8748, auc 0.8754
epoch 6701, loss 0.2559, train acc 87.55%, f1 0.8756, precision 0.8754, recall 0.8757, auc 0.8755
epoch 6801, loss 0.3055, train acc 87.59%, f1 0.8759, precision 0.8759, recall 0.8760, auc 0.8759
epoch 6901, loss 0.2115, train acc 87.65%, f1 0.8765, precision 0.8764, recall 0.8765, auc 0.8765
epoch 7001, loss 0.2922, train acc 87.68%, f1 0.8767, precision 0.8771, recall 0.8763, auc 0.8768
epoch 7101, loss 0.2658, train acc 87.70%, f1 0.8770, precision 0.8768, recall 0.8772, auc 0.8770
epoch 7201, loss 0.2690, train acc 87.72%, f1 0.8772, precision 0.8771, recall 0.8773, auc 0.8772
epoch 7301, loss 0.3000, train acc 87.77%, f1 0.8777, precision 0.8778, recall 0.8777, auc 0.8777
epoch 7401, loss 0.2795, train acc 87.83%, f1 0.8784, precision 0.8781, recall 0.8787, auc 0.8783
epoch 7501, loss 0.4026, train acc 87.90%, f1 0.8791, precision 0.8785, recall 0.8798, auc 0.8790
epoch 7601, loss 0.3377, train acc 87.89%, f1 0.8789, precision 0.8789, recall 0.8789, auc 0.8789
epoch 7701, loss 0.2239, train acc 87.96%, f1 0.8797, precision 0.8793, recall 0.8800, auc 0.8796
epoch 7801, loss 0.2706, train acc 88.02%, f1 0.8802, precision 0.8801, recall 0.8803, auc 0.8802
epoch 7901, loss 0.2079, train acc 88.04%, f1 0.8804, precision 0.8804, recall 0.8804, auc 0.8804
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_4
./test_pima/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6533962264150943

the Fscore is 0.5949367088607596

the precision is 0.44761904761904764

the recall is 0.8867924528301887

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_4
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.4860, train acc 79.85%, f1 0.7978, precision 0.8004, recall 0.7953, auc 0.7985
epoch 201, loss 0.3724, train acc 82.88%, f1 0.8289, precision 0.8287, recall 0.8291, auc 0.8288
epoch 301, loss 0.2754, train acc 84.44%, f1 0.8445, precision 0.8442, recall 0.8448, auc 0.8444
epoch 401, loss 0.3348, train acc 84.87%, f1 0.8487, precision 0.8486, recall 0.8488, auc 0.8487
epoch 501, loss 0.3429, train acc 84.94%, f1 0.8495, precision 0.8490, recall 0.8500, auc 0.8494
epoch 601, loss 0.4167, train acc 85.00%, f1 0.8500, precision 0.8498, recall 0.8502, auc 0.8500
epoch 701, loss 0.3429, train acc 85.03%, f1 0.8504, precision 0.8502, recall 0.8505, auc 0.8503
epoch 801, loss 0.2730, train acc 85.03%, f1 0.8503, precision 0.8502, recall 0.8505, auc 0.8503
epoch 901, loss 0.3538, train acc 85.09%, f1 0.8509, precision 0.8508, recall 0.8511, auc 0.8509
epoch 1001, loss 0.3085, train acc 85.10%, f1 0.8510, precision 0.8509, recall 0.8511, auc 0.8510
epoch 1101, loss 0.2937, train acc 85.08%, f1 0.8508, precision 0.8508, recall 0.8509, auc 0.8508
epoch 1201, loss 0.3196, train acc 85.04%, f1 0.8505, precision 0.8504, recall 0.8505, auc 0.8504
epoch 1301, loss 0.4073, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8507, auc 0.8506
epoch 1401, loss 0.2827, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8507, auc 0.8506
epoch 1501, loss 0.4438, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8508, auc 0.8506
epoch 1601, loss 0.4213, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8508, auc 0.8506
epoch 1701, loss 0.3401, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 1801, loss 0.4351, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8507, auc 0.8506
epoch 1901, loss 0.4379, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8507, auc 0.8506
epoch 2001, loss 0.2999, train acc 85.08%, f1 0.8508, precision 0.8506, recall 0.8510, auc 0.8508
epoch 2101, loss 0.4661, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 2201, loss 0.2694, train acc 85.12%, f1 0.8512, precision 0.8511, recall 0.8513, auc 0.8512
epoch 2301, loss 0.3395, train acc 85.14%, f1 0.8514, precision 0.8515, recall 0.8513, auc 0.8514
epoch 2401, loss 0.3047, train acc 85.17%, f1 0.8517, precision 0.8518, recall 0.8515, auc 0.8517
epoch 2501, loss 0.3759, train acc 85.22%, f1 0.8522, precision 0.8523, recall 0.8521, auc 0.8522
epoch 2601, loss 0.4099, train acc 85.23%, f1 0.8524, precision 0.8520, recall 0.8528, auc 0.8523
epoch 2701, loss 0.2660, train acc 85.32%, f1 0.8531, precision 0.8534, recall 0.8528, auc 0.8532
epoch 2801, loss 0.2947, train acc 85.38%, f1 0.8537, precision 0.8542, recall 0.8532, auc 0.8538
epoch 2901, loss 0.4236, train acc 85.46%, f1 0.8547, precision 0.8542, recall 0.8552, auc 0.8546
epoch 3001, loss 0.3030, train acc 85.53%, f1 0.8553, precision 0.8552, recall 0.8554, auc 0.8553
epoch 3101, loss 0.3160, train acc 85.61%, f1 0.8561, precision 0.8562, recall 0.8560, auc 0.8561
epoch 3201, loss 0.3677, train acc 85.78%, f1 0.8579, precision 0.8574, recall 0.8584, auc 0.8578
epoch 3301, loss 0.3160, train acc 85.86%, f1 0.8587, precision 0.8579, recall 0.8596, auc 0.8586
epoch 3401, loss 0.3386, train acc 85.99%, f1 0.8599, precision 0.8598, recall 0.8600, auc 0.8599
epoch 3501, loss 0.4476, train acc 86.10%, f1 0.8610, precision 0.8609, recall 0.8610, auc 0.8610
epoch 3601, loss 0.3501, train acc 86.24%, f1 0.8625, precision 0.8618, recall 0.8631, auc 0.8624
epoch 3701, loss 0.3246, train acc 86.37%, f1 0.8636, precision 0.8639, recall 0.8633, auc 0.8637
epoch 3801, loss 0.2697, train acc 86.43%, f1 0.8642, precision 0.8644, recall 0.8641, auc 0.8643
epoch 3901, loss 0.3604, train acc 86.56%, f1 0.8655, precision 0.8661, recall 0.8650, auc 0.8656
epoch 4001, loss 0.2906, train acc 86.58%, f1 0.8659, precision 0.8653, recall 0.8664, auc 0.8658
epoch 4101, loss 0.3572, train acc 86.61%, f1 0.8662, precision 0.8655, recall 0.8670, auc 0.8661
epoch 4201, loss 0.3737, train acc 86.71%, f1 0.8671, precision 0.8674, recall 0.8668, auc 0.8671
epoch 4301, loss 0.2927, train acc 86.75%, f1 0.8675, precision 0.8674, recall 0.8677, auc 0.8675
epoch 4401, loss 0.2442, train acc 86.81%, f1 0.8681, precision 0.8679, recall 0.8683, auc 0.8681
epoch 4501, loss 0.3036, train acc 86.85%, f1 0.8686, precision 0.8683, recall 0.8688, auc 0.8685
epoch 4601, loss 0.2555, train acc 86.86%, f1 0.8686, precision 0.8686, recall 0.8687, auc 0.8686
epoch 4701, loss 0.1732, train acc 86.94%, f1 0.8694, precision 0.8692, recall 0.8697, auc 0.8694
epoch 4801, loss 0.2643, train acc 86.92%, f1 0.8692, precision 0.8687, recall 0.8698, auc 0.8692
epoch 4901, loss 0.3427, train acc 86.97%, f1 0.8697, precision 0.8697, recall 0.8698, auc 0.8697
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_4
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6628301886792454

the Fscore is 0.6037735849056604

the precision is 0.4528301886792453

the recall is 0.9056603773584906

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_4
----------------------



epoch 1, loss 0.6933, train acc 48.89%, f1 0.6484, precision 0.4942, recall 0.9426, auc 0.4889
epoch 101, loss 0.5367, train acc 79.04%, f1 0.7898, precision 0.7918, recall 0.7879, auc 0.7904
epoch 201, loss 0.3776, train acc 82.76%, f1 0.8277, precision 0.8272, recall 0.8282, auc 0.8276
epoch 301, loss 0.3522, train acc 84.44%, f1 0.8444, precision 0.8441, recall 0.8447, auc 0.8444
epoch 401, loss 0.3268, train acc 84.94%, f1 0.8495, precision 0.8492, recall 0.8497, auc 0.8494
epoch 501, loss 0.4377, train acc 84.93%, f1 0.8493, precision 0.8491, recall 0.8496, auc 0.8493
epoch 601, loss 0.3843, train acc 85.07%, f1 0.8508, precision 0.8505, recall 0.8512, auc 0.8507
epoch 701, loss 0.4962, train acc 85.13%, f1 0.8514, precision 0.8510, recall 0.8518, auc 0.8513
epoch 801, loss 0.3023, train acc 85.09%, f1 0.8510, precision 0.8508, recall 0.8512, auc 0.8509
epoch 901, loss 0.2989, train acc 85.09%, f1 0.8509, precision 0.8507, recall 0.8511, auc 0.8509
epoch 1001, loss 0.3403, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 1101, loss 0.2089, train acc 85.07%, f1 0.8506, precision 0.8507, recall 0.8506, auc 0.8507
epoch 1201, loss 0.4268, train acc 85.10%, f1 0.8510, precision 0.8510, recall 0.8510, auc 0.8510
epoch 1301, loss 0.3485, train acc 85.08%, f1 0.8508, precision 0.8508, recall 0.8507, auc 0.8508
epoch 1401, loss 0.3669, train acc 85.05%, f1 0.8505, precision 0.8504, recall 0.8507, auc 0.8505
epoch 1501, loss 0.2881, train acc 85.06%, f1 0.8507, precision 0.8505, recall 0.8509, auc 0.8506
epoch 1601, loss 0.4706, train acc 85.06%, f1 0.8506, precision 0.8506, recall 0.8506, auc 0.8506
epoch 1701, loss 0.3605, train acc 85.08%, f1 0.8508, precision 0.8506, recall 0.8509, auc 0.8508
epoch 1801, loss 0.3836, train acc 85.11%, f1 0.8511, precision 0.8509, recall 0.8513, auc 0.8511
epoch 1901, loss 0.4596, train acc 85.10%, f1 0.8511, precision 0.8509, recall 0.8513, auc 0.8510
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_4
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6578301886792453

the Fscore is 0.6

the precision is 0.4485981308411215

the recall is 0.9056603773584906

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_4
----------------------



epoch 1, loss 0.6934, train acc 50.18%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5588, train acc 78.91%, f1 0.7818, precision 0.8067, recall 0.7583, auc 0.7889
epoch 201, loss 0.4858, train acc 82.65%, f1 0.8254, precision 0.8278, recall 0.8230, auc 0.8265
epoch 301, loss 0.3787, train acc 84.35%, f1 0.8440, precision 0.8387, recall 0.8493, auc 0.8435
epoch 401, loss 0.2460, train acc 84.80%, f1 0.8477, precision 0.8467, recall 0.8486, auc 0.8480
epoch 501, loss 0.4203, train acc 84.95%, f1 0.8496, precision 0.8459, recall 0.8533, auc 0.8495
epoch 601, loss 0.3389, train acc 84.94%, f1 0.8492, precision 0.8472, recall 0.8513, auc 0.8494
epoch 701, loss 0.3851, train acc 85.05%, f1 0.8508, precision 0.8461, recall 0.8556, auc 0.8505
epoch 801, loss 0.3111, train acc 85.08%, f1 0.8512, precision 0.8463, recall 0.8561, auc 0.8509
epoch 901, loss 0.4420, train acc 85.07%, f1 0.8508, precision 0.8472, recall 0.8545, auc 0.8508
epoch 1001, loss 0.2941, train acc 85.07%, f1 0.8507, precision 0.8475, recall 0.8540, auc 0.8507
epoch 1101, loss 0.3591, train acc 85.05%, f1 0.8505, precision 0.8477, recall 0.8532, auc 0.8505
epoch 1201, loss 0.2241, train acc 85.11%, f1 0.8508, precision 0.8495, recall 0.8522, auc 0.8511
epoch 1301, loss 0.3567, train acc 85.10%, f1 0.8503, precision 0.8512, recall 0.8495, auc 0.8510
epoch 1401, loss 0.4019, train acc 85.06%, f1 0.8508, precision 0.8469, recall 0.8546, auc 0.8506
epoch 1501, loss 0.3671, train acc 85.08%, f1 0.8507, precision 0.8481, recall 0.8533, auc 0.8508
epoch 1601, loss 0.3704, train acc 85.09%, f1 0.8513, precision 0.8459, recall 0.8568, auc 0.8509
epoch 1701, loss 0.4695, train acc 85.03%, f1 0.8503, precision 0.8470, recall 0.8537, auc 0.8503
epoch 1801, loss 0.3529, train acc 85.06%, f1 0.8511, precision 0.8453, recall 0.8571, auc 0.8507
epoch 1901, loss 0.3498, train acc 85.09%, f1 0.8509, precision 0.8480, recall 0.8538, auc 0.8509
epoch 2001, loss 0.3417, train acc 85.09%, f1 0.8507, precision 0.8487, recall 0.8528, auc 0.8509
epoch 2101, loss 0.3761, train acc 85.05%, f1 0.8503, precision 0.8486, recall 0.8520, auc 0.8505
epoch 2201, loss 0.2761, train acc 85.05%, f1 0.8509, precision 0.8460, recall 0.8558, auc 0.8506
epoch 2301, loss 0.3841, train acc 85.06%, f1 0.8510, precision 0.8457, recall 0.8563, auc 0.8506
epoch 2401, loss 0.2831, train acc 85.05%, f1 0.8500, precision 0.8495, recall 0.8506, auc 0.8505
epoch 2501, loss 0.4065, train acc 85.07%, f1 0.8506, precision 0.8482, recall 0.8531, auc 0.8507
epoch 2601, loss 0.3334, train acc 85.05%, f1 0.8506, precision 0.8471, recall 0.8542, auc 0.8505
epoch 2701, loss 0.4395, train acc 85.06%, f1 0.8508, precision 0.8466, recall 0.8551, auc 0.8506
epoch 2801, loss 0.3162, train acc 85.07%, f1 0.8505, precision 0.8487, recall 0.8524, auc 0.8507
epoch 2901, loss 0.4115, train acc 85.06%, f1 0.8511, precision 0.8456, recall 0.8567, auc 0.8507
epoch 3001, loss 0.3389, train acc 85.11%, f1 0.8509, precision 0.8491, recall 0.8526, auc 0.8511
epoch 3101, loss 0.3579, train acc 85.08%, f1 0.8503, precision 0.8500, recall 0.8507, auc 0.8508
epoch 3201, loss 0.3689, train acc 85.11%, f1 0.8511, precision 0.8481, recall 0.8541, auc 0.8511
epoch 3301, loss 0.3564, train acc 85.10%, f1 0.8511, precision 0.8474, recall 0.8549, auc 0.8510
epoch 3401, loss 0.2915, train acc 85.15%, f1 0.8520, precision 0.8464, recall 0.8577, auc 0.8516
epoch 3501, loss 0.3898, train acc 85.20%, f1 0.8523, precision 0.8474, recall 0.8573, auc 0.8520
epoch 3601, loss 0.4129, train acc 85.22%, f1 0.8521, precision 0.8495, recall 0.8547, auc 0.8522
epoch 3701, loss 0.3614, train acc 85.28%, f1 0.8518, precision 0.8541, recall 0.8496, auc 0.8527
epoch 3801, loss 0.4728, train acc 85.35%, f1 0.8530, precision 0.8525, recall 0.8535, auc 0.8535
epoch 3901, loss 0.3581, train acc 85.46%, f1 0.8541, precision 0.8538, recall 0.8545, auc 0.8546
epoch 4001, loss 0.4016, train acc 85.55%, f1 0.8556, precision 0.8520, recall 0.8593, auc 0.8556
epoch 4101, loss 0.2798, train acc 85.63%, f1 0.8561, precision 0.8545, recall 0.8576, auc 0.8563
epoch 4201, loss 0.2611, train acc 85.68%, f1 0.8572, precision 0.8519, recall 0.8625, auc 0.8568
epoch 4301, loss 0.3406, train acc 85.73%, f1 0.8578, precision 0.8518, recall 0.8639, auc 0.8573
epoch 4401, loss 0.3120, train acc 85.89%, f1 0.8588, precision 0.8562, recall 0.8614, auc 0.8589
epoch 4501, loss 0.3090, train acc 86.00%, f1 0.8599, precision 0.8572, recall 0.8627, auc 0.8600
epoch 4601, loss 0.3006, train acc 86.07%, f1 0.8604, precision 0.8590, recall 0.8617, auc 0.8607
epoch 4701, loss 0.2996, train acc 86.19%, f1 0.8618, precision 0.8594, recall 0.8642, auc 0.8619
epoch 4801, loss 0.4045, train acc 86.25%, f1 0.8628, precision 0.8580, recall 0.8676, auc 0.8625
epoch 4901, loss 0.3015, train acc 86.29%, f1 0.8624, precision 0.8622, recall 0.8627, auc 0.8629
epoch 5001, loss 0.3139, train acc 86.33%, f1 0.8633, precision 0.8606, recall 0.8660, auc 0.8633
epoch 5101, loss 0.2925, train acc 86.44%, f1 0.8638, precision 0.8648, recall 0.8628, auc 0.8644
epoch 5201, loss 0.3356, train acc 86.55%, f1 0.8652, precision 0.8642, recall 0.8662, auc 0.8655
epoch 5301, loss 0.3583, train acc 86.55%, f1 0.8653, precision 0.8639, recall 0.8666, auc 0.8655
epoch 5401, loss 0.2638, train acc 86.65%, f1 0.8662, precision 0.8648, recall 0.8676, auc 0.8665
epoch 5501, loss 0.2637, train acc 86.64%, f1 0.8666, precision 0.8618, recall 0.8715, auc 0.8664
epoch 5601, loss 0.3397, train acc 86.76%, f1 0.8672, precision 0.8665, recall 0.8679, auc 0.8676
epoch 5701, loss 0.2888, train acc 86.73%, f1 0.8678, precision 0.8620, recall 0.8736, auc 0.8674
epoch 5801, loss 0.2867, train acc 86.80%, f1 0.8676, precision 0.8667, recall 0.8685, auc 0.8680
epoch 5901, loss 0.2791, train acc 86.84%, f1 0.8685, precision 0.8646, recall 0.8726, auc 0.8684
epoch 6001, loss 0.3133, train acc 86.88%, f1 0.8687, precision 0.8660, recall 0.8715, auc 0.8688
epoch 6101, loss 0.2862, train acc 86.87%, f1 0.8687, precision 0.8660, recall 0.8714, auc 0.8688
epoch 6201, loss 0.2705, train acc 86.95%, f1 0.8692, precision 0.8678, recall 0.8706, auc 0.8695
epoch 6301, loss 0.2889, train acc 86.94%, f1 0.8695, precision 0.8663, recall 0.8727, auc 0.8695
epoch 6401, loss 0.2744, train acc 86.98%, f1 0.8692, precision 0.8702, recall 0.8682, auc 0.8698
epoch 6501, loss 0.3177, train acc 87.00%, f1 0.8697, precision 0.8689, recall 0.8704, auc 0.8700
epoch 6601, loss 0.4309, train acc 87.04%, f1 0.8705, precision 0.8672, recall 0.8738, auc 0.8705
epoch 6701, loss 0.1856, train acc 87.10%, f1 0.8713, precision 0.8662, recall 0.8765, auc 0.8710
epoch 6801, loss 0.4212, train acc 87.14%, f1 0.8712, precision 0.8695, recall 0.8729, auc 0.8714
epoch 6901, loss 0.2609, train acc 87.17%, f1 0.8717, precision 0.8690, recall 0.8743, auc 0.8717
epoch 7001, loss 0.2639, train acc 87.22%, f1 0.8721, precision 0.8697, recall 0.8746, auc 0.8722
epoch 7101, loss 0.2339, train acc 87.29%, f1 0.8729, precision 0.8695, recall 0.8764, auc 0.8729
epoch 7201, loss 0.3581, train acc 87.34%, f1 0.8733, precision 0.8705, recall 0.8762, auc 0.8734
epoch 7301, loss 0.2843, train acc 87.34%, f1 0.8732, precision 0.8716, recall 0.8747, auc 0.8734
epoch 7401, loss 0.3076, train acc 87.38%, f1 0.8734, precision 0.8734, recall 0.8735, auc 0.8738
epoch 7501, loss 0.3004, train acc 87.40%, f1 0.8738, precision 0.8725, recall 0.8750, auc 0.8740
epoch 7601, loss 0.3731, train acc 87.47%, f1 0.8748, precision 0.8712, recall 0.8784, auc 0.8747
epoch 7701, loss 0.3473, train acc 87.51%, f1 0.8752, precision 0.8716, recall 0.8788, auc 0.8751
epoch 7801, loss 0.4794, train acc 87.55%, f1 0.8753, precision 0.8739, recall 0.8767, auc 0.8755
epoch 7901, loss 0.2629, train acc 87.57%, f1 0.8759, precision 0.8716, recall 0.8803, auc 0.8758
epoch 8001, loss 0.2309, train acc 87.64%, f1 0.8764, precision 0.8732, recall 0.8797, auc 0.8764
epoch 8101, loss 0.3033, train acc 87.70%, f1 0.8768, precision 0.8756, recall 0.8780, auc 0.8770
epoch 8201, loss 0.2963, train acc 87.77%, f1 0.8774, precision 0.8760, recall 0.8788, auc 0.8777
epoch 8301, loss 0.3439, train acc 87.76%, f1 0.8772, precision 0.8766, recall 0.8779, auc 0.8776
epoch 8401, loss 0.1673, train acc 87.77%, f1 0.8774, precision 0.8759, recall 0.8790, auc 0.8777
epoch 8501, loss 0.3955, train acc 87.83%, f1 0.8783, precision 0.8752, recall 0.8815, auc 0.8783
epoch 8601, loss 0.2913, train acc 87.91%, f1 0.8794, precision 0.8743, recall 0.8846, auc 0.8791
epoch 8701, loss 0.2865, train acc 87.96%, f1 0.8797, precision 0.8759, recall 0.8836, auc 0.8797
epoch 8801, loss 0.4133, train acc 88.03%, f1 0.8803, precision 0.8771, recall 0.8835, auc 0.8803
epoch 8901, loss 0.3312, train acc 88.05%, f1 0.8803, precision 0.8789, recall 0.8818, auc 0.8806
epoch 9001, loss 0.2995, train acc 88.08%, f1 0.8804, precision 0.8798, recall 0.8811, auc 0.8808
epoch 9101, loss 0.2460, train acc 88.07%, f1 0.8801, precision 0.8812, recall 0.8790, auc 0.8806
epoch 9201, loss 0.3109, train acc 88.14%, f1 0.8813, precision 0.8787, recall 0.8839, auc 0.8814
epoch 9301, loss 0.2251, train acc 88.21%, f1 0.8821, precision 0.8790, recall 0.8851, auc 0.8821
epoch 9401, loss 0.2982, train acc 88.22%, f1 0.8821, precision 0.8796, recall 0.8846, auc 0.8822
epoch 9501, loss 0.2800, train acc 88.33%, f1 0.8831, precision 0.8808, recall 0.8855, auc 0.8833
epoch 9601, loss 0.3248, train acc 88.32%, f1 0.8832, precision 0.8804, recall 0.8859, auc 0.8832
epoch 9701, loss 0.3252, train acc 88.32%, f1 0.8832, precision 0.8796, recall 0.8869, auc 0.8832
epoch 9801, loss 0.2538, train acc 88.40%, f1 0.8840, precision 0.8812, recall 0.8867, auc 0.8840
epoch 9901, loss 0.2605, train acc 88.45%, f1 0.8847, precision 0.8803, recall 0.8891, auc 0.8845
epoch 10001, loss 0.2708, train acc 88.44%, f1 0.8840, precision 0.8837, recall 0.8844, auc 0.8844
epoch 10101, loss 0.2154, train acc 88.49%, f1 0.8848, precision 0.8822, recall 0.8874, auc 0.8849
epoch 10201, loss 0.1836, train acc 88.52%, f1 0.8846, precision 0.8859, recall 0.8834, auc 0.8852
epoch 10301, loss 0.2909, train acc 88.58%, f1 0.8863, precision 0.8796, recall 0.8930, auc 0.8858
epoch 10401, loss 0.3717, train acc 88.64%, f1 0.8863, precision 0.8838, recall 0.8889, auc 0.8864
epoch 10501, loss 0.1822, train acc 88.64%, f1 0.8864, precision 0.8833, recall 0.8894, auc 0.8864
epoch 10601, loss 0.1931, train acc 88.66%, f1 0.8863, precision 0.8856, recall 0.8870, auc 0.8866
epoch 10701, loss 0.2697, train acc 88.72%, f1 0.8869, precision 0.8865, recall 0.8872, auc 0.8872
epoch 10801, loss 0.2667, train acc 88.74%, f1 0.8873, precision 0.8848, recall 0.8899, auc 0.8874
epoch 10901, loss 0.2246, train acc 88.80%, f1 0.8878, precision 0.8861, recall 0.8894, auc 0.8880
epoch 11001, loss 0.1921, train acc 88.78%, f1 0.8879, precision 0.8844, recall 0.8914, auc 0.8879
epoch 11101, loss 0.2239, train acc 88.86%, f1 0.8881, precision 0.8887, recall 0.8875, auc 0.8886
epoch 11201, loss 0.2307, train acc 88.91%, f1 0.8891, precision 0.8856, recall 0.8926, auc 0.8891
epoch 11301, loss 0.2817, train acc 88.90%, f1 0.8888, precision 0.8877, recall 0.8898, auc 0.8890
epoch 11401, loss 0.2227, train acc 88.97%, f1 0.8893, precision 0.8895, recall 0.8891, auc 0.8897
epoch 11501, loss 0.2156, train acc 89.04%, f1 0.8901, precision 0.8887, recall 0.8916, auc 0.8904
epoch 11601, loss 0.1867, train acc 89.04%, f1 0.8901, precision 0.8894, recall 0.8908, auc 0.8904
epoch 11701, loss 0.1651, train acc 89.07%, f1 0.8905, precision 0.8893, recall 0.8917, auc 0.8907
epoch 11801, loss 0.2632, train acc 89.14%, f1 0.8910, precision 0.8906, recall 0.8915, auc 0.8914
epoch 11901, loss 0.2394, train acc 89.10%, f1 0.8910, precision 0.8880, recall 0.8940, auc 0.8910
epoch 12001, loss 0.2098, train acc 89.22%, f1 0.8919, precision 0.8916, recall 0.8922, auc 0.8922
epoch 12101, loss 0.2372, train acc 89.29%, f1 0.8925, precision 0.8926, recall 0.8925, auc 0.8929
epoch 12201, loss 0.1358, train acc 89.29%, f1 0.8928, precision 0.8906, recall 0.8949, auc 0.8929
epoch 12301, loss 0.2471, train acc 89.35%, f1 0.8935, precision 0.8907, recall 0.8964, auc 0.8936
epoch 12401, loss 0.2100, train acc 89.33%, f1 0.8932, precision 0.8911, recall 0.8953, auc 0.8933
epoch 12501, loss 0.2901, train acc 89.37%, f1 0.8937, precision 0.8904, recall 0.8970, auc 0.8937
epoch 12601, loss 0.2067, train acc 89.42%, f1 0.8940, precision 0.8928, recall 0.8952, auc 0.8942
epoch 12701, loss 0.1920, train acc 89.42%, f1 0.8943, precision 0.8908, recall 0.8977, auc 0.8942
epoch 12801, loss 0.2469, train acc 89.46%, f1 0.8946, precision 0.8913, recall 0.8980, auc 0.8946
epoch 12901, loss 0.2205, train acc 89.53%, f1 0.8951, precision 0.8940, recall 0.8963, auc 0.8954
epoch 13001, loss 0.2077, train acc 89.58%, f1 0.8957, precision 0.8932, recall 0.8982, auc 0.8958
epoch 13101, loss 0.3350, train acc 89.63%, f1 0.8959, precision 0.8954, recall 0.8965, auc 0.8963
epoch 13201, loss 0.3181, train acc 89.63%, f1 0.8964, precision 0.8926, recall 0.9002, auc 0.8963
epoch 13301, loss 0.3060, train acc 89.66%, f1 0.8964, precision 0.8947, recall 0.8982, auc 0.8966
epoch 13401, loss 0.3216, train acc 89.69%, f1 0.8968, precision 0.8941, recall 0.8996, auc 0.8969
epoch 13501, loss 0.2454, train acc 89.78%, f1 0.8977, precision 0.8951, recall 0.9003, auc 0.8978
epoch 13601, loss 0.1772, train acc 89.79%, f1 0.8976, precision 0.8970, recall 0.8982, auc 0.8979
epoch 13701, loss 0.2173, train acc 89.85%, f1 0.8985, precision 0.8954, recall 0.9017, auc 0.8985
epoch 13801, loss 0.2165, train acc 89.83%, f1 0.8981, precision 0.8968, recall 0.8994, auc 0.8983
epoch 13901, loss 0.3277, train acc 89.85%, f1 0.8982, precision 0.8972, recall 0.8993, auc 0.8985
epoch 14001, loss 0.2175, train acc 89.93%, f1 0.8990, precision 0.8988, recall 0.8993, auc 0.8993
epoch 14101, loss 0.2792, train acc 89.89%, f1 0.8988, precision 0.8966, recall 0.9010, auc 0.8989
epoch 14201, loss 0.3627, train acc 89.98%, f1 0.8998, precision 0.8969, recall 0.9027, auc 0.8998
epoch 14301, loss 0.2408, train acc 90.02%, f1 0.9002, precision 0.8967, recall 0.9038, auc 0.9002
epoch 14401, loss 0.2117, train acc 90.04%, f1 0.9000, precision 0.9009, recall 0.8991, auc 0.9004
epoch 14501, loss 0.2229, train acc 90.10%, f1 0.9007, precision 0.8999, recall 0.9016, auc 0.9010
epoch 14601, loss 0.2562, train acc 90.11%, f1 0.9008, precision 0.8996, recall 0.9021, auc 0.9011
epoch 14701, loss 0.2045, train acc 90.15%, f1 0.9016, precision 0.8979, recall 0.9053, auc 0.9016
epoch 14801, loss 0.2018, train acc 90.16%, f1 0.9012, precision 0.9019, recall 0.9005, auc 0.9016
epoch 14901, loss 0.2054, train acc 90.26%, f1 0.9024, precision 0.9005, recall 0.9044, auc 0.9026
epoch 15001, loss 0.2345, train acc 90.21%, f1 0.9018, precision 0.9016, recall 0.9020, auc 0.9021
epoch 15101, loss 0.2403, train acc 90.24%, f1 0.9022, precision 0.9004, recall 0.9041, auc 0.9024
epoch 15201, loss 0.2251, train acc 90.33%, f1 0.9030, precision 0.9021, recall 0.9039, auc 0.9033
epoch 15301, loss 0.1617, train acc 90.37%, f1 0.9034, precision 0.9034, recall 0.9033, auc 0.9037
epoch 15401, loss 0.2561, train acc 90.39%, f1 0.9036, precision 0.9029, recall 0.9044, auc 0.9039
epoch 15501, loss 0.2402, train acc 90.42%, f1 0.9038, precision 0.9043, recall 0.9033, auc 0.9042
epoch 15601, loss 0.2813, train acc 90.46%, f1 0.9043, precision 0.9039, recall 0.9048, auc 0.9046
epoch 15701, loss 0.1884, train acc 90.47%, f1 0.9044, precision 0.9033, recall 0.9056, auc 0.9047
epoch 15801, loss 0.1329, train acc 90.54%, f1 0.9052, precision 0.9037, recall 0.9067, auc 0.9054
epoch 15901, loss 0.1975, train acc 90.55%, f1 0.9056, precision 0.9016, recall 0.9096, auc 0.9055
epoch 16001, loss 0.1592, train acc 90.53%, f1 0.9053, precision 0.9026, recall 0.9080, auc 0.9053
epoch 16101, loss 0.1445, train acc 90.59%, f1 0.9060, precision 0.9020, recall 0.9100, auc 0.9059
epoch 16201, loss 0.1845, train acc 90.67%, f1 0.9066, precision 0.9042, recall 0.9090, auc 0.9067
epoch 16301, loss 0.2745, train acc 90.64%, f1 0.9058, precision 0.9079, recall 0.9038, auc 0.9064
epoch 16401, loss 0.2829, train acc 90.67%, f1 0.9064, precision 0.9062, recall 0.9066, auc 0.9067
epoch 16501, loss 0.2001, train acc 90.70%, f1 0.9065, precision 0.9077, recall 0.9054, auc 0.9070/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.1664, train acc 90.77%, f1 0.9076, precision 0.9051, recall 0.9100, auc 0.9077
epoch 16701, loss 0.1899, train acc 90.78%, f1 0.9075, precision 0.9077, recall 0.9073, auc 0.9078
epoch 16801, loss 0.1886, train acc 90.82%, f1 0.9081, precision 0.9056, recall 0.9107, auc 0.9082
epoch 16901, loss 0.2066, train acc 90.86%, f1 0.9083, precision 0.9085, recall 0.9080, auc 0.9086
epoch 17001, loss 0.2452, train acc 90.89%, f1 0.9087, precision 0.9076, recall 0.9098, auc 0.9089
epoch 17101, loss 0.2102, train acc 90.94%, f1 0.9092, precision 0.9082, recall 0.9102, auc 0.9094
epoch 17201, loss 0.1454, train acc 91.00%, f1 0.9096, precision 0.9105, recall 0.9087, auc 0.9100
epoch 17301, loss 0.1810, train acc 90.98%, f1 0.9097, precision 0.9082, recall 0.9112, auc 0.9099
epoch 17401, loss 0.1933, train acc 91.02%, f1 0.9099, precision 0.9099, recall 0.9099, auc 0.9102
epoch 17501, loss 0.2277, train acc 91.10%, f1 0.9107, precision 0.9102, recall 0.9112, auc 0.9110
epoch 17601, loss 0.2444, train acc 91.12%, f1 0.9109, precision 0.9104, recall 0.9115, auc 0.9112
epoch 17701, loss 0.1993, train acc 91.14%, f1 0.9113, precision 0.9092, recall 0.9135, auc 0.9114
epoch 17801, loss 0.2101, train acc 91.22%, f1 0.9122, precision 0.9092, recall 0.9152, auc 0.9122
epoch 17901, loss 0.2353, train acc 91.27%, f1 0.9124, precision 0.9124, recall 0.9123, auc 0.9127
epoch 18001, loss 0.1753, train acc 91.29%, f1 0.9125, precision 0.9136, recall 0.9114, auc 0.9129
epoch 18101, loss 0.2303, train acc 91.33%, f1 0.9131, precision 0.9116, recall 0.9147, auc 0.9133
epoch 18201, loss 0.2449, train acc 91.35%, f1 0.9132, precision 0.9123, recall 0.9142, auc 0.9135
epoch 18301, loss 0.2775, train acc 91.37%, f1 0.9136, precision 0.9115, recall 0.9157, auc 0.9137
epoch 18401, loss 0.2092, train acc 91.37%, f1 0.9137, precision 0.9112, recall 0.9161, auc 0.9138
epoch 18501, loss 0.1695, train acc 91.46%, f1 0.9143, precision 0.9139, recall 0.9148, auc 0.9146
epoch 18601, loss 0.1757, train acc 91.52%, f1 0.9153, precision 0.9111, recall 0.9195, auc 0.9152
epoch 18701, loss 0.1911, train acc 91.54%, f1 0.9154, precision 0.9125, recall 0.9183, auc 0.9154
epoch 18801, loss 0.1349, train acc 91.58%, f1 0.9157, precision 0.9138, recall 0.9176, auc 0.9158
epoch 18901, loss 0.2388, train acc 91.59%, f1 0.9158, precision 0.9143, recall 0.9172, auc 0.9159
epoch 19001, loss 0.1905, train acc 91.66%, f1 0.9162, precision 0.9169, recall 0.9155, auc 0.9166
epoch 19101, loss 0.1563, train acc 91.67%, f1 0.9164, precision 0.9159, recall 0.9170, auc 0.9167
epoch 19201, loss 0.1880, train acc 91.71%, f1 0.9170, precision 0.9154, recall 0.9185, auc 0.9171
epoch 19301, loss 0.1922, train acc 91.75%, f1 0.9174, precision 0.9154, recall 0.9193, auc 0.9175
epoch 19401, loss 0.2470, train acc 91.80%, f1 0.9180, precision 0.9157, recall 0.9203, auc 0.9181
epoch 19501, loss 0.1167, train acc 91.81%, f1 0.9179, precision 0.9171, recall 0.9188, auc 0.9181
epoch 19601, loss 0.1776, train acc 91.89%, f1 0.9184, precision 0.9200, recall 0.9169, auc 0.9189
epoch 19701, loss 0.0785, train acc 91.90%, f1 0.9189, precision 0.9176, recall 0.9201, auc 0.9190
epoch 19801, loss 0.1839, train acc 91.93%, f1 0.9192, precision 0.9169, recall 0.9216, auc 0.9193
epoch 19901, loss 0.2403, train acc 91.93%, f1 0.9193, precision 0.9163, recall 0.9224, auc 0.9194
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_4
./test_pima/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6839622641509434

the Fscore is 0.6174496644295302

the precision is 0.4791666666666667

the recall is 0.8679245283018868

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_4
----------------------



epoch 1, loss 0.6930, train acc 49.81%, f1 0.6649, precision 0.4981, recall 1.0000, auc 0.5000
epoch 101, loss 0.5466, train acc 78.32%, f1 0.7869, precision 0.7707, recall 0.8038, auc 0.7833
epoch 201, loss 0.3291, train acc 82.67%, f1 0.8254, precision 0.8280, recall 0.8229, auc 0.8266
epoch 301, loss 0.4537, train acc 84.37%, f1 0.8432, precision 0.8428, recall 0.8436, auc 0.8437
epoch 401, loss 0.3308, train acc 84.80%, f1 0.8473, precision 0.8482, recall 0.8463, auc 0.8480
epoch 501, loss 0.3383, train acc 84.96%, f1 0.8485, precision 0.8511, recall 0.8459, auc 0.8495
epoch 601, loss 0.3090, train acc 85.05%, f1 0.8497, precision 0.8512, recall 0.8482, auc 0.8505
epoch 701, loss 0.2883, train acc 85.11%, f1 0.8505, precision 0.8503, recall 0.8508, auc 0.8511
epoch 801, loss 0.3881, train acc 85.07%, f1 0.8503, precision 0.8497, recall 0.8508, auc 0.8507
epoch 901, loss 0.3430, train acc 85.03%, f1 0.8490, precision 0.8535, recall 0.8444, auc 0.8503
epoch 1001, loss 0.4284, train acc 85.02%, f1 0.8489, precision 0.8530, recall 0.8449, auc 0.8502
epoch 1101, loss 0.4341, train acc 85.04%, f1 0.8494, precision 0.8522, recall 0.8466, auc 0.8504
epoch 1201, loss 0.3376, train acc 85.03%, f1 0.8497, precision 0.8498, recall 0.8497, auc 0.8503
epoch 1301, loss 0.4794, train acc 85.01%, f1 0.8494, precision 0.8504, recall 0.8484, auc 0.8501
epoch 1401, loss 0.4545, train acc 85.03%, f1 0.8494, precision 0.8510, recall 0.8477, auc 0.8502
epoch 1501, loss 0.3004, train acc 85.09%, f1 0.8508, precision 0.8479, recall 0.8538, auc 0.8509
epoch 1601, loss 0.3533, train acc 85.09%, f1 0.8508, precision 0.8480, recall 0.8535, auc 0.8509
epoch 1701, loss 0.2156, train acc 85.11%, f1 0.8508, precision 0.8491, recall 0.8526, auc 0.8511
epoch 1801, loss 0.2944, train acc 85.07%, f1 0.8504, precision 0.8493, recall 0.8514, auc 0.8507
epoch 1901, loss 0.3263, train acc 85.09%, f1 0.8505, precision 0.8495, recall 0.8516, auc 0.8509
epoch 2001, loss 0.4271, train acc 85.04%, f1 0.8499, precision 0.8491, recall 0.8507, auc 0.8504
epoch 2101, loss 0.2648, train acc 85.06%, f1 0.8500, precision 0.8502, recall 0.8497, auc 0.8506
epoch 2201, loss 0.2336, train acc 85.05%, f1 0.8501, precision 0.8487, recall 0.8515, auc 0.8505
epoch 2301, loss 0.3360, train acc 85.05%, f1 0.8494, precision 0.8523, recall 0.8464, auc 0.8504
epoch 2401, loss 0.2871, train acc 85.10%, f1 0.8505, precision 0.8499, recall 0.8512, auc 0.8510
epoch 2501, loss 0.3012, train acc 85.08%, f1 0.8505, precision 0.8487, recall 0.8524, auc 0.8508
epoch 2601, loss 0.2801, train acc 85.05%, f1 0.8504, precision 0.8474, recall 0.8534, auc 0.8505
epoch 2701, loss 0.2421, train acc 85.10%, f1 0.8509, precision 0.8484, recall 0.8534, auc 0.8510
epoch 2801, loss 0.3115, train acc 85.09%, f1 0.8505, precision 0.8493, recall 0.8517, auc 0.8509
epoch 2901, loss 0.4081, train acc 85.09%, f1 0.8503, precision 0.8505, recall 0.8500, auc 0.8509
epoch 3001, loss 0.2636, train acc 85.05%, f1 0.8500, precision 0.8497, recall 0.8503, auc 0.8505
epoch 3101, loss 0.3659, train acc 85.12%, f1 0.8509, precision 0.8492, recall 0.8526, auc 0.8512
epoch 3201, loss 0.3674, train acc 85.11%, f1 0.8509, precision 0.8486, recall 0.8532, auc 0.8511
epoch 3301, loss 0.2896, train acc 85.14%, f1 0.8510, precision 0.8503, recall 0.8517, auc 0.8514
epoch 3401, loss 0.3352, train acc 85.13%, f1 0.8515, precision 0.8474, recall 0.8556, auc 0.8514
epoch 3501, loss 0.4549, train acc 85.19%, f1 0.8511, precision 0.8522, recall 0.8501, auc 0.8519
epoch 3601, loss 0.2724, train acc 85.15%, f1 0.8505, precision 0.8533, recall 0.8477, auc 0.8515
epoch 3701, loss 0.3568, train acc 85.28%, f1 0.8523, precision 0.8518, recall 0.8528, auc 0.8528
epoch 3801, loss 0.2131, train acc 85.30%, f1 0.8523, precision 0.8529, recall 0.8516, auc 0.8530
epoch 3901, loss 0.4395, train acc 85.46%, f1 0.8549, precision 0.8499, recall 0.8600, auc 0.8546
epoch 4001, loss 0.2966, train acc 85.48%, f1 0.8543, precision 0.8538, recall 0.8549, auc 0.8548
epoch 4101, loss 0.2456, train acc 85.61%, f1 0.8557, precision 0.8549, recall 0.8565, auc 0.8561
epoch 4201, loss 0.3087, train acc 85.72%, f1 0.8568, precision 0.8559, recall 0.8577, auc 0.8572
epoch 4301, loss 0.2607, train acc 85.80%, f1 0.8577, precision 0.8560, recall 0.8594, auc 0.8580
epoch 4401, loss 0.4376, train acc 85.92%, f1 0.8588, precision 0.8576, recall 0.8600, auc 0.8592
epoch 4501, loss 0.2673, train acc 86.06%, f1 0.8606, precision 0.8568, recall 0.8645, auc 0.8606
epoch 4601, loss 0.3002, train acc 86.12%, f1 0.8613, precision 0.8576, recall 0.8651, auc 0.8613
epoch 4701, loss 0.3497, train acc 86.15%, f1 0.8611, precision 0.8604, recall 0.8618, auc 0.8615
epoch 4801, loss 0.2768, train acc 86.22%, f1 0.8619, precision 0.8604, recall 0.8634, auc 0.8622
epoch 4901, loss 0.4045, train acc 86.39%, f1 0.8637, precision 0.8613, recall 0.8662, auc 0.8639
epoch 5001, loss 0.2520, train acc 86.43%, f1 0.8639, precision 0.8630, recall 0.8648, auc 0.8643
epoch 5101, loss 0.2775, train acc 86.49%, f1 0.8646, precision 0.8632, recall 0.8660, auc 0.8649
epoch 5201, loss 0.3636, train acc 86.50%, f1 0.8645, precision 0.8644, recall 0.8645, auc 0.8650
epoch 5301, loss 0.2446, train acc 86.59%, f1 0.8655, precision 0.8647, recall 0.8663, auc 0.8659
epoch 5401, loss 0.2404, train acc 86.62%, f1 0.8654, precision 0.8674, recall 0.8634, auc 0.8662
epoch 5501, loss 0.3668, train acc 86.65%, f1 0.8661, precision 0.8654, recall 0.8669, auc 0.8665
epoch 5601, loss 0.4808, train acc 86.79%, f1 0.8680, precision 0.8640, recall 0.8720, auc 0.8679
epoch 5701, loss 0.3200, train acc 86.75%, f1 0.8672, precision 0.8660, recall 0.8683, auc 0.8675
epoch 5801, loss 0.4286, train acc 86.73%, f1 0.8670, precision 0.8659, recall 0.8680, auc 0.8673
epoch 5901, loss 0.3945, train acc 86.78%, f1 0.8672, precision 0.8681, recall 0.8663, auc 0.8678
epoch 6001, loss 0.3173, train acc 86.79%, f1 0.8678, precision 0.8647, recall 0.8711, auc 0.8679
epoch 6101, loss 0.3718, train acc 86.86%, f1 0.8685, precision 0.8662, recall 0.8708, auc 0.8686
epoch 6201, loss 0.2142, train acc 86.90%, f1 0.8685, precision 0.8683, recall 0.8687, auc 0.8690
epoch 6301, loss 0.2421, train acc 86.86%, f1 0.8687, precision 0.8648, recall 0.8726, auc 0.8686
epoch 6401, loss 0.2771, train acc 86.99%, f1 0.8693, precision 0.8695, recall 0.8691, auc 0.8699
epoch 6501, loss 0.2086, train acc 86.98%, f1 0.8696, precision 0.8674, recall 0.8718, auc 0.8698
epoch 6601, loss 0.2180, train acc 87.07%, f1 0.8699, precision 0.8719, recall 0.8679, auc 0.8707
epoch 6701, loss 0.3312, train acc 87.15%, f1 0.8718, precision 0.8662, recall 0.8775, auc 0.8715
epoch 6801, loss 0.2217, train acc 87.14%, f1 0.8708, precision 0.8713, recall 0.8704, auc 0.8714
epoch 6901, loss 0.3021, train acc 87.21%, f1 0.8715, precision 0.8722, recall 0.8708, auc 0.8721
epoch 7001, loss 0.3335, train acc 87.27%, f1 0.8726, precision 0.8701, recall 0.8751, auc 0.8727
epoch 7101, loss 0.2626, train acc 87.21%, f1 0.8717, precision 0.8706, recall 0.8729, auc 0.8721
epoch 7201, loss 0.3335, train acc 87.32%, f1 0.8726, precision 0.8731, recall 0.8721, auc 0.8732
epoch 7301, loss 0.2205, train acc 87.33%, f1 0.8732, precision 0.8706, recall 0.8759, auc 0.8734
epoch 7401, loss 0.2943, train acc 87.39%, f1 0.8734, precision 0.8736, recall 0.8732, auc 0.8739
epoch 7501, loss 0.1772, train acc 87.45%, f1 0.8740, precision 0.8740, recall 0.8740, auc 0.8745
epoch 7601, loss 0.2226, train acc 87.52%, f1 0.8751, precision 0.8722, recall 0.8780, auc 0.8752
epoch 7701, loss 0.3280, train acc 87.55%, f1 0.8755, precision 0.8719, recall 0.8790, auc 0.8755
epoch 7801, loss 0.2590, train acc 87.57%, f1 0.8756, precision 0.8727, recall 0.8786, auc 0.8757
epoch 7901, loss 0.2516, train acc 87.62%, f1 0.8758, precision 0.8751, recall 0.8765, auc 0.8762
epoch 8001, loss 0.3927, train acc 87.62%, f1 0.8761, precision 0.8733, recall 0.8790, auc 0.8762
epoch 8101, loss 0.2872, train acc 87.68%, f1 0.8764, precision 0.8759, recall 0.8769, auc 0.8768
epoch 8201, loss 0.3193, train acc 87.71%, f1 0.8767, precision 0.8759, recall 0.8775, auc 0.8771/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.3013, train acc 87.75%, f1 0.8773, precision 0.8757, recall 0.8788, auc 0.8775
epoch 8401, loss 0.2369, train acc 87.83%, f1 0.8782, precision 0.8757, recall 0.8807, auc 0.8783
epoch 8501, loss 0.1604, train acc 87.85%, f1 0.8782, precision 0.8769, recall 0.8795, auc 0.8785
epoch 8601, loss 0.3451, train acc 87.88%, f1 0.8785, precision 0.8774, recall 0.8796, auc 0.8788
epoch 8701, loss 0.3997, train acc 87.92%, f1 0.8789, precision 0.8778, recall 0.8799, auc 0.8792
epoch 8801, loss 0.2266, train acc 87.96%, f1 0.8792, precision 0.8791, recall 0.8793, auc 0.8796
epoch 8901, loss 0.2698, train acc 87.97%, f1 0.8796, precision 0.8775, recall 0.8816, auc 0.8798
epoch 9001, loss 0.3022, train acc 88.04%, f1 0.8801, precision 0.8787, recall 0.8815, auc 0.8804
epoch 9101, loss 0.2955, train acc 88.12%, f1 0.8806, precision 0.8818, recall 0.8793, auc 0.8812
epoch 9201, loss 0.2604, train acc 88.11%, f1 0.8811, precision 0.8782, recall 0.8839, auc 0.8811
epoch 9301, loss 0.2980, train acc 88.10%, f1 0.8808, precision 0.8784, recall 0.8833, auc 0.8810
epoch 9401, loss 0.3397, train acc 88.18%, f1 0.8814, precision 0.8813, recall 0.8816, auc 0.8818
epoch 9501, loss 0.3838, train acc 88.23%, f1 0.8820, precision 0.8805, recall 0.8836, auc 0.8823
epoch 9601, loss 0.3269, train acc 88.23%, f1 0.8816, precision 0.8829, recall 0.8804, auc 0.8822
epoch 9701, loss 0.2106, train acc 88.25%, f1 0.8825, precision 0.8790, recall 0.8861, auc 0.8825
epoch 9801, loss 0.3760, train acc 88.30%, f1 0.8825, precision 0.8827, recall 0.8822, auc 0.8830
epoch 9901, loss 0.3176, train acc 88.27%, f1 0.8827, precision 0.8792, recall 0.8862, auc 0.8827
epoch 10001, loss 0.2384, train acc 88.35%, f1 0.8833, precision 0.8811, recall 0.8856, auc 0.8835
epoch 10101, loss 0.2798, train acc 88.42%, f1 0.8840, precision 0.8819, recall 0.8860, auc 0.8842
epoch 10201, loss 0.3479, train acc 88.35%, f1 0.8832, precision 0.8817, recall 0.8848, auc 0.8835
epoch 10301, loss 0.3694, train acc 88.41%, f1 0.8837, precision 0.8829, recall 0.8845, auc 0.8841
epoch 10401, loss 0.2601, train acc 88.43%, f1 0.8839, precision 0.8830, recall 0.8849, auc 0.8843
epoch 10501, loss 0.3192, train acc 88.50%, f1 0.8843, precision 0.8868, recall 0.8818, auc 0.8850
epoch 10601, loss 0.2445, train acc 88.54%, f1 0.8853, precision 0.8824, recall 0.8883, auc 0.8854
epoch 10701, loss 0.3732, train acc 88.58%, f1 0.8853, precision 0.8855, recall 0.8851, auc 0.8858
epoch 10801, loss 0.3189, train acc 88.58%, f1 0.8854, precision 0.8850, recall 0.8859, auc 0.8858
epoch 10901, loss 0.2795, train acc 88.60%, f1 0.8857, precision 0.8849, recall 0.8864, auc 0.8860
epoch 11001, loss 0.2062, train acc 88.66%, f1 0.8861, precision 0.8865, recall 0.8856, auc 0.8866
epoch 11101, loss 0.2717, train acc 88.66%, f1 0.8859, precision 0.8880, recall 0.8839, auc 0.8866
epoch 11201, loss 0.3658, train acc 88.67%, f1 0.8866, precision 0.8840, recall 0.8892, auc 0.8867
epoch 11301, loss 0.3435, train acc 88.74%, f1 0.8870, precision 0.8864, recall 0.8876, auc 0.8874
epoch 11401, loss 0.2914, train acc 88.78%, f1 0.8875, precision 0.8861, recall 0.8889, auc 0.8878
epoch 11501, loss 0.2278, train acc 88.81%, f1 0.8880, precision 0.8854, recall 0.8905, auc 0.8881
epoch 11601, loss 0.3313, train acc 88.87%, f1 0.8883, precision 0.8878, recall 0.8889, auc 0.8887
epoch 11701, loss 0.3039, train acc 88.89%, f1 0.8890, precision 0.8852, recall 0.8928, auc 0.8889
epoch 11801, loss 0.3031, train acc 88.94%, f1 0.8889, precision 0.8896, recall 0.8883, auc 0.8894
epoch 11901, loss 0.4057, train acc 88.92%, f1 0.8888, precision 0.8884, recall 0.8892, auc 0.8892
epoch 12001, loss 0.2510, train acc 88.98%, f1 0.8893, precision 0.8905, recall 0.8881, auc 0.8898
epoch 12101, loss 0.2671, train acc 89.04%, f1 0.8899, precision 0.8901, recall 0.8898, auc 0.8904
epoch 12201, loss 0.2016, train acc 89.06%, f1 0.8901, precision 0.8904, recall 0.8898, auc 0.8906
epoch 12301, loss 0.1820, train acc 89.08%, f1 0.8903, precision 0.8904, recall 0.8902, auc 0.8908
epoch 12401, loss 0.2728, train acc 89.12%, f1 0.8905, precision 0.8928, recall 0.8883, auc 0.8912
epoch 12501, loss 0.2258, train acc 89.22%, f1 0.8920, precision 0.8901, recall 0.8939, auc 0.8922
epoch 12601, loss 0.2977, train acc 89.24%, f1 0.8919, precision 0.8924, recall 0.8914, auc 0.8924
epoch 12701, loss 0.3737, train acc 89.23%, f1 0.8918, precision 0.8922, recall 0.8915, auc 0.8923
epoch 12801, loss 0.2405, train acc 89.30%, f1 0.8930, precision 0.8893, recall 0.8968, auc 0.8930
epoch 12901, loss 0.2966, train acc 89.28%, f1 0.8925, precision 0.8917, recall 0.8934, auc 0.8928
epoch 13001, loss 0.2243, train acc 89.30%, f1 0.8931, precision 0.8889, recall 0.8974, auc 0.8931
epoch 13101, loss 0.2753, train acc 89.30%, f1 0.8931, precision 0.8886, recall 0.8977, auc 0.8930
epoch 13201, loss 0.2335, train acc 89.38%, f1 0.8935, precision 0.8926, recall 0.8945, auc 0.8938
epoch 13301, loss 0.1620, train acc 89.43%, f1 0.8938, precision 0.8947, recall 0.8929, auc 0.8943
epoch 13401, loss 0.2233, train acc 89.49%, f1 0.8943, precision 0.8958, recall 0.8929, auc 0.8949
epoch 13501, loss 0.2317, train acc 89.49%, f1 0.8945, precision 0.8939, recall 0.8952, auc 0.8949
epoch 13601, loss 0.2559, train acc 89.53%, f1 0.8948, precision 0.8962, recall 0.8933, auc 0.8953
epoch 13701, loss 0.3415, train acc 89.55%, f1 0.8953, precision 0.8940, recall 0.8966, auc 0.8956
epoch 13801, loss 0.2508, train acc 89.60%, f1 0.8957, precision 0.8949, recall 0.8965, auc 0.8960
epoch 13901, loss 0.2833, train acc 89.62%, f1 0.8958, precision 0.8955, recall 0.8962, auc 0.8962
epoch 14001, loss 0.1678, train acc 89.68%, f1 0.8967, precision 0.8942, recall 0.8992, auc 0.8968
epoch 14101, loss 0.2412, train acc 89.69%, f1 0.8966, precision 0.8960, recall 0.8971, auc 0.8969
epoch 14201, loss 0.2917, train acc 89.69%, f1 0.8967, precision 0.8947, recall 0.8987, auc 0.8969
epoch 14301, loss 0.1962, train acc 89.75%, f1 0.8972, precision 0.8958, recall 0.8986, auc 0.8975
epoch 14401, loss 0.2767, train acc 89.76%, f1 0.8976, precision 0.8939, recall 0.9014, auc 0.8976
epoch 14501, loss 0.3467, train acc 89.86%, f1 0.8982, precision 0.8983, recall 0.8981, auc 0.8986
epoch 14601, loss 0.2453, train acc 89.80%, f1 0.8977, precision 0.8973, recall 0.8981, auc 0.8980
epoch 14701, loss 0.2277, train acc 89.89%, f1 0.8985, precision 0.8986, recall 0.8984, auc 0.8989
epoch 14801, loss 0.2409, train acc 89.91%, f1 0.8989, precision 0.8977, recall 0.9001, auc 0.8991
epoch 14901, loss 0.2748, train acc 89.97%, f1 0.8993, precision 0.8992, recall 0.8994, auc 0.8997
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_4
./test_pima/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6445283018867924

the Fscore is 0.5844155844155844

the precision is 0.44554455445544555

the recall is 0.8490566037735849

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_4
----------------------



epoch 1, loss 0.6933, train acc 50.13%, f1 0.6678, precision 0.5013, recall 1.0000, auc 0.5000
epoch 101, loss 0.5536, train acc 78.99%, f1 0.7819, precision 0.8153, recall 0.7511, auc 0.7900
epoch 201, loss 0.5331, train acc 82.58%, f1 0.8255, precision 0.8291, recall 0.8219, auc 0.8258
epoch 301, loss 0.3801, train acc 84.18%, f1 0.8427, precision 0.8405, recall 0.8449, auc 0.8418
epoch 401, loss 0.3015, train acc 84.83%, f1 0.8497, precision 0.8443, recall 0.8551, auc 0.8483
epoch 501, loss 0.2996, train acc 84.94%, f1 0.8511, precision 0.8442, recall 0.8580, auc 0.8494
epoch 601, loss 0.2955, train acc 85.08%, f1 0.8522, precision 0.8467, recall 0.8577, auc 0.8508
epoch 701, loss 0.3013, train acc 85.08%, f1 0.8518, precision 0.8488, recall 0.8547, auc 0.8508
epoch 801, loss 0.3971, train acc 85.09%, f1 0.8519, precision 0.8482, recall 0.8557, auc 0.8509
epoch 901, loss 0.2797, train acc 85.09%, f1 0.8526, precision 0.8455, recall 0.8597, auc 0.8509
epoch 1001, loss 0.3131, train acc 85.05%, f1 0.8515, precision 0.8482, recall 0.8547, auc 0.8505
epoch 1101, loss 0.2520, train acc 84.99%, f1 0.8515, precision 0.8446, recall 0.8586, auc 0.8499
epoch 1201, loss 0.2412, train acc 85.02%, f1 0.8514, precision 0.8469, recall 0.8560, auc 0.8502
epoch 1301, loss 0.3485, train acc 85.03%, f1 0.8511, precision 0.8487, recall 0.8536, auc 0.8503
epoch 1401, loss 0.4049, train acc 85.07%, f1 0.8514, precision 0.8497, recall 0.8531, auc 0.8507
epoch 1501, loss 0.3381, train acc 85.04%, f1 0.8514, precision 0.8481, recall 0.8547, auc 0.8504
epoch 1601, loss 0.2921, train acc 85.09%, f1 0.8520, precision 0.8479, recall 0.8562, auc 0.8509
epoch 1701, loss 0.4276, train acc 85.08%, f1 0.8522, precision 0.8466, recall 0.8579, auc 0.8508
epoch 1801, loss 0.3794, train acc 85.07%, f1 0.8515, precision 0.8495, recall 0.8535, auc 0.8507
epoch 1901, loss 0.3708, train acc 85.07%, f1 0.8520, precision 0.8465, recall 0.8576, auc 0.8506
epoch 2001, loss 0.2883, train acc 85.06%, f1 0.8517, precision 0.8476, recall 0.8559, auc 0.8506
epoch 2101, loss 0.3922, train acc 85.07%, f1 0.8515, precision 0.8492, recall 0.8538, auc 0.8507
epoch 2201, loss 0.3443, train acc 85.06%, f1 0.8509, precision 0.8514, recall 0.8505, auc 0.8506
epoch 2301, loss 0.4093, train acc 85.07%, f1 0.8519, precision 0.8470, recall 0.8569, auc 0.8507
epoch 2401, loss 0.2482, train acc 85.05%, f1 0.8520, precision 0.8460, recall 0.8580, auc 0.8505
epoch 2501, loss 0.3462, train acc 85.02%, f1 0.8514, precision 0.8466, recall 0.8563, auc 0.8501
epoch 2601, loss 0.4128, train acc 85.07%, f1 0.8516, precision 0.8487, recall 0.8546, auc 0.8507
epoch 2701, loss 0.3918, train acc 85.06%, f1 0.8513, precision 0.8494, recall 0.8532, auc 0.8506
epoch 2801, loss 0.3563, train acc 85.09%, f1 0.8517, precision 0.8495, recall 0.8539, auc 0.8509
epoch 2901, loss 0.3723, train acc 85.17%, f1 0.8527, precision 0.8493, recall 0.8563, auc 0.8517
epoch 3001, loss 0.2864, train acc 85.12%, f1 0.8515, precision 0.8519, recall 0.8511, auc 0.8512
epoch 3101, loss 0.2906, train acc 85.20%, f1 0.8532, precision 0.8485, recall 0.8579, auc 0.8519
epoch 3201, loss 0.3542, train acc 85.25%, f1 0.8532, precision 0.8512, recall 0.8552, auc 0.8525
epoch 3301, loss 0.2326, train acc 85.24%, f1 0.8534, precision 0.8499, recall 0.8569, auc 0.8523
epoch 3401, loss 0.2323, train acc 85.34%, f1 0.8544, precision 0.8510, recall 0.8578, auc 0.8534
epoch 3501, loss 0.3017, train acc 85.35%, f1 0.8539, precision 0.8536, recall 0.8543, auc 0.8535
epoch 3601, loss 0.3044, train acc 85.38%, f1 0.8546, precision 0.8523, recall 0.8568, auc 0.8538
epoch 3701, loss 0.3385, train acc 85.47%, f1 0.8558, precision 0.8515, recall 0.8603, auc 0.8547
epoch 3801, loss 0.4881, train acc 85.56%, f1 0.8565, precision 0.8534, recall 0.8597, auc 0.8556
epoch 3901, loss 0.3122, train acc 85.60%, f1 0.8557, precision 0.8599, recall 0.8516, auc 0.8560
epoch 4001, loss 0.3014, train acc 85.68%, f1 0.8579, precision 0.8540, recall 0.8618, auc 0.8568
epoch 4101, loss 0.2945, train acc 85.75%, f1 0.8575, precision 0.8595, recall 0.8555, auc 0.8575
epoch 4201, loss 0.2922, train acc 85.91%, f1 0.8599, precision 0.8575, recall 0.8623, auc 0.8591
epoch 4301, loss 0.4738, train acc 85.99%, f1 0.8608, precision 0.8575, recall 0.8641, auc 0.8599
epoch 4401, loss 0.2133, train acc 86.04%, f1 0.8606, precision 0.8617, recall 0.8595, auc 0.8604
epoch 4501, loss 0.4729, train acc 86.13%, f1 0.8617, precision 0.8614, recall 0.8620, auc 0.8613
epoch 4601, loss 0.3417, train acc 86.17%, f1 0.8626, precision 0.8590, recall 0.8662, auc 0.8617
epoch 4701, loss 0.3655, train acc 86.27%, f1 0.8635, precision 0.8610, recall 0.8660, auc 0.8627
epoch 4801, loss 0.3023, train acc 86.41%, f1 0.8645, precision 0.8642, recall 0.8649, auc 0.8641
epoch 4901, loss 0.3685, train acc 86.44%, f1 0.8650, precision 0.8632, recall 0.8668, auc 0.8644
epoch 5001, loss 0.2771, train acc 86.51%, f1 0.8656, precision 0.8645, recall 0.8667, auc 0.8651
epoch 5101, loss 0.1927, train acc 86.55%, f1 0.8653, precision 0.8692, recall 0.8615, auc 0.8655
epoch 5201, loss 0.3629, train acc 86.59%, f1 0.8663, precision 0.8657, recall 0.8670, auc 0.8659
epoch 5301, loss 0.3050, train acc 86.61%, f1 0.8667, precision 0.8648, recall 0.8687, auc 0.8661
epoch 5401, loss 0.3065, train acc 86.70%, f1 0.8675, precision 0.8668, recall 0.8682, auc 0.8670
epoch 5501, loss 0.2841, train acc 86.73%, f1 0.8675, precision 0.8685, recall 0.8664, auc 0.8673
epoch 5601, loss 0.2961, train acc 86.77%, f1 0.8686, precision 0.8652, recall 0.8719, auc 0.8677
epoch 5701, loss 0.3504, train acc 86.76%, f1 0.8683, precision 0.8664, recall 0.8702, auc 0.8676
epoch 5801, loss 0.2717, train acc 86.78%, f1 0.8684, precision 0.8667, recall 0.8702, auc 0.8678
epoch 5901, loss 0.3227, train acc 86.85%, f1 0.8694, precision 0.8657, recall 0.8731, auc 0.8684
epoch 6001, loss 0.4144, train acc 86.91%, f1 0.8699, precision 0.8670, recall 0.8727, auc 0.8691
epoch 6101, loss 0.2681, train acc 86.90%, f1 0.8698, precision 0.8667, recall 0.8728, auc 0.8690
epoch 6201, loss 0.3942, train acc 86.96%, f1 0.8702, precision 0.8687, recall 0.8717, auc 0.8696
epoch 6301, loss 0.4812, train acc 86.99%, f1 0.8707, precision 0.8675, recall 0.8740, auc 0.8699
epoch 6401, loss 0.3191, train acc 87.02%, f1 0.8704, precision 0.8711, recall 0.8696, auc 0.8702
epoch 6501, loss 0.2562, train acc 87.07%, f1 0.8709, precision 0.8718, recall 0.8700, auc 0.8707
epoch 6601, loss 0.2887, train acc 87.10%, f1 0.8712, precision 0.8719, recall 0.8705, auc 0.8710
epoch 6701, loss 0.2392, train acc 87.18%, f1 0.8721, precision 0.8728, recall 0.8713, auc 0.8718
epoch 6801, loss 0.2702, train acc 87.16%, f1 0.8722, precision 0.8707, recall 0.8737, auc 0.8716
epoch 6901, loss 0.3233, train acc 87.17%, f1 0.8730, precision 0.8669, recall 0.8792, auc 0.8717
epoch 7001, loss 0.2080, train acc 87.23%, f1 0.8724, precision 0.8742, recall 0.8706, auc 0.8724
epoch 7101, loss 0.2767, train acc 87.29%, f1 0.8733, precision 0.8729, recall 0.8737, auc 0.8729
epoch 7201, loss 0.4122, train acc 87.29%, f1 0.8733, precision 0.8733, recall 0.8732, auc 0.8729
epoch 7301, loss 0.3067, train acc 87.35%, f1 0.8741, precision 0.8725, recall 0.8757, auc 0.8735
epoch 7401, loss 0.2807, train acc 87.39%, f1 0.8744, precision 0.8729, recall 0.8760, auc 0.8739
epoch 7501, loss 0.1739, train acc 87.43%, f1 0.8747, precision 0.8744, recall 0.8750, auc 0.8743
epoch 7601, loss 0.2565, train acc 87.43%, f1 0.8747, precision 0.8739, recall 0.8756, auc 0.8743
epoch 7701, loss 0.3975, train acc 87.46%, f1 0.8752, precision 0.8735, recall 0.8768, auc 0.8746
epoch 7801, loss 0.3102, train acc 87.55%, f1 0.8759, precision 0.8751, recall 0.8767, auc 0.8755
epoch 7901, loss 0.3280, train acc 87.56%, f1 0.8758, precision 0.8763, recall 0.8753, auc 0.8756
epoch 8001, loss 0.2373, train acc 87.60%, f1 0.8765, precision 0.8752, recall 0.8779, auc 0.8760
epoch 8101, loss 0.3214, train acc 87.61%, f1 0.8769, precision 0.8732, recall 0.8806, auc 0.8761
epoch 8201, loss 0.4379, train acc 87.65%, f1 0.8768, precision 0.8765, recall 0.8771, auc 0.8765/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2277, train acc 87.67%, f1 0.8770, precision 0.8774, recall 0.8765, auc 0.8767
epoch 8401, loss 0.2104, train acc 87.71%, f1 0.8774, precision 0.8774, recall 0.8774, auc 0.8771
epoch 8501, loss 0.1578, train acc 87.76%, f1 0.8779, precision 0.8778, recall 0.8780, auc 0.8776
epoch 8601, loss 0.2398, train acc 87.77%, f1 0.8779, precision 0.8787, recall 0.8770, auc 0.8777
epoch 8701, loss 0.3304, train acc 87.83%, f1 0.8788, precision 0.8772, recall 0.8804, auc 0.8783
epoch 8801, loss 0.3351, train acc 87.87%, f1 0.8792, precision 0.8780, recall 0.8803, auc 0.8787
epoch 8901, loss 0.2507, train acc 87.94%, f1 0.8801, precision 0.8776, recall 0.8825, auc 0.8794
epoch 9001, loss 0.2899, train acc 87.91%, f1 0.8792, precision 0.8809, recall 0.8775, auc 0.8791
epoch 9101, loss 0.2803, train acc 87.94%, f1 0.8798, precision 0.8792, recall 0.8804, auc 0.8794
epoch 9201, loss 0.2921, train acc 88.01%, f1 0.8802, precision 0.8817, recall 0.8787, auc 0.8801
epoch 9301, loss 0.2105, train acc 88.02%, f1 0.8804, precision 0.8808, recall 0.8800, auc 0.8802
epoch 9401, loss 0.3087, train acc 88.05%, f1 0.8814, precision 0.8775, recall 0.8853, auc 0.8805
epoch 9501, loss 0.2400, train acc 88.08%, f1 0.8811, precision 0.8813, recall 0.8809, auc 0.8808
epoch 9601, loss 0.2721, train acc 88.14%, f1 0.8822, precision 0.8786, recall 0.8859, auc 0.8814
epoch 9701, loss 0.2901, train acc 88.16%, f1 0.8815, precision 0.8848, recall 0.8782, auc 0.8816
epoch 9801, loss 0.2379, train acc 88.20%, f1 0.8822, precision 0.8836, recall 0.8807, auc 0.8821
epoch 9901, loss 0.2298, train acc 88.20%, f1 0.8826, precision 0.8804, recall 0.8848, auc 0.8820
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_4
./test_pima/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6145283018867924

the Fscore is 0.5625

the precision is 0.4205607476635514

the recall is 0.8490566037735849

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_4
----------------------



epoch 1, loss 0.6929, train acc 49.89%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6083, train acc 79.13%, f1 0.7881, precision 0.8023, recall 0.7744, auc 0.7914
epoch 201, loss 0.3997, train acc 82.63%, f1 0.8261, precision 0.8286, recall 0.8236, auc 0.8263
epoch 301, loss 0.3996, train acc 84.42%, f1 0.8451, precision 0.8422, recall 0.8480, auc 0.8442
epoch 401, loss 0.4134, train acc 84.88%, f1 0.8491, precision 0.8496, recall 0.8486, auc 0.8488
epoch 501, loss 0.3607, train acc 85.05%, f1 0.8514, precision 0.8480, recall 0.8548, auc 0.8505
epoch 601, loss 0.3681, train acc 85.08%, f1 0.8514, precision 0.8499, recall 0.8529, auc 0.8508
epoch 701, loss 0.3463, train acc 85.07%, f1 0.8519, precision 0.8467, recall 0.8572, auc 0.8506
epoch 801, loss 0.2381, train acc 85.06%, f1 0.8514, precision 0.8485, recall 0.8544, auc 0.8506
epoch 901, loss 0.2542, train acc 85.05%, f1 0.8510, precision 0.8502, recall 0.8517, auc 0.8505
epoch 1001, loss 0.3268, train acc 85.05%, f1 0.8511, precision 0.8492, recall 0.8531, auc 0.8505
epoch 1101, loss 0.4969, train acc 85.02%, f1 0.8510, precision 0.8484, recall 0.8536, auc 0.8502
epoch 1201, loss 0.2374, train acc 85.06%, f1 0.8510, precision 0.8502, recall 0.8518, auc 0.8506
epoch 1301, loss 0.2943, train acc 85.05%, f1 0.8504, precision 0.8529, recall 0.8480, auc 0.8506
epoch 1401, loss 0.3586, train acc 85.10%, f1 0.8509, precision 0.8534, recall 0.8483, auc 0.8510
epoch 1501, loss 0.2428, train acc 85.05%, f1 0.8508, precision 0.8507, recall 0.8508, auc 0.8505
epoch 1601, loss 0.3232, train acc 85.08%, f1 0.8517, precision 0.8483, recall 0.8552, auc 0.8508
epoch 1701, loss 0.4457, train acc 85.02%, f1 0.8507, precision 0.8494, recall 0.8521, auc 0.8502
epoch 1801, loss 0.2378, train acc 85.04%, f1 0.8508, precision 0.8502, recall 0.8514, auc 0.8504
epoch 1901, loss 0.3483, train acc 85.08%, f1 0.8514, precision 0.8500, recall 0.8529, auc 0.8508
epoch 2001, loss 0.3066, train acc 85.06%, f1 0.8514, precision 0.8485, recall 0.8543, auc 0.8506
epoch 2101, loss 0.3747, train acc 85.07%, f1 0.8512, precision 0.8502, recall 0.8522, auc 0.8507
epoch 2201, loss 0.3402, train acc 85.04%, f1 0.8503, precision 0.8526, recall 0.8480, auc 0.8504
epoch 2301, loss 0.3870, train acc 85.02%, f1 0.8507, precision 0.8499, recall 0.8515, auc 0.8502
epoch 2401, loss 0.3580, train acc 85.07%, f1 0.8515, precision 0.8491, recall 0.8538, auc 0.8507
epoch 2501, loss 0.3005, train acc 85.05%, f1 0.8504, precision 0.8528, recall 0.8480, auc 0.8505
epoch 2601, loss 0.2942, train acc 85.11%, f1 0.8509, precision 0.8538, recall 0.8481, auc 0.8511
epoch 2701, loss 0.2438, train acc 85.06%, f1 0.8512, precision 0.8498, recall 0.8525, auc 0.8506
epoch 2801, loss 0.3485, train acc 85.08%, f1 0.8515, precision 0.8494, recall 0.8535, auc 0.8508
epoch 2901, loss 0.4211, train acc 85.08%, f1 0.8520, precision 0.8471, recall 0.8569, auc 0.8508
epoch 3001, loss 0.3774, train acc 85.09%, f1 0.8512, precision 0.8514, recall 0.8510, auc 0.8509
epoch 3101, loss 0.3208, train acc 85.11%, f1 0.8515, precision 0.8509, recall 0.8522, auc 0.8511
epoch 3201, loss 0.3851, train acc 85.11%, f1 0.8517, precision 0.8502, recall 0.8533, auc 0.8511
epoch 3301, loss 0.2604, train acc 85.10%, f1 0.8512, precision 0.8518, recall 0.8507, auc 0.8510
epoch 3401, loss 0.3510, train acc 85.17%, f1 0.8517, precision 0.8532, recall 0.8502, auc 0.8517
epoch 3501, loss 0.4201, train acc 85.18%, f1 0.8521, precision 0.8519, recall 0.8522, auc 0.8518
epoch 3601, loss 0.2455, train acc 85.20%, f1 0.8522, precision 0.8528, recall 0.8517, auc 0.8520
epoch 3701, loss 0.2731, train acc 85.24%, f1 0.8526, precision 0.8530, recall 0.8522, auc 0.8524
epoch 3801, loss 0.2419, train acc 85.27%, f1 0.8530, precision 0.8532, recall 0.8528, auc 0.8527
epoch 3901, loss 0.3415, train acc 85.34%, f1 0.8539, precision 0.8527, recall 0.8552, auc 0.8534
epoch 4001, loss 0.3900, train acc 85.48%, f1 0.8553, precision 0.8541, recall 0.8564, auc 0.8548
epoch 4101, loss 0.2550, train acc 85.57%, f1 0.8565, precision 0.8534, recall 0.8596, auc 0.8557
epoch 4201, loss 0.4047, train acc 85.64%, f1 0.8565, precision 0.8579, recall 0.8551, auc 0.8564
epoch 4301, loss 0.2504, train acc 85.71%, f1 0.8579, precision 0.8551, recall 0.8606, auc 0.8571
epoch 4401, loss 0.3558, train acc 85.81%, f1 0.8586, precision 0.8575, recall 0.8596, auc 0.8581
epoch 4501, loss 0.2053, train acc 85.87%, f1 0.8590, precision 0.8591, recall 0.8589, auc 0.8587
epoch 4601, loss 0.3179, train acc 86.03%, f1 0.8610, precision 0.8587, recall 0.8634, auc 0.8603
epoch 4701, loss 0.2938, train acc 86.14%, f1 0.8615, precision 0.8625, recall 0.8606, auc 0.8614
epoch 4801, loss 0.3270, train acc 86.25%, f1 0.8629, precision 0.8622, recall 0.8637, auc 0.8625
epoch 4901, loss 0.2648, train acc 86.29%, f1 0.8635, precision 0.8617, recall 0.8652, auc 0.8629
epoch 5001, loss 0.3157, train acc 86.36%, f1 0.8639, precision 0.8639, recall 0.8639, auc 0.8636
epoch 5101, loss 0.3433, train acc 86.42%, f1 0.8647, precision 0.8632, recall 0.8662, auc 0.8642
epoch 5201, loss 0.2828, train acc 86.46%, f1 0.8648, precision 0.8654, recall 0.8643, auc 0.8646
epoch 5301, loss 0.2683, train acc 86.55%, f1 0.8659, precision 0.8648, recall 0.8670, auc 0.8655
epoch 5401, loss 0.3907, train acc 86.60%, f1 0.8665, precision 0.8655, recall 0.8675, auc 0.8660
epoch 5501, loss 0.2645, train acc 86.60%, f1 0.8657, precision 0.8691, recall 0.8625, auc 0.8660
epoch 5601, loss 0.2297, train acc 86.71%, f1 0.8677, precision 0.8651, recall 0.8704, auc 0.8671
epoch 5701, loss 0.1904, train acc 86.71%, f1 0.8669, precision 0.8696, recall 0.8642, auc 0.8671
epoch 5801, loss 0.3878, train acc 86.77%, f1 0.8684, precision 0.8659, recall 0.8710, auc 0.8677
epoch 5901, loss 0.3549, train acc 86.83%, f1 0.8687, precision 0.8681, recall 0.8692, auc 0.8683
epoch 6001, loss 0.3269, train acc 86.87%, f1 0.8693, precision 0.8671, recall 0.8716, auc 0.8687
epoch 6101, loss 0.3339, train acc 86.93%, f1 0.8699, precision 0.8678, recall 0.8721, auc 0.8693
epoch 6201, loss 0.3052, train acc 86.95%, f1 0.8696, precision 0.8713, recall 0.8678, auc 0.8696
epoch 6301, loss 0.3106, train acc 86.98%, f1 0.8701, precision 0.8701, recall 0.8700, auc 0.8698
epoch 6401, loss 0.3330, train acc 87.03%, f1 0.8708, precision 0.8696, recall 0.8720, auc 0.8703
epoch 6501, loss 0.3300, train acc 87.03%, f1 0.8703, precision 0.8723, recall 0.8683, auc 0.8703
epoch 6601, loss 0.2408, train acc 87.11%, f1 0.8712, precision 0.8723, recall 0.8701, auc 0.8711
epoch 6701, loss 0.3695, train acc 87.14%, f1 0.8719, precision 0.8704, recall 0.8734, auc 0.8714
epoch 6801, loss 0.2582, train acc 87.17%, f1 0.8723, precision 0.8700, recall 0.8745, auc 0.8717
epoch 6901, loss 0.2109, train acc 87.22%, f1 0.8725, precision 0.8727, recall 0.8722, auc 0.8722
epoch 7001, loss 0.2740, train acc 87.20%, f1 0.8718, precision 0.8748, recall 0.8689, auc 0.8720
epoch 7101, loss 0.3671, train acc 87.32%, f1 0.8732, precision 0.8745, recall 0.8720, auc 0.8732
epoch 7201, loss 0.2060, train acc 87.36%, f1 0.8740, precision 0.8732, recall 0.8748, auc 0.8736
epoch 7301, loss 0.2710, train acc 87.39%, f1 0.8746, precision 0.8715, recall 0.8777, auc 0.8739
epoch 7401, loss 0.2374, train acc 87.40%, f1 0.8745, precision 0.8728, recall 0.8761, auc 0.8739
epoch 7501, loss 0.2891, train acc 87.47%, f1 0.8753, precision 0.8729, recall 0.8778, auc 0.8747
epoch 7601, loss 0.3574, train acc 87.53%, f1 0.8754, precision 0.8769, recall 0.8738, auc 0.8753
epoch 7701, loss 0.2700, train acc 87.55%, f1 0.8759, precision 0.8753, recall 0.8765, auc 0.8755
epoch 7801, loss 0.1936, train acc 87.59%, f1 0.8766, precision 0.8738, recall 0.8793, auc 0.8759
epoch 7901, loss 0.3798, train acc 87.63%, f1 0.8767, precision 0.8758, recall 0.8777, auc 0.8763
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_4
./test_pima/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6183962264150944

the Fscore is 0.5696969696969697

the precision is 0.41964285714285715

the recall is 0.8867924528301887

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_4
----------------------



epoch 1, loss 0.6933, train acc 50.08%, f1 0.0046, precision 0.6250, recall 0.0023, auc 0.5005
epoch 101, loss 0.5634, train acc 78.97%, f1 0.7901, precision 0.7881, recall 0.7920, auc 0.7897
epoch 201, loss 0.3999, train acc 82.49%, f1 0.8252, precision 0.8234, recall 0.8270, auc 0.8249
epoch 301, loss 0.2645, train acc 84.39%, f1 0.8437, precision 0.8441, recall 0.8433, auc 0.8439
epoch 401, loss 0.3567, train acc 84.86%, f1 0.8480, precision 0.8507, recall 0.8454, auc 0.8486
epoch 501, loss 0.2952, train acc 85.02%, f1 0.8500, precision 0.8508, recall 0.8492, auc 0.8502
epoch 601, loss 0.3634, train acc 85.08%, f1 0.8504, precision 0.8521, recall 0.8488, auc 0.8508
epoch 701, loss 0.3946, train acc 85.10%, f1 0.8506, precision 0.8526, recall 0.8485, auc 0.8510
epoch 801, loss 0.3879, train acc 85.06%, f1 0.8502, precision 0.8520, recall 0.8483, auc 0.8506
epoch 901, loss 0.3377, train acc 85.10%, f1 0.8510, precision 0.8506, recall 0.8514, auc 0.8510
epoch 1001, loss 0.3826, train acc 85.06%, f1 0.8505, precision 0.8506, recall 0.8504, auc 0.8506
epoch 1101, loss 0.3549, train acc 85.05%, f1 0.8506, precision 0.8494, recall 0.8517, auc 0.8505
epoch 1201, loss 0.3352, train acc 85.08%, f1 0.8507, precision 0.8508, recall 0.8506, auc 0.8508
epoch 1301, loss 0.3395, train acc 85.10%, f1 0.8506, precision 0.8526, recall 0.8485, auc 0.8510
epoch 1401, loss 0.2788, train acc 85.09%, f1 0.8512, precision 0.8489, recall 0.8535, auc 0.8509
epoch 1501, loss 0.3367, train acc 85.07%, f1 0.8502, precision 0.8523, recall 0.8481, auc 0.8507
epoch 1601, loss 0.3120, train acc 85.04%, f1 0.8501, precision 0.8515, recall 0.8486, auc 0.8504
epoch 1701, loss 0.2819, train acc 85.08%, f1 0.8510, precision 0.8493, recall 0.8527, auc 0.8508
epoch 1801, loss 0.3238, train acc 85.08%, f1 0.8508, precision 0.8503, recall 0.8514, auc 0.8508
epoch 1901, loss 0.3037, train acc 85.02%, f1 0.8492, precision 0.8541, recall 0.8443, auc 0.8501
epoch 2001, loss 0.3737, train acc 85.11%, f1 0.8509, precision 0.8515, recall 0.8503, auc 0.8511
epoch 2101, loss 0.3774, train acc 85.07%, f1 0.8507, precision 0.8502, recall 0.8513, auc 0.8507
epoch 2201, loss 0.2766, train acc 85.05%, f1 0.8500, precision 0.8522, recall 0.8479, auc 0.8505
epoch 2301, loss 0.3349, train acc 85.12%, f1 0.8513, precision 0.8503, recall 0.8523, auc 0.8512
epoch 2401, loss 0.2336, train acc 85.11%, f1 0.8511, precision 0.8504, recall 0.8518, auc 0.8511
epoch 2501, loss 0.2634, train acc 85.08%, f1 0.8508, precision 0.8502, recall 0.8514, auc 0.8508
epoch 2601, loss 0.3294, train acc 85.12%, f1 0.8512, precision 0.8505, recall 0.8520, auc 0.8512
epoch 2701, loss 0.2339, train acc 85.06%, f1 0.8507, precision 0.8499, recall 0.8515, auc 0.8506
epoch 2801, loss 0.3722, train acc 85.08%, f1 0.8504, precision 0.8520, recall 0.8487, auc 0.8508
epoch 2901, loss 0.4173, train acc 85.05%, f1 0.8505, precision 0.8502, recall 0.8508, auc 0.8505
epoch 3001, loss 0.3573, train acc 85.09%, f1 0.8505, precision 0.8519, recall 0.8492, auc 0.8509
epoch 3101, loss 0.2657, train acc 85.08%, f1 0.8500, precision 0.8541, recall 0.8458, auc 0.8508
epoch 3201, loss 0.3110, train acc 85.08%, f1 0.8506, precision 0.8511, recall 0.8502, auc 0.8508
epoch 3301, loss 0.2406, train acc 85.11%, f1 0.8509, precision 0.8516, recall 0.8502, auc 0.8511
epoch 3401, loss 0.3940, train acc 85.21%, f1 0.8519, precision 0.8525, recall 0.8512, auc 0.8521
epoch 3501, loss 0.5583, train acc 85.17%, f1 0.8516, precision 0.8514, recall 0.8518, auc 0.8517
epoch 3601, loss 0.3681, train acc 85.22%, f1 0.8525, precision 0.8502, recall 0.8548, auc 0.8522
epoch 3701, loss 0.2943, train acc 85.27%, f1 0.8528, precision 0.8516, recall 0.8539, auc 0.8527
epoch 3801, loss 0.2182, train acc 85.31%, f1 0.8529, precision 0.8532, recall 0.8527, auc 0.8531
epoch 3901, loss 0.4066, train acc 85.35%, f1 0.8534, precision 0.8538, recall 0.8529, auc 0.8535
epoch 4001, loss 0.3950, train acc 85.48%, f1 0.8544, precision 0.8561, recall 0.8527, auc 0.8548
epoch 4101, loss 0.2408, train acc 85.54%, f1 0.8558, precision 0.8528, recall 0.8588, auc 0.8554
epoch 4201, loss 0.4307, train acc 85.61%, f1 0.8562, precision 0.8549, recall 0.8575, auc 0.8561
epoch 4301, loss 0.3125, train acc 85.73%, f1 0.8573, precision 0.8568, recall 0.8578, auc 0.8573
epoch 4401, loss 0.3007, train acc 85.83%, f1 0.8582, precision 0.8583, recall 0.8580, auc 0.8583
epoch 4501, loss 0.4034, train acc 85.91%, f1 0.8593, precision 0.8575, recall 0.8610, auc 0.8591
epoch 4601, loss 0.3635, train acc 86.00%, f1 0.8599, precision 0.8601, recall 0.8597, auc 0.8600
epoch 4701, loss 0.3494, train acc 86.11%, f1 0.8606, precision 0.8630, recall 0.8583, auc 0.8611
epoch 4801, loss 0.2947, train acc 86.20%, f1 0.8617, precision 0.8627, recall 0.8607, auc 0.8620
epoch 4901, loss 0.3291, train acc 86.33%, f1 0.8625, precision 0.8669, recall 0.8581, auc 0.8633
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_4
./test_pima/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6683962264150943

the Fscore is 0.6064516129032258

the precision is 0.46078431372549017

the recall is 0.8867924528301887

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_4
----------------------



epoch 1, loss 0.6931, train acc 49.80%, f1 0.6649, precision 0.4980, recall 1.0000, auc 0.5000
epoch 101, loss 0.5827, train acc 78.68%, f1 0.7830, precision 0.7939, recall 0.7725, auc 0.7867
epoch 201, loss 0.3597, train acc 82.83%, f1 0.8270, precision 0.8296, recall 0.8245, auc 0.8282
epoch 301, loss 0.3851, train acc 84.34%, f1 0.8424, precision 0.8441, recall 0.8407, auc 0.8433
epoch 401, loss 0.2916, train acc 84.86%, f1 0.8482, precision 0.8474, recall 0.8490, auc 0.8486
epoch 501, loss 0.3251, train acc 85.06%, f1 0.8500, precision 0.8500, recall 0.8501, auc 0.8506
epoch 601, loss 0.4220, train acc 85.04%, f1 0.8500, precision 0.8490, recall 0.8509, auc 0.8504
epoch 701, loss 0.2353, train acc 85.14%, f1 0.8509, precision 0.8504, recall 0.8515, auc 0.8514
epoch 801, loss 0.3532, train acc 85.05%, f1 0.8501, precision 0.8491, recall 0.8510, auc 0.8505
epoch 901, loss 0.2938, train acc 85.08%, f1 0.8506, precision 0.8484, recall 0.8527, auc 0.8508
epoch 1001, loss 0.1956, train acc 85.06%, f1 0.8502, precision 0.8492, recall 0.8512, auc 0.8506
epoch 1101, loss 0.4037, train acc 85.05%, f1 0.8502, precision 0.8487, recall 0.8516, auc 0.8505
epoch 1201, loss 0.2676, train acc 85.08%, f1 0.8503, precision 0.8499, recall 0.8507, auc 0.8508
epoch 1301, loss 0.3504, train acc 85.04%, f1 0.8497, precision 0.8503, recall 0.8491, auc 0.8504
epoch 1401, loss 0.2660, train acc 85.05%, f1 0.8497, precision 0.8510, recall 0.8483, auc 0.8505
epoch 1501, loss 0.2825, train acc 85.11%, f1 0.8498, precision 0.8540, recall 0.8457, auc 0.8511
epoch 1601, loss 0.2237, train acc 85.06%, f1 0.8501, precision 0.8495, recall 0.8506, auc 0.8506
epoch 1701, loss 0.2366, train acc 85.08%, f1 0.8506, precision 0.8485, recall 0.8526, auc 0.8508
epoch 1801, loss 0.2322, train acc 85.08%, f1 0.8504, precision 0.8492, recall 0.8516, auc 0.8508
epoch 1901, loss 0.3068, train acc 85.05%, f1 0.8498, precision 0.8508, recall 0.8487, auc 0.8505
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_4
./test_pima/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6478301886792452

the Fscore is 0.5925925925925927

the precision is 0.44036697247706424

the recall is 0.9056603773584906

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_4
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5944, train acc 78.97%, f1 0.7924, precision 0.7821, recall 0.8030, auc 0.7897
epoch 201, loss 0.4761, train acc 81.37%, f1 0.8140, precision 0.8126, recall 0.8153, auc 0.8137
epoch 301, loss 0.3728, train acc 83.43%, f1 0.8343, precision 0.8344, recall 0.8342, auc 0.8343
epoch 401, loss 0.3946, train acc 84.38%, f1 0.8436, precision 0.8443, recall 0.8430, auc 0.8438
epoch 501, loss 0.3469, train acc 84.82%, f1 0.8481, precision 0.8485, recall 0.8477, auc 0.8482
epoch 601, loss 0.3355, train acc 84.93%, f1 0.8492, precision 0.8498, recall 0.8486, auc 0.8493
epoch 701, loss 0.3060, train acc 85.06%, f1 0.8505, precision 0.8512, recall 0.8498, auc 0.8506
epoch 801, loss 0.2513, train acc 85.06%, f1 0.8505, precision 0.8511, recall 0.8499, auc 0.8506
epoch 901, loss 0.2658, train acc 85.09%, f1 0.8509, precision 0.8511, recall 0.8506, auc 0.8509
epoch 1001, loss 0.3748, train acc 85.10%, f1 0.8509, precision 0.8513, recall 0.8505, auc 0.8510
epoch 1101, loss 0.2057, train acc 85.11%, f1 0.8510, precision 0.8514, recall 0.8506, auc 0.8511
epoch 1201, loss 0.3328, train acc 85.08%, f1 0.8508, precision 0.8513, recall 0.8502, auc 0.8508
epoch 1301, loss 0.3875, train acc 85.08%, f1 0.8508, precision 0.8510, recall 0.8506, auc 0.8508
epoch 1401, loss 0.2930, train acc 85.08%, f1 0.8507, precision 0.8509, recall 0.8506, auc 0.8508
epoch 1501, loss 0.3502, train acc 85.02%, f1 0.8502, precision 0.8503, recall 0.8501, auc 0.8502
epoch 1601, loss 0.3081, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8508, auc 0.8509
epoch 1701, loss 0.3341, train acc 85.11%, f1 0.8511, precision 0.8512, recall 0.8511, auc 0.8511
epoch 1801, loss 0.3049, train acc 85.08%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8508
epoch 1901, loss 0.3913, train acc 85.06%, f1 0.8506, precision 0.8507, recall 0.8505, auc 0.8506
epoch 2001, loss 0.3175, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 2101, loss 0.2854, train acc 85.06%, f1 0.8506, precision 0.8507, recall 0.8505, auc 0.8506
epoch 2201, loss 0.3709, train acc 85.08%, f1 0.8508, precision 0.8509, recall 0.8508, auc 0.8508
epoch 2301, loss 0.3116, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 2401, loss 0.3121, train acc 85.11%, f1 0.8511, precision 0.8510, recall 0.8511, auc 0.8511
epoch 2501, loss 0.2580, train acc 85.07%, f1 0.8507, precision 0.8506, recall 0.8509, auc 0.8507
epoch 2601, loss 0.3669, train acc 85.04%, f1 0.8504, precision 0.8504, recall 0.8503, auc 0.8504
epoch 2701, loss 0.2522, train acc 85.08%, f1 0.8508, precision 0.8507, recall 0.8509, auc 0.8508
epoch 2801, loss 0.3843, train acc 85.06%, f1 0.8505, precision 0.8507, recall 0.8504, auc 0.8506
epoch 2901, loss 0.3038, train acc 85.12%, f1 0.8512, precision 0.8511, recall 0.8513, auc 0.8512
epoch 3001, loss 0.2940, train acc 85.04%, f1 0.8504, precision 0.8504, recall 0.8504, auc 0.8504
epoch 3101, loss 0.2653, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8505
epoch 3201, loss 0.3632, train acc 85.04%, f1 0.8504, precision 0.8504, recall 0.8504, auc 0.8504
epoch 3301, loss 0.2324, train acc 85.04%, f1 0.8505, precision 0.8504, recall 0.8505, auc 0.8504
epoch 3401, loss 0.3355, train acc 85.08%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8508
epoch 3501, loss 0.2694, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 3601, loss 0.3339, train acc 85.08%, f1 0.8509, precision 0.8508, recall 0.8509, auc 0.8508
epoch 3701, loss 0.3059, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8506, auc 0.8505
epoch 3801, loss 0.4056, train acc 85.08%, f1 0.8508, precision 0.8507, recall 0.8508, auc 0.8508
epoch 3901, loss 0.4251, train acc 85.09%, f1 0.8509, precision 0.8508, recall 0.8509, auc 0.8509
epoch 4001, loss 0.2707, train acc 85.07%, f1 0.8507, precision 0.8508, recall 0.8506, auc 0.8507
epoch 4101, loss 0.3772, train acc 85.06%, f1 0.8506, precision 0.8507, recall 0.8505, auc 0.8506
epoch 4201, loss 0.4354, train acc 85.08%, f1 0.8508, precision 0.8508, recall 0.8508, auc 0.8508
epoch 4301, loss 0.3196, train acc 85.03%, f1 0.8503, precision 0.8502, recall 0.8504, auc 0.8503
epoch 4401, loss 0.3066, train acc 85.03%, f1 0.8503, precision 0.8504, recall 0.8503, auc 0.8503
epoch 4501, loss 0.3258, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8506, auc 0.8506
epoch 4601, loss 0.4909, train acc 85.08%, f1 0.8508, precision 0.8510, recall 0.8505, auc 0.8508
epoch 4701, loss 0.2690, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8506, auc 0.8505
epoch 4801, loss 0.4808, train acc 85.11%, f1 0.8510, precision 0.8511, recall 0.8510, auc 0.8511
epoch 4901, loss 0.2527, train acc 85.12%, f1 0.8512, precision 0.8513, recall 0.8510, auc 0.8512
epoch 5001, loss 0.3300, train acc 85.03%, f1 0.8503, precision 0.8504, recall 0.8503, auc 0.8503
epoch 5101, loss 0.3716, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8505, auc 0.8505
epoch 5201, loss 0.3946, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8504, auc 0.8505
epoch 5301, loss 0.2472, train acc 85.04%, f1 0.8504, precision 0.8506, recall 0.8502, auc 0.8504
epoch 5401, loss 0.3408, train acc 85.06%, f1 0.8505, precision 0.8507, recall 0.8503, auc 0.8506
epoch 5501, loss 0.4391, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8504, auc 0.8505
epoch 5601, loss 0.3626, train acc 85.11%, f1 0.8511, precision 0.8513, recall 0.8509, auc 0.8511
epoch 5701, loss 0.3459, train acc 85.08%, f1 0.8508, precision 0.8510, recall 0.8506, auc 0.8508
epoch 5801, loss 0.3287, train acc 85.06%, f1 0.8505, precision 0.8508, recall 0.8502, auc 0.8506
epoch 5901, loss 0.2316, train acc 85.10%, f1 0.8510, precision 0.8511, recall 0.8509, auc 0.8510
epoch 6001, loss 0.3157, train acc 85.14%, f1 0.8514, precision 0.8515, recall 0.8512, auc 0.8514
epoch 6101, loss 0.4371, train acc 85.11%, f1 0.8511, precision 0.8513, recall 0.8509, auc 0.8511
epoch 6201, loss 0.2720, train acc 85.07%, f1 0.8507, precision 0.8509, recall 0.8506, auc 0.8507
epoch 6301, loss 0.3051, train acc 85.11%, f1 0.8510, precision 0.8514, recall 0.8507, auc 0.8511
epoch 6401, loss 0.2402, train acc 85.11%, f1 0.8510, precision 0.8513, recall 0.8508, auc 0.8511
epoch 6501, loss 0.3656, train acc 85.14%, f1 0.8513, precision 0.8515, recall 0.8512, auc 0.8514
epoch 6601, loss 0.3487, train acc 85.08%, f1 0.8507, precision 0.8509, recall 0.8505, auc 0.8508
epoch 6701, loss 0.4041, train acc 85.11%, f1 0.8510, precision 0.8513, recall 0.8507, auc 0.8511
epoch 6801, loss 0.3826, train acc 85.15%, f1 0.8514, precision 0.8519, recall 0.8510, auc 0.8515
epoch 6901, loss 0.3300, train acc 85.12%, f1 0.8512, precision 0.8513, recall 0.8511, auc 0.8512
epoch 7001, loss 0.4129, train acc 85.11%, f1 0.8510, precision 0.8512, recall 0.8508, auc 0.8511
epoch 7101, loss 0.3120, train acc 85.11%, f1 0.8511, precision 0.8513, recall 0.8509, auc 0.8511
epoch 7201, loss 0.2691, train acc 85.16%, f1 0.8515, precision 0.8519, recall 0.8511, auc 0.8516
epoch 7301, loss 0.2115, train acc 85.11%, f1 0.8510, precision 0.8511, recall 0.8510, auc 0.8511
epoch 7401, loss 0.3219, train acc 85.16%, f1 0.8515, precision 0.8520, recall 0.8511, auc 0.8516
epoch 7501, loss 0.3104, train acc 85.18%, f1 0.8518, precision 0.8519, recall 0.8517, auc 0.8518
epoch 7601, loss 0.3764, train acc 85.17%, f1 0.8516, precision 0.8520, recall 0.8512, auc 0.8517
epoch 7701, loss 0.2980, train acc 85.22%, f1 0.8521, precision 0.8524, recall 0.8519, auc 0.8522
epoch 7801, loss 0.2094, train acc 85.24%, f1 0.8523, precision 0.8526, recall 0.8520, auc 0.8524
epoch 7901, loss 0.4703, train acc 85.24%, f1 0.8524, precision 0.8524, recall 0.8524, auc 0.8524
epoch 8001, loss 0.3100, train acc 85.23%, f1 0.8523, precision 0.8524, recall 0.8522, auc 0.8523
epoch 8101, loss 0.4489, train acc 85.22%, f1 0.8522, precision 0.8524, recall 0.8519, auc 0.8522
epoch 8201, loss 0.2983, train acc 85.23%, f1 0.8522, precision 0.8525, recall 0.8520, auc 0.8523
epoch 8301, loss 0.3251, train acc 85.24%, f1 0.8523, precision 0.8527, recall 0.8520, auc 0.8524
epoch 8401, loss 0.2977, train acc 85.28%, f1 0.8528, precision 0.8528, recall 0.8528, auc 0.8528
epoch 8501, loss 0.2487, train acc 85.29%, f1 0.8529, precision 0.8530, recall 0.8528, auc 0.8529
epoch 8601, loss 0.2922, train acc 85.26%, f1 0.8526, precision 0.8527, recall 0.8524, auc 0.8526
epoch 8701, loss 0.3200, train acc 85.33%, f1 0.8532, precision 0.8538, recall 0.8526, auc 0.8533
epoch 8801, loss 0.2672, train acc 85.34%, f1 0.8534, precision 0.8536, recall 0.8531, auc 0.8534
epoch 8901, loss 0.4499, train acc 85.34%, f1 0.8534, precision 0.8536, recall 0.8532, auc 0.8534
epoch 9001, loss 0.3092, train acc 85.35%, f1 0.8535, precision 0.8536, recall 0.8534, auc 0.8535
epoch 9101, loss 0.2279, train acc 85.30%, f1 0.8529, precision 0.8531, recall 0.8528, auc 0.8530
epoch 9201, loss 0.2460, train acc 85.32%, f1 0.8532, precision 0.8534, recall 0.8529, auc 0.8532
epoch 9301, loss 0.2229, train acc 85.42%, f1 0.8541, precision 0.8544, recall 0.8538, auc 0.8542
epoch 9401, loss 0.3424, train acc 85.34%, f1 0.8534, precision 0.8534, recall 0.8534, auc 0.8534
epoch 9501, loss 0.3077, train acc 85.39%, f1 0.8539, precision 0.8542, recall 0.8535, auc 0.8539
epoch 9601, loss 0.2242, train acc 85.45%, f1 0.8546, precision 0.8544, recall 0.8547, auc 0.8545
epoch 9701, loss 0.3473, train acc 85.43%, f1 0.8543, precision 0.8542, recall 0.8544, auc 0.8543
epoch 9801, loss 0.3842, train acc 85.38%, f1 0.8537, precision 0.8541, recall 0.8534, auc 0.8538
epoch 9901, loss 0.3921, train acc 85.42%, f1 0.8542, precision 0.8542, recall 0.8542, auc 0.8542
epoch 10001, loss 0.4002, train acc 85.38%, f1 0.8538, precision 0.8542, recall 0.8534, auc 0.8538
epoch 10101, loss 0.3708, train acc 85.46%, f1 0.8546, precision 0.8548, recall 0.8544, auc 0.8546
epoch 10201, loss 0.2711, train acc 85.45%, f1 0.8546, precision 0.8544, recall 0.8547, auc 0.8545
epoch 10301, loss 0.3896, train acc 85.49%, f1 0.8549, precision 0.8549, recall 0.8550, auc 0.8549
epoch 10401, loss 0.3961, train acc 85.52%, f1 0.8552, precision 0.8552, recall 0.8552, auc 0.8552
epoch 10501, loss 0.3463, train acc 85.51%, f1 0.8550, precision 0.8553, recall 0.8547, auc 0.8551
epoch 10601, loss 0.2151, train acc 85.53%, f1 0.8553, precision 0.8555, recall 0.8550, auc 0.8553
epoch 10701, loss 0.2984, train acc 85.53%, f1 0.8553, precision 0.8553, recall 0.8552, auc 0.8553
epoch 10801, loss 0.2218, train acc 85.50%, f1 0.8551, precision 0.8548, recall 0.8554, auc 0.8550
epoch 10901, loss 0.3025, train acc 85.53%, f1 0.8553, precision 0.8553, recall 0.8553, auc 0.8553
epoch 11001, loss 0.2025, train acc 85.53%, f1 0.8552, precision 0.8555, recall 0.8549, auc 0.8553
epoch 11101, loss 0.3525, train acc 85.53%, f1 0.8553, precision 0.8554, recall 0.8552, auc 0.8553
epoch 11201, loss 0.4069, train acc 85.60%, f1 0.8560, precision 0.8561, recall 0.8560, auc 0.8560
epoch 11301, loss 0.2840, train acc 85.60%, f1 0.8560, precision 0.8560, recall 0.8560, auc 0.8560
epoch 11401, loss 0.3702, train acc 85.58%, f1 0.8558, precision 0.8556, recall 0.8560, auc 0.8558
epoch 11501, loss 0.2774, train acc 85.58%, f1 0.8557, precision 0.8559, recall 0.8555, auc 0.8558
epoch 11601, loss 0.3360, train acc 85.59%, f1 0.8559, precision 0.8561, recall 0.8557, auc 0.8559
epoch 11701, loss 0.3569, train acc 85.62%, f1 0.8561, precision 0.8563, recall 0.8559, auc 0.8562
epoch 11801, loss 0.2435, train acc 85.63%, f1 0.8562, precision 0.8565, recall 0.8560, auc 0.8563
epoch 11901, loss 0.3109, train acc 85.64%, f1 0.8563, precision 0.8564, recall 0.8563, auc 0.8564
epoch 12001, loss 0.3289, train acc 85.65%, f1 0.8565, precision 0.8564, recall 0.8566, auc 0.8565
epoch 12101, loss 0.2727, train acc 85.65%, f1 0.8564, precision 0.8566, recall 0.8563, auc 0.8565
epoch 12201, loss 0.4098, train acc 85.64%, f1 0.8564, precision 0.8564, recall 0.8563, auc 0.8564
epoch 12301, loss 0.2830, train acc 85.66%, f1 0.8566, precision 0.8567, recall 0.8565, auc 0.8566
epoch 12401, loss 0.3977, train acc 85.70%, f1 0.8570, precision 0.8570, recall 0.8570, auc 0.8570
epoch 12501, loss 0.4218, train acc 85.69%, f1 0.8569, precision 0.8569, recall 0.8568, auc 0.8569
epoch 12601, loss 0.2305, train acc 85.70%, f1 0.8570, precision 0.8569, recall 0.8572, auc 0.8570
epoch 12701, loss 0.2545, train acc 85.74%, f1 0.8574, precision 0.8575, recall 0.8574, auc 0.8574
epoch 12801, loss 0.3372, train acc 85.76%, f1 0.8576, precision 0.8576, recall 0.8577, auc 0.8576
epoch 12901, loss 0.2986, train acc 85.69%, f1 0.8569, precision 0.8571, recall 0.8567, auc 0.8569
epoch 13001, loss 0.4931, train acc 85.73%, f1 0.8572, precision 0.8574, recall 0.8571, auc 0.8573
epoch 13101, loss 0.3323, train acc 85.71%, f1 0.8571, precision 0.8572, recall 0.8570, auc 0.8571
epoch 13201, loss 0.3188, train acc 85.72%, f1 0.8573, precision 0.8572, recall 0.8573, auc 0.8572
epoch 13301, loss 0.3017, train acc 85.76%, f1 0.8576, precision 0.8577, recall 0.8575, auc 0.8576
epoch 13401, loss 0.3731, train acc 85.75%, f1 0.8575, precision 0.8577, recall 0.8572, auc 0.8575
epoch 13501, loss 0.3092, train acc 85.77%, f1 0.8578, precision 0.8577, recall 0.8578, auc 0.8577
epoch 13601, loss 0.2394, train acc 85.78%, f1 0.8578, precision 0.8579, recall 0.8577, auc 0.8578
epoch 13701, loss 0.3394, train acc 85.83%, f1 0.8584, precision 0.8582, recall 0.8585, auc 0.8583
epoch 13801, loss 0.2416, train acc 85.72%, f1 0.8572, precision 0.8572, recall 0.8572, auc 0.8572
epoch 13901, loss 0.2941, train acc 85.78%, f1 0.8578, precision 0.8577, recall 0.8579, auc 0.8578
epoch 14001, loss 0.4166, train acc 85.83%, f1 0.8583, precision 0.8584, recall 0.8583, auc 0.8583
epoch 14101, loss 0.2806, train acc 85.87%, f1 0.8587, precision 0.8589, recall 0.8585, auc 0.8587
epoch 14201, loss 0.2503, train acc 85.87%, f1 0.8587, precision 0.8588, recall 0.8586, auc 0.8587
epoch 14301, loss 0.3558, train acc 85.78%, f1 0.8579, precision 0.8577, recall 0.8580, auc 0.8578
epoch 14401, loss 0.2840, train acc 85.84%, f1 0.8584, precision 0.8582, recall 0.8585, auc 0.8584
epoch 14501, loss 0.2095, train acc 85.85%, f1 0.8585, precision 0.8585, recall 0.8585, auc 0.8585
epoch 14601, loss 0.2486, train acc 85.85%, f1 0.8586, precision 0.8584, recall 0.8587, auc 0.8585
epoch 14701, loss 0.3459, train acc 85.80%, f1 0.8579, precision 0.8580, recall 0.8579, auc 0.8580
epoch 14801, loss 0.2820, train acc 85.88%, f1 0.8588, precision 0.8589, recall 0.8587, auc 0.8588
epoch 14901, loss 0.4225, train acc 85.89%, f1 0.8589, precision 0.8590, recall 0.8588, auc 0.8589
epoch 15001, loss 0.2840, train acc 85.90%, f1 0.8590, precision 0.8590, recall 0.8590, auc 0.8590
epoch 15101, loss 0.2317, train acc 85.97%, f1 0.8597, precision 0.8597, recall 0.8597, auc 0.8597
epoch 15201, loss 0.4140, train acc 85.96%, f1 0.8596, precision 0.8595, recall 0.8597, auc 0.8596
epoch 15301, loss 0.3210, train acc 85.93%, f1 0.8593, precision 0.8594, recall 0.8591, auc 0.8593
epoch 15401, loss 0.3382, train acc 85.88%, f1 0.8588, precision 0.8589, recall 0.8588, auc 0.8588
epoch 15501, loss 0.2859, train acc 85.98%, f1 0.8598, precision 0.8598, recall 0.8598, auc 0.8598
epoch 15601, loss 0.3886, train acc 85.95%, f1 0.8595, precision 0.8594, recall 0.8596, auc 0.8595
epoch 15701, loss 0.3398, train acc 85.95%, f1 0.8596, precision 0.8594, recall 0.8597, auc 0.8595
epoch 15801, loss 0.2559, train acc 85.95%, f1 0.8595, precision 0.8593, recall 0.8597, auc 0.8595
epoch 15901, loss 0.3247, train acc 85.98%, f1 0.8598, precision 0.8597, recall 0.8598, auc 0.8598
epoch 16001, loss 0.2724, train acc 86.00%, f1 0.8600, precision 0.8599, recall 0.8601, auc 0.8600
epoch 16101, loss 0.4050, train acc 86.03%, f1 0.8603, precision 0.8604, recall 0.8602, auc 0.8603
epoch 16201, loss 0.2636, train acc 85.94%, f1 0.8595, precision 0.8593, recall 0.8596, auc 0.8594
epoch 16301, loss 0.2472, train acc 85.99%, f1 0.8600, precision 0.8597, recall 0.8602, auc 0.8599
epoch 16401, loss 0.2492, train acc 86.05%, f1 0.8605, precision 0.8606, recall 0.8605, auc 0.8605
epoch 16501, loss 0.2742, train acc 86.00%, f1 0.8600, precision 0.8600, recall 0.8600, auc 0.8600/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.2741, train acc 85.96%, f1 0.8596, precision 0.8596, recall 0.8597, auc 0.8596
epoch 16701, loss 0.2801, train acc 85.99%, f1 0.8599, precision 0.8601, recall 0.8597, auc 0.8599
epoch 16801, loss 0.2063, train acc 86.04%, f1 0.8605, precision 0.8604, recall 0.8605, auc 0.8604
epoch 16901, loss 0.3238, train acc 86.06%, f1 0.8606, precision 0.8605, recall 0.8606, auc 0.8606
epoch 17001, loss 0.3205, train acc 86.01%, f1 0.8601, precision 0.8600, recall 0.8602, auc 0.8601
epoch 17101, loss 0.2570, train acc 86.07%, f1 0.8607, precision 0.8607, recall 0.8607, auc 0.8607
epoch 17201, loss 0.2768, train acc 86.06%, f1 0.8606, precision 0.8605, recall 0.8607, auc 0.8606
epoch 17301, loss 0.4154, train acc 86.06%, f1 0.8605, precision 0.8607, recall 0.8604, auc 0.8606
epoch 17401, loss 0.3373, train acc 86.07%, f1 0.8607, precision 0.8607, recall 0.8607, auc 0.8607
epoch 17501, loss 0.2000, train acc 86.06%, f1 0.8606, precision 0.8604, recall 0.8608, auc 0.8606
epoch 17601, loss 0.2931, train acc 86.10%, f1 0.8610, precision 0.8611, recall 0.8609, auc 0.8610
epoch 17701, loss 0.3651, train acc 86.10%, f1 0.8611, precision 0.8608, recall 0.8613, auc 0.8610
epoch 17801, loss 0.2520, train acc 86.09%, f1 0.8609, precision 0.8609, recall 0.8610, auc 0.8609
epoch 17901, loss 0.3205, train acc 86.11%, f1 0.8611, precision 0.8609, recall 0.8613, auc 0.8611
epoch 18001, loss 0.2840, train acc 86.11%, f1 0.8611, precision 0.8611, recall 0.8611, auc 0.8611
epoch 18101, loss 0.4039, train acc 86.13%, f1 0.8613, precision 0.8613, recall 0.8614, auc 0.8613
epoch 18201, loss 0.4039, train acc 86.16%, f1 0.8617, precision 0.8615, recall 0.8618, auc 0.8616
epoch 18301, loss 0.3082, train acc 86.20%, f1 0.8621, precision 0.8618, recall 0.8623, auc 0.8620
epoch 18401, loss 0.3213, train acc 86.13%, f1 0.8613, precision 0.8613, recall 0.8613, auc 0.8613
epoch 18501, loss 0.3479, train acc 86.18%, f1 0.8618, precision 0.8617, recall 0.8618, auc 0.8618
epoch 18601, loss 0.2867, train acc 86.20%, f1 0.8621, precision 0.8617, recall 0.8625, auc 0.8620
epoch 18701, loss 0.2610, train acc 86.21%, f1 0.8621, precision 0.8620, recall 0.8623, auc 0.8621
epoch 18801, loss 0.2917, train acc 86.23%, f1 0.8624, precision 0.8620, recall 0.8627, auc 0.8623
epoch 18901, loss 0.2248, train acc 86.23%, f1 0.8623, precision 0.8621, recall 0.8625, auc 0.8623
epoch 19001, loss 0.2356, train acc 86.22%, f1 0.8622, precision 0.8621, recall 0.8622, auc 0.8622
epoch 19101, loss 0.3460, train acc 86.25%, f1 0.8625, precision 0.8625, recall 0.8625, auc 0.8625
epoch 19201, loss 0.3910, train acc 86.22%, f1 0.8623, precision 0.8620, recall 0.8625, auc 0.8622
epoch 19301, loss 0.3244, train acc 86.24%, f1 0.8624, precision 0.8623, recall 0.8624, auc 0.8624
epoch 19401, loss 0.3433, train acc 86.23%, f1 0.8624, precision 0.8621, recall 0.8626, auc 0.8623
epoch 19501, loss 0.3457, train acc 86.25%, f1 0.8625, precision 0.8623, recall 0.8627, auc 0.8625
epoch 19601, loss 0.2449, train acc 86.23%, f1 0.8624, precision 0.8620, recall 0.8628, auc 0.8623
epoch 19701, loss 0.3399, train acc 86.29%, f1 0.8630, precision 0.8627, recall 0.8632, auc 0.8629
epoch 19801, loss 0.3398, train acc 86.23%, f1 0.8623, precision 0.8621, recall 0.8626, auc 0.8623
epoch 19901, loss 0.3111, train acc 86.30%, f1 0.8630, precision 0.8627, recall 0.8633, auc 0.8630
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_4
./test_pima/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6566981132075471

the Fscore is 0.6024096385542169

the precision is 0.4424778761061947

the recall is 0.9433962264150944

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_4
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5858, train acc 78.30%, f1 0.7854, precision 0.7768, recall 0.7941, auc 0.7830
epoch 201, loss 0.4425, train acc 81.00%, f1 0.8103, precision 0.8091, recall 0.8116, auc 0.8100
epoch 301, loss 0.4040, train acc 83.35%, f1 0.8336, precision 0.8334, recall 0.8337, auc 0.8335
epoch 401, loss 0.4113, train acc 84.45%, f1 0.8445, precision 0.8447, recall 0.8443, auc 0.8445
epoch 501, loss 0.3672, train acc 84.81%, f1 0.8480, precision 0.8488, recall 0.8471, auc 0.8481
epoch 601, loss 0.3716, train acc 84.91%, f1 0.8490, precision 0.8495, recall 0.8486, auc 0.8491
epoch 701, loss 0.3225, train acc 85.01%, f1 0.8499, precision 0.8506, recall 0.8492, auc 0.8501
epoch 801, loss 0.3331, train acc 85.06%, f1 0.8506, precision 0.8510, recall 0.8502, auc 0.8506
epoch 901, loss 0.2680, train acc 85.10%, f1 0.8510, precision 0.8514, recall 0.8505, auc 0.8510
epoch 1001, loss 0.3420, train acc 85.04%, f1 0.8504, precision 0.8507, recall 0.8500, auc 0.8504
epoch 1101, loss 0.5317, train acc 85.09%, f1 0.8509, precision 0.8512, recall 0.8506, auc 0.8509
epoch 1201, loss 0.3731, train acc 85.06%, f1 0.8506, precision 0.8510, recall 0.8502, auc 0.8506
epoch 1301, loss 0.2677, train acc 85.09%, f1 0.8509, precision 0.8511, recall 0.8508, auc 0.8509
epoch 1401, loss 0.2938, train acc 85.09%, f1 0.8509, precision 0.8511, recall 0.8507, auc 0.8509
epoch 1501, loss 0.4054, train acc 85.08%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8508
epoch 1601, loss 0.4981, train acc 85.11%, f1 0.8511, precision 0.8513, recall 0.8508, auc 0.8511
epoch 1701, loss 0.2958, train acc 85.08%, f1 0.8508, precision 0.8508, recall 0.8508, auc 0.8508
epoch 1801, loss 0.2289, train acc 85.11%, f1 0.8511, precision 0.8512, recall 0.8510, auc 0.8511
epoch 1901, loss 0.3804, train acc 85.09%, f1 0.8509, precision 0.8510, recall 0.8508, auc 0.8509
epoch 2001, loss 0.3226, train acc 85.10%, f1 0.8510, precision 0.8510, recall 0.8510, auc 0.8510
epoch 2101, loss 0.2822, train acc 85.08%, f1 0.8507, precision 0.8508, recall 0.8507, auc 0.8507
epoch 2201, loss 0.2744, train acc 85.10%, f1 0.8510, precision 0.8511, recall 0.8508, auc 0.8510
epoch 2301, loss 0.2636, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 2401, loss 0.5005, train acc 85.07%, f1 0.8508, precision 0.8507, recall 0.8508, auc 0.8507
epoch 2501, loss 0.3370, train acc 85.11%, f1 0.8511, precision 0.8511, recall 0.8510, auc 0.8511
epoch 2601, loss 0.4296, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8506, auc 0.8506
epoch 2701, loss 0.3073, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8507, auc 0.8506
epoch 2801, loss 0.4131, train acc 85.08%, f1 0.8508, precision 0.8507, recall 0.8509, auc 0.8508
epoch 2901, loss 0.2480, train acc 85.12%, f1 0.8512, precision 0.8512, recall 0.8513, auc 0.8512
epoch 3001, loss 0.4177, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8505
epoch 3101, loss 0.2714, train acc 85.04%, f1 0.8504, precision 0.8501, recall 0.8507, auc 0.8504
epoch 3201, loss 0.3401, train acc 85.05%, f1 0.8505, precision 0.8504, recall 0.8506, auc 0.8505
epoch 3301, loss 0.3485, train acc 85.09%, f1 0.8509, precision 0.8507, recall 0.8511, auc 0.8509
epoch 3401, loss 0.4137, train acc 85.08%, f1 0.8508, precision 0.8506, recall 0.8510, auc 0.8508
epoch 3501, loss 0.3820, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8507, auc 0.8506
epoch 3601, loss 0.4923, train acc 85.07%, f1 0.8507, precision 0.8506, recall 0.8508, auc 0.8507
epoch 3701, loss 0.4109, train acc 85.04%, f1 0.8505, precision 0.8503, recall 0.8507, auc 0.8504
epoch 3801, loss 0.3196, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8508, auc 0.8506
epoch 3901, loss 0.3328, train acc 85.05%, f1 0.8506, precision 0.8504, recall 0.8507, auc 0.8505
epoch 4001, loss 0.3539, train acc 85.08%, f1 0.8508, precision 0.8507, recall 0.8510, auc 0.8508
epoch 4101, loss 0.4127, train acc 85.05%, f1 0.8506, precision 0.8504, recall 0.8507, auc 0.8505
epoch 4201, loss 0.2460, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8507, auc 0.8506
epoch 4301, loss 0.4196, train acc 85.04%, f1 0.8504, precision 0.8503, recall 0.8506, auc 0.8504
epoch 4401, loss 0.2550, train acc 85.08%, f1 0.8509, precision 0.8508, recall 0.8509, auc 0.8508
epoch 4501, loss 0.3967, train acc 85.06%, f1 0.8507, precision 0.8505, recall 0.8508, auc 0.8506
epoch 4601, loss 0.4001, train acc 85.07%, f1 0.8508, precision 0.8504, recall 0.8512, auc 0.8507
epoch 4701, loss 0.3429, train acc 85.05%, f1 0.8506, precision 0.8501, recall 0.8510, auc 0.8505
epoch 4801, loss 0.3434, train acc 85.08%, f1 0.8508, precision 0.8506, recall 0.8511, auc 0.8508
epoch 4901, loss 0.2922, train acc 85.05%, f1 0.8506, precision 0.8502, recall 0.8510, auc 0.8505
epoch 5001, loss 0.5134, train acc 85.08%, f1 0.8509, precision 0.8507, recall 0.8510, auc 0.8508
epoch 5101, loss 0.2177, train acc 85.08%, f1 0.8509, precision 0.8502, recall 0.8516, auc 0.8508
epoch 5201, loss 0.3423, train acc 85.07%, f1 0.8507, precision 0.8504, recall 0.8511, auc 0.8507
epoch 5301, loss 0.2327, train acc 85.12%, f1 0.8512, precision 0.8507, recall 0.8517, auc 0.8512
epoch 5401, loss 0.2850, train acc 85.11%, f1 0.8512, precision 0.8508, recall 0.8515, auc 0.8511
epoch 5501, loss 0.2839, train acc 85.08%, f1 0.8508, precision 0.8505, recall 0.8512, auc 0.8508
epoch 5601, loss 0.3871, train acc 85.08%, f1 0.8509, precision 0.8505, recall 0.8512, auc 0.8508
epoch 5701, loss 0.3833, train acc 85.12%, f1 0.8513, precision 0.8507, recall 0.8518, auc 0.8512
epoch 5801, loss 0.2195, train acc 85.11%, f1 0.8511, precision 0.8507, recall 0.8515, auc 0.8511
epoch 5901, loss 0.2604, train acc 85.09%, f1 0.8510, precision 0.8507, recall 0.8513, auc 0.8509
epoch 6001, loss 0.3026, train acc 85.06%, f1 0.8506, precision 0.8503, recall 0.8509, auc 0.8506
epoch 6101, loss 0.3513, train acc 85.11%, f1 0.8512, precision 0.8507, recall 0.8517, auc 0.8511
epoch 6201, loss 0.3388, train acc 85.11%, f1 0.8512, precision 0.8508, recall 0.8516, auc 0.8511
epoch 6301, loss 0.4208, train acc 85.16%, f1 0.8517, precision 0.8510, recall 0.8525, auc 0.8516
epoch 6401, loss 0.2837, train acc 85.12%, f1 0.8513, precision 0.8509, recall 0.8518, auc 0.8512
epoch 6501, loss 0.4673, train acc 85.10%, f1 0.8510, precision 0.8506, recall 0.8515, auc 0.8510
epoch 6601, loss 0.2755, train acc 85.11%, f1 0.8512, precision 0.8508, recall 0.8515, auc 0.8511
epoch 6701, loss 0.2558, train acc 85.13%, f1 0.8514, precision 0.8511, recall 0.8517, auc 0.8513
epoch 6801, loss 0.4000, train acc 85.14%, f1 0.8515, precision 0.8511, recall 0.8518, auc 0.8514
epoch 6901, loss 0.3246, train acc 85.15%, f1 0.8516, precision 0.8511, recall 0.8520, auc 0.8515
epoch 7001, loss 0.3068, train acc 85.15%, f1 0.8516, precision 0.8510, recall 0.8522, auc 0.8515
epoch 7101, loss 0.3054, train acc 85.16%, f1 0.8517, precision 0.8511, recall 0.8523, auc 0.8516
epoch 7201, loss 0.4729, train acc 85.18%, f1 0.8519, precision 0.8517, recall 0.8520, auc 0.8518
epoch 7301, loss 0.2843, train acc 85.18%, f1 0.8519, precision 0.8515, recall 0.8524, auc 0.8518
epoch 7401, loss 0.2436, train acc 85.17%, f1 0.8518, precision 0.8515, recall 0.8520, auc 0.8517
epoch 7501, loss 0.1930, train acc 85.18%, f1 0.8519, precision 0.8514, recall 0.8524, auc 0.8518
epoch 7601, loss 0.3709, train acc 85.20%, f1 0.8521, precision 0.8516, recall 0.8525, auc 0.8520
epoch 7701, loss 0.3281, train acc 85.28%, f1 0.8529, precision 0.8526, recall 0.8531, auc 0.8528
epoch 7801, loss 0.2933, train acc 85.26%, f1 0.8526, precision 0.8522, recall 0.8530, auc 0.8526
epoch 7901, loss 0.3550, train acc 85.25%, f1 0.8526, precision 0.8524, recall 0.8527, auc 0.8525
epoch 8001, loss 0.3389, train acc 85.23%, f1 0.8523, precision 0.8522, recall 0.8524, auc 0.8523
epoch 8101, loss 0.2770, train acc 85.26%, f1 0.8527, precision 0.8524, recall 0.8529, auc 0.8526
epoch 8201, loss 0.2716, train acc 85.25%, f1 0.8526, precision 0.8524, recall 0.8527, auc 0.8525/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2533, train acc 85.28%, f1 0.8528, precision 0.8527, recall 0.8530, auc 0.8528
epoch 8401, loss 0.4018, train acc 85.29%, f1 0.8529, precision 0.8528, recall 0.8531, auc 0.8529
epoch 8501, loss 0.3865, train acc 85.33%, f1 0.8533, precision 0.8532, recall 0.8534, auc 0.8533
epoch 8601, loss 0.2974, train acc 85.34%, f1 0.8534, precision 0.8533, recall 0.8534, auc 0.8534
epoch 8701, loss 0.3159, train acc 85.29%, f1 0.8529, precision 0.8527, recall 0.8531, auc 0.8529
epoch 8801, loss 0.2161, train acc 85.34%, f1 0.8535, precision 0.8534, recall 0.8535, auc 0.8534
epoch 8901, loss 0.3241, train acc 85.39%, f1 0.8540, precision 0.8539, recall 0.8541, auc 0.8539
epoch 9001, loss 0.3284, train acc 85.41%, f1 0.8542, precision 0.8541, recall 0.8542, auc 0.8541
epoch 9101, loss 0.3724, train acc 85.41%, f1 0.8542, precision 0.8540, recall 0.8543, auc 0.8541
epoch 9201, loss 0.3900, train acc 85.43%, f1 0.8543, precision 0.8542, recall 0.8543, auc 0.8543
epoch 9301, loss 0.3347, train acc 85.43%, f1 0.8543, precision 0.8544, recall 0.8543, auc 0.8543
epoch 9401, loss 0.2794, train acc 85.45%, f1 0.8545, precision 0.8544, recall 0.8546, auc 0.8545
epoch 9501, loss 0.2484, train acc 85.45%, f1 0.8546, precision 0.8545, recall 0.8546, auc 0.8545
epoch 9601, loss 0.3233, train acc 85.46%, f1 0.8546, precision 0.8546, recall 0.8547, auc 0.8546
epoch 9701, loss 0.3508, train acc 85.44%, f1 0.8543, precision 0.8545, recall 0.8541, auc 0.8544
epoch 9801, loss 0.3653, train acc 85.52%, f1 0.8552, precision 0.8551, recall 0.8554, auc 0.8552
epoch 9901, loss 0.2332, train acc 85.48%, f1 0.8548, precision 0.8549, recall 0.8547, auc 0.8548
epoch 10001, loss 0.2392, train acc 85.50%, f1 0.8549, precision 0.8551, recall 0.8548, auc 0.8550
epoch 10101, loss 0.3423, train acc 85.50%, f1 0.8550, precision 0.8550, recall 0.8550, auc 0.8550
epoch 10201, loss 0.4039, train acc 85.54%, f1 0.8554, precision 0.8554, recall 0.8554, auc 0.8554
epoch 10301, loss 0.3705, train acc 85.57%, f1 0.8557, precision 0.8558, recall 0.8556, auc 0.8557
epoch 10401, loss 0.4267, train acc 85.55%, f1 0.8555, precision 0.8555, recall 0.8555, auc 0.8555
epoch 10501, loss 0.3354, train acc 85.55%, f1 0.8555, precision 0.8555, recall 0.8555, auc 0.8555
epoch 10601, loss 0.3301, train acc 85.59%, f1 0.8558, precision 0.8559, recall 0.8558, auc 0.8559
epoch 10701, loss 0.3751, train acc 85.54%, f1 0.8554, precision 0.8553, recall 0.8556, auc 0.8554
epoch 10801, loss 0.2773, train acc 85.55%, f1 0.8555, precision 0.8556, recall 0.8555, auc 0.8555
epoch 10901, loss 0.3604, train acc 85.62%, f1 0.8562, precision 0.8561, recall 0.8562, auc 0.8562
epoch 11001, loss 0.3737, train acc 85.56%, f1 0.8556, precision 0.8558, recall 0.8554, auc 0.8556
epoch 11101, loss 0.2141, train acc 85.60%, f1 0.8560, precision 0.8559, recall 0.8560, auc 0.8560
epoch 11201, loss 0.4023, train acc 85.65%, f1 0.8565, precision 0.8566, recall 0.8564, auc 0.8565
epoch 11301, loss 0.2880, train acc 85.62%, f1 0.8562, precision 0.8563, recall 0.8561, auc 0.8562
epoch 11401, loss 0.3108, train acc 85.62%, f1 0.8562, precision 0.8564, recall 0.8559, auc 0.8562
epoch 11501, loss 0.3921, train acc 85.66%, f1 0.8566, precision 0.8565, recall 0.8567, auc 0.8566
epoch 11601, loss 0.3417, train acc 85.69%, f1 0.8569, precision 0.8570, recall 0.8568, auc 0.8569
epoch 11701, loss 0.3647, train acc 85.69%, f1 0.8569, precision 0.8570, recall 0.8569, auc 0.8569
epoch 11801, loss 0.2801, train acc 85.66%, f1 0.8566, precision 0.8566, recall 0.8566, auc 0.8566
epoch 11901, loss 0.4271, train acc 85.72%, f1 0.8572, precision 0.8573, recall 0.8572, auc 0.8572
epoch 12001, loss 0.2981, train acc 85.63%, f1 0.8563, precision 0.8563, recall 0.8564, auc 0.8563
epoch 12101, loss 0.2849, train acc 85.64%, f1 0.8564, precision 0.8564, recall 0.8563, auc 0.8564
epoch 12201, loss 0.2912, train acc 85.73%, f1 0.8573, precision 0.8572, recall 0.8573, auc 0.8573
epoch 12301, loss 0.3099, train acc 85.73%, f1 0.8573, precision 0.8573, recall 0.8573, auc 0.8573
epoch 12401, loss 0.3436, train acc 85.73%, f1 0.8573, precision 0.8576, recall 0.8570, auc 0.8573
epoch 12501, loss 0.2640, train acc 85.75%, f1 0.8575, precision 0.8575, recall 0.8575, auc 0.8575
epoch 12601, loss 0.3562, train acc 85.74%, f1 0.8574, precision 0.8574, recall 0.8574, auc 0.8574
epoch 12701, loss 0.2914, train acc 85.81%, f1 0.8581, precision 0.8581, recall 0.8581, auc 0.8581
epoch 12801, loss 0.2743, train acc 85.79%, f1 0.8579, precision 0.8580, recall 0.8579, auc 0.8579
epoch 12901, loss 0.3045, train acc 85.82%, f1 0.8582, precision 0.8581, recall 0.8582, auc 0.8582
epoch 13001, loss 0.3340, train acc 85.75%, f1 0.8574, precision 0.8576, recall 0.8573, auc 0.8575
epoch 13101, loss 0.4161, train acc 85.73%, f1 0.8573, precision 0.8574, recall 0.8572, auc 0.8573
epoch 13201, loss 0.3742, train acc 85.80%, f1 0.8580, precision 0.8580, recall 0.8579, auc 0.8580
epoch 13301, loss 0.3610, train acc 85.80%, f1 0.8580, precision 0.8581, recall 0.8579, auc 0.8580
epoch 13401, loss 0.3697, train acc 85.83%, f1 0.8582, precision 0.8584, recall 0.8580, auc 0.8583
epoch 13501, loss 0.1754, train acc 85.86%, f1 0.8586, precision 0.8589, recall 0.8583, auc 0.8586
epoch 13601, loss 0.2603, train acc 85.90%, f1 0.8589, precision 0.8593, recall 0.8586, auc 0.8590
epoch 13701, loss 0.3068, train acc 85.82%, f1 0.8582, precision 0.8583, recall 0.8581, auc 0.8582
epoch 13801, loss 0.3291, train acc 85.89%, f1 0.8589, precision 0.8591, recall 0.8587, auc 0.8589
epoch 13901, loss 0.3232, train acc 85.90%, f1 0.8590, precision 0.8592, recall 0.8588, auc 0.8590
epoch 14001, loss 0.2695, train acc 85.90%, f1 0.8590, precision 0.8591, recall 0.8589, auc 0.8590
epoch 14101, loss 0.2711, train acc 85.92%, f1 0.8592, precision 0.8595, recall 0.8589, auc 0.8592
epoch 14201, loss 0.3545, train acc 85.85%, f1 0.8584, precision 0.8586, recall 0.8583, auc 0.8585
epoch 14301, loss 0.3049, train acc 85.92%, f1 0.8592, precision 0.8595, recall 0.8588, auc 0.8592
epoch 14401, loss 0.3379, train acc 85.84%, f1 0.8584, precision 0.8585, recall 0.8584, auc 0.8584
epoch 14501, loss 0.4110, train acc 85.89%, f1 0.8588, precision 0.8593, recall 0.8583, auc 0.8589
epoch 14601, loss 0.2447, train acc 85.93%, f1 0.8593, precision 0.8594, recall 0.8592, auc 0.8593
epoch 14701, loss 0.2665, train acc 85.97%, f1 0.8597, precision 0.8597, recall 0.8597, auc 0.8597
epoch 14801, loss 0.2505, train acc 86.00%, f1 0.8599, precision 0.8602, recall 0.8597, auc 0.8600
epoch 14901, loss 0.3112, train acc 86.00%, f1 0.8600, precision 0.8601, recall 0.8598, auc 0.8600
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_4
./test_pima/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6383962264150944

the Fscore is 0.5838509316770186

the precision is 0.4351851851851852

the recall is 0.8867924528301887

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_4
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6133, train acc 78.59%, f1 0.7859, precision 0.7859, recall 0.7859, auc 0.7859
epoch 201, loss 0.5230, train acc 81.33%, f1 0.8133, precision 0.8133, recall 0.8133, auc 0.8133
epoch 301, loss 0.3492, train acc 83.33%, f1 0.8333, precision 0.8333, recall 0.8333, auc 0.8333
epoch 401, loss 0.3779, train acc 84.52%, f1 0.8452, precision 0.8452, recall 0.8452, auc 0.8452
epoch 501, loss 0.3540, train acc 84.82%, f1 0.8482, precision 0.8482, recall 0.8482, auc 0.8482
epoch 601, loss 0.3254, train acc 84.94%, f1 0.8494, precision 0.8494, recall 0.8494, auc 0.8494
epoch 701, loss 0.3083, train acc 85.02%, f1 0.8502, precision 0.8502, recall 0.8502, auc 0.8502
epoch 801, loss 0.4507, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 901, loss 0.3844, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 1001, loss 0.3681, train acc 85.08%, f1 0.8508, precision 0.8507, recall 0.8508, auc 0.8508
epoch 1101, loss 0.2947, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 1201, loss 0.3161, train acc 85.08%, f1 0.8508, precision 0.8508, recall 0.8508, auc 0.8508
epoch 1301, loss 0.4176, train acc 85.09%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8509
epoch 1401, loss 0.2967, train acc 85.07%, f1 0.8507, precision 0.8506, recall 0.8508, auc 0.8507
epoch 1501, loss 0.3667, train acc 85.10%, f1 0.8510, precision 0.8510, recall 0.8510, auc 0.8510
epoch 1601, loss 0.3887, train acc 85.10%, f1 0.8509, precision 0.8511, recall 0.8508, auc 0.8510
epoch 1701, loss 0.3381, train acc 85.09%, f1 0.8509, precision 0.8511, recall 0.8507, auc 0.8509
epoch 1801, loss 0.2817, train acc 85.10%, f1 0.8510, precision 0.8510, recall 0.8509, auc 0.8510
epoch 1901, loss 0.2728, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 2001, loss 0.2273, train acc 85.08%, f1 0.8507, precision 0.8508, recall 0.8506, auc 0.8508
epoch 2101, loss 0.3409, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 2201, loss 0.3694, train acc 85.08%, f1 0.8508, precision 0.8510, recall 0.8506, auc 0.8508
epoch 2301, loss 0.3335, train acc 85.09%, f1 0.8508, precision 0.8509, recall 0.8508, auc 0.8509
epoch 2401, loss 0.2136, train acc 85.10%, f1 0.8510, precision 0.8511, recall 0.8508, auc 0.8510
epoch 2501, loss 0.4303, train acc 85.13%, f1 0.8513, precision 0.8515, recall 0.8511, auc 0.8513
epoch 2601, loss 0.4069, train acc 85.12%, f1 0.8512, precision 0.8514, recall 0.8510, auc 0.8512
epoch 2701, loss 0.2484, train acc 85.08%, f1 0.8507, precision 0.8510, recall 0.8505, auc 0.8508
epoch 2801, loss 0.3526, train acc 85.09%, f1 0.8509, precision 0.8511, recall 0.8507, auc 0.8509
epoch 2901, loss 0.3133, train acc 85.08%, f1 0.8508, precision 0.8510, recall 0.8505, auc 0.8508
epoch 3001, loss 0.2815, train acc 85.07%, f1 0.8506, precision 0.8509, recall 0.8504, auc 0.8507
epoch 3101, loss 0.2622, train acc 85.04%, f1 0.8504, precision 0.8506, recall 0.8502, auc 0.8504
epoch 3201, loss 0.3848, train acc 85.06%, f1 0.8506, precision 0.8508, recall 0.8504, auc 0.8506
epoch 3301, loss 0.2509, train acc 85.06%, f1 0.8506, precision 0.8508, recall 0.8505, auc 0.8506
epoch 3401, loss 0.3002, train acc 85.07%, f1 0.8507, precision 0.8508, recall 0.8506, auc 0.8507
epoch 3501, loss 0.2240, train acc 85.09%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8509
epoch 3601, loss 0.3477, train acc 85.05%, f1 0.8505, precision 0.8507, recall 0.8502, auc 0.8505
epoch 3701, loss 0.3300, train acc 85.02%, f1 0.8502, precision 0.8503, recall 0.8501, auc 0.8502
epoch 3801, loss 0.5042, train acc 85.06%, f1 0.8505, precision 0.8508, recall 0.8503, auc 0.8506
epoch 3901, loss 0.2860, train acc 85.10%, f1 0.8510, precision 0.8511, recall 0.8508, auc 0.8510
epoch 4001, loss 0.3445, train acc 85.09%, f1 0.8509, precision 0.8510, recall 0.8508, auc 0.8509
epoch 4101, loss 0.3735, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8504, auc 0.8505
epoch 4201, loss 0.3783, train acc 85.04%, f1 0.8504, precision 0.8505, recall 0.8502, auc 0.8504
epoch 4301, loss 0.3182, train acc 85.03%, f1 0.8502, precision 0.8504, recall 0.8501, auc 0.8503
epoch 4401, loss 0.3827, train acc 85.06%, f1 0.8506, precision 0.8507, recall 0.8506, auc 0.8506
epoch 4501, loss 0.3271, train acc 85.06%, f1 0.8505, precision 0.8506, recall 0.8505, auc 0.8506
epoch 4601, loss 0.3047, train acc 85.10%, f1 0.8510, precision 0.8511, recall 0.8509, auc 0.8510
epoch 4701, loss 0.3815, train acc 85.10%, f1 0.8510, precision 0.8511, recall 0.8508, auc 0.8510
epoch 4801, loss 0.4380, train acc 85.07%, f1 0.8506, precision 0.8508, recall 0.8505, auc 0.8507
epoch 4901, loss 0.3734, train acc 85.09%, f1 0.8509, precision 0.8510, recall 0.8508, auc 0.8509
epoch 5001, loss 0.2849, train acc 85.10%, f1 0.8510, precision 0.8512, recall 0.8507, auc 0.8510
epoch 5101, loss 0.2751, train acc 85.06%, f1 0.8505, precision 0.8508, recall 0.8503, auc 0.8506
epoch 5201, loss 0.3553, train acc 85.12%, f1 0.8511, precision 0.8514, recall 0.8508, auc 0.8512
epoch 5301, loss 0.2867, train acc 85.09%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8509
epoch 5401, loss 0.3425, train acc 85.08%, f1 0.8508, precision 0.8509, recall 0.8507, auc 0.8508
epoch 5501, loss 0.3348, train acc 85.07%, f1 0.8508, precision 0.8507, recall 0.8508, auc 0.8507
epoch 5601, loss 0.2764, train acc 85.07%, f1 0.8506, precision 0.8509, recall 0.8504, auc 0.8507
epoch 5701, loss 0.4489, train acc 85.11%, f1 0.8511, precision 0.8512, recall 0.8509, auc 0.8511
epoch 5801, loss 0.3096, train acc 85.11%, f1 0.8511, precision 0.8512, recall 0.8509, auc 0.8511
epoch 5901, loss 0.2968, train acc 85.08%, f1 0.8508, precision 0.8509, recall 0.8506, auc 0.8508
epoch 6001, loss 0.3214, train acc 85.10%, f1 0.8510, precision 0.8512, recall 0.8507, auc 0.8510
epoch 6101, loss 0.4617, train acc 85.07%, f1 0.8506, precision 0.8508, recall 0.8504, auc 0.8507
epoch 6201, loss 0.4293, train acc 85.07%, f1 0.8507, precision 0.8509, recall 0.8505, auc 0.8507
epoch 6301, loss 0.3823, train acc 85.07%, f1 0.8507, precision 0.8510, recall 0.8503, auc 0.8507
epoch 6401, loss 0.3652, train acc 85.10%, f1 0.8510, precision 0.8514, recall 0.8505, auc 0.8510
epoch 6501, loss 0.3249, train acc 85.10%, f1 0.8510, precision 0.8511, recall 0.8509, auc 0.8510
epoch 6601, loss 0.4417, train acc 85.11%, f1 0.8510, precision 0.8514, recall 0.8506, auc 0.8511
epoch 6701, loss 0.3643, train acc 85.09%, f1 0.8508, precision 0.8514, recall 0.8503, auc 0.8509
epoch 6801, loss 0.3420, train acc 85.12%, f1 0.8512, precision 0.8513, recall 0.8511, auc 0.8512
epoch 6901, loss 0.3618, train acc 85.07%, f1 0.8506, precision 0.8511, recall 0.8501, auc 0.8507
epoch 7001, loss 0.4938, train acc 85.09%, f1 0.8509, precision 0.8514, recall 0.8503, auc 0.8509
epoch 7101, loss 0.2349, train acc 85.10%, f1 0.8509, precision 0.8512, recall 0.8507, auc 0.8510
epoch 7201, loss 0.3895, train acc 85.12%, f1 0.8512, precision 0.8516, recall 0.8507, auc 0.8512
epoch 7301, loss 0.2727, train acc 85.13%, f1 0.8512, precision 0.8515, recall 0.8510, auc 0.8513
epoch 7401, loss 0.3869, train acc 85.09%, f1 0.8508, precision 0.8513, recall 0.8502, auc 0.8509
epoch 7501, loss 0.3377, train acc 85.16%, f1 0.8515, precision 0.8520, recall 0.8510, auc 0.8516
epoch 7601, loss 0.3112, train acc 85.13%, f1 0.8513, precision 0.8517, recall 0.8508, auc 0.8513
epoch 7701, loss 0.2756, train acc 85.15%, f1 0.8514, precision 0.8519, recall 0.8510, auc 0.8515
epoch 7801, loss 0.3416, train acc 85.13%, f1 0.8512, precision 0.8518, recall 0.8507, auc 0.8513
epoch 7901, loss 0.3192, train acc 85.18%, f1 0.8518, precision 0.8520, recall 0.8515, auc 0.8518
epoch 8001, loss 0.3344, train acc 85.15%, f1 0.8515, precision 0.8516, recall 0.8515, auc 0.8515
epoch 8101, loss 0.3422, train acc 85.18%, f1 0.8518, precision 0.8521, recall 0.8514, auc 0.8518
epoch 8201, loss 0.4184, train acc 85.16%, f1 0.8515, precision 0.8520, recall 0.8511, auc 0.8516/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.4898, train acc 85.17%, f1 0.8517, precision 0.8521, recall 0.8513, auc 0.8517
epoch 8401, loss 0.3081, train acc 85.21%, f1 0.8521, precision 0.8521, recall 0.8520, auc 0.8521
epoch 8501, loss 0.4385, train acc 85.17%, f1 0.8516, precision 0.8521, recall 0.8512, auc 0.8517
epoch 8601, loss 0.2869, train acc 85.17%, f1 0.8517, precision 0.8518, recall 0.8515, auc 0.8517
epoch 8701, loss 0.2713, train acc 85.20%, f1 0.8520, precision 0.8520, recall 0.8519, auc 0.8520
epoch 8801, loss 0.2468, train acc 85.24%, f1 0.8523, precision 0.8525, recall 0.8521, auc 0.8524
epoch 8901, loss 0.2679, train acc 85.22%, f1 0.8521, precision 0.8526, recall 0.8516, auc 0.8522
epoch 9001, loss 0.3389, train acc 85.24%, f1 0.8524, precision 0.8527, recall 0.8521, auc 0.8524
epoch 9101, loss 0.3765, train acc 85.24%, f1 0.8524, precision 0.8525, recall 0.8522, auc 0.8524
epoch 9201, loss 0.3171, train acc 85.28%, f1 0.8527, precision 0.8532, recall 0.8522, auc 0.8528
epoch 9301, loss 0.4132, train acc 85.27%, f1 0.8527, precision 0.8529, recall 0.8524, auc 0.8527
epoch 9401, loss 0.2886, train acc 85.32%, f1 0.8531, precision 0.8534, recall 0.8529, auc 0.8532
epoch 9501, loss 0.3375, train acc 85.34%, f1 0.8533, precision 0.8537, recall 0.8529, auc 0.8534
epoch 9601, loss 0.4442, train acc 85.30%, f1 0.8529, precision 0.8532, recall 0.8526, auc 0.8530
epoch 9701, loss 0.3555, train acc 85.40%, f1 0.8540, precision 0.8539, recall 0.8541, auc 0.8540
epoch 9801, loss 0.2737, train acc 85.37%, f1 0.8537, precision 0.8537, recall 0.8537, auc 0.8537
epoch 9901, loss 0.2867, train acc 85.40%, f1 0.8540, precision 0.8537, recall 0.8543, auc 0.8540
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_4
./test_pima/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6572641509433963

the Fscore is 0.6012269938650308

the precision is 0.44545454545454544

the recall is 0.9245283018867925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_4
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6003, train acc 75.91%, f1 0.7882, precision 0.7033, recall 0.8962, auc 0.7591
epoch 201, loss 0.4918, train acc 81.19%, f1 0.8175, precision 0.7939, recall 0.8425, auc 0.8119
epoch 301, loss 0.4803, train acc 83.46%, f1 0.8354, precision 0.8315, recall 0.8393, auc 0.8346
epoch 401, loss 0.3787, train acc 84.44%, f1 0.8439, precision 0.8468, recall 0.8411, auc 0.8444
epoch 501, loss 0.4151, train acc 84.84%, f1 0.8475, precision 0.8527, recall 0.8423, auc 0.8484
epoch 601, loss 0.3769, train acc 84.95%, f1 0.8482, precision 0.8556, recall 0.8409, auc 0.8495
epoch 701, loss 0.2968, train acc 85.01%, f1 0.8485, precision 0.8574, recall 0.8399, auc 0.8501
epoch 801, loss 0.3760, train acc 85.00%, f1 0.8485, precision 0.8573, recall 0.8399, auc 0.8500
epoch 901, loss 0.3013, train acc 85.04%, f1 0.8488, precision 0.8577, recall 0.8401, auc 0.8504
epoch 1001, loss 0.2914, train acc 85.05%, f1 0.8492, precision 0.8569, recall 0.8417, auc 0.8505
epoch 1101, loss 0.2641, train acc 85.09%, f1 0.8494, precision 0.8584, recall 0.8406, auc 0.8509
epoch 1201, loss 0.4364, train acc 85.09%, f1 0.8496, precision 0.8570, recall 0.8423, auc 0.8509
epoch 1301, loss 0.3597, train acc 85.09%, f1 0.8499, precision 0.8556, recall 0.8442, auc 0.8509
epoch 1401, loss 0.3044, train acc 85.08%, f1 0.8496, precision 0.8561, recall 0.8432, auc 0.8508
epoch 1501, loss 0.3966, train acc 85.07%, f1 0.8496, precision 0.8559, recall 0.8435, auc 0.8507
epoch 1601, loss 0.3734, train acc 85.06%, f1 0.8501, precision 0.8531, recall 0.8470, auc 0.8506
epoch 1701, loss 0.3172, train acc 85.10%, f1 0.8502, precision 0.8545, recall 0.8460, auc 0.8510
epoch 1801, loss 0.2820, train acc 85.08%, f1 0.8503, precision 0.8532, recall 0.8473, auc 0.8508
epoch 1901, loss 0.3039, train acc 85.07%, f1 0.8500, precision 0.8538, recall 0.8463, auc 0.8507
epoch 2001, loss 0.3454, train acc 85.10%, f1 0.8506, precision 0.8529, recall 0.8483, auc 0.8510
epoch 2101, loss 0.3238, train acc 85.05%, f1 0.8503, precision 0.8517, recall 0.8489, auc 0.8505
epoch 2201, loss 0.3054, train acc 85.09%, f1 0.8504, precision 0.8529, recall 0.8480, auc 0.8509
epoch 2301, loss 0.2801, train acc 85.06%, f1 0.8503, precision 0.8522, recall 0.8483, auc 0.8506
epoch 2401, loss 0.2959, train acc 85.06%, f1 0.8502, precision 0.8521, recall 0.8483, auc 0.8506
epoch 2501, loss 0.3747, train acc 85.04%, f1 0.8500, precision 0.8527, recall 0.8473, auc 0.8504
epoch 2601, loss 0.3805, train acc 85.07%, f1 0.8505, precision 0.8520, recall 0.8489, auc 0.8507
epoch 2701, loss 0.4189, train acc 85.07%, f1 0.8505, precision 0.8517, recall 0.8493, auc 0.8507
epoch 2801, loss 0.3454, train acc 85.07%, f1 0.8506, precision 0.8512, recall 0.8500, auc 0.8507
epoch 2901, loss 0.4035, train acc 85.06%, f1 0.8504, precision 0.8513, recall 0.8496, auc 0.8506
epoch 3001, loss 0.3157, train acc 85.08%, f1 0.8505, precision 0.8523, recall 0.8487, auc 0.8508
epoch 3101, loss 0.3639, train acc 85.10%, f1 0.8509, precision 0.8516, recall 0.8501, auc 0.8510
epoch 3201, loss 0.3908, train acc 85.07%, f1 0.8505, precision 0.8515, recall 0.8495, auc 0.8507
epoch 3301, loss 0.2750, train acc 85.06%, f1 0.8505, precision 0.8513, recall 0.8497, auc 0.8506
epoch 3401, loss 0.3143, train acc 85.07%, f1 0.8507, precision 0.8509, recall 0.8504, auc 0.8507
epoch 3501, loss 0.2627, train acc 85.09%, f1 0.8507, precision 0.8519, recall 0.8495, auc 0.8509
epoch 3601, loss 0.3281, train acc 85.04%, f1 0.8502, precision 0.8511, recall 0.8492, auc 0.8504
epoch 3701, loss 0.2958, train acc 85.02%, f1 0.8502, precision 0.8506, recall 0.8497, auc 0.8502
epoch 3801, loss 0.3757, train acc 85.07%, f1 0.8506, precision 0.8510, recall 0.8502, auc 0.8507
epoch 3901, loss 0.3630, train acc 85.11%, f1 0.8511, precision 0.8513, recall 0.8508, auc 0.8511
epoch 4001, loss 0.2767, train acc 85.09%, f1 0.8508, precision 0.8514, recall 0.8502, auc 0.8509
epoch 4101, loss 0.4368, train acc 85.03%, f1 0.8502, precision 0.8508, recall 0.8495, auc 0.8503
epoch 4201, loss 0.3465, train acc 85.09%, f1 0.8508, precision 0.8513, recall 0.8503, auc 0.8509
epoch 4301, loss 0.2807, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8508, auc 0.8509
epoch 4401, loss 0.2884, train acc 85.05%, f1 0.8505, precision 0.8503, recall 0.8507, auc 0.8505
epoch 4501, loss 0.2977, train acc 85.07%, f1 0.8506, precision 0.8508, recall 0.8504, auc 0.8507
epoch 4601, loss 0.3654, train acc 85.08%, f1 0.8508, precision 0.8510, recall 0.8506, auc 0.8508
epoch 4701, loss 0.4468, train acc 85.06%, f1 0.8505, precision 0.8513, recall 0.8497, auc 0.8506
epoch 4801, loss 0.2186, train acc 85.08%, f1 0.8507, precision 0.8509, recall 0.8505, auc 0.8508
epoch 4901, loss 0.3124, train acc 85.07%, f1 0.8506, precision 0.8509, recall 0.8503, auc 0.8507
epoch 5001, loss 0.3784, train acc 85.10%, f1 0.8509, precision 0.8512, recall 0.8506, auc 0.8510
epoch 5101, loss 0.2945, train acc 85.08%, f1 0.8506, precision 0.8513, recall 0.8500, auc 0.8508
epoch 5201, loss 0.3937, train acc 85.10%, f1 0.8509, precision 0.8514, recall 0.8505, auc 0.8510
epoch 5301, loss 0.4476, train acc 85.07%, f1 0.8507, precision 0.8506, recall 0.8508, auc 0.8507
epoch 5401, loss 0.2763, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8504, auc 0.8505
epoch 5501, loss 0.3417, train acc 85.07%, f1 0.8506, precision 0.8511, recall 0.8501, auc 0.8507
epoch 5601, loss 0.2969, train acc 85.03%, f1 0.8503, precision 0.8503, recall 0.8502, auc 0.8503
epoch 5701, loss 0.1965, train acc 85.09%, f1 0.8508, precision 0.8514, recall 0.8503, auc 0.8509
epoch 5801, loss 0.3983, train acc 85.09%, f1 0.8508, precision 0.8513, recall 0.8502, auc 0.8509
epoch 5901, loss 0.3654, train acc 85.10%, f1 0.8510, precision 0.8511, recall 0.8509, auc 0.8510
epoch 6001, loss 0.3130, train acc 85.09%, f1 0.8509, precision 0.8510, recall 0.8508, auc 0.8509
epoch 6101, loss 0.3870, train acc 85.14%, f1 0.8514, precision 0.8514, recall 0.8514, auc 0.8514
epoch 6201, loss 0.3866, train acc 85.13%, f1 0.8513, precision 0.8514, recall 0.8512, auc 0.8513
epoch 6301, loss 0.2786, train acc 85.10%, f1 0.8509, precision 0.8518, recall 0.8500, auc 0.8510
epoch 6401, loss 0.3447, train acc 85.12%, f1 0.8511, precision 0.8516, recall 0.8506, auc 0.8512
epoch 6501, loss 0.2603, train acc 85.09%, f1 0.8509, precision 0.8508, recall 0.8509, auc 0.8509
epoch 6601, loss 0.3572, train acc 85.13%, f1 0.8512, precision 0.8517, recall 0.8508, auc 0.8513
epoch 6701, loss 0.2948, train acc 85.15%, f1 0.8515, precision 0.8514, recall 0.8515, auc 0.8515
epoch 6801, loss 0.2948, train acc 85.14%, f1 0.8513, precision 0.8520, recall 0.8507, auc 0.8514
epoch 6901, loss 0.3188, train acc 85.13%, f1 0.8512, precision 0.8516, recall 0.8508, auc 0.8513
epoch 7001, loss 0.3402, train acc 85.16%, f1 0.8515, precision 0.8520, recall 0.8511, auc 0.8516
epoch 7101, loss 0.2090, train acc 85.17%, f1 0.8517, precision 0.8517, recall 0.8517, auc 0.8517
epoch 7201, loss 0.3571, train acc 85.17%, f1 0.8516, precision 0.8520, recall 0.8512, auc 0.8517
epoch 7301, loss 0.3220, train acc 85.21%, f1 0.8521, precision 0.8522, recall 0.8519, auc 0.8521
epoch 7401, loss 0.2888, train acc 85.22%, f1 0.8520, precision 0.8528, recall 0.8512, auc 0.8522
epoch 7501, loss 0.2433, train acc 85.21%, f1 0.8520, precision 0.8525, recall 0.8516, auc 0.8521
epoch 7601, loss 0.3664, train acc 85.19%, f1 0.8518, precision 0.8521, recall 0.8515, auc 0.8519
epoch 7701, loss 0.2710, train acc 85.19%, f1 0.8519, precision 0.8520, recall 0.8518, auc 0.8519
epoch 7801, loss 0.3025, train acc 85.20%, f1 0.8520, precision 0.8520, recall 0.8520, auc 0.8520
epoch 7901, loss 0.3991, train acc 85.20%, f1 0.8520, precision 0.8520, recall 0.8520, auc 0.8520
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_4
./test_pima/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6522641509433962

the Fscore is 0.5975609756097561

the precision is 0.44144144144144143

the recall is 0.9245283018867925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_4
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5940, train acc 75.56%, f1 0.7862, precision 0.6989, recall 0.8984, auc 0.7556
epoch 201, loss 0.5005, train acc 81.23%, f1 0.8185, precision 0.7920, recall 0.8469, auc 0.8123
epoch 301, loss 0.4167, train acc 83.26%, f1 0.8338, precision 0.8278, recall 0.8399, auc 0.8326
epoch 401, loss 0.4246, train acc 84.32%, f1 0.8427, precision 0.8455, recall 0.8400, auc 0.8432
epoch 501, loss 0.3078, train acc 84.73%, f1 0.8461, precision 0.8526, recall 0.8398, auc 0.8473
epoch 601, loss 0.3425, train acc 84.88%, f1 0.8477, precision 0.8542, recall 0.8413, auc 0.8488
epoch 701, loss 0.3885, train acc 84.96%, f1 0.8479, precision 0.8576, recall 0.8384, auc 0.8496
epoch 801, loss 0.4150, train acc 85.00%, f1 0.8483, precision 0.8578, recall 0.8390, auc 0.8500
epoch 901, loss 0.3278, train acc 85.01%, f1 0.8486, precision 0.8571, recall 0.8403, auc 0.8501
epoch 1001, loss 0.3822, train acc 85.07%, f1 0.8496, precision 0.8560, recall 0.8433, auc 0.8507
epoch 1101, loss 0.3523, train acc 85.03%, f1 0.8489, precision 0.8566, recall 0.8415, auc 0.8503
epoch 1201, loss 0.3084, train acc 85.08%, f1 0.8498, precision 0.8557, recall 0.8440, auc 0.8508
epoch 1301, loss 0.2331, train acc 85.07%, f1 0.8495, precision 0.8564, recall 0.8427, auc 0.8507
epoch 1401, loss 0.3097, train acc 85.09%, f1 0.8499, precision 0.8557, recall 0.8442, auc 0.8509
epoch 1501, loss 0.3768, train acc 85.05%, f1 0.8496, precision 0.8549, recall 0.8443, auc 0.8505
epoch 1601, loss 0.3254, train acc 85.09%, f1 0.8499, precision 0.8555, recall 0.8444, auc 0.8509
epoch 1701, loss 0.2327, train acc 85.05%, f1 0.8497, precision 0.8545, recall 0.8450, auc 0.8505
epoch 1801, loss 0.3342, train acc 85.07%, f1 0.8498, precision 0.8548, recall 0.8448, auc 0.8507
epoch 1901, loss 0.3217, train acc 85.09%, f1 0.8503, precision 0.8539, recall 0.8468, auc 0.8509
epoch 2001, loss 0.3186, train acc 85.08%, f1 0.8501, precision 0.8539, recall 0.8464, auc 0.8508
epoch 2101, loss 0.3645, train acc 85.08%, f1 0.8503, precision 0.8528, recall 0.8479, auc 0.8508
epoch 2201, loss 0.3838, train acc 85.09%, f1 0.8503, precision 0.8536, recall 0.8470, auc 0.8509
epoch 2301, loss 0.2512, train acc 85.08%, f1 0.8503, precision 0.8533, recall 0.8473, auc 0.8508
epoch 2401, loss 0.3863, train acc 85.06%, f1 0.8505, precision 0.8512, recall 0.8497, auc 0.8506
epoch 2501, loss 0.2595, train acc 85.08%, f1 0.8506, precision 0.8520, recall 0.8492, auc 0.8508
epoch 2601, loss 0.3755, train acc 85.08%, f1 0.8505, precision 0.8522, recall 0.8488, auc 0.8508
epoch 2701, loss 0.3707, train acc 85.08%, f1 0.8504, precision 0.8527, recall 0.8481, auc 0.8508
epoch 2801, loss 0.3584, train acc 85.07%, f1 0.8507, precision 0.8511, recall 0.8502, auc 0.8507
epoch 2901, loss 0.3349, train acc 85.06%, f1 0.8502, precision 0.8524, recall 0.8481, auc 0.8506
epoch 3001, loss 0.2604, train acc 85.07%, f1 0.8504, precision 0.8520, recall 0.8488, auc 0.8507
epoch 3101, loss 0.3430, train acc 85.05%, f1 0.8502, precision 0.8520, recall 0.8485, auc 0.8505
epoch 3201, loss 0.3504, train acc 85.04%, f1 0.8503, precision 0.8509, recall 0.8497, auc 0.8504
epoch 3301, loss 0.3554, train acc 85.07%, f1 0.8505, precision 0.8516, recall 0.8495, auc 0.8507
epoch 3401, loss 0.2953, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8508, auc 0.8509
epoch 3501, loss 0.3597, train acc 85.04%, f1 0.8502, precision 0.8512, recall 0.8492, auc 0.8504
epoch 3601, loss 0.2422, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8506, auc 0.8506
epoch 3701, loss 0.4131, train acc 85.11%, f1 0.8509, precision 0.8517, recall 0.8501, auc 0.8511
epoch 3801, loss 0.3384, train acc 85.08%, f1 0.8507, precision 0.8512, recall 0.8502, auc 0.8508
epoch 3901, loss 0.3613, train acc 85.08%, f1 0.8507, precision 0.8514, recall 0.8500, auc 0.8508
epoch 4001, loss 0.3629, train acc 85.03%, f1 0.8503, precision 0.8502, recall 0.8504, auc 0.8503
epoch 4101, loss 0.4724, train acc 85.07%, f1 0.8507, precision 0.8509, recall 0.8505, auc 0.8507
epoch 4201, loss 0.3402, train acc 85.09%, f1 0.8509, precision 0.8510, recall 0.8508, auc 0.8509
epoch 4301, loss 0.3230, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8505
epoch 4401, loss 0.1671, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 4501, loss 0.2715, train acc 85.06%, f1 0.8505, precision 0.8507, recall 0.8503, auc 0.8506
epoch 4601, loss 0.2946, train acc 85.06%, f1 0.8505, precision 0.8508, recall 0.8502, auc 0.8506
epoch 4701, loss 0.3264, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 4801, loss 0.4595, train acc 85.07%, f1 0.8507, precision 0.8510, recall 0.8504, auc 0.8507
epoch 4901, loss 0.4719, train acc 85.10%, f1 0.8510, precision 0.8512, recall 0.8507, auc 0.8510
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_4
./test_pima/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6228301886792453

the Fscore is 0.5748502994011976

the precision is 0.42105263157894735

the recall is 0.9056603773584906

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_4
----------------------



epoch 1, loss 0.6931, train acc 50.03%, f1 0.0011, precision 0.9412, recall 0.0006, auc 0.5003
epoch 101, loss 0.6321, train acc 78.43%, f1 0.7835, precision 0.7864, recall 0.7806, auc 0.7843
epoch 201, loss 0.4926, train acc 81.53%, f1 0.8152, precision 0.8156, recall 0.8149, auc 0.8153
epoch 301, loss 0.4449, train acc 83.54%, f1 0.8355, precision 0.8352, recall 0.8358, auc 0.8354
epoch 401, loss 0.3735, train acc 84.51%, f1 0.8451, precision 0.8449, recall 0.8453, auc 0.8451
epoch 501, loss 0.3678, train acc 84.86%, f1 0.8487, precision 0.8482, recall 0.8492, auc 0.8486
epoch 601, loss 0.3307, train acc 84.97%, f1 0.8498, precision 0.8495, recall 0.8500, auc 0.8497
epoch 701, loss 0.3760, train acc 84.99%, f1 0.8499, precision 0.8497, recall 0.8502, auc 0.8499
epoch 801, loss 0.4153, train acc 85.04%, f1 0.8505, precision 0.8502, recall 0.8508, auc 0.8504
epoch 901, loss 0.4301, train acc 85.12%, f1 0.8513, precision 0.8510, recall 0.8516, auc 0.8512
epoch 1001, loss 0.2902, train acc 85.11%, f1 0.8511, precision 0.8510, recall 0.8513, auc 0.8511
epoch 1101, loss 0.3891, train acc 85.10%, f1 0.8510, precision 0.8508, recall 0.8511, auc 0.8510
epoch 1201, loss 0.3863, train acc 85.09%, f1 0.8509, precision 0.8507, recall 0.8511, auc 0.8509
epoch 1301, loss 0.2901, train acc 85.14%, f1 0.8514, precision 0.8513, recall 0.8516, auc 0.8514
epoch 1401, loss 0.3146, train acc 85.13%, f1 0.8513, precision 0.8513, recall 0.8514, auc 0.8513
epoch 1501, loss 0.3611, train acc 85.04%, f1 0.8504, precision 0.8504, recall 0.8504, auc 0.8504
epoch 1601, loss 0.3250, train acc 85.04%, f1 0.8504, precision 0.8504, recall 0.8505, auc 0.8504
epoch 1701, loss 0.3852, train acc 85.06%, f1 0.8506, precision 0.8506, recall 0.8507, auc 0.8506
epoch 1801, loss 0.4487, train acc 85.04%, f1 0.8504, precision 0.8505, recall 0.8504, auc 0.8504
epoch 1901, loss 0.2777, train acc 85.07%, f1 0.8506, precision 0.8507, recall 0.8506, auc 0.8507
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_4
./test_pima/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6528301886792454

the Fscore is 0.5962732919254657

the precision is 0.4444444444444444

the recall is 0.9056603773584906

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_4
----------------------



epoch 1, loss 0.6931, train acc 49.98%, f1 0.6665, precision 0.4998, recall 1.0000, auc 0.5000
epoch 101, loss 0.5935, train acc 77.42%, f1 0.7900, precision 0.7382, recall 0.8495, auc 0.7742
epoch 201, loss 0.5100, train acc 81.26%, f1 0.8155, precision 0.8028, recall 0.8286, auc 0.8126
epoch 301, loss 0.4253, train acc 83.36%, f1 0.8333, precision 0.8346, recall 0.8320, auc 0.8336
epoch 401, loss 0.4397, train acc 84.28%, f1 0.8424, precision 0.8444, recall 0.8404, auc 0.8428
epoch 501, loss 0.3802, train acc 84.80%, f1 0.8468, precision 0.8530, recall 0.8408, auc 0.8480
epoch 601, loss 0.3644, train acc 84.94%, f1 0.8488, precision 0.8523, recall 0.8453, auc 0.8494
epoch 701, loss 0.3089, train acc 85.00%, f1 0.8494, precision 0.8523, recall 0.8465, auc 0.8500
epoch 801, loss 0.3578, train acc 85.09%, f1 0.8499, precision 0.8550, recall 0.8449, auc 0.8509
epoch 901, loss 0.3393, train acc 85.12%, f1 0.8500, precision 0.8564, recall 0.8438, auc 0.8512
epoch 1001, loss 0.4560, train acc 85.12%, f1 0.8502, precision 0.8559, recall 0.8445, auc 0.8512
epoch 1101, loss 0.3249, train acc 85.05%, f1 0.8498, precision 0.8537, recall 0.8458, auc 0.8505
epoch 1201, loss 0.2962, train acc 85.09%, f1 0.8504, precision 0.8531, recall 0.8476, auc 0.8509
epoch 1301, loss 0.3713, train acc 85.09%, f1 0.8500, precision 0.8550, recall 0.8450, auc 0.8509
epoch 1401, loss 0.3322, train acc 85.10%, f1 0.8507, precision 0.8525, recall 0.8488, auc 0.8510
epoch 1501, loss 0.4064, train acc 85.13%, f1 0.8506, precision 0.8547, recall 0.8465, auc 0.8513
epoch 1601, loss 0.3509, train acc 85.07%, f1 0.8500, precision 0.8537, recall 0.8463, auc 0.8507
epoch 1701, loss 0.4311, train acc 85.09%, f1 0.8506, precision 0.8517, recall 0.8495, auc 0.8509
epoch 1801, loss 0.2330, train acc 85.08%, f1 0.8501, precision 0.8536, recall 0.8466, auc 0.8508
epoch 1901, loss 0.3618, train acc 85.09%, f1 0.8501, precision 0.8542, recall 0.8461, auc 0.8509
epoch 2001, loss 0.2576, train acc 85.09%, f1 0.8501, precision 0.8547, recall 0.8454, auc 0.8509
epoch 2101, loss 0.2308, train acc 85.10%, f1 0.8504, precision 0.8535, recall 0.8474, auc 0.8510
epoch 2201, loss 0.2691, train acc 85.08%, f1 0.8502, precision 0.8531, recall 0.8474, auc 0.8508
epoch 2301, loss 0.3989, train acc 85.12%, f1 0.8507, precision 0.8537, recall 0.8476, auc 0.8512
epoch 2401, loss 0.3401, train acc 85.08%, f1 0.8505, precision 0.8516, recall 0.8495, auc 0.8508
epoch 2501, loss 0.3576, train acc 85.10%, f1 0.8509, precision 0.8514, recall 0.8504, auc 0.8510
epoch 2601, loss 0.2861, train acc 85.12%, f1 0.8504, precision 0.8548, recall 0.8459, auc 0.8512
epoch 2701, loss 0.2792, train acc 85.13%, f1 0.8507, precision 0.8539, recall 0.8474, auc 0.8513
epoch 2801, loss 0.2194, train acc 85.13%, f1 0.8507, precision 0.8543, recall 0.8471, auc 0.8513
epoch 2901, loss 0.3379, train acc 85.10%, f1 0.8506, precision 0.8526, recall 0.8486, auc 0.8510
epoch 3001, loss 0.2831, train acc 85.08%, f1 0.8506, precision 0.8513, recall 0.8500, auc 0.8508
epoch 3101, loss 0.2883, train acc 85.12%, f1 0.8509, precision 0.8526, recall 0.8492, auc 0.8512
epoch 3201, loss 0.2446, train acc 85.09%, f1 0.8504, precision 0.8531, recall 0.8477, auc 0.8509
epoch 3301, loss 0.3310, train acc 85.09%, f1 0.8506, precision 0.8516, recall 0.8496, auc 0.8509
epoch 3401, loss 0.3511, train acc 85.07%, f1 0.8505, precision 0.8514, recall 0.8496, auc 0.8507
epoch 3501, loss 0.3509, train acc 85.08%, f1 0.8511, precision 0.8492, recall 0.8530, auc 0.8508
epoch 3601, loss 0.3603, train acc 85.04%, f1 0.8500, precision 0.8519, recall 0.8481, auc 0.8504
epoch 3701, loss 0.2953, train acc 85.06%, f1 0.8499, precision 0.8537, recall 0.8461, auc 0.8506
epoch 3801, loss 0.2827, train acc 85.05%, f1 0.8501, precision 0.8521, recall 0.8482, auc 0.8505
epoch 3901, loss 0.3271, train acc 85.02%, f1 0.8504, precision 0.8486, recall 0.8523, auc 0.8502
epoch 4001, loss 0.4748, train acc 85.09%, f1 0.8507, precision 0.8516, recall 0.8497, auc 0.8509
epoch 4101, loss 0.3234, train acc 85.13%, f1 0.8507, precision 0.8540, recall 0.8474, auc 0.8513
epoch 4201, loss 0.3183, train acc 85.05%, f1 0.8505, precision 0.8503, recall 0.8508, auc 0.8505
epoch 4301, loss 0.4090, train acc 85.10%, f1 0.8509, precision 0.8517, recall 0.8500, auc 0.8510
epoch 4401, loss 0.3379, train acc 85.11%, f1 0.8506, precision 0.8536, recall 0.8476, auc 0.8511
epoch 4501, loss 0.3167, train acc 85.04%, f1 0.8506, precision 0.8495, recall 0.8517, auc 0.8504
epoch 4601, loss 0.2847, train acc 85.01%, f1 0.8502, precision 0.8497, recall 0.8506, auc 0.8501
epoch 4701, loss 0.3939, train acc 85.06%, f1 0.8506, precision 0.8504, recall 0.8507, auc 0.8506
epoch 4801, loss 0.3825, train acc 85.10%, f1 0.8505, precision 0.8535, recall 0.8474, auc 0.8510
epoch 4901, loss 0.2881, train acc 85.09%, f1 0.8507, precision 0.8511, recall 0.8503, auc 0.8509
epoch 5001, loss 0.2543, train acc 85.12%, f1 0.8507, precision 0.8531, recall 0.8483, auc 0.8512
epoch 5101, loss 0.4063, train acc 85.07%, f1 0.8502, precision 0.8528, recall 0.8475, auc 0.8507
epoch 5201, loss 0.5461, train acc 85.11%, f1 0.8511, precision 0.8507, recall 0.8515, auc 0.8511
epoch 5301, loss 0.3239, train acc 85.06%, f1 0.8508, precision 0.8495, recall 0.8521, auc 0.8506
epoch 5401, loss 0.3197, train acc 85.11%, f1 0.8506, precision 0.8533, recall 0.8478, auc 0.8511
epoch 5501, loss 0.4101, train acc 85.07%, f1 0.8507, precision 0.8500, recall 0.8515, auc 0.8507
epoch 5601, loss 0.3165, train acc 85.11%, f1 0.8514, precision 0.8494, recall 0.8534, auc 0.8511
epoch 5701, loss 0.3754, train acc 85.15%, f1 0.8512, precision 0.8524, recall 0.8500, auc 0.8515
epoch 5801, loss 0.2496, train acc 85.13%, f1 0.8508, precision 0.8537, recall 0.8480, auc 0.8513
epoch 5901, loss 0.3128, train acc 85.08%, f1 0.8507, precision 0.8510, recall 0.8504, auc 0.8508
epoch 6001, loss 0.2231, train acc 85.09%, f1 0.8507, precision 0.8517, recall 0.8498, auc 0.8509
epoch 6101, loss 0.3085, train acc 85.13%, f1 0.8513, precision 0.8508, recall 0.8518, auc 0.8513
epoch 6201, loss 0.2780, train acc 85.11%, f1 0.8508, precision 0.8523, recall 0.8493, auc 0.8511
epoch 6301, loss 0.3362, train acc 85.14%, f1 0.8511, precision 0.8523, recall 0.8500, auc 0.8514
epoch 6401, loss 0.4288, train acc 85.13%, f1 0.8511, precision 0.8524, recall 0.8497, auc 0.8513
epoch 6501, loss 0.3207, train acc 85.14%, f1 0.8514, precision 0.8511, recall 0.8516, auc 0.8514
epoch 6601, loss 0.3765, train acc 85.17%, f1 0.8516, precision 0.8516, recall 0.8516, auc 0.8517
epoch 6701, loss 0.2980, train acc 85.18%, f1 0.8516, precision 0.8522, recall 0.8510, auc 0.8518
epoch 6801, loss 0.3457, train acc 85.21%, f1 0.8521, precision 0.8517, recall 0.8525, auc 0.8521
epoch 6901, loss 0.3609, train acc 85.18%, f1 0.8522, precision 0.8498, recall 0.8545, auc 0.8518
epoch 7001, loss 0.2452, train acc 85.18%, f1 0.8516, precision 0.8525, recall 0.8507, auc 0.8518
epoch 7101, loss 0.4629, train acc 85.19%, f1 0.8517, precision 0.8527, recall 0.8507, auc 0.8519
epoch 7201, loss 0.3872, train acc 85.15%, f1 0.8516, precision 0.8511, recall 0.8520, auc 0.8515
epoch 7301, loss 0.3422, train acc 85.19%, f1 0.8515, precision 0.8537, recall 0.8493, auc 0.8519
epoch 7401, loss 0.2919, train acc 85.26%, f1 0.8526, precision 0.8524, recall 0.8528, auc 0.8526
epoch 7501, loss 0.3736, train acc 85.28%, f1 0.8521, precision 0.8559, recall 0.8483, auc 0.8528
epoch 7601, loss 0.3335, train acc 85.26%, f1 0.8528, precision 0.8516, recall 0.8540, auc 0.8526
epoch 7701, loss 0.4167, train acc 85.29%, f1 0.8532, precision 0.8512, recall 0.8553, auc 0.8529
epoch 7801, loss 0.3315, train acc 85.26%, f1 0.8526, precision 0.8522, recall 0.8531, auc 0.8526
epoch 7901, loss 0.2429, train acc 85.34%, f1 0.8534, precision 0.8532, recall 0.8536, auc 0.8534
epoch 8001, loss 0.3369, train acc 85.29%, f1 0.8531, precision 0.8518, recall 0.8545, auc 0.8529
epoch 8101, loss 0.3425, train acc 85.34%, f1 0.8532, precision 0.8540, recall 0.8524, auc 0.8534
epoch 8201, loss 0.2716, train acc 85.38%, f1 0.8537, precision 0.8540, recall 0.8534, auc 0.8538
epoch 8301, loss 0.4414, train acc 85.38%, f1 0.8536, precision 0.8547, recall 0.8525, auc 0.8538
epoch 8401, loss 0.2920, train acc 85.41%, f1 0.8538, precision 0.8553, recall 0.8523, auc 0.8541
epoch 8501, loss 0.3175, train acc 85.37%, f1 0.8534, precision 0.8548, recall 0.8520, auc 0.8537
epoch 8601, loss 0.3009, train acc 85.39%, f1 0.8540, precision 0.8532, recall 0.8548, auc 0.8539
epoch 8701, loss 0.2927, train acc 85.39%, f1 0.8536, precision 0.8548, recall 0.8524, auc 0.8539
epoch 8801, loss 0.3194, train acc 85.40%, f1 0.8537, precision 0.8549, recall 0.8526, auc 0.8540
epoch 8901, loss 0.3119, train acc 85.49%, f1 0.8545, precision 0.8568, recall 0.8522, auc 0.8549
epoch 9001, loss 0.2529, train acc 85.40%, f1 0.8533, precision 0.8574, recall 0.8492, auc 0.8540
epoch 9101, loss 0.3874, train acc 85.41%, f1 0.8540, precision 0.8544, recall 0.8535, auc 0.8541
epoch 9201, loss 0.3206, train acc 85.48%, f1 0.8548, precision 0.8548, recall 0.8547, auc 0.8548
epoch 9301, loss 0.3569, train acc 85.51%, f1 0.8552, precision 0.8542, recall 0.8563, auc 0.8551
epoch 9401, loss 0.2687, train acc 85.51%, f1 0.8555, precision 0.8532, recall 0.8578, auc 0.8551
epoch 9501, loss 0.3622, train acc 85.50%, f1 0.8546, precision 0.8566, recall 0.8526, auc 0.8550
epoch 9601, loss 0.3389, train acc 85.50%, f1 0.8553, precision 0.8535, recall 0.8570, auc 0.8550
epoch 9701, loss 0.3088, train acc 85.53%, f1 0.8552, precision 0.8552, recall 0.8553, auc 0.8553
epoch 9801, loss 0.3420, train acc 85.58%, f1 0.8557, precision 0.8563, recall 0.8551, auc 0.8558
epoch 9901, loss 0.4073, train acc 85.52%, f1 0.8550, precision 0.8559, recall 0.8541, auc 0.8552
epoch 10001, loss 0.3064, train acc 85.59%, f1 0.8560, precision 0.8552, recall 0.8567, auc 0.8559
epoch 10101, loss 0.3447, train acc 85.59%, f1 0.8557, precision 0.8567, recall 0.8546, auc 0.8559
epoch 10201, loss 0.2042, train acc 85.56%, f1 0.8554, precision 0.8561, recall 0.8547, auc 0.8556
epoch 10301, loss 0.3505, train acc 85.56%, f1 0.8551, precision 0.8577, recall 0.8525, auc 0.8556
epoch 10401, loss 0.3055, train acc 85.61%, f1 0.8557, precision 0.8575, recall 0.8540, auc 0.8561
epoch 10501, loss 0.3462, train acc 85.62%, f1 0.8564, precision 0.8548, recall 0.8580, auc 0.8562
epoch 10601, loss 0.3450, train acc 85.63%, f1 0.8560, precision 0.8574, recall 0.8546, auc 0.8563
epoch 10701, loss 0.4108, train acc 85.66%, f1 0.8564, precision 0.8577, recall 0.8550, auc 0.8566
epoch 10801, loss 0.3317, train acc 85.68%, f1 0.8570, precision 0.8558, recall 0.8582, auc 0.8568
epoch 10901, loss 0.2626, train acc 85.66%, f1 0.8562, precision 0.8583, recall 0.8541, auc 0.8566
epoch 11001, loss 0.3705, train acc 85.72%, f1 0.8572, precision 0.8568, recall 0.8577, auc 0.8572
epoch 11101, loss 0.2765, train acc 85.75%, f1 0.8574, precision 0.8577, recall 0.8572, auc 0.8575
epoch 11201, loss 0.3115, train acc 85.68%, f1 0.8569, precision 0.8561, recall 0.8576, auc 0.8568
epoch 11301, loss 0.3176, train acc 85.69%, f1 0.8568, precision 0.8573, recall 0.8563, auc 0.8569
epoch 11401, loss 0.3607, train acc 85.67%, f1 0.8569, precision 0.8554, recall 0.8584, auc 0.8567
epoch 11501, loss 0.3587, train acc 85.70%, f1 0.8567, precision 0.8581, recall 0.8554, auc 0.8570
epoch 11601, loss 0.2963, train acc 85.70%, f1 0.8569, precision 0.8572, recall 0.8566, auc 0.8570
epoch 11701, loss 0.2614, train acc 85.81%, f1 0.8579, precision 0.8589, recall 0.8568, auc 0.8581
epoch 11801, loss 0.3620, train acc 85.79%, f1 0.8577, precision 0.8588, recall 0.8566, auc 0.8579
epoch 11901, loss 0.3198, train acc 85.81%, f1 0.8583, precision 0.8572, recall 0.8593, auc 0.8581
epoch 12001, loss 0.4213, train acc 85.69%, f1 0.8565, precision 0.8585, recall 0.8545, auc 0.8569
epoch 12101, loss 0.3101, train acc 85.73%, f1 0.8574, precision 0.8566, recall 0.8582, auc 0.8573
epoch 12201, loss 0.2499, train acc 85.79%, f1 0.8576, precision 0.8594, recall 0.8558, auc 0.8579
epoch 12301, loss 0.2722, train acc 85.75%, f1 0.8578, precision 0.8555, recall 0.8602, auc 0.8575
epoch 12401, loss 0.3564, train acc 85.77%, f1 0.8573, precision 0.8591, recall 0.8556, auc 0.8577
epoch 12501, loss 0.2499, train acc 85.77%, f1 0.8575, precision 0.8580, recall 0.8571, auc 0.8577
epoch 12601, loss 0.3155, train acc 85.80%, f1 0.8578, precision 0.8584, recall 0.8573, auc 0.8580
epoch 12701, loss 0.2965, train acc 85.82%, f1 0.8579, precision 0.8591, recall 0.8568, auc 0.8582
epoch 12801, loss 0.3560, train acc 85.81%, f1 0.8582, precision 0.8571, recall 0.8593, auc 0.8581
epoch 12901, loss 0.2986, train acc 85.83%, f1 0.8577, precision 0.8612, recall 0.8543, auc 0.8583
epoch 13001, loss 0.3681, train acc 85.84%, f1 0.8581, precision 0.8599, recall 0.8563, auc 0.8584
epoch 13101, loss 0.3001, train acc 85.83%, f1 0.8582, precision 0.8586, recall 0.8578, auc 0.8583
epoch 13201, loss 0.3783, train acc 85.88%, f1 0.8587, precision 0.8593, recall 0.8580, auc 0.8588
epoch 13301, loss 0.3759, train acc 85.86%, f1 0.8584, precision 0.8590, recall 0.8579, auc 0.8586
epoch 13401, loss 0.3905, train acc 85.86%, f1 0.8583, precision 0.8598, recall 0.8567, auc 0.8586
epoch 13501, loss 0.3126, train acc 85.84%, f1 0.8584, precision 0.8581, recall 0.8588, auc 0.8584
epoch 13601, loss 0.4063, train acc 85.91%, f1 0.8594, precision 0.8574, recall 0.8614, auc 0.8591
epoch 13701, loss 0.2671, train acc 85.90%, f1 0.8587, precision 0.8600, recall 0.8573, auc 0.8590
epoch 13801, loss 0.3612, train acc 85.86%, f1 0.8584, precision 0.8595, recall 0.8572, auc 0.8586
epoch 13901, loss 0.2245, train acc 85.97%, f1 0.8598, precision 0.8592, recall 0.8604, auc 0.8597
epoch 14001, loss 0.3226, train acc 85.97%, f1 0.8593, precision 0.8617, recall 0.8569, auc 0.8597
epoch 14101, loss 0.4160, train acc 85.93%, f1 0.8592, precision 0.8594, recall 0.8590, auc 0.8593
epoch 14201, loss 0.2661, train acc 85.91%, f1 0.8587, precision 0.8609, recall 0.8565, auc 0.8591
epoch 14301, loss 0.2975, train acc 85.99%, f1 0.8594, precision 0.8620, recall 0.8569, auc 0.8599
epoch 14401, loss 0.3861, train acc 85.99%, f1 0.8596, precision 0.8610, recall 0.8583, auc 0.8599
epoch 14501, loss 0.3645, train acc 85.99%, f1 0.8597, precision 0.8607, recall 0.8586, auc 0.8599
epoch 14601, loss 0.2249, train acc 85.97%, f1 0.8598, precision 0.8586, recall 0.8610, auc 0.8597
epoch 14701, loss 0.2775, train acc 85.97%, f1 0.8591, precision 0.8625, recall 0.8558, auc 0.8597
epoch 14801, loss 0.3417, train acc 86.05%, f1 0.8604, precision 0.8607, recall 0.8601, auc 0.8605
epoch 14901, loss 0.3782, train acc 86.01%, f1 0.8601, precision 0.8596, recall 0.8606, auc 0.8601
epoch 15001, loss 0.3272, train acc 86.08%, f1 0.8606, precision 0.8613, recall 0.8600, auc 0.8608
epoch 15101, loss 0.3332, train acc 86.09%, f1 0.8608, precision 0.8610, recall 0.8605, auc 0.8609
epoch 15201, loss 0.2632, train acc 86.02%, f1 0.8600, precision 0.8612, recall 0.8588, auc 0.8602
epoch 15301, loss 0.4089, train acc 86.06%, f1 0.8605, precision 0.8608, recall 0.8602, auc 0.8606
epoch 15401, loss 0.2324, train acc 86.06%, f1 0.8602, precision 0.8622, recall 0.8583, auc 0.8606
epoch 15501, loss 0.2037, train acc 86.07%, f1 0.8603, precision 0.8626, recall 0.8580, auc 0.8607
epoch 15601, loss 0.2808, train acc 86.08%, f1 0.8610, precision 0.8593, recall 0.8628, auc 0.8608
epoch 15701, loss 0.2753, train acc 86.11%, f1 0.8609, precision 0.8615, recall 0.8604, auc 0.8611
epoch 15801, loss 0.3820, train acc 86.04%, f1 0.8601, precision 0.8614, recall 0.8588, auc 0.8604
epoch 15901, loss 0.2386, train acc 86.13%, f1 0.8607, precision 0.8643, recall 0.8572, auc 0.8613
epoch 16001, loss 0.3158, train acc 86.11%, f1 0.8608, precision 0.8623, recall 0.8593, auc 0.8611
epoch 16101, loss 0.3033, train acc 86.07%, f1 0.8604, precision 0.8618, recall 0.8590, auc 0.8607
epoch 16201, loss 0.2396, train acc 86.12%, f1 0.8610, precision 0.8620, recall 0.8600, auc 0.8612
epoch 16301, loss 0.3161, train acc 86.18%, f1 0.8617, precision 0.8623, recall 0.8611, auc 0.8618
epoch 16401, loss 0.3183, train acc 86.16%, f1 0.8616, precision 0.8613, recall 0.8619, auc 0.8616
epoch 16501, loss 0.3201, train acc 86.18%, f1 0.8619, precision 0.8615, recall 0.8622, auc 0.8618/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.3656, train acc 86.19%, f1 0.8615, precision 0.8634, recall 0.8597, auc 0.8619
epoch 16701, loss 0.2542, train acc 86.13%, f1 0.8610, precision 0.8630, recall 0.8590, auc 0.8613
epoch 16801, loss 0.4627, train acc 86.18%, f1 0.8615, precision 0.8632, recall 0.8598, auc 0.8618
epoch 16901, loss 0.3368, train acc 86.13%, f1 0.8610, precision 0.8630, recall 0.8589, auc 0.8613
epoch 17001, loss 0.3044, train acc 86.15%, f1 0.8612, precision 0.8633, recall 0.8590, auc 0.8615
epoch 17101, loss 0.2772, train acc 86.22%, f1 0.8617, precision 0.8643, recall 0.8591, auc 0.8622
epoch 17201, loss 0.2605, train acc 86.14%, f1 0.8615, precision 0.8605, recall 0.8625, auc 0.8614
epoch 17301, loss 0.2903, train acc 86.22%, f1 0.8621, precision 0.8624, recall 0.8618, auc 0.8622
epoch 17401, loss 0.2929, train acc 86.19%, f1 0.8616, precision 0.8634, recall 0.8598, auc 0.8619
epoch 17501, loss 0.3645, train acc 86.22%, f1 0.8623, precision 0.8611, recall 0.8636, auc 0.8622
epoch 17601, loss 0.3574, train acc 86.22%, f1 0.8622, precision 0.8621, recall 0.8622, auc 0.8622
epoch 17701, loss 0.3176, train acc 86.22%, f1 0.8620, precision 0.8628, recall 0.8612, auc 0.8622
epoch 17801, loss 0.2960, train acc 86.27%, f1 0.8622, precision 0.8651, recall 0.8594, auc 0.8627
epoch 17901, loss 0.3309, train acc 86.26%, f1 0.8625, precision 0.8629, recall 0.8621, auc 0.8626
epoch 18001, loss 0.3309, train acc 86.25%, f1 0.8624, precision 0.8630, recall 0.8617, auc 0.8625
epoch 18101, loss 0.2569, train acc 86.28%, f1 0.8630, precision 0.8620, recall 0.8640, auc 0.8628
epoch 18201, loss 0.4219, train acc 86.29%, f1 0.8625, precision 0.8648, recall 0.8601, auc 0.8629
epoch 18301, loss 0.3196, train acc 86.24%, f1 0.8625, precision 0.8618, recall 0.8632, auc 0.8624
epoch 18401, loss 0.3494, train acc 86.28%, f1 0.8633, precision 0.8603, recall 0.8662, auc 0.8628
epoch 18501, loss 0.3418, train acc 86.27%, f1 0.8630, precision 0.8614, recall 0.8645, auc 0.8627
epoch 18601, loss 0.3425, train acc 86.30%, f1 0.8631, precision 0.8622, recall 0.8640, auc 0.8630
epoch 18701, loss 0.3641, train acc 86.33%, f1 0.8629, precision 0.8652, recall 0.8606, auc 0.8633
epoch 18801, loss 0.2982, train acc 86.30%, f1 0.8630, precision 0.8628, recall 0.8632, auc 0.8630
epoch 18901, loss 0.3529, train acc 86.34%, f1 0.8631, precision 0.8644, recall 0.8619, auc 0.8634
epoch 19001, loss 0.2628, train acc 86.34%, f1 0.8632, precision 0.8641, recall 0.8624, auc 0.8634
epoch 19101, loss 0.3240, train acc 86.32%, f1 0.8634, precision 0.8622, recall 0.8646, auc 0.8632
epoch 19201, loss 0.3422, train acc 86.34%, f1 0.8633, precision 0.8634, recall 0.8632, auc 0.8634
epoch 19301, loss 0.3332, train acc 86.30%, f1 0.8631, precision 0.8621, recall 0.8642, auc 0.8630
epoch 19401, loss 0.2960, train acc 86.33%, f1 0.8634, precision 0.8624, recall 0.8644, auc 0.8633
epoch 19501, loss 0.4043, train acc 86.38%, f1 0.8637, precision 0.8636, recall 0.8639, auc 0.8638
epoch 19601, loss 0.2400, train acc 86.37%, f1 0.8635, precision 0.8643, recall 0.8628, auc 0.8637
epoch 19701, loss 0.2914, train acc 86.33%, f1 0.8630, precision 0.8651, recall 0.8609, auc 0.8633
epoch 19801, loss 0.3062, train acc 86.37%, f1 0.8633, precision 0.8654, recall 0.8613, auc 0.8637
epoch 19901, loss 0.2420, train acc 86.38%, f1 0.8638, precision 0.8634, recall 0.8641, auc 0.8638
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_4
./test_pima/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6416981132075472

the Fscore is 0.5917159763313609

the precision is 0.43103448275862066

the recall is 0.9433962264150944

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_4
----------------------



epoch 1, loss 0.6934, train acc 49.99%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6119, train acc 75.39%, f1 0.7087, precision 0.8684, recall 0.5986, auc 0.7539
epoch 201, loss 0.4930, train acc 81.15%, f1 0.8034, precision 0.8394, recall 0.7704, auc 0.8115
epoch 301, loss 0.4185, train acc 83.28%, f1 0.8313, precision 0.8389, recall 0.8238, auc 0.8328
epoch 401, loss 0.2751, train acc 84.35%, f1 0.8438, precision 0.8425, recall 0.8451, auc 0.8435
epoch 501, loss 0.3085, train acc 84.70%, f1 0.8478, precision 0.8434, recall 0.8523, auc 0.8470
epoch 601, loss 0.3328, train acc 84.83%, f1 0.8497, precision 0.8422, recall 0.8572, auc 0.8483
epoch 701, loss 0.3526, train acc 84.96%, f1 0.8515, precision 0.8414, recall 0.8618, auc 0.8496
epoch 801, loss 0.2949, train acc 85.01%, f1 0.8516, precision 0.8432, recall 0.8603, auc 0.8501
epoch 901, loss 0.4724, train acc 85.06%, f1 0.8520, precision 0.8440, recall 0.8602, auc 0.8506
epoch 1001, loss 0.3009, train acc 85.09%, f1 0.8528, precision 0.8424, recall 0.8634, auc 0.8509
epoch 1101, loss 0.3857, train acc 85.06%, f1 0.8524, precision 0.8424, recall 0.8627, auc 0.8506
epoch 1201, loss 0.4665, train acc 85.03%, f1 0.8517, precision 0.8439, recall 0.8597, auc 0.8503
epoch 1301, loss 0.3075, train acc 85.08%, f1 0.8523, precision 0.8441, recall 0.8607, auc 0.8508
epoch 1401, loss 0.3843, train acc 85.09%, f1 0.8522, precision 0.8452, recall 0.8594, auc 0.8509
epoch 1501, loss 0.3260, train acc 85.10%, f1 0.8523, precision 0.8455, recall 0.8591, auc 0.8510
epoch 1601, loss 0.2135, train acc 85.08%, f1 0.8517, precision 0.8467, recall 0.8567, auc 0.8508
epoch 1701, loss 0.4567, train acc 85.09%, f1 0.8521, precision 0.8456, recall 0.8587, auc 0.8509
epoch 1801, loss 0.3053, train acc 85.05%, f1 0.8515, precision 0.8465, recall 0.8565, auc 0.8505
epoch 1901, loss 0.1995, train acc 85.09%, f1 0.8520, precision 0.8463, recall 0.8577, auc 0.8509
epoch 2001, loss 0.2550, train acc 85.11%, f1 0.8519, precision 0.8477, recall 0.8561, auc 0.8511
epoch 2101, loss 0.2758, train acc 85.04%, f1 0.8511, precision 0.8473, recall 0.8550, auc 0.8504
epoch 2201, loss 0.3664, train acc 85.05%, f1 0.8514, precision 0.8466, recall 0.8563, auc 0.8505
epoch 2301, loss 0.2893, train acc 85.06%, f1 0.8512, precision 0.8479, recall 0.8546, auc 0.8506
epoch 2401, loss 0.3451, train acc 85.07%, f1 0.8517, precision 0.8466, recall 0.8569, auc 0.8507
epoch 2501, loss 0.3024, train acc 85.08%, f1 0.8513, precision 0.8484, recall 0.8543, auc 0.8508
epoch 2601, loss 0.3522, train acc 85.11%, f1 0.8519, precision 0.8476, recall 0.8563, auc 0.8511
epoch 2701, loss 0.4189, train acc 85.13%, f1 0.8519, precision 0.8485, recall 0.8554, auc 0.8513
epoch 2801, loss 0.3193, train acc 85.07%, f1 0.8511, precision 0.8488, recall 0.8534, auc 0.8507
epoch 2901, loss 0.3978, train acc 85.04%, f1 0.8508, precision 0.8486, recall 0.8531, auc 0.8504
epoch 3001, loss 0.2856, train acc 85.09%, f1 0.8515, precision 0.8483, recall 0.8548, auc 0.8509
epoch 3101, loss 0.4020, train acc 85.10%, f1 0.8521, precision 0.8463, recall 0.8580, auc 0.8510
epoch 3201, loss 0.3014, train acc 85.09%, f1 0.8510, precision 0.8503, recall 0.8518, auc 0.8509
epoch 3301, loss 0.2894, train acc 85.07%, f1 0.8511, precision 0.8493, recall 0.8529, auc 0.8507
epoch 3401, loss 0.3191, train acc 85.06%, f1 0.8511, precision 0.8485, recall 0.8537, auc 0.8506
epoch 3501, loss 0.4733, train acc 85.06%, f1 0.8512, precision 0.8478, recall 0.8546, auc 0.8506
epoch 3601, loss 0.3368, train acc 85.06%, f1 0.8511, precision 0.8486, recall 0.8536, auc 0.8506
epoch 3701, loss 0.3168, train acc 85.08%, f1 0.8516, precision 0.8475, recall 0.8557, auc 0.8508
epoch 3801, loss 0.3220, train acc 85.09%, f1 0.8512, precision 0.8493, recall 0.8532, auc 0.8509
epoch 3901, loss 0.3465, train acc 85.07%, f1 0.8507, precision 0.8506, recall 0.8509, auc 0.8507
epoch 4001, loss 0.3062, train acc 85.09%, f1 0.8513, precision 0.8497, recall 0.8529, auc 0.8509
epoch 4101, loss 0.4200, train acc 85.07%, f1 0.8512, precision 0.8483, recall 0.8542, auc 0.8507
epoch 4201, loss 0.1754, train acc 85.09%, f1 0.8515, precision 0.8483, recall 0.8548, auc 0.8509
epoch 4301, loss 0.3622, train acc 85.07%, f1 0.8512, precision 0.8487, recall 0.8536, auc 0.8507
epoch 4401, loss 0.2767, train acc 85.08%, f1 0.8512, precision 0.8493, recall 0.8532, auc 0.8508
epoch 4501, loss 0.2373, train acc 85.03%, f1 0.8505, precision 0.8499, recall 0.8511, auc 0.8503
epoch 4601, loss 0.2504, train acc 85.10%, f1 0.8509, precision 0.8514, recall 0.8505, auc 0.8510
epoch 4701, loss 0.2613, train acc 85.13%, f1 0.8517, precision 0.8497, recall 0.8537, auc 0.8513
epoch 4801, loss 0.2683, train acc 85.08%, f1 0.8508, precision 0.8506, recall 0.8510, auc 0.8508
epoch 4901, loss 0.2999, train acc 85.07%, f1 0.8508, precision 0.8507, recall 0.8509, auc 0.8507
epoch 5001, loss 0.3724, train acc 85.10%, f1 0.8512, precision 0.8498, recall 0.8527, auc 0.8510
epoch 5101, loss 0.2410, train acc 85.08%, f1 0.8518, precision 0.8468, recall 0.8568, auc 0.8508
epoch 5201, loss 0.3245, train acc 85.06%, f1 0.8510, precision 0.8492, recall 0.8527, auc 0.8506
epoch 5301, loss 0.2626, train acc 85.06%, f1 0.8507, precision 0.8502, recall 0.8513, auc 0.8506
epoch 5401, loss 0.3146, train acc 85.08%, f1 0.8513, precision 0.8486, recall 0.8540, auc 0.8508
epoch 5501, loss 0.3876, train acc 85.09%, f1 0.8512, precision 0.8500, recall 0.8523, auc 0.8509
epoch 5601, loss 0.4138, train acc 85.07%, f1 0.8508, precision 0.8505, recall 0.8510, auc 0.8507
epoch 5701, loss 0.4326, train acc 85.10%, f1 0.8511, precision 0.8510, recall 0.8512, auc 0.8510
epoch 5801, loss 0.3830, train acc 85.08%, f1 0.8511, precision 0.8500, recall 0.8522, auc 0.8508
epoch 5901, loss 0.3755, train acc 85.13%, f1 0.8517, precision 0.8497, recall 0.8537, auc 0.8513
epoch 6001, loss 0.4937, train acc 85.14%, f1 0.8517, precision 0.8501, recall 0.8534, auc 0.8514
epoch 6101, loss 0.4551, train acc 85.12%, f1 0.8516, precision 0.8496, recall 0.8536, auc 0.8512
epoch 6201, loss 0.3082, train acc 85.08%, f1 0.8511, precision 0.8499, recall 0.8522, auc 0.8508
epoch 6301, loss 0.2440, train acc 85.11%, f1 0.8512, precision 0.8511, recall 0.8513, auc 0.8511
epoch 6401, loss 0.2120, train acc 85.12%, f1 0.8515, precision 0.8500, recall 0.8530, auc 0.8512
epoch 6501, loss 0.3973, train acc 85.14%, f1 0.8515, precision 0.8511, recall 0.8518, auc 0.8514
epoch 6601, loss 0.3130, train acc 85.11%, f1 0.8514, precision 0.8497, recall 0.8532, auc 0.8511
epoch 6701, loss 0.3543, train acc 85.11%, f1 0.8517, precision 0.8486, recall 0.8547, auc 0.8511
epoch 6801, loss 0.2329, train acc 85.14%, f1 0.8517, precision 0.8502, recall 0.8532, auc 0.8514
epoch 6901, loss 0.3643, train acc 85.13%, f1 0.8510, precision 0.8527, recall 0.8493, auc 0.8513
epoch 7001, loss 0.3838, train acc 85.18%, f1 0.8519, precision 0.8516, recall 0.8522, auc 0.8518
epoch 7101, loss 0.3251, train acc 85.17%, f1 0.8515, precision 0.8530, recall 0.8500, auc 0.8517
epoch 7201, loss 0.3159, train acc 85.13%, f1 0.8516, precision 0.8500, recall 0.8531, auc 0.8513
epoch 7301, loss 0.3007, train acc 85.17%, f1 0.8517, precision 0.8524, recall 0.8509, auc 0.8517
epoch 7401, loss 0.3206, train acc 85.23%, f1 0.8527, precision 0.8503, recall 0.8551, auc 0.8523
epoch 7501, loss 0.3582, train acc 85.15%, f1 0.8519, precision 0.8500, recall 0.8538, auc 0.8515
epoch 7601, loss 0.2949, train acc 85.18%, f1 0.8518, precision 0.8521, recall 0.8515, auc 0.8518
epoch 7701, loss 0.4049, train acc 85.22%, f1 0.8526, precision 0.8501, recall 0.8552, auc 0.8522
epoch 7801, loss 0.2993, train acc 85.20%, f1 0.8522, precision 0.8513, recall 0.8532, auc 0.8520
epoch 7901, loss 0.3038, train acc 85.18%, f1 0.8523, precision 0.8496, recall 0.8550, auc 0.8518
epoch 8001, loss 0.3844, train acc 85.23%, f1 0.8527, precision 0.8507, recall 0.8548, auc 0.8523
epoch 8101, loss 0.2620, train acc 85.22%, f1 0.8523, precision 0.8520, recall 0.8527, auc 0.8522
epoch 8201, loss 0.3634, train acc 85.26%, f1 0.8531, precision 0.8506, recall 0.8556, auc 0.8526/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.4483, train acc 85.26%, f1 0.8529, precision 0.8515, recall 0.8543, auc 0.8526
epoch 8401, loss 0.2406, train acc 85.29%, f1 0.8530, precision 0.8526, recall 0.8535, auc 0.8529
epoch 8501, loss 0.2950, train acc 85.24%, f1 0.8520, precision 0.8547, recall 0.8493, auc 0.8524
epoch 8601, loss 0.4356, train acc 85.27%, f1 0.8531, precision 0.8509, recall 0.8553, auc 0.8527
epoch 8701, loss 0.2944, train acc 85.31%, f1 0.8527, precision 0.8553, recall 0.8501, auc 0.8531
epoch 8801, loss 0.3302, train acc 85.34%, f1 0.8533, precision 0.8541, recall 0.8526, auc 0.8534
epoch 8901, loss 0.3206, train acc 85.36%, f1 0.8538, precision 0.8525, recall 0.8552, auc 0.8536
epoch 9001, loss 0.2986, train acc 85.39%, f1 0.8540, precision 0.8538, recall 0.8542, auc 0.8539
epoch 9101, loss 0.2606, train acc 85.39%, f1 0.8541, precision 0.8533, recall 0.8548, auc 0.8539
epoch 9201, loss 0.3266, train acc 85.42%, f1 0.8544, precision 0.8531, recall 0.8558, auc 0.8542
epoch 9301, loss 0.3224, train acc 85.40%, f1 0.8538, precision 0.8553, recall 0.8523, auc 0.8540
epoch 9401, loss 0.4650, train acc 85.45%, f1 0.8541, precision 0.8567, recall 0.8515, auc 0.8545
epoch 9501, loss 0.2764, train acc 85.45%, f1 0.8547, precision 0.8541, recall 0.8553, auc 0.8545
epoch 9601, loss 0.3417, train acc 85.44%, f1 0.8546, precision 0.8536, recall 0.8557, auc 0.8544
epoch 9701, loss 0.3633, train acc 85.44%, f1 0.8547, precision 0.8530, recall 0.8564, auc 0.8544
epoch 9801, loss 0.2567, train acc 85.47%, f1 0.8549, precision 0.8541, recall 0.8558, auc 0.8547
epoch 9901, loss 0.2414, train acc 85.52%, f1 0.8555, precision 0.8539, recall 0.8571, auc 0.8552
epoch 10001, loss 0.3428, train acc 85.51%, f1 0.8553, precision 0.8545, recall 0.8561, auc 0.8551
epoch 10101, loss 0.3059, train acc 85.54%, f1 0.8554, precision 0.8560, recall 0.8547, auc 0.8554
epoch 10201, loss 0.3397, train acc 85.52%, f1 0.8554, precision 0.8543, recall 0.8565, auc 0.8552
epoch 10301, loss 0.2882, train acc 85.52%, f1 0.8553, precision 0.8548, recall 0.8559, auc 0.8552
epoch 10401, loss 0.3219, train acc 85.52%, f1 0.8552, precision 0.8553, recall 0.8552, auc 0.8552
epoch 10501, loss 0.3862, train acc 85.56%, f1 0.8555, precision 0.8563, recall 0.8546, auc 0.8556
epoch 10601, loss 0.3255, train acc 85.58%, f1 0.8563, precision 0.8534, recall 0.8593, auc 0.8558
epoch 10701, loss 0.2919, train acc 85.58%, f1 0.8561, precision 0.8548, recall 0.8574, auc 0.8558
epoch 10801, loss 0.3531, train acc 85.60%, f1 0.8559, precision 0.8567, recall 0.8551, auc 0.8560
epoch 10901, loss 0.2487, train acc 85.61%, f1 0.8558, precision 0.8577, recall 0.8539, auc 0.8561
epoch 11001, loss 0.3230, train acc 85.67%, f1 0.8568, precision 0.8563, recall 0.8572, auc 0.8567
epoch 11101, loss 0.4373, train acc 85.60%, f1 0.8562, precision 0.8551, recall 0.8572, auc 0.8560
epoch 11201, loss 0.4857, train acc 85.65%, f1 0.8569, precision 0.8553, recall 0.8585, auc 0.8565
epoch 11301, loss 0.3256, train acc 85.68%, f1 0.8569, precision 0.8562, recall 0.8576, auc 0.8568
epoch 11401, loss 0.2610, train acc 85.63%, f1 0.8568, precision 0.8544, recall 0.8591, auc 0.8563
epoch 11501, loss 0.4561, train acc 85.69%, f1 0.8570, precision 0.8568, recall 0.8572, auc 0.8569
epoch 11601, loss 0.2826, train acc 85.70%, f1 0.8571, precision 0.8570, recall 0.8572, auc 0.8570
epoch 11701, loss 0.2879, train acc 85.73%, f1 0.8574, precision 0.8573, recall 0.8575, auc 0.8573
epoch 11801, loss 0.3124, train acc 85.77%, f1 0.8577, precision 0.8576, recall 0.8579, auc 0.8577
epoch 11901, loss 0.3353, train acc 85.78%, f1 0.8579, precision 0.8578, recall 0.8580, auc 0.8578
epoch 12001, loss 0.3507, train acc 85.81%, f1 0.8587, precision 0.8554, recall 0.8621, auc 0.8581
epoch 12101, loss 0.3643, train acc 85.78%, f1 0.8582, precision 0.8564, recall 0.8599, auc 0.8578
epoch 12201, loss 0.3791, train acc 85.78%, f1 0.8579, precision 0.8573, recall 0.8586, auc 0.8578
epoch 12301, loss 0.4165, train acc 85.80%, f1 0.8584, precision 0.8564, recall 0.8604, auc 0.8580
epoch 12401, loss 0.3829, train acc 85.84%, f1 0.8582, precision 0.8593, recall 0.8571, auc 0.8584
epoch 12501, loss 0.4259, train acc 85.79%, f1 0.8575, precision 0.8602, recall 0.8549, auc 0.8579
epoch 12601, loss 0.2791, train acc 85.80%, f1 0.8582, precision 0.8576, recall 0.8588, auc 0.8580
epoch 12701, loss 0.3494, train acc 85.78%, f1 0.8573, precision 0.8606, recall 0.8541, auc 0.8578
epoch 12801, loss 0.3758, train acc 85.91%, f1 0.8593, precision 0.8579, recall 0.8608, auc 0.8591
epoch 12901, loss 0.2585, train acc 85.86%, f1 0.8583, precision 0.8602, recall 0.8566, auc 0.8586
epoch 13001, loss 0.3376, train acc 85.83%, f1 0.8579, precision 0.8605, recall 0.8554, auc 0.8583
epoch 13101, loss 0.3029, train acc 85.84%, f1 0.8582, precision 0.8597, recall 0.8568, auc 0.8584
epoch 13201, loss 0.4322, train acc 85.87%, f1 0.8591, precision 0.8568, recall 0.8614, auc 0.8587
epoch 13301, loss 0.2831, train acc 85.82%, f1 0.8582, precision 0.8583, recall 0.8582, auc 0.8582
epoch 13401, loss 0.2458, train acc 85.91%, f1 0.8592, precision 0.8591, recall 0.8593, auc 0.8591
epoch 13501, loss 0.2740, train acc 85.85%, f1 0.8585, precision 0.8589, recall 0.8580, auc 0.8585
epoch 13601, loss 0.4947, train acc 85.92%, f1 0.8591, precision 0.8600, recall 0.8582, auc 0.8592
epoch 13701, loss 0.3382, train acc 85.95%, f1 0.8592, precision 0.8610, recall 0.8575, auc 0.8595
epoch 13801, loss 0.2937, train acc 85.91%, f1 0.8589, precision 0.8602, recall 0.8576, auc 0.8591
epoch 13901, loss 0.3990, train acc 85.96%, f1 0.8600, precision 0.8578, recall 0.8622, auc 0.8596
epoch 14001, loss 0.3151, train acc 85.84%, f1 0.8584, precision 0.8584, recall 0.8584, auc 0.8584
epoch 14101, loss 0.2642, train acc 85.90%, f1 0.8587, precision 0.8604, recall 0.8571, auc 0.8590
epoch 14201, loss 0.3891, train acc 85.89%, f1 0.8591, precision 0.8582, recall 0.8600, auc 0.8589
epoch 14301, loss 0.2497, train acc 85.99%, f1 0.8598, precision 0.8603, recall 0.8593, auc 0.8599
epoch 14401, loss 0.1848, train acc 85.97%, f1 0.8597, precision 0.8597, recall 0.8597, auc 0.8597
epoch 14501, loss 0.3571, train acc 86.02%, f1 0.8603, precision 0.8601, recall 0.8605, auc 0.8602
epoch 14601, loss 0.2300, train acc 86.02%, f1 0.8606, precision 0.8587, recall 0.8625, auc 0.8602
epoch 14701, loss 0.3193, train acc 86.01%, f1 0.8604, precision 0.8590, recall 0.8617, auc 0.8601
epoch 14801, loss 0.2271, train acc 85.98%, f1 0.8598, precision 0.8597, recall 0.8600, auc 0.8598
epoch 14901, loss 0.2466, train acc 86.00%, f1 0.8606, precision 0.8573, recall 0.8638, auc 0.8600
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_4
./test_pima/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6283962264150944

the Fscore is 0.5766871165644172

the precision is 0.42727272727272725

the recall is 0.8867924528301887

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_4
----------------------



epoch 1, loss 0.6935, train acc 50.10%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5953, train acc 77.53%, f1 0.7571, precision 0.8222, recall 0.7015, auc 0.7752
epoch 201, loss 0.4825, train acc 80.92%, f1 0.8072, precision 0.8141, recall 0.8004, auc 0.8092
epoch 301, loss 0.4135, train acc 83.17%, f1 0.8313, precision 0.8315, recall 0.8312, auc 0.8317
epoch 401, loss 0.4949, train acc 84.34%, f1 0.8435, precision 0.8410, recall 0.8461, auc 0.8434
epoch 501, loss 0.2734, train acc 84.77%, f1 0.8485, precision 0.8426, recall 0.8545, auc 0.8478
epoch 601, loss 0.2970, train acc 84.95%, f1 0.8496, precision 0.8471, recall 0.8522, auc 0.8495
epoch 701, loss 0.4458, train acc 85.05%, f1 0.8511, precision 0.8459, recall 0.8564, auc 0.8505
epoch 801, loss 0.3696, train acc 85.04%, f1 0.8508, precision 0.8464, recall 0.8553, auc 0.8504
epoch 901, loss 0.3803, train acc 85.09%, f1 0.8514, precision 0.8469, recall 0.8559, auc 0.8509
epoch 1001, loss 0.2731, train acc 85.11%, f1 0.8517, precision 0.8467, recall 0.8568, auc 0.8511
epoch 1101, loss 0.4439, train acc 85.08%, f1 0.8512, precision 0.8472, recall 0.8552, auc 0.8508
epoch 1201, loss 0.3077, train acc 85.10%, f1 0.8514, precision 0.8471, recall 0.8559, auc 0.8510
epoch 1301, loss 0.4120, train acc 85.07%, f1 0.8510, precision 0.8475, recall 0.8545, auc 0.8507
epoch 1401, loss 0.3231, train acc 85.08%, f1 0.8513, precision 0.8468, recall 0.8559, auc 0.8508
epoch 1501, loss 0.2924, train acc 85.11%, f1 0.8511, precision 0.8492, recall 0.8530, auc 0.8511
epoch 1601, loss 0.3183, train acc 85.05%, f1 0.8506, precision 0.8485, recall 0.8527, auc 0.8505
epoch 1701, loss 0.2984, train acc 85.07%, f1 0.8510, precision 0.8476, recall 0.8545, auc 0.8507
epoch 1801, loss 0.3208, train acc 85.09%, f1 0.8507, precision 0.8503, recall 0.8511, auc 0.8509
epoch 1901, loss 0.4413, train acc 85.10%, f1 0.8517, precision 0.8460, recall 0.8575, auc 0.8510
epoch 2001, loss 0.3620, train acc 85.09%, f1 0.8512, precision 0.8478, recall 0.8546, auc 0.8509
epoch 2101, loss 0.3142, train acc 85.08%, f1 0.8508, precision 0.8492, recall 0.8524, auc 0.8508
epoch 2201, loss 0.2313, train acc 85.07%, f1 0.8506, precision 0.8495, recall 0.8518, auc 0.8507
epoch 2301, loss 0.3786, train acc 85.06%, f1 0.8506, precision 0.8489, recall 0.8523, auc 0.8506
epoch 2401, loss 0.4774, train acc 85.05%, f1 0.8504, precision 0.8498, recall 0.8509, auc 0.8505
epoch 2501, loss 0.4145, train acc 85.08%, f1 0.8505, precision 0.8504, recall 0.8506, auc 0.8508
epoch 2601, loss 0.3353, train acc 85.09%, f1 0.8508, precision 0.8495, recall 0.8522, auc 0.8509
epoch 2701, loss 0.2945, train acc 85.08%, f1 0.8513, precision 0.8468, recall 0.8559, auc 0.8508
epoch 2801, loss 0.4216, train acc 85.06%, f1 0.8509, precision 0.8473, recall 0.8546, auc 0.8506
epoch 2901, loss 0.2817, train acc 85.08%, f1 0.8508, precision 0.8492, recall 0.8525, auc 0.8508
epoch 3001, loss 0.4286, train acc 85.02%, f1 0.8501, precision 0.8491, recall 0.8512, auc 0.8502
epoch 3101, loss 0.3624, train acc 85.08%, f1 0.8505, precision 0.8503, recall 0.8508, auc 0.8508
epoch 3201, loss 0.3851, train acc 85.04%, f1 0.8503, precision 0.8490, recall 0.8517, auc 0.8504
epoch 3301, loss 0.3453, train acc 85.04%, f1 0.8502, precision 0.8498, recall 0.8506, auc 0.8504
epoch 3401, loss 0.3471, train acc 85.06%, f1 0.8507, precision 0.8482, recall 0.8533, auc 0.8506
epoch 3501, loss 0.3021, train acc 85.06%, f1 0.8502, precision 0.8507, recall 0.8497, auc 0.8506
epoch 3601, loss 0.2598, train acc 85.11%, f1 0.8508, precision 0.8508, recall 0.8507, auc 0.8511
epoch 3701, loss 0.2406, train acc 85.09%, f1 0.8510, precision 0.8488, recall 0.8532, auc 0.8509
epoch 3801, loss 0.4019, train acc 85.10%, f1 0.8509, precision 0.8496, recall 0.8523, auc 0.8510
epoch 3901, loss 0.3002, train acc 85.10%, f1 0.8508, precision 0.8502, recall 0.8515, auc 0.8510
epoch 4001, loss 0.3700, train acc 85.10%, f1 0.8506, precision 0.8507, recall 0.8506, auc 0.8510
epoch 4101, loss 0.4593, train acc 85.06%, f1 0.8502, precision 0.8511, recall 0.8493, auc 0.8506
epoch 4201, loss 0.3684, train acc 84.99%, f1 0.8495, precision 0.8498, recall 0.8493, auc 0.8499
epoch 4301, loss 0.2861, train acc 85.03%, f1 0.8497, precision 0.8518, recall 0.8476, auc 0.8503
epoch 4401, loss 0.3593, train acc 85.10%, f1 0.8506, precision 0.8514, recall 0.8498, auc 0.8510
epoch 4501, loss 0.4170, train acc 85.11%, f1 0.8512, precision 0.8491, recall 0.8534, auc 0.8511
epoch 4601, loss 0.3188, train acc 85.04%, f1 0.8497, precision 0.8519, recall 0.8476, auc 0.8504
epoch 4701, loss 0.3004, train acc 85.06%, f1 0.8501, precision 0.8512, recall 0.8491, auc 0.8506
epoch 4801, loss 0.4029, train acc 85.06%, f1 0.8501, precision 0.8512, recall 0.8491, auc 0.8506
epoch 4901, loss 0.3292, train acc 85.09%, f1 0.8506, precision 0.8509, recall 0.8503, auc 0.8509
epoch 5001, loss 0.3672, train acc 85.06%, f1 0.8500, precision 0.8518, recall 0.8482, auc 0.8506
epoch 5101, loss 0.2935, train acc 85.07%, f1 0.8504, precision 0.8505, recall 0.8503, auc 0.8507
epoch 5201, loss 0.3217, train acc 85.07%, f1 0.8508, precision 0.8482, recall 0.8535, auc 0.8507
epoch 5301, loss 0.3184, train acc 85.09%, f1 0.8506, precision 0.8507, recall 0.8504, auc 0.8509
epoch 5401, loss 0.3684, train acc 85.11%, f1 0.8509, precision 0.8505, recall 0.8513, auc 0.8511
epoch 5501, loss 0.3403, train acc 85.10%, f1 0.8511, precision 0.8490, recall 0.8532, auc 0.8510
epoch 5601, loss 0.3827, train acc 85.07%, f1 0.8507, precision 0.8493, recall 0.8520, auc 0.8507
epoch 5701, loss 0.2741, train acc 85.11%, f1 0.8509, precision 0.8504, recall 0.8513, auc 0.8511
epoch 5801, loss 0.3870, train acc 85.09%, f1 0.8503, precision 0.8523, recall 0.8482, auc 0.8509
epoch 5901, loss 0.3274, train acc 85.03%, f1 0.8495, precision 0.8522, recall 0.8468, auc 0.8502
epoch 6001, loss 0.3925, train acc 85.11%, f1 0.8504, precision 0.8529, recall 0.8479, auc 0.8511
epoch 6101, loss 0.2738, train acc 85.15%, f1 0.8513, precision 0.8506, recall 0.8521, auc 0.8515
epoch 6201, loss 0.3235, train acc 85.13%, f1 0.8506, precision 0.8529, recall 0.8483, auc 0.8513
epoch 6301, loss 0.3659, train acc 85.22%, f1 0.8519, precision 0.8516, recall 0.8522, auc 0.8522
epoch 6401, loss 0.3081, train acc 85.10%, f1 0.8512, precision 0.8487, recall 0.8536, auc 0.8510
epoch 6501, loss 0.3339, train acc 85.13%, f1 0.8511, precision 0.8509, recall 0.8513, auc 0.8513
epoch 6601, loss 0.3142, train acc 85.13%, f1 0.8513, precision 0.8499, recall 0.8526, auc 0.8513
epoch 6701, loss 0.3381, train acc 85.21%, f1 0.8521, precision 0.8506, recall 0.8535, auc 0.8521
epoch 6801, loss 0.2807, train acc 85.17%, f1 0.8512, precision 0.8525, recall 0.8500, auc 0.8517
epoch 6901, loss 0.3351, train acc 85.14%, f1 0.8506, precision 0.8534, recall 0.8478, auc 0.8514
epoch 7001, loss 0.3507, train acc 85.19%, f1 0.8516, precision 0.8513, recall 0.8519, auc 0.8519
epoch 7101, loss 0.3468, train acc 85.26%, f1 0.8523, precision 0.8527, recall 0.8518, auc 0.8526
epoch 7201, loss 0.3078, train acc 85.20%, f1 0.8521, precision 0.8497, recall 0.8546, auc 0.8520
epoch 7301, loss 0.2699, train acc 85.25%, f1 0.8522, precision 0.8523, recall 0.8522, auc 0.8525
epoch 7401, loss 0.3346, train acc 85.24%, f1 0.8514, precision 0.8555, recall 0.8473, auc 0.8524
epoch 7501, loss 0.2900, train acc 85.25%, f1 0.8521, precision 0.8528, recall 0.8515, auc 0.8525
epoch 7601, loss 0.2722, train acc 85.23%, f1 0.8521, precision 0.8519, recall 0.8522, auc 0.8523
epoch 7701, loss 0.4882, train acc 85.23%, f1 0.8520, precision 0.8518, recall 0.8523, auc 0.8523
epoch 7801, loss 0.3158, train acc 85.27%, f1 0.8520, precision 0.8542, recall 0.8498, auc 0.8527
epoch 7901, loss 0.3047, train acc 85.28%, f1 0.8526, precision 0.8521, recall 0.8532, auc 0.8528
epoch 8001, loss 0.3861, train acc 85.30%, f1 0.8531, precision 0.8513, recall 0.8549, auc 0.8531
epoch 8101, loss 0.3473, train acc 85.29%, f1 0.8525, precision 0.8531, recall 0.8519, auc 0.8529
epoch 8201, loss 0.3638, train acc 85.34%, f1 0.8531, precision 0.8533, recall 0.8530, auc 0.8534/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2360, train acc 85.33%, f1 0.8533, precision 0.8519, recall 0.8547, auc 0.8534
epoch 8401, loss 0.3664, train acc 85.36%, f1 0.8531, precision 0.8542, recall 0.8521, auc 0.8536
epoch 8501, loss 0.3891, train acc 85.37%, f1 0.8535, precision 0.8533, recall 0.8536, auc 0.8537
epoch 8601, loss 0.3889, train acc 85.35%, f1 0.8534, precision 0.8523, recall 0.8545, auc 0.8535
epoch 8701, loss 0.3251, train acc 85.36%, f1 0.8537, precision 0.8516, recall 0.8559, auc 0.8536
epoch 8801, loss 0.2371, train acc 85.37%, f1 0.8533, precision 0.8539, recall 0.8526, auc 0.8537
epoch 8901, loss 0.2308, train acc 85.34%, f1 0.8529, precision 0.8537, recall 0.8522, auc 0.8534
epoch 9001, loss 0.3222, train acc 85.46%, f1 0.8546, precision 0.8532, recall 0.8559, auc 0.8546
epoch 9101, loss 0.3637, train acc 85.43%, f1 0.8542, precision 0.8533, recall 0.8552, auc 0.8543
epoch 9201, loss 0.2246, train acc 85.48%, f1 0.8548, precision 0.8531, recall 0.8564, auc 0.8548
epoch 9301, loss 0.3085, train acc 85.47%, f1 0.8546, precision 0.8535, recall 0.8556, auc 0.8547
epoch 9401, loss 0.3931, train acc 85.47%, f1 0.8550, precision 0.8516, recall 0.8583, auc 0.8547
epoch 9501, loss 0.4445, train acc 85.47%, f1 0.8543, precision 0.8550, recall 0.8535, auc 0.8547
epoch 9601, loss 0.3077, train acc 85.52%, f1 0.8555, precision 0.8520, recall 0.8590, auc 0.8552
epoch 9701, loss 0.2806, train acc 85.51%, f1 0.8549, precision 0.8543, recall 0.8556, auc 0.8551
epoch 9801, loss 0.3188, train acc 85.53%, f1 0.8551, precision 0.8545, recall 0.8558, auc 0.8553
epoch 9901, loss 0.2642, train acc 85.55%, f1 0.8552, precision 0.8553, recall 0.8552, auc 0.8555
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_4
./test_pima/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6378301886792452

the Fscore is 0.5853658536585367

the precision is 0.43243243243243246

the recall is 0.9056603773584906

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_4
----------------------



epoch 1, loss 0.6933, train acc 50.38%, f1 0.6700, precision 0.5038, recall 1.0000, auc 0.5000
epoch 101, loss 0.6336, train acc 77.16%, f1 0.7938, precision 0.7281, recall 0.8725, auc 0.7709
epoch 201, loss 0.4275, train acc 80.99%, f1 0.8134, precision 0.8047, recall 0.8222, auc 0.8098
epoch 301, loss 0.4113, train acc 83.21%, f1 0.8328, precision 0.8354, recall 0.8303, auc 0.8321
epoch 401, loss 0.3451, train acc 84.37%, f1 0.8439, precision 0.8492, recall 0.8386, auc 0.8437
epoch 501, loss 0.3467, train acc 84.75%, f1 0.8472, precision 0.8551, recall 0.8395, auc 0.8475
epoch 601, loss 0.3968, train acc 84.87%, f1 0.8487, precision 0.8556, recall 0.8419, auc 0.8488
epoch 701, loss 0.3256, train acc 84.94%, f1 0.8491, precision 0.8573, recall 0.8411, auc 0.8495
epoch 801, loss 0.2594, train acc 85.03%, f1 0.8494, precision 0.8611, recall 0.8381, auc 0.8504
epoch 901, loss 0.2676, train acc 85.05%, f1 0.8500, precision 0.8594, recall 0.8407, auc 0.8506
epoch 1001, loss 0.4699, train acc 85.10%, f1 0.8508, precision 0.8584, recall 0.8434, auc 0.8511
epoch 1101, loss 0.3666, train acc 85.12%, f1 0.8514, precision 0.8567, recall 0.8462, auc 0.8513
epoch 1201, loss 0.3490, train acc 85.11%, f1 0.8509, precision 0.8582, recall 0.8437, auc 0.8511
epoch 1301, loss 0.3442, train acc 85.11%, f1 0.8508, precision 0.8590, recall 0.8428, auc 0.8512
epoch 1401, loss 0.4520, train acc 85.12%, f1 0.8509, precision 0.8588, recall 0.8432, auc 0.8512
epoch 1501, loss 0.4022, train acc 85.07%, f1 0.8506, precision 0.8576, recall 0.8437, auc 0.8508
epoch 1601, loss 0.3040, train acc 85.10%, f1 0.8511, precision 0.8569, recall 0.8453, auc 0.8510
epoch 1701, loss 0.2898, train acc 85.09%, f1 0.8507, precision 0.8586, recall 0.8429, auc 0.8510
epoch 1801, loss 0.3133, train acc 85.07%, f1 0.8506, precision 0.8576, recall 0.8436, auc 0.8507
epoch 1901, loss 0.4301, train acc 85.09%, f1 0.8508, precision 0.8578, recall 0.8438, auc 0.8509
epoch 2001, loss 0.3126, train acc 85.08%, f1 0.8506, precision 0.8581, recall 0.8433, auc 0.8508
epoch 2101, loss 0.2753, train acc 85.10%, f1 0.8506, precision 0.8594, recall 0.8421, auc 0.8511
epoch 2201, loss 0.3545, train acc 85.10%, f1 0.8509, precision 0.8576, recall 0.8443, auc 0.8510
epoch 2301, loss 0.3296, train acc 85.10%, f1 0.8515, precision 0.8552, recall 0.8479, auc 0.8511
epoch 2401, loss 0.3441, train acc 85.07%, f1 0.8508, precision 0.8566, recall 0.8452, auc 0.8508
epoch 2501, loss 0.4251, train acc 85.07%, f1 0.8506, precision 0.8574, recall 0.8440, auc 0.8507
epoch 2601, loss 0.3262, train acc 85.07%, f1 0.8510, precision 0.8561, recall 0.8458, auc 0.8508
epoch 2701, loss 0.2463, train acc 85.08%, f1 0.8512, precision 0.8558, recall 0.8465, auc 0.8509
epoch 2801, loss 0.3312, train acc 85.04%, f1 0.8509, precision 0.8545, recall 0.8474, auc 0.8505
epoch 2901, loss 0.3112, train acc 85.11%, f1 0.8514, precision 0.8558, recall 0.8471, auc 0.8511
epoch 3001, loss 0.2758, train acc 85.08%, f1 0.8510, precision 0.8560, recall 0.8462, auc 0.8508
epoch 3101, loss 0.3407, train acc 85.09%, f1 0.8515, precision 0.8548, recall 0.8482, auc 0.8509
epoch 3201, loss 0.3545, train acc 85.04%, f1 0.8507, precision 0.8552, recall 0.8463, auc 0.8504
epoch 3301, loss 0.3134, train acc 85.11%, f1 0.8516, precision 0.8553, recall 0.8480, auc 0.8512
epoch 3401, loss 0.4314, train acc 85.07%, f1 0.8512, precision 0.8549, recall 0.8475, auc 0.8507
epoch 3501, loss 0.3855, train acc 85.07%, f1 0.8513, precision 0.8542, recall 0.8485, auc 0.8507
epoch 3601, loss 0.2658, train acc 85.11%, f1 0.8515, precision 0.8554, recall 0.8476, auc 0.8511
epoch 3701, loss 0.3393, train acc 85.10%, f1 0.8512, precision 0.8566, recall 0.8459, auc 0.8511
epoch 3801, loss 0.2546, train acc 85.06%, f1 0.8506, precision 0.8569, recall 0.8445, auc 0.8507
epoch 3901, loss 0.3530, train acc 85.08%, f1 0.8513, precision 0.8546, recall 0.8481, auc 0.8508
epoch 4001, loss 0.3302, train acc 85.06%, f1 0.8506, precision 0.8566, recall 0.8448, auc 0.8506
epoch 4101, loss 0.3397, train acc 85.06%, f1 0.8511, precision 0.8550, recall 0.8472, auc 0.8507
epoch 4201, loss 0.3002, train acc 85.10%, f1 0.8517, precision 0.8538, recall 0.8497, auc 0.8510
epoch 4301, loss 0.3549, train acc 85.08%, f1 0.8512, precision 0.8549, recall 0.8475, auc 0.8508
epoch 4401, loss 0.3252, train acc 85.08%, f1 0.8515, precision 0.8539, recall 0.8492, auc 0.8508
epoch 4501, loss 0.3732, train acc 85.05%, f1 0.8510, precision 0.8545, recall 0.8475, auc 0.8505
epoch 4601, loss 0.3966, train acc 85.08%, f1 0.8516, precision 0.8538, recall 0.8494, auc 0.8508
epoch 4701, loss 0.3739, train acc 85.07%, f1 0.8516, precision 0.8529, recall 0.8503, auc 0.8507
epoch 4801, loss 0.2400, train acc 85.07%, f1 0.8509, precision 0.8566, recall 0.8452, auc 0.8508
epoch 4901, loss 0.2224, train acc 85.12%, f1 0.8517, precision 0.8551, recall 0.8484, auc 0.8512
epoch 5001, loss 0.2807, train acc 85.12%, f1 0.8518, precision 0.8551, recall 0.8484, auc 0.8513
epoch 5101, loss 0.3866, train acc 85.05%, f1 0.8513, precision 0.8533, recall 0.8494, auc 0.8506
epoch 5201, loss 0.3280, train acc 85.09%, f1 0.8514, precision 0.8553, recall 0.8475, auc 0.8510
epoch 5301, loss 0.3720, train acc 85.11%, f1 0.8518, precision 0.8543, recall 0.8493, auc 0.8511
epoch 5401, loss 0.2954, train acc 85.06%, f1 0.8513, precision 0.8538, recall 0.8488, auc 0.8506
epoch 5501, loss 0.4311, train acc 85.09%, f1 0.8514, precision 0.8547, recall 0.8481, auc 0.8509
epoch 5601, loss 0.2377, train acc 85.05%, f1 0.8507, precision 0.8558, recall 0.8457, auc 0.8505
epoch 5701, loss 0.3847, train acc 85.08%, f1 0.8517, precision 0.8530, recall 0.8504, auc 0.8508
epoch 5801, loss 0.3764, train acc 85.11%, f1 0.8519, precision 0.8535, recall 0.8504, auc 0.8511
epoch 5901, loss 0.4885, train acc 85.10%, f1 0.8517, precision 0.8539, recall 0.8496, auc 0.8510
epoch 6001, loss 0.4049, train acc 85.13%, f1 0.8518, precision 0.8555, recall 0.8480, auc 0.8513
epoch 6101, loss 0.2512, train acc 85.11%, f1 0.8515, precision 0.8552, recall 0.8479, auc 0.8511
epoch 6201, loss 0.3989, train acc 85.12%, f1 0.8514, precision 0.8566, recall 0.8462, auc 0.8512
epoch 6301, loss 0.2282, train acc 85.12%, f1 0.8516, precision 0.8554, recall 0.8479, auc 0.8512
epoch 6401, loss 0.4957, train acc 85.11%, f1 0.8517, precision 0.8546, recall 0.8489, auc 0.8512
epoch 6501, loss 0.4682, train acc 85.11%, f1 0.8516, precision 0.8549, recall 0.8483, auc 0.8511
epoch 6601, loss 0.3779, train acc 85.10%, f1 0.8511, precision 0.8567, recall 0.8455, auc 0.8510
epoch 6701, loss 0.3765, train acc 85.10%, f1 0.8512, precision 0.8563, recall 0.8462, auc 0.8510
epoch 6801, loss 0.5282, train acc 85.11%, f1 0.8517, precision 0.8546, recall 0.8489, auc 0.8511
epoch 6901, loss 0.4324, train acc 85.12%, f1 0.8517, precision 0.8547, recall 0.8488, auc 0.8512
epoch 7001, loss 0.2748, train acc 85.10%, f1 0.8512, precision 0.8564, recall 0.8461, auc 0.8510
epoch 7101, loss 0.3497, train acc 85.15%, f1 0.8520, precision 0.8558, recall 0.8481, auc 0.8516
epoch 7201, loss 0.2453, train acc 85.18%, f1 0.8522, precision 0.8561, recall 0.8483, auc 0.8518
epoch 7301, loss 0.4270, train acc 85.18%, f1 0.8524, precision 0.8555, recall 0.8494, auc 0.8518
epoch 7401, loss 0.3018, train acc 85.11%, f1 0.8519, precision 0.8538, recall 0.8500, auc 0.8511
epoch 7501, loss 0.2883, train acc 85.20%, f1 0.8522, precision 0.8574, recall 0.8471, auc 0.8520
epoch 7601, loss 0.1959, train acc 85.22%, f1 0.8522, precision 0.8585, recall 0.8459, auc 0.8522
epoch 7701, loss 0.3529, train acc 85.15%, f1 0.8517, precision 0.8566, recall 0.8469, auc 0.8515
epoch 7801, loss 0.4011, train acc 85.17%, f1 0.8518, precision 0.8575, recall 0.8461, auc 0.8517
epoch 7901, loss 0.3483, train acc 85.22%, f1 0.8529, precision 0.8549, recall 0.8509, auc 0.8522
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_4
./test_pima/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6078301886792453

the Fscore is 0.5647058823529412

the precision is 0.41025641025641024

the recall is 0.9056603773584906

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_4
----------------------



epoch 1, loss 0.6933, train acc 49.69%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6053, train acc 77.88%, f1 0.7858, precision 0.7659, recall 0.8068, auc 0.7786
epoch 201, loss 0.4647, train acc 81.20%, f1 0.8140, precision 0.8105, recall 0.8175, auc 0.8120
epoch 301, loss 0.3918, train acc 83.31%, f1 0.8345, precision 0.8326, recall 0.8364, auc 0.8331
epoch 401, loss 0.4208, train acc 84.37%, f1 0.8444, precision 0.8461, recall 0.8426, auc 0.8438
epoch 501, loss 0.3173, train acc 84.71%, f1 0.8477, precision 0.8498, recall 0.8456, auc 0.8471
epoch 601, loss 0.2948, train acc 84.90%, f1 0.8495, precision 0.8521, recall 0.8469, auc 0.8491
epoch 701, loss 0.3490, train acc 85.00%, f1 0.8508, precision 0.8515, recall 0.8502, auc 0.8500
epoch 801, loss 0.3169, train acc 85.06%, f1 0.8510, precision 0.8539, recall 0.8482, auc 0.8506
epoch 901, loss 0.3537, train acc 85.07%, f1 0.8511, precision 0.8541, recall 0.8480, auc 0.8507
epoch 1001, loss 0.3596, train acc 85.08%, f1 0.8509, precision 0.8557, recall 0.8462, auc 0.8509
epoch 1101, loss 0.3310, train acc 85.08%, f1 0.8515, precision 0.8527, recall 0.8504, auc 0.8508
epoch 1201, loss 0.4406, train acc 85.09%, f1 0.8514, precision 0.8538, recall 0.8490, auc 0.8509
epoch 1301, loss 0.2095, train acc 85.07%, f1 0.8511, precision 0.8541, recall 0.8482, auc 0.8507
epoch 1401, loss 0.3563, train acc 85.12%, f1 0.8513, precision 0.8557, recall 0.8470, auc 0.8512
epoch 1501, loss 0.3697, train acc 85.07%, f1 0.8514, precision 0.8526, recall 0.8503, auc 0.8507
epoch 1601, loss 0.2985, train acc 85.07%, f1 0.8512, precision 0.8537, recall 0.8487, auc 0.8507
epoch 1701, loss 0.2641, train acc 85.05%, f1 0.8509, precision 0.8536, recall 0.8483, auc 0.8505
epoch 1801, loss 0.3121, train acc 85.08%, f1 0.8514, precision 0.8528, recall 0.8501, auc 0.8508
epoch 1901, loss 0.3758, train acc 85.07%, f1 0.8510, precision 0.8544, recall 0.8476, auc 0.8507
epoch 2001, loss 0.3735, train acc 85.09%, f1 0.8514, precision 0.8538, recall 0.8491, auc 0.8509
epoch 2101, loss 0.3440, train acc 85.04%, f1 0.8507, precision 0.8544, recall 0.8470, auc 0.8504
epoch 2201, loss 0.3797, train acc 85.07%, f1 0.8513, precision 0.8530, recall 0.8496, auc 0.8507
epoch 2301, loss 0.3622, train acc 85.13%, f1 0.8520, precision 0.8532, recall 0.8509, auc 0.8513
epoch 2401, loss 0.2684, train acc 85.12%, f1 0.8516, precision 0.8550, recall 0.8481, auc 0.8513
epoch 2501, loss 0.2563, train acc 85.10%, f1 0.8517, precision 0.8527, recall 0.8507, auc 0.8510
epoch 2601, loss 0.3281, train acc 85.13%, f1 0.8518, precision 0.8544, recall 0.8491, auc 0.8513
epoch 2701, loss 0.2857, train acc 85.08%, f1 0.8516, precision 0.8524, recall 0.8507, auc 0.8508
epoch 2801, loss 0.3158, train acc 85.08%, f1 0.8511, precision 0.8551, recall 0.8471, auc 0.8509
epoch 2901, loss 0.2266, train acc 85.05%, f1 0.8511, precision 0.8526, recall 0.8497, auc 0.8505
epoch 3001, loss 0.3260, train acc 85.05%, f1 0.8510, precision 0.8534, recall 0.8487, auc 0.8505
epoch 3101, loss 0.3677, train acc 85.04%, f1 0.8508, precision 0.8535, recall 0.8481, auc 0.8504
epoch 3201, loss 0.4348, train acc 85.06%, f1 0.8517, precision 0.8506, recall 0.8529, auc 0.8506
epoch 3301, loss 0.4111, train acc 85.06%, f1 0.8512, precision 0.8530, recall 0.8495, auc 0.8506
epoch 3401, loss 0.3039, train acc 85.10%, f1 0.8519, precision 0.8522, recall 0.8515, auc 0.8510
epoch 3501, loss 0.2754, train acc 85.08%, f1 0.8516, precision 0.8520, recall 0.8513, auc 0.8508
epoch 3601, loss 0.3418, train acc 85.09%, f1 0.8516, precision 0.8532, recall 0.8499, auc 0.8509
epoch 3701, loss 0.3215, train acc 85.09%, f1 0.8514, precision 0.8537, recall 0.8491, auc 0.8509
epoch 3801, loss 0.3057, train acc 85.07%, f1 0.8516, precision 0.8515, recall 0.8517, auc 0.8506
epoch 3901, loss 0.4286, train acc 85.10%, f1 0.8522, precision 0.8501, recall 0.8543, auc 0.8509
epoch 4001, loss 0.3945, train acc 85.05%, f1 0.8511, precision 0.8533, recall 0.8489, auc 0.8506
epoch 4101, loss 0.2905, train acc 85.04%, f1 0.8510, precision 0.8531, recall 0.8488, auc 0.8504
epoch 4201, loss 0.3621, train acc 85.05%, f1 0.8511, precision 0.8530, recall 0.8493, auc 0.8505
epoch 4301, loss 0.2966, train acc 85.04%, f1 0.8515, precision 0.8505, recall 0.8525, auc 0.8504
epoch 4401, loss 0.3970, train acc 85.05%, f1 0.8509, precision 0.8534, recall 0.8484, auc 0.8505
epoch 4501, loss 0.3282, train acc 85.09%, f1 0.8517, precision 0.8523, recall 0.8510, auc 0.8509
epoch 4601, loss 0.3505, train acc 85.06%, f1 0.8509, precision 0.8543, recall 0.8475, auc 0.8506
epoch 4701, loss 0.3311, train acc 85.07%, f1 0.8515, precision 0.8522, recall 0.8508, auc 0.8507
epoch 4801, loss 0.2316, train acc 85.07%, f1 0.8511, precision 0.8541, recall 0.8480, auc 0.8507
epoch 4901, loss 0.3815, train acc 85.09%, f1 0.8515, precision 0.8533, recall 0.8497, auc 0.8509
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_4
./test_pima/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6572641509433963

the Fscore is 0.6012269938650308

the precision is 0.44545454545454544

the recall is 0.9245283018867925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_4
----------------------



epoch 1, loss 0.6937, train acc 50.45%, f1 0.6706, precision 0.5045, recall 1.0000, auc 0.5000
epoch 101, loss 0.6289, train acc 78.12%, f1 0.7995, precision 0.7434, recall 0.8648, auc 0.7805
epoch 201, loss 0.4729, train acc 81.81%, f1 0.8218, precision 0.8124, recall 0.8315, auc 0.8180
epoch 301, loss 0.4871, train acc 83.38%, f1 0.8344, precision 0.8389, recall 0.8299, auc 0.8338
epoch 401, loss 0.3200, train acc 84.35%, f1 0.8441, precision 0.8484, recall 0.8397, auc 0.8435
epoch 501, loss 0.4429, train acc 84.75%, f1 0.8479, precision 0.8532, recall 0.8427, auc 0.8475
epoch 601, loss 0.3527, train acc 84.91%, f1 0.8497, precision 0.8539, recall 0.8456, auc 0.8491
epoch 701, loss 0.2641, train acc 84.98%, f1 0.8500, precision 0.8562, recall 0.8439, auc 0.8498
epoch 801, loss 0.3627, train acc 85.06%, f1 0.8512, precision 0.8552, recall 0.8473, auc 0.8506
epoch 901, loss 0.3722, train acc 85.02%, f1 0.8504, precision 0.8570, recall 0.8439, auc 0.8503
epoch 1001, loss 0.3956, train acc 85.08%, f1 0.8513, precision 0.8561, recall 0.8465, auc 0.8508
epoch 1101, loss 0.3937, train acc 85.06%, f1 0.8510, precision 0.8562, recall 0.8458, auc 0.8506
epoch 1201, loss 0.4762, train acc 85.06%, f1 0.8510, precision 0.8568, recall 0.8452, auc 0.8507
epoch 1301, loss 0.2808, train acc 85.05%, f1 0.8512, precision 0.8551, recall 0.8473, auc 0.8506
epoch 1401, loss 0.2032, train acc 85.10%, f1 0.8517, precision 0.8552, recall 0.8483, auc 0.8510
epoch 1501, loss 0.3668, train acc 85.08%, f1 0.8513, precision 0.8558, recall 0.8469, auc 0.8508
epoch 1601, loss 0.3309, train acc 85.02%, f1 0.8512, precision 0.8531, recall 0.8494, auc 0.8503
epoch 1701, loss 0.3252, train acc 85.02%, f1 0.8508, precision 0.8551, recall 0.8466, auc 0.8503
epoch 1801, loss 0.2752, train acc 85.07%, f1 0.8513, precision 0.8554, recall 0.8473, auc 0.8507
epoch 1901, loss 0.2942, train acc 85.06%, f1 0.8512, precision 0.8552, recall 0.8473, auc 0.8506
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_4
./test_pima/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6378301886792452

the Fscore is 0.5853658536585367

the precision is 0.43243243243243246

the recall is 0.9056603773584906

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_4
----------------------



epoch 1, loss 0.6937, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6226, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5942, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5671, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.5423, train acc 67.15%, f1 0.1513, precision 0.7826, recall 0.0837, auc 0.5356
epoch 501, loss 0.5221, train acc 71.54%, f1 0.3636, precision 0.8333, recall 0.2326, auc 0.6038
epoch 601, loss 0.5063, train acc 74.47%, f1 0.4952, precision 0.8021, recall 0.3581, auc 0.6553
epoch 701, loss 0.4940, train acc 76.10%, f1 0.5739, precision 0.7615, recall 0.4605, auc 0.6915
epoch 801, loss 0.4845, train acc 77.56%, f1 0.6209, precision 0.7584, recall 0.5256, auc 0.7178
epoch 901, loss 0.4772, train acc 78.21%, f1 0.6474, precision 0.7455, recall 0.5721, auc 0.7335
epoch 1001, loss 0.4717, train acc 78.05%, f1 0.6494, precision 0.7353, recall 0.5814, auc 0.7344
epoch 1101, loss 0.4676, train acc 79.02%, f1 0.6718, precision 0.7416, recall 0.6140, auc 0.7495
epoch 1201, loss 0.4642, train acc 79.19%, f1 0.6784, precision 0.7377, recall 0.6279, auc 0.7540
epoch 1301, loss 0.4609, train acc 79.02%, f1 0.6783, precision 0.7312, recall 0.6326, auc 0.7538
epoch 1401, loss 0.4566, train acc 78.70%, f1 0.6749, precision 0.7234, recall 0.6326, auc 0.7513
epoch 1501, loss 0.4516, train acc 78.86%, f1 0.6782, precision 0.7249, recall 0.6372, auc 0.7536
epoch 1601, loss 0.4464, train acc 79.35%, f1 0.6833, precision 0.7366, recall 0.6372, auc 0.7574
epoch 1701, loss 0.4417, train acc 79.51%, f1 0.6850, precision 0.7405, recall 0.6372, auc 0.7586
epoch 1801, loss 0.4376, train acc 80.00%, f1 0.6917, precision 0.7500, recall 0.6419, auc 0.7634
epoch 1901, loss 0.4341, train acc 79.51%, f1 0.6850, precision 0.7405, recall 0.6372, auc 0.7586
epoch 2001, loss 0.4310, train acc 79.51%, f1 0.6866, precision 0.7380, recall 0.6419, auc 0.7597
epoch 2101, loss 0.4283, train acc 79.84%, f1 0.6946, precision 0.7382, recall 0.6558, auc 0.7654
epoch 2201, loss 0.4257, train acc 79.67%, f1 0.6959, precision 0.7296, recall 0.6651, auc 0.7663
epoch 2301, loss 0.4234, train acc 79.02%, f1 0.6877, precision 0.7172, recall 0.6605, auc 0.7602
epoch 2401, loss 0.4211, train acc 79.02%, f1 0.6861, precision 0.7194, recall 0.6558, auc 0.7592
epoch 2501, loss 0.4191, train acc 79.19%, f1 0.6878, precision 0.7231, recall 0.6558, auc 0.7604
epoch 2601, loss 0.4171, train acc 79.19%, f1 0.6878, precision 0.7231, recall 0.6558, auc 0.7604
epoch 2701, loss 0.4152, train acc 79.35%, f1 0.6910, precision 0.7245, recall 0.6605, auc 0.7627
epoch 2801, loss 0.4131, train acc 79.51%, f1 0.6942, precision 0.7259, recall 0.6651, auc 0.7651
epoch 2901, loss 0.4109, train acc 79.35%, f1 0.6925, precision 0.7222, recall 0.6651, auc 0.7638
epoch 3001, loss 0.4087, train acc 80.16%, f1 0.7039, precision 0.7360, recall 0.6744, auc 0.7722
epoch 3101, loss 0.4066, train acc 80.16%, f1 0.7039, precision 0.7360, recall 0.6744, auc 0.7722
epoch 3201, loss 0.4044, train acc 80.00%, f1 0.7007, precision 0.7347, recall 0.6698, auc 0.7699
epoch 3301, loss 0.4023, train acc 79.84%, f1 0.7005, precision 0.7286, recall 0.6744, auc 0.7697
epoch 3401, loss 0.4003, train acc 80.16%, f1 0.7067, precision 0.7313, recall 0.6837, auc 0.7744
epoch 3501, loss 0.3983, train acc 80.65%, f1 0.7146, precision 0.7376, recall 0.6930, auc 0.7803
epoch 3601, loss 0.3965, train acc 80.65%, f1 0.7146, precision 0.7376, recall 0.6930, auc 0.7803
epoch 3701, loss 0.3947, train acc 80.33%, f1 0.7084, precision 0.7350, recall 0.6837, auc 0.7756
epoch 3801, loss 0.3930, train acc 80.33%, f1 0.7084, precision 0.7350, recall 0.6837, auc 0.7756
epoch 3901, loss 0.3914, train acc 79.84%, f1 0.6990, precision 0.7310, recall 0.6698, auc 0.7686
epoch 4001, loss 0.3898, train acc 79.84%, f1 0.6990, precision 0.7310, recall 0.6698, auc 0.7686
epoch 4101, loss 0.3883, train acc 80.00%, f1 0.7022, precision 0.7323, recall 0.6744, auc 0.7710
epoch 4201, loss 0.3867, train acc 80.33%, f1 0.7084, precision 0.7350, recall 0.6837, auc 0.7756
epoch 4301, loss 0.3851, train acc 80.16%, f1 0.7053, precision 0.7337, recall 0.6791, auc 0.7733
epoch 4401, loss 0.3834, train acc 80.33%, f1 0.7084, precision 0.7350, recall 0.6837, auc 0.7756
epoch 4501, loss 0.3817, train acc 80.81%, f1 0.7136, precision 0.7462, recall 0.6837, auc 0.7794
epoch 4601, loss 0.3799, train acc 81.30%, f1 0.7202, precision 0.7551, recall 0.6884, auc 0.7842
epoch 4701, loss 0.3782, train acc 81.46%, f1 0.7233, precision 0.7563, recall 0.6930, auc 0.7865
epoch 4801, loss 0.3763, train acc 81.63%, f1 0.7251, precision 0.7602, recall 0.6930, auc 0.7878
epoch 4901, loss 0.3743, train acc 81.79%, f1 0.7295, precision 0.7588, recall 0.7023, auc 0.7912
epoch 5001, loss 0.3720, train acc 81.95%, f1 0.7325, precision 0.7600, recall 0.7070, auc 0.7935
epoch 5101, loss 0.3692, train acc 82.28%, f1 0.7386, precision 0.7624, recall 0.7163, auc 0.7981
epoch 5201, loss 0.3668, train acc 82.11%, f1 0.7368, precision 0.7586, recall 0.7163, auc 0.7969
epoch 5301, loss 0.3646, train acc 82.44%, f1 0.7429, precision 0.7610, recall 0.7256, auc 0.8015
epoch 5401, loss 0.3624, train acc 82.76%, f1 0.7488, precision 0.7633, recall 0.7349, auc 0.8062
epoch 5501, loss 0.3601, train acc 82.76%, f1 0.7476, precision 0.7659, recall 0.7302, auc 0.8051
epoch 5601, loss 0.3578, train acc 83.41%, f1 0.7548, precision 0.7811, recall 0.7302, auc 0.8101
epoch 5701, loss 0.3551, train acc 83.74%, f1 0.7585, precision 0.7889, recall 0.7302, auc 0.8126
epoch 5801, loss 0.3517, train acc 83.74%, f1 0.7585, precision 0.7889, recall 0.7302, auc 0.8126
epoch 5901, loss 0.3476, train acc 84.23%, f1 0.7663, precision 0.7950, recall 0.7395, auc 0.8185
epoch 6001, loss 0.3428, train acc 84.72%, f1 0.7751, precision 0.7980, recall 0.7535, auc 0.8255
epoch 6101, loss 0.3367, train acc 85.04%, f1 0.7799, precision 0.8030, recall 0.7581, auc 0.8291
epoch 6201, loss 0.3317, train acc 84.72%, f1 0.7762, precision 0.7951, recall 0.7581, auc 0.8266
epoch 6301, loss 0.3274, train acc 84.72%, f1 0.7762, precision 0.7951, recall 0.7581, auc 0.8266
epoch 6401, loss 0.3232, train acc 84.88%, f1 0.7780, precision 0.7990, recall 0.7581, auc 0.8278
epoch 6501, loss 0.3190, train acc 85.04%, f1 0.7799, precision 0.8030, recall 0.7581, auc 0.8291
epoch 6601, loss 0.3149, train acc 85.53%, f1 0.7876, precision 0.8088, recall 0.7674, auc 0.8350
epoch 6701, loss 0.3114, train acc 86.34%, f1 0.8000, precision 0.8195, recall 0.7814, auc 0.8444
epoch 6801, loss 0.3082, train acc 86.50%, f1 0.8029, precision 0.8204, recall 0.7860, auc 0.8468
epoch 6901, loss 0.3052, train acc 86.83%, f1 0.8076, precision 0.8252, recall 0.7907, auc 0.8503
epoch 7001, loss 0.3024, train acc 86.99%, f1 0.8104, precision 0.8261, recall 0.7953, auc 0.8527
epoch 7101, loss 0.2998, train acc 86.99%, f1 0.8095, precision 0.8293, recall 0.7907, auc 0.8516
epoch 7201, loss 0.2973, train acc 86.99%, f1 0.8095, precision 0.8293, recall 0.7907, auc 0.8516
epoch 7301, loss 0.2950, train acc 87.64%, f1 0.8199, precision 0.8357, recall 0.8047, auc 0.8598
epoch 7401, loss 0.2928, train acc 87.64%, f1 0.8199, precision 0.8357, recall 0.8047, auc 0.8598
epoch 7501, loss 0.2907, train acc 87.64%, f1 0.8199, precision 0.8357, recall 0.8047, auc 0.8598
epoch 7601, loss 0.2887, train acc 87.48%, f1 0.8180, precision 0.8317, recall 0.8047, auc 0.8586
epoch 7701, loss 0.2860, train acc 88.13%, f1 0.8282, precision 0.8381, recall 0.8186, auc 0.8668
epoch 7801, loss 0.2836, train acc 87.97%, f1 0.8255, precision 0.8373, recall 0.8140, auc 0.8645
epoch 7901, loss 0.2814, train acc 87.97%, f1 0.8255, precision 0.8373, recall 0.8140, auc 0.8645
epoch 8001, loss 0.2795, train acc 88.13%, f1 0.8282, precision 0.8381, recall 0.8186, auc 0.8668
epoch 8101, loss 0.2776, train acc 88.29%, f1 0.8310, precision 0.8389, recall 0.8233, auc 0.8691
epoch 8201, loss 0.2758, train acc 88.29%, f1 0.8302, precision 0.8421, recall 0.8186, auc 0.8681
epoch 8301, loss 0.2740, train acc 88.29%, f1 0.8302, precision 0.8421, recall 0.8186, auc 0.8681
epoch 8401, loss 0.2722, train acc 88.29%, f1 0.8302, precision 0.8421, recall 0.8186, auc 0.8681
epoch 8501, loss 0.2703, train acc 88.29%, f1 0.8302, precision 0.8421, recall 0.8186, auc 0.8681
epoch 8601, loss 0.2685, train acc 88.46%, f1 0.8329, precision 0.8429, recall 0.8233, auc 0.8704
epoch 8701, loss 0.2666, train acc 88.78%, f1 0.8376, precision 0.8476, recall 0.8279, auc 0.8740
epoch 8801, loss 0.2648, train acc 88.94%, f1 0.8396, precision 0.8517, recall 0.8279, auc 0.8752
epoch 8901, loss 0.2630, train acc 88.94%, f1 0.8396, precision 0.8517, recall 0.8279, auc 0.8752
epoch 9001, loss 0.2613, train acc 88.94%, f1 0.8396, precision 0.8517, recall 0.8279, auc 0.8752
epoch 9101, loss 0.2596, train acc 89.27%, f1 0.8443, precision 0.8565, recall 0.8326, auc 0.8788
epoch 9201, loss 0.2579, train acc 89.27%, f1 0.8443, precision 0.8565, recall 0.8326, auc 0.8788
epoch 9301, loss 0.2563, train acc 89.43%, f1 0.8463, precision 0.8606, recall 0.8326, auc 0.8800
epoch 9401, loss 0.2548, train acc 89.43%, f1 0.8463, precision 0.8606, recall 0.8326, auc 0.8800
epoch 9501, loss 0.2533, train acc 89.59%, f1 0.8491, precision 0.8612, recall 0.8372, auc 0.8824
epoch 9601, loss 0.2520, train acc 89.76%, f1 0.8518, precision 0.8619, recall 0.8419, auc 0.8847
epoch 9701, loss 0.2507, train acc 89.59%, f1 0.8498, precision 0.8578, recall 0.8419, auc 0.8834
epoch 9801, loss 0.2494, train acc 89.76%, f1 0.8525, precision 0.8585, recall 0.8465, auc 0.8858
epoch 9901, loss 0.2482, train acc 89.76%, f1 0.8525, precision 0.8585, recall 0.8465, auc 0.8858
epoch 10001, loss 0.2470, train acc 89.76%, f1 0.8525, precision 0.8585, recall 0.8465, auc 0.8858
epoch 10101, loss 0.2459, train acc 89.76%, f1 0.8525, precision 0.8585, recall 0.8465, auc 0.8858
epoch 10201, loss 0.2447, train acc 89.76%, f1 0.8525, precision 0.8585, recall 0.8465, auc 0.8858
epoch 10301, loss 0.2436, train acc 89.59%, f1 0.8505, precision 0.8545, recall 0.8465, auc 0.8845
epoch 10401, loss 0.2426, train acc 89.76%, f1 0.8531, precision 0.8551, recall 0.8512, auc 0.8868
epoch 10501, loss 0.2415, train acc 89.76%, f1 0.8531, precision 0.8551, recall 0.8512, auc 0.8868
epoch 10601, loss 0.2405, train acc 89.76%, f1 0.8531, precision 0.8551, recall 0.8512, auc 0.8868
epoch 10701, loss 0.2395, train acc 89.92%, f1 0.8558, precision 0.8558, recall 0.8558, auc 0.8892
epoch 10801, loss 0.2385, train acc 89.92%, f1 0.8558, precision 0.8558, recall 0.8558, auc 0.8892
epoch 10901, loss 0.2376, train acc 89.76%, f1 0.8538, precision 0.8519, recall 0.8558, auc 0.8879
epoch 11001, loss 0.2366, train acc 89.92%, f1 0.8565, precision 0.8525, recall 0.8605, auc 0.8902
epoch 11101, loss 0.2357, train acc 89.92%, f1 0.8565, precision 0.8525, recall 0.8605, auc 0.8902
epoch 11201, loss 0.2345, train acc 89.76%, f1 0.8538, precision 0.8519, recall 0.8558, auc 0.8879
epoch 11301, loss 0.2335, train acc 89.92%, f1 0.8558, precision 0.8558, recall 0.8558, auc 0.8892
epoch 11401, loss 0.2326, train acc 89.92%, f1 0.8558, precision 0.8558, recall 0.8558, auc 0.8892
epoch 11501, loss 0.2317, train acc 90.08%, f1 0.8578, precision 0.8598, recall 0.8558, auc 0.8904
epoch 11601, loss 0.2308, train acc 90.08%, f1 0.8578, precision 0.8598, recall 0.8558, auc 0.8904
epoch 11701, loss 0.2300, train acc 90.24%, f1 0.8605, precision 0.8605, recall 0.8605, auc 0.8927
epoch 11801, loss 0.2292, train acc 90.41%, f1 0.8625, precision 0.8645, recall 0.8605, auc 0.8940
epoch 11901, loss 0.2280, train acc 89.76%, f1 0.8525, precision 0.8585, recall 0.8465, auc 0.8858
epoch 12001, loss 0.2271, train acc 89.76%, f1 0.8525, precision 0.8585, recall 0.8465, auc 0.8858
epoch 12101, loss 0.2263, train acc 90.24%, f1 0.8598, precision 0.8638, recall 0.8558, auc 0.8917
epoch 12201, loss 0.2255, train acc 90.24%, f1 0.8598, precision 0.8638, recall 0.8558, auc 0.8917
epoch 12301, loss 0.2247, train acc 90.24%, f1 0.8598, precision 0.8638, recall 0.8558, auc 0.8917
epoch 12401, loss 0.2240, train acc 90.41%, f1 0.8618, precision 0.8679, recall 0.8558, auc 0.8929
epoch 12501, loss 0.2233, train acc 90.41%, f1 0.8618, precision 0.8679, recall 0.8558, auc 0.8929
epoch 12601, loss 0.2226, train acc 90.41%, f1 0.8618, precision 0.8679, recall 0.8558, auc 0.8929
epoch 12701, loss 0.2218, train acc 90.41%, f1 0.8618, precision 0.8679, recall 0.8558, auc 0.8929
epoch 12801, loss 0.2209, train acc 90.24%, f1 0.8592, precision 0.8673, recall 0.8512, auc 0.8906
epoch 12901, loss 0.2198, train acc 90.24%, f1 0.8592, precision 0.8673, recall 0.8512, auc 0.8906
epoch 13001, loss 0.2189, train acc 90.24%, f1 0.8592, precision 0.8673, recall 0.8512, auc 0.8906
epoch 13101, loss 0.2181, train acc 90.41%, f1 0.8612, precision 0.8714, recall 0.8512, auc 0.8918
epoch 13201, loss 0.2174, train acc 90.24%, f1 0.8592, precision 0.8673, recall 0.8512, auc 0.8906
epoch 13301, loss 0.2168, train acc 90.73%, f1 0.8665, precision 0.8726, recall 0.8605, auc 0.8965
epoch 13401, loss 0.2160, train acc 90.73%, f1 0.8671, precision 0.8692, recall 0.8651, auc 0.8976
epoch 13501, loss 0.2153, train acc 91.06%, f1 0.8724, precision 0.8704, recall 0.8744, auc 0.9022
epoch 13601, loss 0.2147, train acc 91.22%, f1 0.8744, precision 0.8744, recall 0.8744, auc 0.9035
epoch 13701, loss 0.2140, train acc 91.38%, f1 0.8765, precision 0.8785, recall 0.8744, auc 0.9047
epoch 13801, loss 0.2134, train acc 91.38%, f1 0.8765, precision 0.8785, recall 0.8744, auc 0.9047
epoch 13901, loss 0.2127, train acc 91.38%, f1 0.8765, precision 0.8785, recall 0.8744, auc 0.9047
epoch 14001, loss 0.2121, train acc 91.38%, f1 0.8770, precision 0.8750, recall 0.8791, auc 0.9058
epoch 14101, loss 0.2114, train acc 91.38%, f1 0.8770, precision 0.8750, recall 0.8791, auc 0.9058
epoch 14201, loss 0.2108, train acc 91.38%, f1 0.8770, precision 0.8750, recall 0.8791, auc 0.9058
epoch 14301, loss 0.2102, train acc 91.38%, f1 0.8770, precision 0.8750, recall 0.8791, auc 0.9058
epoch 14401, loss 0.2095, train acc 91.38%, f1 0.8770, precision 0.8750, recall 0.8791, auc 0.9058
epoch 14501, loss 0.2089, train acc 91.38%, f1 0.8776, precision 0.8716, recall 0.8837, auc 0.9069
epoch 14601, loss 0.2083, train acc 91.38%, f1 0.8776, precision 0.8716, recall 0.8837, auc 0.9069
epoch 14701, loss 0.2074, train acc 91.06%, f1 0.8730, precision 0.8670, recall 0.8791, auc 0.9033
epoch 14801, loss 0.2061, train acc 90.89%, f1 0.8710, precision 0.8630, recall 0.8791, auc 0.9020
epoch 14901, loss 0.2053, train acc 90.89%, f1 0.8710, precision 0.8630, recall 0.8791, auc 0.9020
epoch 15001, loss 0.2047, train acc 90.89%, f1 0.8710, precision 0.8630, recall 0.8791, auc 0.9020
epoch 15101, loss 0.2041, train acc 90.89%, f1 0.8710, precision 0.8630, recall 0.8791, auc 0.9020
epoch 15201, loss 0.2035, train acc 90.89%, f1 0.8710, precision 0.8630, recall 0.8791, auc 0.9020
epoch 15301, loss 0.2030, train acc 90.89%, f1 0.8710, precision 0.8630, recall 0.8791, auc 0.9020
epoch 15401, loss 0.2024, train acc 91.22%, f1 0.8756, precision 0.8676, recall 0.8837, auc 0.9056
epoch 15501, loss 0.2019, train acc 91.22%, f1 0.8756, precision 0.8676, recall 0.8837, auc 0.9056
epoch 15601, loss 0.2013, train acc 91.22%, f1 0.8756, precision 0.8676, recall 0.8837, auc 0.9056
epoch 15701, loss 0.2008, train acc 91.54%, f1 0.8807, precision 0.8688, recall 0.8930, auc 0.9103
epoch 15801, loss 0.2003, train acc 91.54%, f1 0.8807, precision 0.8688, recall 0.8930, auc 0.9103
epoch 15901, loss 0.1998, train acc 91.38%, f1 0.8787, precision 0.8649, recall 0.8930, auc 0.9090
epoch 16001, loss 0.1993, train acc 91.38%, f1 0.8787, precision 0.8649, recall 0.8930, auc 0.9090
epoch 16101, loss 0.1988, train acc 91.38%, f1 0.8787, precision 0.8649, recall 0.8930, auc 0.9090
epoch 16201, loss 0.1983, train acc 91.38%, f1 0.8787, precision 0.8649, recall 0.8930, auc 0.9090
epoch 16301, loss 0.1978, train acc 91.38%, f1 0.8787, precision 0.8649, recall 0.8930, auc 0.9090
epoch 16401, loss 0.1973, train acc 91.22%, f1 0.8761, precision 0.8643, recall 0.8884, auc 0.9067
epoch 16501, loss 0.1968, train acc 91.22%, f1 0.8761, precision 0.8643, recall 0.8884, auc 0.9067/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.1964, train acc 91.22%, f1 0.8761, precision 0.8643, recall 0.8884, auc 0.9067
epoch 16701, loss 0.1959, train acc 91.38%, f1 0.8787, precision 0.8649, recall 0.8930, auc 0.9090
epoch 16801, loss 0.1955, train acc 91.54%, f1 0.8807, precision 0.8688, recall 0.8930, auc 0.9103
epoch 16901, loss 0.1950, train acc 91.54%, f1 0.8807, precision 0.8688, recall 0.8930, auc 0.9103
epoch 17001, loss 0.1945, train acc 91.54%, f1 0.8807, precision 0.8688, recall 0.8930, auc 0.9103
epoch 17101, loss 0.1940, train acc 92.03%, f1 0.8884, precision 0.8705, recall 0.9070, auc 0.9172
epoch 17201, loss 0.1935, train acc 92.03%, f1 0.8884, precision 0.8705, recall 0.9070, auc 0.9172
epoch 17301, loss 0.1930, train acc 92.03%, f1 0.8884, precision 0.8705, recall 0.9070, auc 0.9172
epoch 17401, loss 0.1926, train acc 92.03%, f1 0.8884, precision 0.8705, recall 0.9070, auc 0.9172
epoch 17501, loss 0.1922, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 17601, loss 0.1917, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 17701, loss 0.1913, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 17801, loss 0.1909, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 17901, loss 0.1905, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18001, loss 0.1901, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18101, loss 0.1897, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18201, loss 0.1893, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18301, loss 0.1889, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18401, loss 0.1885, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18501, loss 0.1881, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18601, loss 0.1877, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18701, loss 0.1873, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18801, loss 0.1870, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 18901, loss 0.1866, train acc 92.03%, f1 0.8879, precision 0.8739, recall 0.9023, auc 0.9162
epoch 19001, loss 0.1862, train acc 92.03%, f1 0.8879, precision 0.8739, recall 0.9023, auc 0.9162
epoch 19101, loss 0.1858, train acc 92.03%, f1 0.8879, precision 0.8739, recall 0.9023, auc 0.9162
epoch 19201, loss 0.1854, train acc 92.03%, f1 0.8879, precision 0.8739, recall 0.9023, auc 0.9162
epoch 19301, loss 0.1851, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 19401, loss 0.1847, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 19501, loss 0.1843, train acc 92.20%, f1 0.8904, precision 0.8744, recall 0.9070, auc 0.9185
epoch 19601, loss 0.1840, train acc 92.36%, f1 0.8929, precision 0.8750, recall 0.9116, auc 0.9208
epoch 19701, loss 0.1836, train acc 92.36%, f1 0.8929, precision 0.8750, recall 0.9116, auc 0.9208
epoch 19801, loss 0.1833, train acc 92.36%, f1 0.8929, precision 0.8750, recall 0.9116, auc 0.9208
epoch 19901, loss 0.1829, train acc 92.52%, f1 0.8955, precision 0.8756, recall 0.9163, auc 0.9231
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_normal_20000
normal
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_4
./test_pima/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.6747169811320755

the Fscore is 0.5625

the precision is 0.627906976744186

the recall is 0.5094339622641509

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_4
----------------------



epoch 1, loss 0.6946, train acc 37.40%, f1 0.5157, precision 0.3534, recall 0.9535, auc 0.5080
epoch 101, loss 0.6231, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5946, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5677, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.5429, train acc 67.15%, f1 0.1441, precision 0.8095, recall 0.0791, auc 0.5345
epoch 501, loss 0.5227, train acc 71.71%, f1 0.3650, precision 0.8475, recall 0.2326, auc 0.6050
epoch 601, loss 0.5068, train acc 74.63%, f1 0.4968, precision 0.8105, recall 0.3581, auc 0.6566
epoch 701, loss 0.4943, train acc 76.10%, f1 0.5739, precision 0.7615, recall 0.4605, auc 0.6915
epoch 801, loss 0.4847, train acc 77.56%, f1 0.6209, precision 0.7584, recall 0.5256, auc 0.7178
epoch 901, loss 0.4774, train acc 78.21%, f1 0.6474, precision 0.7455, recall 0.5721, auc 0.7335
epoch 1001, loss 0.4719, train acc 78.05%, f1 0.6494, precision 0.7353, recall 0.5814, auc 0.7344
epoch 1101, loss 0.4678, train acc 79.02%, f1 0.6718, precision 0.7416, recall 0.6140, auc 0.7495
epoch 1201, loss 0.4646, train acc 79.02%, f1 0.6767, precision 0.7337, recall 0.6279, auc 0.7527
epoch 1301, loss 0.4618, train acc 79.19%, f1 0.6832, precision 0.7302, recall 0.6419, auc 0.7572
epoch 1401, loss 0.4586, train acc 78.70%, f1 0.6765, precision 0.7211, recall 0.6372, auc 0.7524
epoch 1501, loss 0.4543, train acc 79.02%, f1 0.6815, precision 0.7263, recall 0.6419, auc 0.7559
epoch 1601, loss 0.4494, train acc 79.19%, f1 0.6847, precision 0.7277, recall 0.6465, auc 0.7583
epoch 1701, loss 0.4443, train acc 79.02%, f1 0.6783, precision 0.7312, recall 0.6326, auc 0.7538
epoch 1801, loss 0.4395, train acc 79.84%, f1 0.6869, precision 0.7514, recall 0.6326, auc 0.7600
epoch 1901, loss 0.4352, train acc 79.84%, f1 0.6915, precision 0.7433, recall 0.6465, auc 0.7633
epoch 2001, loss 0.4315, train acc 79.67%, f1 0.6929, precision 0.7344, recall 0.6558, auc 0.7642
epoch 2101, loss 0.4283, train acc 79.35%, f1 0.6895, precision 0.7268, recall 0.6558, auc 0.7617
epoch 2201, loss 0.4253, train acc 79.02%, f1 0.6877, precision 0.7172, recall 0.6605, auc 0.7602
epoch 2301, loss 0.4226, train acc 79.02%, f1 0.6877, precision 0.7172, recall 0.6605, auc 0.7602
epoch 2401, loss 0.4201, train acc 79.02%, f1 0.6861, precision 0.7194, recall 0.6558, auc 0.7592
epoch 2501, loss 0.4176, train acc 79.02%, f1 0.6861, precision 0.7194, recall 0.6558, auc 0.7592
epoch 2601, loss 0.4149, train acc 79.19%, f1 0.6893, precision 0.7208, recall 0.6605, auc 0.7615
epoch 2701, loss 0.4123, train acc 79.51%, f1 0.6957, precision 0.7236, recall 0.6698, auc 0.7661
epoch 2801, loss 0.4101, train acc 79.67%, f1 0.6988, precision 0.7250, recall 0.6744, auc 0.7685
epoch 2901, loss 0.4081, train acc 79.67%, f1 0.7002, precision 0.7228, recall 0.6791, auc 0.7695
epoch 3001, loss 0.4062, train acc 80.16%, f1 0.7067, precision 0.7313, recall 0.6837, auc 0.7744
epoch 3101, loss 0.4044, train acc 80.33%, f1 0.7084, precision 0.7350, recall 0.6837, auc 0.7756
epoch 3201, loss 0.4025, train acc 80.16%, f1 0.7053, precision 0.7337, recall 0.6791, auc 0.7733
epoch 3301, loss 0.4004, train acc 80.33%, f1 0.7084, precision 0.7350, recall 0.6837, auc 0.7756
epoch 3401, loss 0.3983, train acc 80.98%, f1 0.7181, precision 0.7450, recall 0.6930, auc 0.7828
epoch 3501, loss 0.3964, train acc 80.81%, f1 0.7150, precision 0.7437, recall 0.6884, auc 0.7804
epoch 3601, loss 0.3945, train acc 80.81%, f1 0.7177, precision 0.7389, recall 0.6977, auc 0.7826
epoch 3701, loss 0.3927, train acc 80.65%, f1 0.7133, precision 0.7400, recall 0.6884, auc 0.7792
epoch 3801, loss 0.3909, train acc 80.81%, f1 0.7163, precision 0.7413, recall 0.6930, auc 0.7815
epoch 3901, loss 0.3890, train acc 81.14%, f1 0.7225, precision 0.7438, recall 0.7023, auc 0.7862
epoch 4001, loss 0.3870, train acc 81.63%, f1 0.7316, precision 0.7476, recall 0.7163, auc 0.7931
epoch 4101, loss 0.3848, train acc 81.63%, f1 0.7316, precision 0.7476, recall 0.7163, auc 0.7931
epoch 4201, loss 0.3825, train acc 82.28%, f1 0.7411, precision 0.7573, recall 0.7256, auc 0.8003
epoch 4301, loss 0.3798, train acc 82.44%, f1 0.7416, precision 0.7635, recall 0.7209, auc 0.8005
epoch 4401, loss 0.3766, train acc 82.60%, f1 0.7446, precision 0.7647, recall 0.7256, auc 0.8028
epoch 4501, loss 0.3731, train acc 83.09%, f1 0.7512, precision 0.7734, recall 0.7302, auc 0.8076
epoch 4601, loss 0.3700, train acc 83.58%, f1 0.7589, precision 0.7794, recall 0.7395, auc 0.8135
epoch 4701, loss 0.3670, train acc 83.58%, f1 0.7589, precision 0.7794, recall 0.7395, auc 0.8135
epoch 4801, loss 0.3641, train acc 83.41%, f1 0.7560, precision 0.7783, recall 0.7349, auc 0.8112
epoch 4901, loss 0.3611, train acc 83.90%, f1 0.7626, precision 0.7871, recall 0.7395, auc 0.8160
epoch 5001, loss 0.3580, train acc 84.39%, f1 0.7703, precision 0.7931, recall 0.7488, auc 0.8219
epoch 5101, loss 0.3549, train acc 84.07%, f1 0.7644, precision 0.7910, recall 0.7395, auc 0.8173
epoch 5201, loss 0.3512, train acc 84.55%, f1 0.7733, precision 0.7941, recall 0.7535, auc 0.8242
epoch 5301, loss 0.3469, train acc 84.88%, f1 0.7770, precision 0.8020, recall 0.7535, auc 0.8267
epoch 5401, loss 0.3423, train acc 84.88%, f1 0.7759, precision 0.8050, recall 0.7488, auc 0.8257
epoch 5501, loss 0.3374, train acc 85.20%, f1 0.7807, precision 0.8100, recall 0.7535, auc 0.8292
epoch 5601, loss 0.3311, train acc 85.04%, f1 0.7767, precision 0.8122, recall 0.7442, auc 0.8258
epoch 5701, loss 0.3254, train acc 85.20%, f1 0.7797, precision 0.8131, recall 0.7488, auc 0.8282
epoch 5801, loss 0.3199, train acc 85.53%, f1 0.7866, precision 0.8119, recall 0.7628, auc 0.8339
epoch 5901, loss 0.3141, train acc 85.85%, f1 0.7914, precision 0.8168, recall 0.7674, auc 0.8375
epoch 6001, loss 0.3086, train acc 85.85%, f1 0.7904, precision 0.8200, recall 0.7628, auc 0.8364
epoch 6101, loss 0.3038, train acc 86.02%, f1 0.7923, precision 0.8241, recall 0.7628, auc 0.8376
epoch 6201, loss 0.2995, train acc 86.34%, f1 0.7990, precision 0.8227, recall 0.7767, auc 0.8434
epoch 6301, loss 0.2955, train acc 87.15%, f1 0.8124, precision 0.8301, recall 0.7953, auc 0.8539
epoch 6401, loss 0.2920, train acc 87.15%, f1 0.8124, precision 0.8301, recall 0.7953, auc 0.8539
epoch 6501, loss 0.2886, train acc 87.48%, f1 0.8171, precision 0.8350, recall 0.8000, auc 0.8575
epoch 6601, loss 0.2852, train acc 87.80%, f1 0.8227, precision 0.8365, recall 0.8093, auc 0.8622
epoch 6701, loss 0.2817, train acc 88.13%, f1 0.8266, precision 0.8447, recall 0.8093, auc 0.8647
epoch 6801, loss 0.2784, train acc 88.13%, f1 0.8266, precision 0.8447, recall 0.8093, auc 0.8647
epoch 6901, loss 0.2753, train acc 88.13%, f1 0.8266, precision 0.8447, recall 0.8093, auc 0.8647
epoch 7001, loss 0.2724, train acc 88.46%, f1 0.8314, precision 0.8495, recall 0.8140, auc 0.8682
epoch 7101, loss 0.2696, train acc 88.78%, f1 0.8369, precision 0.8510, recall 0.8233, auc 0.8729
epoch 7201, loss 0.2668, train acc 89.43%, f1 0.8463, precision 0.8606, recall 0.8326, auc 0.8800
epoch 7301, loss 0.2640, train acc 89.76%, f1 0.8518, precision 0.8619, recall 0.8419, auc 0.8847
epoch 7401, loss 0.2611, train acc 89.92%, f1 0.8551, precision 0.8592, recall 0.8512, auc 0.8881
epoch 7501, loss 0.2582, train acc 89.76%, f1 0.8531, precision 0.8551, recall 0.8512, auc 0.8868
epoch 7601, loss 0.2557, train acc 89.59%, f1 0.8505, precision 0.8545, recall 0.8465, auc 0.8845
epoch 7701, loss 0.2531, train acc 89.92%, f1 0.8551, precision 0.8592, recall 0.8512, auc 0.8881
epoch 7801, loss 0.2499, train acc 89.59%, f1 0.8505, precision 0.8545, recall 0.8465, auc 0.8845
epoch 7901, loss 0.2466, train acc 89.76%, f1 0.8531, precision 0.8551, recall 0.8512, auc 0.8868
epoch 8001, loss 0.2433, train acc 90.73%, f1 0.8671, precision 0.8692, recall 0.8651, auc 0.8976
epoch 8101, loss 0.2394, train acc 91.06%, f1 0.8706, precision 0.8810, recall 0.8605, auc 0.8990
epoch 8201, loss 0.2354, train acc 90.73%, f1 0.8671, precision 0.8692, recall 0.8651, auc 0.8976/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2319, train acc 91.22%, f1 0.8744, precision 0.8744, recall 0.8744, auc 0.9035
epoch 8401, loss 0.2282, train acc 91.38%, f1 0.8765, precision 0.8785, recall 0.8744, auc 0.9047
epoch 8501, loss 0.2242, train acc 92.20%, f1 0.8873, precision 0.8957, recall 0.8791, auc 0.9120
epoch 8601, loss 0.2198, train acc 92.36%, f1 0.8894, precision 0.9000, recall 0.8791, auc 0.9133
epoch 8701, loss 0.2158, train acc 92.52%, f1 0.8915, precision 0.9043, recall 0.8791, auc 0.9145
epoch 8801, loss 0.2121, train acc 92.52%, f1 0.8910, precision 0.9082, recall 0.8744, auc 0.9135
epoch 8901, loss 0.2086, train acc 92.68%, f1 0.8936, precision 0.9087, recall 0.8791, auc 0.9158
epoch 9001, loss 0.2052, train acc 92.85%, f1 0.8962, precision 0.9091, recall 0.8837, auc 0.9181
epoch 9101, loss 0.2018, train acc 93.01%, f1 0.8983, precision 0.9135, recall 0.8837, auc 0.9194
epoch 9201, loss 0.1990, train acc 93.01%, f1 0.8983, precision 0.9135, recall 0.8837, auc 0.9194
epoch 9301, loss 0.1965, train acc 93.17%, f1 0.9005, precision 0.9179, recall 0.8837, auc 0.9206
epoch 9401, loss 0.1942, train acc 93.33%, f1 0.9026, precision 0.9223, recall 0.8837, auc 0.9219
epoch 9501, loss 0.1920, train acc 93.50%, f1 0.9052, precision 0.9227, recall 0.8884, auc 0.9242
epoch 9601, loss 0.1900, train acc 93.98%, f1 0.9121, precision 0.9320, recall 0.8930, auc 0.9290
epoch 9701, loss 0.1880, train acc 93.98%, f1 0.9121, precision 0.9320, recall 0.8930, auc 0.9290
epoch 9801, loss 0.1862, train acc 94.15%, f1 0.9143, precision 0.9366, recall 0.8930, auc 0.9303
epoch 9901, loss 0.1844, train acc 94.31%, f1 0.9169, precision 0.9369, recall 0.8977, auc 0.9326
epoch 10001, loss 0.1826, train acc 94.47%, f1 0.9194, precision 0.9372, recall 0.9023, auc 0.9349
epoch 10101, loss 0.1803, train acc 93.82%, f1 0.9100, precision 0.9275, recall 0.8930, auc 0.9278
epoch 10201, loss 0.1785, train acc 93.98%, f1 0.9125, precision 0.9279, recall 0.8977, auc 0.9301
epoch 10301, loss 0.1768, train acc 94.15%, f1 0.9151, precision 0.9282, recall 0.9023, auc 0.9324
epoch 10401, loss 0.1751, train acc 94.15%, f1 0.9151, precision 0.9282, recall 0.9023, auc 0.9324
epoch 10501, loss 0.1734, train acc 94.47%, f1 0.9198, precision 0.9330, recall 0.9070, auc 0.9360
epoch 10601, loss 0.1717, train acc 94.31%, f1 0.9173, precision 0.9327, recall 0.9023, auc 0.9337
epoch 10701, loss 0.1698, train acc 94.15%, f1 0.9147, precision 0.9324, recall 0.8977, auc 0.9313
epoch 10801, loss 0.1680, train acc 94.31%, f1 0.9173, precision 0.9327, recall 0.9023, auc 0.9337
epoch 10901, loss 0.1663, train acc 94.47%, f1 0.9198, precision 0.9330, recall 0.9070, auc 0.9360
epoch 11001, loss 0.1647, train acc 94.63%, f1 0.9224, precision 0.9333, recall 0.9116, auc 0.9383
epoch 11101, loss 0.1631, train acc 94.96%, f1 0.9274, precision 0.9340, recall 0.9209, auc 0.9430
epoch 11201, loss 0.1615, train acc 95.12%, f1 0.9296, precision 0.9384, recall 0.9209, auc 0.9442
epoch 11301, loss 0.1599, train acc 94.96%, f1 0.9274, precision 0.9340, recall 0.9209, auc 0.9430
epoch 11401, loss 0.1584, train acc 95.28%, f1 0.9321, precision 0.9387, recall 0.9256, auc 0.9465
epoch 11501, loss 0.1569, train acc 95.45%, f1 0.9346, precision 0.9390, recall 0.9302, auc 0.9489
epoch 11601, loss 0.1555, train acc 95.45%, f1 0.9346, precision 0.9390, recall 0.9302, auc 0.9489
epoch 11701, loss 0.1541, train acc 95.45%, f1 0.9346, precision 0.9390, recall 0.9302, auc 0.9489
epoch 11801, loss 0.1528, train acc 95.45%, f1 0.9346, precision 0.9390, recall 0.9302, auc 0.9489
epoch 11901, loss 0.1514, train acc 95.45%, f1 0.9346, precision 0.9390, recall 0.9302, auc 0.9489
epoch 12001, loss 0.1502, train acc 95.61%, f1 0.9368, precision 0.9434, recall 0.9302, auc 0.9501
epoch 12101, loss 0.1489, train acc 95.61%, f1 0.9368, precision 0.9434, recall 0.9302, auc 0.9501
epoch 12201, loss 0.1476, train acc 95.61%, f1 0.9368, precision 0.9434, recall 0.9302, auc 0.9501
epoch 12301, loss 0.1464, train acc 95.77%, f1 0.9390, precision 0.9479, recall 0.9302, auc 0.9514
epoch 12401, loss 0.1452, train acc 95.77%, f1 0.9390, precision 0.9479, recall 0.9302, auc 0.9514
epoch 12501, loss 0.1440, train acc 95.93%, f1 0.9412, precision 0.9524, recall 0.9302, auc 0.9526
epoch 12601, loss 0.1428, train acc 95.93%, f1 0.9412, precision 0.9524, recall 0.9302, auc 0.9526
epoch 12701, loss 0.1416, train acc 95.93%, f1 0.9412, precision 0.9524, recall 0.9302, auc 0.9526
epoch 12801, loss 0.1404, train acc 95.93%, f1 0.9412, precision 0.9524, recall 0.9302, auc 0.9526
epoch 12901, loss 0.1393, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13001, loss 0.1381, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13101, loss 0.1369, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13201, loss 0.1358, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13301, loss 0.1347, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13401, loss 0.1336, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13501, loss 0.1326, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13601, loss 0.1316, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13701, loss 0.1306, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13801, loss 0.1296, train acc 95.93%, f1 0.9415, precision 0.9481, recall 0.9349, auc 0.9537
epoch 13901, loss 0.1286, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14001, loss 0.1277, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14101, loss 0.1267, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14201, loss 0.1258, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14301, loss 0.1249, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14401, loss 0.1241, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14501, loss 0.1232, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14601, loss 0.1223, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14701, loss 0.1215, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14801, loss 0.1207, train acc 96.10%, f1 0.9437, precision 0.9526, recall 0.9349, auc 0.9549
epoch 14901, loss 0.1199, train acc 96.26%, f1 0.9461, precision 0.9528, recall 0.9395, auc 0.9573
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_normal_15000
normal
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_4
./test_pima/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.6352830188679245

the Fscore is 0.5148514851485149

the precision is 0.5416666666666666

the recall is 0.49056603773584906

Done
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_4
----------------------



epoch 1, loss 0.6983, train acc 34.96%, f1 0.5181, precision 0.3496, recall 1.0000, auc 0.5000
epoch 101, loss 0.6246, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5958, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5691, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.5443, train acc 66.99%, f1 0.1288, precision 0.8333, recall 0.0698, auc 0.5311
epoch 501, loss 0.5240, train acc 70.89%, f1 0.3346, precision 0.8333, recall 0.2093, auc 0.5934
epoch 601, loss 0.5080, train acc 74.15%, f1 0.4821, precision 0.8043, recall 0.3442, auc 0.6496
epoch 701, loss 0.4954, train acc 76.26%, f1 0.5731, precision 0.7717, recall 0.4558, auc 0.6917
epoch 801, loss 0.4857, train acc 77.24%, f1 0.6133, precision 0.7551, recall 0.5163, auc 0.7131
epoch 901, loss 0.4782, train acc 78.05%, f1 0.6438, precision 0.7439, recall 0.5674, auc 0.7312
epoch 1001, loss 0.4726, train acc 77.89%, f1 0.6458, precision 0.7337, recall 0.5767, auc 0.7321
epoch 1101, loss 0.4683, train acc 79.02%, f1 0.6718, precision 0.7416, recall 0.6140, auc 0.7495
epoch 1201, loss 0.4651, train acc 79.02%, f1 0.6751, precision 0.7363, recall 0.6233, auc 0.7516
epoch 1301, loss 0.4624, train acc 79.19%, f1 0.6832, precision 0.7302, recall 0.6419, auc 0.7572
epoch 1401, loss 0.4594, train acc 79.19%, f1 0.6847, precision 0.7277, recall 0.6465, auc 0.7583
epoch 1501, loss 0.4553, train acc 78.86%, f1 0.6782, precision 0.7249, recall 0.6372, auc 0.7536
epoch 1601, loss 0.4502, train acc 79.19%, f1 0.6847, precision 0.7277, recall 0.6465, auc 0.7583
epoch 1701, loss 0.4449, train acc 79.19%, f1 0.6800, precision 0.7351, recall 0.6326, auc 0.7550
epoch 1801, loss 0.4399, train acc 79.51%, f1 0.6818, precision 0.7459, recall 0.6279, auc 0.7565
epoch 1901, loss 0.4354, train acc 79.84%, f1 0.6900, precision 0.7459, recall 0.6419, auc 0.7622
epoch 2001, loss 0.4314, train acc 79.35%, f1 0.6833, precision 0.7366, recall 0.6372, auc 0.7574
epoch 2101, loss 0.4278, train acc 79.35%, f1 0.6864, precision 0.7316, recall 0.6465, auc 0.7595
epoch 2201, loss 0.4246, train acc 79.51%, f1 0.6881, precision 0.7354, recall 0.6465, auc 0.7608
epoch 2301, loss 0.4217, train acc 79.67%, f1 0.6944, precision 0.7320, recall 0.6605, auc 0.7652
epoch 2401, loss 0.4190, train acc 79.51%, f1 0.6927, precision 0.7282, recall 0.6605, auc 0.7640
epoch 2501, loss 0.4165, train acc 79.35%, f1 0.6910, precision 0.7245, recall 0.6605, auc 0.7627
epoch 2601, loss 0.4141, train acc 79.35%, f1 0.6910, precision 0.7245, recall 0.6605, auc 0.7627
epoch 2701, loss 0.4119, train acc 79.84%, f1 0.6990, precision 0.7310, recall 0.6698, auc 0.7686
epoch 2801, loss 0.4097, train acc 79.84%, f1 0.7005, precision 0.7286, recall 0.6744, auc 0.7697
epoch 2901, loss 0.4076, train acc 79.67%, f1 0.6973, precision 0.7273, recall 0.6698, auc 0.7674
epoch 3001, loss 0.4055, train acc 79.84%, f1 0.6990, precision 0.7310, recall 0.6698, auc 0.7686
epoch 3101, loss 0.4032, train acc 80.00%, f1 0.7022, precision 0.7323, recall 0.6744, auc 0.7710
epoch 3201, loss 0.4009, train acc 79.67%, f1 0.6973, precision 0.7273, recall 0.6698, auc 0.7674
epoch 3301, loss 0.3987, train acc 80.00%, f1 0.7036, precision 0.7300, recall 0.6791, auc 0.7720
epoch 3401, loss 0.3966, train acc 80.16%, f1 0.7053, precision 0.7337, recall 0.6791, auc 0.7733
epoch 3501, loss 0.3946, train acc 80.81%, f1 0.7150, precision 0.7437, recall 0.6884, auc 0.7804
epoch 3601, loss 0.3926, train acc 81.30%, f1 0.7229, precision 0.7500, recall 0.6977, auc 0.7863
epoch 3701, loss 0.3908, train acc 81.30%, f1 0.7215, precision 0.7525, recall 0.6930, auc 0.7853
epoch 3801, loss 0.3890, train acc 81.63%, f1 0.7264, precision 0.7576, recall 0.6977, auc 0.7888
epoch 3901, loss 0.3874, train acc 81.46%, f1 0.7233, precision 0.7563, recall 0.6930, auc 0.7865
epoch 4001, loss 0.3857, train acc 81.14%, f1 0.7184, precision 0.7513, recall 0.6884, auc 0.7829
epoch 4101, loss 0.3841, train acc 81.14%, f1 0.7184, precision 0.7513, recall 0.6884, auc 0.7829
epoch 4201, loss 0.3824, train acc 81.46%, f1 0.7220, precision 0.7590, recall 0.6884, auc 0.7854
epoch 4301, loss 0.3803, train acc 81.79%, f1 0.7268, precision 0.7641, recall 0.6930, auc 0.7890
epoch 4401, loss 0.3779, train acc 81.95%, f1 0.7312, precision 0.7626, recall 0.7023, auc 0.7924
epoch 4501, loss 0.3755, train acc 82.11%, f1 0.7356, precision 0.7612, recall 0.7116, auc 0.7958
epoch 4601, loss 0.3732, train acc 82.11%, f1 0.7368, precision 0.7586, recall 0.7163, auc 0.7969
epoch 4701, loss 0.3707, train acc 81.63%, f1 0.7303, precision 0.7500, recall 0.7116, auc 0.7921
epoch 4801, loss 0.3678, train acc 81.63%, f1 0.7303, precision 0.7500, recall 0.7116, auc 0.7921
epoch 4901, loss 0.3652, train acc 82.28%, f1 0.7399, precision 0.7598, recall 0.7209, auc 0.7992
epoch 5001, loss 0.3626, train acc 82.11%, f1 0.7368, precision 0.7586, recall 0.7163, auc 0.7969
epoch 5101, loss 0.3602, train acc 82.44%, f1 0.7416, precision 0.7635, recall 0.7209, auc 0.8005
epoch 5201, loss 0.3578, train acc 82.60%, f1 0.7446, precision 0.7647, recall 0.7256, auc 0.8028
epoch 5301, loss 0.3555, train acc 82.93%, f1 0.7518, precision 0.7644, recall 0.7395, auc 0.8085
epoch 5401, loss 0.3534, train acc 83.25%, f1 0.7576, precision 0.7667, recall 0.7488, auc 0.8132
epoch 5501, loss 0.3513, train acc 83.74%, f1 0.7630, precision 0.7778, recall 0.7488, auc 0.8169
epoch 5601, loss 0.3489, train acc 83.74%, f1 0.7619, precision 0.7805, recall 0.7442, auc 0.8158
epoch 5701, loss 0.3466, train acc 83.74%, f1 0.7630, precision 0.7778, recall 0.7488, auc 0.8169
epoch 5801, loss 0.3440, train acc 83.58%, f1 0.7601, precision 0.7767, recall 0.7442, auc 0.8146
epoch 5901, loss 0.3415, train acc 83.90%, f1 0.7660, precision 0.7788, recall 0.7535, auc 0.8192
epoch 6001, loss 0.3389, train acc 83.74%, f1 0.7642, precision 0.7751, recall 0.7535, auc 0.8180
epoch 6101, loss 0.3361, train acc 83.74%, f1 0.7642, precision 0.7751, recall 0.7535, auc 0.8180
epoch 6201, loss 0.3334, train acc 84.39%, f1 0.7725, precision 0.7874, recall 0.7581, auc 0.8241
epoch 6301, loss 0.3309, train acc 84.55%, f1 0.7743, precision 0.7913, recall 0.7581, auc 0.8253
epoch 6401, loss 0.3285, train acc 84.72%, f1 0.7773, precision 0.7923, recall 0.7628, auc 0.8276
epoch 6501, loss 0.3262, train acc 84.88%, f1 0.7791, precision 0.7961, recall 0.7628, auc 0.8289
epoch 6601, loss 0.3240, train acc 85.20%, f1 0.7838, precision 0.8010, recall 0.7674, auc 0.8325
epoch 6701, loss 0.3218, train acc 85.53%, f1 0.7886, precision 0.8058, recall 0.7721, auc 0.8360
epoch 6801, loss 0.3197, train acc 85.53%, f1 0.7886, precision 0.8058, recall 0.7721, auc 0.8360
epoch 6901, loss 0.3177, train acc 85.85%, f1 0.7943, precision 0.8077, recall 0.7814, auc 0.8407
epoch 7001, loss 0.3157, train acc 86.02%, f1 0.7972, precision 0.8086, recall 0.7860, auc 0.8430
epoch 7101, loss 0.3138, train acc 86.18%, f1 0.7981, precision 0.8155, recall 0.7814, auc 0.8432
epoch 7201, loss 0.3117, train acc 86.67%, f1 0.8066, precision 0.8182, recall 0.7953, auc 0.8502
epoch 7301, loss 0.3085, train acc 86.67%, f1 0.8066, precision 0.8182, recall 0.7953, auc 0.8502
epoch 7401, loss 0.3049, train acc 86.67%, f1 0.8057, precision 0.8213, recall 0.7907, auc 0.8491
epoch 7501, loss 0.3020, train acc 86.67%, f1 0.8057, precision 0.8213, recall 0.7907, auc 0.8491
epoch 7601, loss 0.2994, train acc 86.50%, f1 0.8038, precision 0.8173, recall 0.7907, auc 0.8478
epoch 7701, loss 0.2962, train acc 86.67%, f1 0.8066, precision 0.8182, recall 0.7953, auc 0.8502
epoch 7801, loss 0.2917, train acc 87.15%, f1 0.8141, precision 0.8238, recall 0.8047, auc 0.8561
epoch 7901, loss 0.2876, train acc 87.80%, f1 0.8235, precision 0.8333, recall 0.8140, auc 0.8632
epoch 8001, loss 0.2837, train acc 88.13%, f1 0.8290, precision 0.8349, recall 0.8233, auc 0.8679
epoch 8101, loss 0.2802, train acc 88.29%, f1 0.8302, precision 0.8421, recall 0.8186, auc 0.8681
epoch 8201, loss 0.2770, train acc 88.46%, f1 0.8322, precision 0.8462, recall 0.8186, auc 0.8693/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2740, train acc 88.62%, f1 0.8349, precision 0.8469, recall 0.8233, auc 0.8716
epoch 8401, loss 0.2712, train acc 88.78%, f1 0.8369, precision 0.8510, recall 0.8233, auc 0.8729
epoch 8501, loss 0.2685, train acc 89.11%, f1 0.8424, precision 0.8524, recall 0.8326, auc 0.8775
epoch 8601, loss 0.2660, train acc 89.43%, f1 0.8478, precision 0.8538, recall 0.8419, auc 0.8822
epoch 8701, loss 0.2637, train acc 89.59%, f1 0.8498, precision 0.8578, recall 0.8419, auc 0.8834
epoch 8801, loss 0.2610, train acc 89.43%, f1 0.8471, precision 0.8571, recall 0.8372, auc 0.8811
epoch 8901, loss 0.2578, train acc 89.27%, f1 0.8451, precision 0.8531, recall 0.8372, auc 0.8799
epoch 9001, loss 0.2553, train acc 89.27%, f1 0.8443, precision 0.8565, recall 0.8326, auc 0.8788
epoch 9101, loss 0.2532, train acc 89.27%, f1 0.8443, precision 0.8565, recall 0.8326, auc 0.8788
epoch 9201, loss 0.2513, train acc 89.59%, f1 0.8491, precision 0.8612, recall 0.8372, auc 0.8824
epoch 9301, loss 0.2493, train acc 89.11%, f1 0.8424, precision 0.8524, recall 0.8326, auc 0.8775
epoch 9401, loss 0.2470, train acc 89.27%, f1 0.8443, precision 0.8565, recall 0.8326, auc 0.8788
epoch 9501, loss 0.2449, train acc 89.27%, f1 0.8443, precision 0.8565, recall 0.8326, auc 0.8788
epoch 9601, loss 0.2429, train acc 89.43%, f1 0.8463, precision 0.8606, recall 0.8326, auc 0.8800
epoch 9701, loss 0.2411, train acc 89.59%, f1 0.8491, precision 0.8612, recall 0.8372, auc 0.8824
epoch 9801, loss 0.2394, train acc 89.59%, f1 0.8498, precision 0.8578, recall 0.8419, auc 0.8834
epoch 9901, loss 0.2378, train acc 89.59%, f1 0.8498, precision 0.8578, recall 0.8419, auc 0.8834
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_normal_10000
normal
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_4
./test_pima/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.6231132075471698

the Fscore is 0.4719101123595506

the precision is 0.5833333333333334

the recall is 0.39622641509433965

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_4
----------------------



epoch 1, loss 0.6966, train acc 34.96%, f1 0.5181, precision 0.3496, recall 1.0000, auc 0.5000
epoch 101, loss 0.6241, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5954, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5686, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.5437, train acc 67.15%, f1 0.1368, precision 0.8421, recall 0.0744, auc 0.5335
epoch 501, loss 0.5234, train acc 71.54%, f1 0.3590, precision 0.8448, recall 0.2279, auc 0.6027
epoch 601, loss 0.5074, train acc 74.63%, f1 0.4968, precision 0.8105, recall 0.3581, auc 0.6566
epoch 701, loss 0.4949, train acc 76.26%, f1 0.5731, precision 0.7717, recall 0.4558, auc 0.6917
epoch 801, loss 0.4853, train acc 77.24%, f1 0.6133, precision 0.7551, recall 0.5163, auc 0.7131
epoch 901, loss 0.4779, train acc 78.21%, f1 0.6474, precision 0.7455, recall 0.5721, auc 0.7335
epoch 1001, loss 0.4723, train acc 77.89%, f1 0.6458, precision 0.7337, recall 0.5767, auc 0.7321
epoch 1101, loss 0.4681, train acc 79.02%, f1 0.6718, precision 0.7416, recall 0.6140, auc 0.7495
epoch 1201, loss 0.4648, train acc 78.86%, f1 0.6734, precision 0.7322, recall 0.6233, auc 0.7504
epoch 1301, loss 0.4618, train acc 79.02%, f1 0.6799, precision 0.7287, recall 0.6372, auc 0.7549
epoch 1401, loss 0.4581, train acc 78.54%, f1 0.6733, precision 0.7196, recall 0.6326, auc 0.7500
epoch 1501, loss 0.4536, train acc 79.02%, f1 0.6783, precision 0.7312, recall 0.6326, auc 0.7538
epoch 1601, loss 0.4487, train acc 79.19%, f1 0.6832, precision 0.7302, recall 0.6419, auc 0.7572
epoch 1701, loss 0.4440, train acc 79.02%, f1 0.6767, precision 0.7337, recall 0.6279, auc 0.7527
epoch 1801, loss 0.4396, train acc 79.84%, f1 0.6884, precision 0.7486, recall 0.6372, auc 0.7611
epoch 1901, loss 0.4355, train acc 79.84%, f1 0.6900, precision 0.7459, recall 0.6419, auc 0.7622
epoch 2001, loss 0.4317, train acc 79.35%, f1 0.6817, precision 0.7391, recall 0.6326, auc 0.7563
epoch 2101, loss 0.4283, train acc 79.35%, f1 0.6849, precision 0.7340, recall 0.6419, auc 0.7584
epoch 2201, loss 0.4252, train acc 79.35%, f1 0.6895, precision 0.7268, recall 0.6558, auc 0.7617
epoch 2301, loss 0.4223, train acc 79.35%, f1 0.6910, precision 0.7245, recall 0.6605, auc 0.7627
epoch 2401, loss 0.4196, train acc 79.67%, f1 0.6959, precision 0.7296, recall 0.6651, auc 0.7663
epoch 2501, loss 0.4170, train acc 79.35%, f1 0.6910, precision 0.7245, recall 0.6605, auc 0.7627
epoch 2601, loss 0.4145, train acc 79.35%, f1 0.6910, precision 0.7245, recall 0.6605, auc 0.7627
epoch 2701, loss 0.4120, train acc 80.00%, f1 0.7022, precision 0.7323, recall 0.6744, auc 0.7710
epoch 2801, loss 0.4097, train acc 79.51%, f1 0.6957, precision 0.7236, recall 0.6698, auc 0.7661
epoch 2901, loss 0.4075, train acc 79.67%, f1 0.6973, precision 0.7273, recall 0.6698, auc 0.7674
epoch 3001, loss 0.4053, train acc 79.67%, f1 0.6973, precision 0.7273, recall 0.6698, auc 0.7674
epoch 3101, loss 0.4032, train acc 79.67%, f1 0.6988, precision 0.7250, recall 0.6744, auc 0.7685
epoch 3201, loss 0.4010, train acc 79.84%, f1 0.6990, precision 0.7310, recall 0.6698, auc 0.7686
epoch 3301, loss 0.3987, train acc 80.00%, f1 0.7036, precision 0.7300, recall 0.6791, auc 0.7720
epoch 3401, loss 0.3965, train acc 80.33%, f1 0.7098, precision 0.7327, recall 0.6884, auc 0.7767
epoch 3501, loss 0.3945, train acc 80.98%, f1 0.7194, precision 0.7426, recall 0.6977, auc 0.7838
epoch 3601, loss 0.3926, train acc 80.81%, f1 0.7163, precision 0.7413, recall 0.6930, auc 0.7815
epoch 3701, loss 0.3908, train acc 81.14%, f1 0.7198, precision 0.7487, recall 0.6930, auc 0.7840
epoch 3801, loss 0.3890, train acc 81.30%, f1 0.7229, precision 0.7500, recall 0.6977, auc 0.7863
epoch 3901, loss 0.3872, train acc 81.63%, f1 0.7277, precision 0.7550, recall 0.7023, auc 0.7899
epoch 4001, loss 0.3853, train acc 81.30%, f1 0.7229, precision 0.7500, recall 0.6977, auc 0.7863
epoch 4101, loss 0.3832, train acc 81.46%, f1 0.7260, precision 0.7512, recall 0.7023, auc 0.7887
epoch 4201, loss 0.3809, train acc 81.30%, f1 0.7229, precision 0.7500, recall 0.6977, auc 0.7863
epoch 4301, loss 0.3783, train acc 81.30%, f1 0.7229, precision 0.7500, recall 0.6977, auc 0.7863
epoch 4401, loss 0.3756, train acc 81.63%, f1 0.7290, precision 0.7525, recall 0.7070, auc 0.7910
epoch 4501, loss 0.3729, train acc 81.95%, f1 0.7338, precision 0.7574, recall 0.7116, auc 0.7946
epoch 4601, loss 0.3702, train acc 81.95%, f1 0.7338, precision 0.7574, recall 0.7116, auc 0.7946
epoch 4701, loss 0.3675, train acc 82.60%, f1 0.7446, precision 0.7647, recall 0.7256, auc 0.8028
epoch 4801, loss 0.3649, train acc 82.44%, f1 0.7416, precision 0.7635, recall 0.7209, auc 0.8005
epoch 4901, loss 0.3623, train acc 82.76%, f1 0.7464, precision 0.7685, recall 0.7256, auc 0.8040
epoch 5001, loss 0.3596, train acc 82.60%, f1 0.7434, precision 0.7673, recall 0.7209, auc 0.8017
epoch 5101, loss 0.3568, train acc 82.76%, f1 0.7452, precision 0.7711, recall 0.7209, auc 0.8030
epoch 5201, loss 0.3539, train acc 82.93%, f1 0.7482, precision 0.7723, recall 0.7256, auc 0.8053
epoch 5301, loss 0.3511, train acc 83.09%, f1 0.7512, precision 0.7734, recall 0.7302, auc 0.8076
epoch 5401, loss 0.3486, train acc 83.09%, f1 0.7524, precision 0.7707, recall 0.7349, auc 0.8087
epoch 5501, loss 0.3460, train acc 83.58%, f1 0.7589, precision 0.7794, recall 0.7395, auc 0.8135
epoch 5601, loss 0.3434, train acc 84.39%, f1 0.7703, precision 0.7931, recall 0.7488, auc 0.8219
epoch 5701, loss 0.3406, train acc 84.55%, f1 0.7733, precision 0.7941, recall 0.7535, auc 0.8242
epoch 5801, loss 0.3379, train acc 84.55%, f1 0.7722, precision 0.7970, recall 0.7488, auc 0.8232
epoch 5901, loss 0.3355, train acc 84.88%, f1 0.7770, precision 0.8020, recall 0.7535, auc 0.8267
epoch 6001, loss 0.3332, train acc 85.04%, f1 0.7799, precision 0.8030, recall 0.7581, auc 0.8291
epoch 6101, loss 0.3309, train acc 85.20%, f1 0.7828, precision 0.8039, recall 0.7628, auc 0.8314
epoch 6201, loss 0.3287, train acc 85.04%, f1 0.7810, precision 0.8000, recall 0.7628, auc 0.8301
epoch 6301, loss 0.3266, train acc 85.20%, f1 0.7828, precision 0.8039, recall 0.7628, auc 0.8314
epoch 6401, loss 0.3246, train acc 85.20%, f1 0.7838, precision 0.8010, recall 0.7674, auc 0.8325
epoch 6501, loss 0.3227, train acc 85.20%, f1 0.7838, precision 0.8010, recall 0.7674, auc 0.8325
epoch 6601, loss 0.3207, train acc 85.53%, f1 0.7886, precision 0.8058, recall 0.7721, auc 0.8360
epoch 6701, loss 0.3179, train acc 85.85%, f1 0.7933, precision 0.8107, recall 0.7767, auc 0.8396
epoch 6801, loss 0.3138, train acc 86.02%, f1 0.7952, precision 0.8146, recall 0.7767, auc 0.8409
epoch 6901, loss 0.3106, train acc 86.02%, f1 0.7962, precision 0.8116, recall 0.7814, auc 0.8419
epoch 7001, loss 0.3078, train acc 86.02%, f1 0.7962, precision 0.8116, recall 0.7814, auc 0.8419
epoch 7101, loss 0.3050, train acc 86.02%, f1 0.7962, precision 0.8116, recall 0.7814, auc 0.8419
epoch 7201, loss 0.3024, train acc 86.02%, f1 0.7972, precision 0.8086, recall 0.7860, auc 0.8430
epoch 7301, loss 0.2999, train acc 86.18%, f1 0.7991, precision 0.8125, recall 0.7860, auc 0.8443
epoch 7401, loss 0.2976, train acc 86.34%, f1 0.8009, precision 0.8164, recall 0.7860, auc 0.8455
epoch 7501, loss 0.2953, train acc 86.34%, f1 0.8019, precision 0.8134, recall 0.7907, auc 0.8466
epoch 7601, loss 0.2930, train acc 86.83%, f1 0.8085, precision 0.8221, recall 0.7953, auc 0.8514
epoch 7701, loss 0.2904, train acc 87.15%, f1 0.8124, precision 0.8301, recall 0.7953, auc 0.8539
epoch 7801, loss 0.2880, train acc 87.64%, f1 0.8190, precision 0.8390, recall 0.8000, auc 0.8588
epoch 7901, loss 0.2858, train acc 87.97%, f1 0.8238, precision 0.8439, recall 0.8047, auc 0.8623
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_normal_8000
normal
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_4
./test_pima/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.6331132075471698

the Fscore is 0.48275862068965514

the precision is 0.6176470588235294

the recall is 0.39622641509433965

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_4
----------------------



epoch 1, loss 0.6902, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6219, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5937, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5663, train acc 65.20%, f1 0.0093, precision 1.0000, recall 0.0047, auc 0.5023
epoch 401, loss 0.5413, train acc 67.32%, f1 0.1660, precision 0.7692, recall 0.0930, auc 0.5390
epoch 501, loss 0.5211, train acc 71.71%, f1 0.3696, precision 0.8361, recall 0.2372, auc 0.6061
epoch 601, loss 0.5053, train acc 74.63%, f1 0.5032, precision 0.7980, recall 0.3674, auc 0.6587
epoch 701, loss 0.4930, train acc 75.93%, f1 0.5723, precision 0.7557, recall 0.4605, auc 0.6902
epoch 801, loss 0.4836, train acc 77.56%, f1 0.6209, precision 0.7584, recall 0.5256, auc 0.7178
epoch 901, loss 0.4765, train acc 78.21%, f1 0.6474, precision 0.7455, recall 0.5721, auc 0.7335
epoch 1001, loss 0.4711, train acc 78.05%, f1 0.6494, precision 0.7353, recall 0.5814, auc 0.7344
epoch 1101, loss 0.4671, train acc 79.19%, f1 0.6751, precision 0.7430, recall 0.6186, auc 0.7518
epoch 1201, loss 0.4640, train acc 79.19%, f1 0.6800, precision 0.7351, recall 0.6326, auc 0.7550
epoch 1301, loss 0.4611, train acc 78.86%, f1 0.6782, precision 0.7249, recall 0.6372, auc 0.7536
epoch 1401, loss 0.4574, train acc 78.54%, f1 0.6733, precision 0.7196, recall 0.6326, auc 0.7500
epoch 1501, loss 0.4526, train acc 79.19%, f1 0.6816, precision 0.7326, recall 0.6372, auc 0.7561
epoch 1601, loss 0.4474, train acc 79.02%, f1 0.6799, precision 0.7287, recall 0.6372, auc 0.7549
epoch 1701, loss 0.4422, train acc 79.35%, f1 0.6801, precision 0.7418, recall 0.6279, auc 0.7552
epoch 1801, loss 0.4375, train acc 79.67%, f1 0.6851, precision 0.7473, recall 0.6326, auc 0.7588
epoch 1901, loss 0.4332, train acc 79.67%, f1 0.6867, precision 0.7446, recall 0.6372, auc 0.7599
epoch 2001, loss 0.4293, train acc 79.19%, f1 0.6832, precision 0.7302, recall 0.6419, auc 0.7572
epoch 2101, loss 0.4259, train acc 79.67%, f1 0.6898, precision 0.7394, recall 0.6465, auc 0.7620
epoch 2201, loss 0.4228, train acc 79.35%, f1 0.6880, precision 0.7292, recall 0.6512, auc 0.7606
epoch 2301, loss 0.4199, train acc 79.51%, f1 0.6927, precision 0.7282, recall 0.6605, auc 0.7640
epoch 2401, loss 0.4172, train acc 79.19%, f1 0.6878, precision 0.7231, recall 0.6558, auc 0.7604
epoch 2501, loss 0.4147, train acc 79.19%, f1 0.6878, precision 0.7231, recall 0.6558, auc 0.7604
epoch 2601, loss 0.4122, train acc 80.00%, f1 0.7007, precision 0.7347, recall 0.6698, auc 0.7699
epoch 2701, loss 0.4099, train acc 80.16%, f1 0.7039, precision 0.7360, recall 0.6744, auc 0.7722
epoch 2801, loss 0.4076, train acc 80.00%, f1 0.7007, precision 0.7347, recall 0.6698, auc 0.7699
epoch 2901, loss 0.4056, train acc 79.67%, f1 0.6944, precision 0.7320, recall 0.6605, auc 0.7652
epoch 3001, loss 0.4038, train acc 79.35%, f1 0.6895, precision 0.7268, recall 0.6558, auc 0.7617
epoch 3101, loss 0.4021, train acc 79.67%, f1 0.6944, precision 0.7320, recall 0.6605, auc 0.7652
epoch 3201, loss 0.4004, train acc 80.00%, f1 0.6993, precision 0.7371, recall 0.6651, auc 0.7688
epoch 3301, loss 0.3988, train acc 80.00%, f1 0.7007, precision 0.7347, recall 0.6698, auc 0.7699
epoch 3401, loss 0.3968, train acc 79.84%, f1 0.6976, precision 0.7333, recall 0.6651, auc 0.7676
epoch 3501, loss 0.3950, train acc 80.16%, f1 0.7039, precision 0.7360, recall 0.6744, auc 0.7722
epoch 3601, loss 0.3932, train acc 80.33%, f1 0.7056, precision 0.7398, recall 0.6744, auc 0.7735
epoch 3701, loss 0.3915, train acc 80.49%, f1 0.7087, precision 0.7411, recall 0.6791, auc 0.7758
epoch 3801, loss 0.3898, train acc 80.65%, f1 0.7105, precision 0.7449, recall 0.6791, auc 0.7770
epoch 3901, loss 0.3881, train acc 80.98%, f1 0.7167, precision 0.7475, recall 0.6884, auc 0.7817
epoch 4001, loss 0.3865, train acc 81.14%, f1 0.7184, precision 0.7513, recall 0.6884, auc 0.7829
epoch 4101, loss 0.3849, train acc 81.46%, f1 0.7246, precision 0.7538, recall 0.6977, auc 0.7876
epoch 4201, loss 0.3834, train acc 81.79%, f1 0.7295, precision 0.7588, recall 0.7023, auc 0.7912
epoch 4301, loss 0.3818, train acc 81.63%, f1 0.7277, precision 0.7550, recall 0.7023, auc 0.7899
epoch 4401, loss 0.3802, train acc 81.63%, f1 0.7277, precision 0.7550, recall 0.7023, auc 0.7899
epoch 4501, loss 0.3785, train acc 81.46%, f1 0.7246, precision 0.7538, recall 0.6977, auc 0.7876
epoch 4601, loss 0.3765, train acc 81.63%, f1 0.7264, precision 0.7576, recall 0.6977, auc 0.7888
epoch 4701, loss 0.3742, train acc 81.95%, f1 0.7325, precision 0.7600, recall 0.7070, auc 0.7935
epoch 4801, loss 0.3718, train acc 81.79%, f1 0.7295, precision 0.7588, recall 0.7023, auc 0.7912
epoch 4901, loss 0.3692, train acc 81.79%, f1 0.7295, precision 0.7588, recall 0.7023, auc 0.7912
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_normal_5000
normal
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_4
./test_pima/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.6852830188679245

the Fscore is 0.5714285714285714

the precision is 0.6842105263157895

the recall is 0.49056603773584906

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_4
----------------------



epoch 1, loss 0.6932, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6230, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5946, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5675, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.5426, train acc 67.15%, f1 0.1513, precision 0.7826, recall 0.0837, auc 0.5356
epoch 501, loss 0.5224, train acc 71.54%, f1 0.3636, precision 0.8333, recall 0.2326, auc 0.6038
epoch 601, loss 0.5064, train acc 74.47%, f1 0.4952, precision 0.8021, recall 0.3581, auc 0.6553
epoch 701, loss 0.4940, train acc 76.10%, f1 0.5739, precision 0.7615, recall 0.4605, auc 0.6915
epoch 801, loss 0.4845, train acc 77.56%, f1 0.6209, precision 0.7584, recall 0.5256, auc 0.7178
epoch 901, loss 0.4772, train acc 78.21%, f1 0.6474, precision 0.7455, recall 0.5721, auc 0.7335
epoch 1001, loss 0.4717, train acc 78.05%, f1 0.6494, precision 0.7353, recall 0.5814, auc 0.7344
epoch 1101, loss 0.4675, train acc 79.02%, f1 0.6718, precision 0.7416, recall 0.6140, auc 0.7495
epoch 1201, loss 0.4642, train acc 79.02%, f1 0.6767, precision 0.7337, recall 0.6279, auc 0.7527
epoch 1301, loss 0.4611, train acc 78.70%, f1 0.6749, precision 0.7234, recall 0.6326, auc 0.7513
epoch 1401, loss 0.4572, train acc 78.70%, f1 0.6749, precision 0.7234, recall 0.6326, auc 0.7513
epoch 1501, loss 0.4522, train acc 79.19%, f1 0.6832, precision 0.7302, recall 0.6419, auc 0.7572
epoch 1601, loss 0.4468, train acc 79.19%, f1 0.6832, precision 0.7302, recall 0.6419, auc 0.7572
epoch 1701, loss 0.4418, train acc 79.35%, f1 0.6817, precision 0.7391, recall 0.6326, auc 0.7563
epoch 1801, loss 0.4375, train acc 80.00%, f1 0.6902, precision 0.7527, recall 0.6372, auc 0.7624
epoch 1901, loss 0.4339, train acc 79.67%, f1 0.6898, precision 0.7394, recall 0.6465, auc 0.7620
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_normal_2000
normal
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_4
./test_pima/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.6902830188679245

the Fscore is 0.5777777777777777

the precision is 0.7027027027027027

the recall is 0.49056603773584906

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_4
----------------------



epoch 1, loss 0.6933, train acc 72.41%, f1 0.6381, precision 0.9272, recall 0.4864, auc 0.7241
epoch 101, loss 0.3017, train acc 88.27%, f1 0.8827, precision 0.8824, recall 0.8830, auc 0.8827
epoch 201, loss 0.1484, train acc 97.38%, f1 0.9738, precision 0.9738, recall 0.9738, auc 0.9738
epoch 301, loss 0.0542, train acc 98.47%, f1 0.9847, precision 0.9847, recall 0.9847, auc 0.9847
epoch 401, loss 0.0543, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 501, loss 0.0361, train acc 99.04%, f1 0.9904, precision 0.9905, recall 0.9904, auc 0.9904
epoch 601, loss 0.0240, train acc 99.18%, f1 0.9918, precision 0.9918, recall 0.9917, auc 0.9918
epoch 701, loss 0.0158, train acc 99.28%, f1 0.9928, precision 0.9929, recall 0.9928, auc 0.9928
epoch 801, loss 0.0171, train acc 99.37%, f1 0.9937, precision 0.9937, recall 0.9936, auc 0.9937
epoch 901, loss 0.0140, train acc 99.42%, f1 0.9942, precision 0.9943, recall 0.9942, auc 0.9942
epoch 1001, loss 0.0261, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 1101, loss 0.0190, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9953, auc 0.9954
epoch 1201, loss 0.0067, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 1301, loss 0.0111, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1401, loss 0.0082, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1501, loss 0.0074, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 1601, loss 0.0084, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 1701, loss 0.0075, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 1801, loss 0.0080, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 1901, loss 0.0077, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 2001, loss 0.0090, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9979, auc 0.9978
epoch 2101, loss 0.0129, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 2201, loss 0.0094, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9980
epoch 2301, loss 0.0069, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2401, loss 0.0092, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 2501, loss 0.0112, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 2601, loss 0.0064, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 2701, loss 0.0101, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9986, auc 0.9985
epoch 2801, loss 0.0043, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 2901, loss 0.0055, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 3001, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 3101, loss 0.0044, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 3201, loss 0.0047, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 3301, loss 0.0095, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3401, loss 0.0052, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 3501, loss 0.0049, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 3601, loss 0.0006, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 3701, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 3801, loss 0.0050, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 3901, loss 0.0032, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4001, loss 0.0037, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 4101, loss 0.0030, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 4201, loss 0.0037, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4301, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4401, loss 0.0040, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4501, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4601, loss 0.0046, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 4701, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 4801, loss 0.0043, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 4901, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 5001, loss 0.0028, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 5101, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 5201, loss 0.0051, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5301, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 5401, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 5501, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 5601, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 5701, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 5801, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 5901, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6001, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 6101, loss 0.0025, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6201, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 6301, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 6401, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6501, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 6601, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 6701, loss 0.0021, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 6801, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 6901, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7001, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 7101, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 7201, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7301, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7401, loss 0.0001, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 7501, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 7601, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7701, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 7801, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 7901, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8001, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8101, loss 0.0005, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8201, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8301, loss 0.0003, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8401, loss 0.0006, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8501, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8601, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 8801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_4
./test_vehicle0/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.8711240310077519

the Fscore is 0.8450704225352113

the precision is 0.967741935483871

the recall is 0.75

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_4
----------------------



epoch 1, loss 0.6932, train acc 80.08%, f1 0.7674, precision 0.9218, recall 0.6573, auc 0.8008
epoch 101, loss 0.3642, train acc 88.16%, f1 0.8815, precision 0.8818, recall 0.8813, auc 0.8816
epoch 201, loss 0.1185, train acc 97.40%, f1 0.9740, precision 0.9741, recall 0.9739, auc 0.9740
epoch 301, loss 0.0942, train acc 98.51%, f1 0.9851, precision 0.9851, recall 0.9851, auc 0.9851
epoch 401, loss 0.0715, train acc 98.83%, f1 0.9883, precision 0.9883, recall 0.9884, auc 0.9883
epoch 501, loss 0.0460, train acc 99.05%, f1 0.9905, precision 0.9904, recall 0.9906, auc 0.9905
epoch 601, loss 0.0553, train acc 99.17%, f1 0.9917, precision 0.9916, recall 0.9917, auc 0.9917
epoch 701, loss 0.0196, train acc 99.26%, f1 0.9926, precision 0.9926, recall 0.9926, auc 0.9926
epoch 801, loss 0.0169, train acc 99.36%, f1 0.9936, precision 0.9936, recall 0.9937, auc 0.9936
epoch 901, loss 0.0305, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9943, auc 0.9943
epoch 1001, loss 0.0305, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1101, loss 0.0103, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9953, auc 0.9953
epoch 1201, loss 0.0196, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 1301, loss 0.0163, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1401, loss 0.0109, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1501, loss 0.0088, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 1601, loss 0.0099, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 1701, loss 0.0159, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 1801, loss 0.0075, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 1901, loss 0.0061, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2001, loss 0.0066, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2101, loss 0.0063, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 2201, loss 0.0050, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2301, loss 0.0042, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 2401, loss 0.0091, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2501, loss 0.0109, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 2601, loss 0.0059, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 2701, loss 0.0067, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 2801, loss 0.0065, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 2901, loss 0.0053, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 3001, loss 0.0055, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 3101, loss 0.0055, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9984, auc 0.9985
epoch 3201, loss 0.0018, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3301, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3401, loss 0.0037, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 3501, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 3601, loss 0.0046, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 3701, loss 0.0052, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 3801, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9987, auc 0.9989
epoch 3901, loss 0.0033, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4001, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 4101, loss 0.0060, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 4201, loss 0.0062, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 4301, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4401, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 4501, loss 0.0038, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4601, loss 0.0044, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4701, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4801, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 4901, loss 0.0034, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5001, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 5101, loss 0.0011, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5201, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 5301, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5401, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 5501, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5601, loss 0.0029, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5701, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 5801, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 5901, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6001, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6101, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6201, loss 0.0035, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6301, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 6401, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 6501, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 6601, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6701, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 6801, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 6901, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7001, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 7101, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7201, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 7301, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7401, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7501, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 7601, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 7701, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7801, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 7901, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8001, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8101, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 8201, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 8401, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8501, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 8601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 8801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0005, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_4
./test_vehicle0/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.921124031007752

the Fscore is 0.9066666666666667

the precision is 0.9714285714285714

the recall is 0.85

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_4
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.3285, train acc 87.89%, f1 0.8791, precision 0.8775, recall 0.8807, auc 0.8789
epoch 201, loss 0.1456, train acc 97.31%, f1 0.9731, precision 0.9730, recall 0.9733, auc 0.9731
epoch 301, loss 0.0655, train acc 98.50%, f1 0.9850, precision 0.9850, recall 0.9849, auc 0.9850
epoch 401, loss 0.0669, train acc 98.85%, f1 0.9885, precision 0.9886, recall 0.9884, auc 0.9885
epoch 501, loss 0.0457, train acc 99.10%, f1 0.9910, precision 0.9910, recall 0.9909, auc 0.9910
epoch 601, loss 0.0278, train acc 99.17%, f1 0.9917, precision 0.9919, recall 0.9916, auc 0.9917
epoch 701, loss 0.0192, train acc 99.29%, f1 0.9929, precision 0.9930, recall 0.9928, auc 0.9929
epoch 801, loss 0.0214, train acc 99.37%, f1 0.9937, precision 0.9937, recall 0.9936, auc 0.9937
epoch 901, loss 0.0260, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9942, auc 0.9943
epoch 1001, loss 0.0170, train acc 99.48%, f1 0.9948, precision 0.9949, recall 0.9947, auc 0.9948
epoch 1101, loss 0.0139, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1201, loss 0.0122, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 1301, loss 0.0163, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1401, loss 0.0082, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1501, loss 0.0081, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 1601, loss 0.0078, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 1701, loss 0.0113, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9974, auc 0.9974
epoch 1801, loss 0.0106, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 1901, loss 0.0059, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 2001, loss 0.0037, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 2101, loss 0.0087, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 2201, loss 0.0043, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2301, loss 0.0052, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 2401, loss 0.0125, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 2501, loss 0.0051, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 2601, loss 0.0036, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 2701, loss 0.0101, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9986, auc 0.9984
epoch 2801, loss 0.0058, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 2901, loss 0.0025, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3001, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 3101, loss 0.0060, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3201, loss 0.0026, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 3301, loss 0.0029, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 3401, loss 0.0041, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 3501, loss 0.0046, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 3601, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 3701, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 3801, loss 0.0062, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 3901, loss 0.0070, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4001, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 4101, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 4201, loss 0.0027, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4301, loss 0.0035, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 4401, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 4501, loss 0.0044, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 4601, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 4701, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 4801, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 4901, loss 0.0044, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5001, loss 0.0049, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5101, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5201, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 5301, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 5401, loss 0.0034, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 5501, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 5601, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 5701, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 5801, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 5901, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6001, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 6101, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6201, loss 0.0029, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 6301, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 6401, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 6501, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 6601, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 6701, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 6801, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 6901, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7001, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7101, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7201, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7301, loss 0.0021, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 7401, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 7501, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 7601, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 7701, loss 0.0015, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 7801, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 7901, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8001, loss 0.0007, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8101, loss 0.0007, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8201, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0001, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 8501, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 8601, loss 0.0008, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 8801, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8901, loss 0.0009, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 9001, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 9101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 9201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_4
./test_vehicle0/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9086240310077519

the Fscore is 0.8918918918918919

the precision is 0.9705882352941176

the recall is 0.825

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_4
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3054, train acc 88.54%, f1 0.8843, precision 0.8933, recall 0.8754, auc 0.8854
epoch 201, loss 0.1386, train acc 97.39%, f1 0.9739, precision 0.9745, recall 0.9733, auc 0.9739
epoch 301, loss 0.0821, train acc 98.45%, f1 0.9845, precision 0.9844, recall 0.9846, auc 0.9845
epoch 401, loss 0.0508, train acc 98.84%, f1 0.9884, precision 0.9883, recall 0.9886, auc 0.9884
epoch 501, loss 0.0368, train acc 99.07%, f1 0.9907, precision 0.9904, recall 0.9910, auc 0.9907
epoch 601, loss 0.0332, train acc 99.20%, f1 0.9920, precision 0.9918, recall 0.9923, auc 0.9920
epoch 701, loss 0.0169, train acc 99.30%, f1 0.9930, precision 0.9929, recall 0.9930, auc 0.9930
epoch 801, loss 0.0314, train acc 99.37%, f1 0.9937, precision 0.9936, recall 0.9938, auc 0.9937
epoch 901, loss 0.0251, train acc 99.44%, f1 0.9944, precision 0.9942, recall 0.9946, auc 0.9944
epoch 1001, loss 0.0155, train acc 99.51%, f1 0.9951, precision 0.9950, recall 0.9952, auc 0.9951
epoch 1101, loss 0.0150, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9956, auc 0.9956
epoch 1201, loss 0.0170, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9961, auc 0.9961
epoch 1301, loss 0.0164, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 1401, loss 0.0095, train acc 99.68%, f1 0.9968, precision 0.9967, recall 0.9968, auc 0.9968
epoch 1501, loss 0.0066, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 1601, loss 0.0125, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 1701, loss 0.0042, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 1801, loss 0.0061, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 1901, loss 0.0039, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2001, loss 0.0081, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 2101, loss 0.0059, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 2201, loss 0.0032, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 2301, loss 0.0037, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 2401, loss 0.0042, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 2501, loss 0.0072, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 2601, loss 0.0035, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 2701, loss 0.0057, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 2801, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 2901, loss 0.0042, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 3001, loss 0.0043, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9984, auc 0.9986
epoch 3101, loss 0.0027, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3201, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 3301, loss 0.0023, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3401, loss 0.0019, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 3501, loss 0.0036, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 3601, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 3701, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 3801, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 3901, loss 0.0009, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 4001, loss 0.0062, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4101, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 4201, loss 0.0049, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4301, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4401, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4501, loss 0.0035, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 4601, loss 0.0039, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 4701, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 4801, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 4901, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 5001, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 5101, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 5201, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 5301, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5401, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 5501, loss 0.0030, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 5601, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 5701, loss 0.0043, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 5801, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 5901, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 6001, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6101, loss 0.0025, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 6201, loss 0.0021, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6301, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6401, loss 0.0033, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 6501, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 6601, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 6701, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 6801, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 6901, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 7001, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7101, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 7201, loss 0.0024, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 7301, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 7401, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 7501, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7601, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 7701, loss 0.0023, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7801, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7901, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_4
./test_vehicle0/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9547480620155038

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_4
----------------------



epoch 1, loss 0.6932, train acc 63.76%, f1 0.4318, precision 0.9988, recall 0.2755, auc 0.6376
epoch 101, loss 0.3381, train acc 88.04%, f1 0.8804, precision 0.8803, recall 0.8805, auc 0.8804
epoch 201, loss 0.1309, train acc 97.42%, f1 0.9742, precision 0.9742, recall 0.9742, auc 0.9742
epoch 301, loss 0.0652, train acc 98.52%, f1 0.9852, precision 0.9853, recall 0.9852, auc 0.9852
epoch 401, loss 0.0641, train acc 98.81%, f1 0.9881, precision 0.9881, recall 0.9881, auc 0.9881
epoch 501, loss 0.0311, train acc 99.04%, f1 0.9904, precision 0.9904, recall 0.9904, auc 0.9904
epoch 601, loss 0.0323, train acc 99.18%, f1 0.9918, precision 0.9918, recall 0.9918, auc 0.9918
epoch 701, loss 0.0246, train acc 99.29%, f1 0.9929, precision 0.9929, recall 0.9930, auc 0.9929
epoch 801, loss 0.0368, train acc 99.37%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9937
epoch 901, loss 0.0100, train acc 99.43%, f1 0.9943, precision 0.9942, recall 0.9943, auc 0.9943
epoch 1001, loss 0.0211, train acc 99.47%, f1 0.9947, precision 0.9948, recall 0.9947, auc 0.9947
epoch 1101, loss 0.0130, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1201, loss 0.0090, train acc 99.57%, f1 0.9957, precision 0.9958, recall 0.9957, auc 0.9957
epoch 1301, loss 0.0090, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 1401, loss 0.0237, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9966, auc 0.9965
epoch 1501, loss 0.0085, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 1601, loss 0.0075, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 1701, loss 0.0117, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 1801, loss 0.0089, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 1901, loss 0.0159, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 2001, loss 0.0050, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2101, loss 0.0066, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2201, loss 0.0096, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2301, loss 0.0051, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2401, loss 0.0093, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9982
epoch 2501, loss 0.0058, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 2601, loss 0.0073, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 2701, loss 0.0045, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 2801, loss 0.0020, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 2901, loss 0.0064, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 3001, loss 0.0034, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3101, loss 0.0034, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 3201, loss 0.0013, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 3301, loss 0.0019, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3401, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3501, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3601, loss 0.0018, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 3701, loss 0.0034, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 3801, loss 0.0021, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 3901, loss 0.0042, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4001, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4101, loss 0.0048, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4201, loss 0.0053, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4301, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 4401, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4501, loss 0.0047, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 4601, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 4701, loss 0.0024, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 4801, loss 0.0033, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 4901, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_4
./test_vehicle0/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9633720930232557

the Fscore is 0.9382716049382716

the precision is 0.926829268292683

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_4
----------------------



epoch 1, loss 0.6930, train acc 50.00%, f1 0.0002, precision 1.0000, recall 0.0001, auc 0.5000
epoch 101, loss 0.3233, train acc 88.79%, f1 0.8878, precision 0.8881, recall 0.8876, auc 0.8879
epoch 201, loss 0.1339, train acc 97.42%, f1 0.9742, precision 0.9743, recall 0.9742, auc 0.9742
epoch 301, loss 0.0643, train acc 98.47%, f1 0.9847, precision 0.9848, recall 0.9847, auc 0.9847
epoch 401, loss 0.0377, train acc 98.84%, f1 0.9884, precision 0.9884, recall 0.9884, auc 0.9884
epoch 501, loss 0.0386, train acc 99.06%, f1 0.9906, precision 0.9906, recall 0.9906, auc 0.9906
epoch 601, loss 0.0301, train acc 99.19%, f1 0.9919, precision 0.9919, recall 0.9919, auc 0.9919
epoch 701, loss 0.0294, train acc 99.29%, f1 0.9929, precision 0.9929, recall 0.9929, auc 0.9929
epoch 801, loss 0.0170, train acc 99.35%, f1 0.9935, precision 0.9935, recall 0.9935, auc 0.9935
epoch 901, loss 0.0259, train acc 99.42%, f1 0.9942, precision 0.9942, recall 0.9942, auc 0.9942
epoch 1001, loss 0.0155, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1101, loss 0.0190, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1201, loss 0.0121, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1301, loss 0.0075, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1401, loss 0.0104, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1501, loss 0.0110, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1601, loss 0.0123, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 1701, loss 0.0041, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9973, auc 0.9973
epoch 1801, loss 0.0110, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 1901, loss 0.0089, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_4
./test_vehicle0/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9517441860465117

the Fscore is 0.9047619047619048

the precision is 0.8636363636363636

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_4
----------------------



epoch 1, loss 0.6932, train acc 49.77%, f1 0.6646, precision 0.4977, recall 1.0000, auc 0.5000
epoch 101, loss 0.2916, train acc 88.03%, f1 0.8795, precision 0.8817, recall 0.8772, auc 0.8803
epoch 201, loss 0.1442, train acc 97.28%, f1 0.9726, precision 0.9721, recall 0.9732, auc 0.9728
epoch 301, loss 0.0657, train acc 98.50%, f1 0.9850, precision 0.9850, recall 0.9849, auc 0.9850
epoch 401, loss 0.0463, train acc 98.79%, f1 0.9878, precision 0.9877, recall 0.9879, auc 0.9879
epoch 501, loss 0.0378, train acc 99.04%, f1 0.9904, precision 0.9900, recall 0.9907, auc 0.9904
epoch 601, loss 0.0415, train acc 99.15%, f1 0.9915, precision 0.9912, recall 0.9918, auc 0.9915
epoch 701, loss 0.0225, train acc 99.29%, f1 0.9929, precision 0.9928, recall 0.9929, auc 0.9929
epoch 801, loss 0.0161, train acc 99.31%, f1 0.9931, precision 0.9928, recall 0.9934, auc 0.9931
epoch 901, loss 0.0322, train acc 99.42%, f1 0.9941, precision 0.9940, recall 0.9943, auc 0.9942
epoch 1001, loss 0.0296, train acc 99.47%, f1 0.9947, precision 0.9945, recall 0.9949, auc 0.9947
epoch 1101, loss 0.0137, train acc 99.53%, f1 0.9953, precision 0.9949, recall 0.9956, auc 0.9953
epoch 1201, loss 0.0074, train acc 99.57%, f1 0.9957, precision 0.9958, recall 0.9956, auc 0.9957
epoch 1301, loss 0.0166, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9964, auc 0.9962
epoch 1401, loss 0.0069, train acc 99.66%, f1 0.9965, precision 0.9963, recall 0.9968, auc 0.9966
epoch 1501, loss 0.0155, train acc 99.67%, f1 0.9967, precision 0.9962, recall 0.9972, auc 0.9967
epoch 1601, loss 0.0104, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 1701, loss 0.0115, train acc 99.73%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9973
epoch 1801, loss 0.0148, train acc 99.75%, f1 0.9974, precision 0.9970, recall 0.9979, auc 0.9975
epoch 1901, loss 0.0044, train acc 99.74%, f1 0.9974, precision 0.9971, recall 0.9977, auc 0.9974
epoch 2001, loss 0.0126, train acc 99.77%, f1 0.9976, precision 0.9972, recall 0.9981, auc 0.9977
epoch 2101, loss 0.0097, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9978, auc 0.9977
epoch 2201, loss 0.0033, train acc 99.78%, f1 0.9978, precision 0.9980, recall 0.9977, auc 0.9978
epoch 2301, loss 0.0088, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9980, auc 0.9979
epoch 2401, loss 0.0043, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 2501, loss 0.0051, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 2601, loss 0.0034, train acc 99.82%, f1 0.9982, precision 0.9977, recall 0.9987, auc 0.9982
epoch 2701, loss 0.0058, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9986, auc 0.9983
epoch 2801, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9981, recall 0.9987, auc 0.9984
epoch 2901, loss 0.0114, train acc 99.85%, f1 0.9985, precision 0.9981, recall 0.9989, auc 0.9985
epoch 3001, loss 0.0043, train acc 99.85%, f1 0.9985, precision 0.9982, recall 0.9988, auc 0.9985
epoch 3101, loss 0.0070, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9989, auc 0.9986
epoch 3201, loss 0.0030, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 3301, loss 0.0076, train acc 99.87%, f1 0.9987, precision 0.9984, recall 0.9989, auc 0.9987
epoch 3401, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 3501, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9985, recall 0.9990, auc 0.9988
epoch 3601, loss 0.0062, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 3701, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9990, auc 0.9988
epoch 3801, loss 0.0055, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 3901, loss 0.0063, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 4001, loss 0.0069, train acc 99.89%, f1 0.9989, precision 0.9986, recall 0.9993, auc 0.9989
epoch 4101, loss 0.0047, train acc 99.90%, f1 0.9990, precision 0.9987, recall 0.9992, auc 0.9990
epoch 4201, loss 0.0025, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 4301, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 4401, loss 0.0020, train acc 99.91%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9991
epoch 4501, loss 0.0031, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 4601, loss 0.0032, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 4701, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 4801, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 4901, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 5001, loss 0.0052, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9995, auc 0.9992
epoch 5101, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 5201, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 5301, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 5401, loss 0.0006, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 5501, loss 0.0059, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 5601, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 5701, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9990, recall 0.9996, auc 0.9993
epoch 5801, loss 0.0033, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 5901, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 6001, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9992, recall 0.9997, auc 0.9995
epoch 6101, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 6201, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 6301, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6401, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6501, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9997, auc 0.9995
epoch 6601, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 6701, loss 0.0030, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 6801, loss 0.0030, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 6901, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7001, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 7101, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7201, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 7301, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 7401, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7501, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 7601, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7701, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 7801, loss 0.0023, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7901, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 8001, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 8101, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8201, loss 0.0002, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 8301, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8401, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 8501, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8601, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 8701, loss 0.0003, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8801, loss 0.0013, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8901, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0007, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 9201, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0005, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 9401, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_4
./test_vehicle0/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.921124031007752

the Fscore is 0.9066666666666667

the precision is 0.9714285714285714

the recall is 0.85

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_4
----------------------



epoch 1, loss 0.6937, train acc 49.94%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3325, train acc 87.29%, f1 0.8705, precision 0.8879, recall 0.8539, auc 0.8729
epoch 201, loss 0.1239, train acc 97.19%, f1 0.9719, precision 0.9730, recall 0.9707, auc 0.9719
epoch 301, loss 0.0708, train acc 98.52%, f1 0.9852, precision 0.9845, recall 0.9860, auc 0.9852
epoch 401, loss 0.0589, train acc 98.79%, f1 0.9880, precision 0.9874, recall 0.9886, auc 0.9879
epoch 501, loss 0.0496, train acc 99.02%, f1 0.9902, precision 0.9895, recall 0.9909, auc 0.9902
epoch 601, loss 0.0444, train acc 99.18%, f1 0.9918, precision 0.9914, recall 0.9922, auc 0.9918
epoch 701, loss 0.0360, train acc 99.28%, f1 0.9928, precision 0.9923, recall 0.9934, auc 0.9928
epoch 801, loss 0.0198, train acc 99.36%, f1 0.9936, precision 0.9933, recall 0.9938, auc 0.9936
epoch 901, loss 0.0200, train acc 99.43%, f1 0.9943, precision 0.9941, recall 0.9945, auc 0.9943
epoch 1001, loss 0.0163, train acc 99.48%, f1 0.9948, precision 0.9944, recall 0.9952, auc 0.9948
epoch 1101, loss 0.0113, train acc 99.53%, f1 0.9953, precision 0.9947, recall 0.9958, auc 0.9953
epoch 1201, loss 0.0195, train acc 99.58%, f1 0.9958, precision 0.9953, recall 0.9962, auc 0.9958
epoch 1301, loss 0.0181, train acc 99.63%, f1 0.9963, precision 0.9961, recall 0.9964, auc 0.9963
epoch 1401, loss 0.0077, train acc 99.66%, f1 0.9966, precision 0.9962, recall 0.9969, auc 0.9966
epoch 1501, loss 0.0098, train acc 99.70%, f1 0.9970, precision 0.9967, recall 0.9972, auc 0.9970
epoch 1601, loss 0.0052, train acc 99.72%, f1 0.9972, precision 0.9969, recall 0.9975, auc 0.9972
epoch 1701, loss 0.0032, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9976, auc 0.9974
epoch 1801, loss 0.0078, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9978, auc 0.9976
epoch 1901, loss 0.0046, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2001, loss 0.0045, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2101, loss 0.0067, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2201, loss 0.0063, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 2301, loss 0.0080, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 2401, loss 0.0109, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 2501, loss 0.0136, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 2601, loss 0.0076, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 2701, loss 0.0041, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9988, auc 0.9986
epoch 2801, loss 0.0043, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 2901, loss 0.0065, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9985, auc 0.9987
epoch 3001, loss 0.0077, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 3101, loss 0.0052, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 3201, loss 0.0068, train acc 99.88%, f1 0.9988, precision 0.9985, recall 0.9991, auc 0.9988
epoch 3301, loss 0.0030, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 3401, loss 0.0033, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 3501, loss 0.0047, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 3601, loss 0.0015, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 3701, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9990
epoch 3801, loss 0.0046, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9990
epoch 3901, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9993, auc 0.9991
epoch 4001, loss 0.0030, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9991
epoch 4101, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4201, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 4301, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 4401, loss 0.0011, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 4501, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9995, auc 0.9993
epoch 4601, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 4701, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9990, recall 0.9996, auc 0.9993
epoch 4801, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9996, auc 0.9994
epoch 4901, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9995, auc 0.9993
epoch 5001, loss 0.0005, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9996, auc 0.9994
epoch 5101, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 5201, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 5301, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9997, auc 0.9995
epoch 5401, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 5501, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 5601, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 5701, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9997, auc 0.9996
epoch 5801, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 5901, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 6001, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 6101, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 6201, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 6301, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 6401, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 6501, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 6601, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 6701, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 6801, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 6901, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 7001, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 7101, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7201, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 7301, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 7401, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 7501, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 7601, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 7701, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 7801, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 7901, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 8001, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8101, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 8301, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 8401, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8501, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8601, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 8701, loss 0.0012, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 8801, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8901, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9001, loss 0.0005, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 9101, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 9201, loss 0.0003, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 9301, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 9401, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 9501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 9901, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_4
./test_vehicle0/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.958624031007752

the Fscore is 0.9487179487179489

the precision is 0.9736842105263158

the recall is 0.925

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_4
----------------------



epoch 1, loss 0.6932, train acc 75.93%, f1 0.7158, precision 0.8729, recall 0.6066, auc 0.7592
epoch 101, loss 0.3491, train acc 88.23%, f1 0.8815, precision 0.8870, recall 0.8761, auc 0.8823
epoch 201, loss 0.1281, train acc 97.33%, f1 0.9732, precision 0.9741, recall 0.9723, auc 0.9733
epoch 301, loss 0.0633, train acc 98.50%, f1 0.9850, precision 0.9863, recall 0.9836, auc 0.9850
epoch 401, loss 0.0444, train acc 98.81%, f1 0.9880, precision 0.9890, recall 0.9871, auc 0.9881
epoch 501, loss 0.0359, train acc 99.04%, f1 0.9904, precision 0.9909, recall 0.9899, auc 0.9904
epoch 601, loss 0.0495, train acc 99.17%, f1 0.9917, precision 0.9924, recall 0.9910, auc 0.9917
epoch 701, loss 0.0284, train acc 99.28%, f1 0.9928, precision 0.9928, recall 0.9929, auc 0.9928
epoch 801, loss 0.0327, train acc 99.36%, f1 0.9936, precision 0.9937, recall 0.9936, auc 0.9936
epoch 901, loss 0.0153, train acc 99.41%, f1 0.9941, precision 0.9942, recall 0.9940, auc 0.9941
epoch 1001, loss 0.0130, train acc 99.48%, f1 0.9948, precision 0.9950, recall 0.9945, auc 0.9948
epoch 1101, loss 0.0129, train acc 99.50%, f1 0.9950, precision 0.9954, recall 0.9947, auc 0.9950
epoch 1201, loss 0.0162, train acc 99.55%, f1 0.9955, precision 0.9959, recall 0.9951, auc 0.9955
epoch 1301, loss 0.0178, train acc 99.60%, f1 0.9960, precision 0.9961, recall 0.9959, auc 0.9960
epoch 1401, loss 0.0091, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9963, auc 0.9964
epoch 1501, loss 0.0167, train acc 99.67%, f1 0.9967, precision 0.9970, recall 0.9964, auc 0.9967
epoch 1601, loss 0.0066, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9967, auc 0.9969
epoch 1701, loss 0.0054, train acc 99.71%, f1 0.9971, precision 0.9972, recall 0.9970, auc 0.9971
epoch 1801, loss 0.0073, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9974
epoch 1901, loss 0.0049, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2001, loss 0.0056, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9973, auc 0.9975
epoch 2101, loss 0.0030, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9977, auc 0.9976
epoch 2201, loss 0.0046, train acc 99.79%, f1 0.9979, precision 0.9981, recall 0.9977, auc 0.9979
epoch 2301, loss 0.0136, train acc 99.79%, f1 0.9979, precision 0.9982, recall 0.9977, auc 0.9979
epoch 2401, loss 0.0075, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 2501, loss 0.0027, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9982, auc 0.9980
epoch 2601, loss 0.0055, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 2701, loss 0.0070, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9980, auc 0.9982
epoch 2801, loss 0.0068, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 2901, loss 0.0070, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 3001, loss 0.0095, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3101, loss 0.0045, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3201, loss 0.0017, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 3301, loss 0.0059, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9989, auc 0.9986
epoch 3401, loss 0.0038, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 3501, loss 0.0043, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 3601, loss 0.0024, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 3701, loss 0.0067, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 3801, loss 0.0036, train acc 99.89%, f1 0.9989, precision 0.9986, recall 0.9992, auc 0.9989
epoch 3901, loss 0.0053, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 4001, loss 0.0042, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 4101, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 4201, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 4301, loss 0.0035, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 4401, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9990
epoch 4501, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9993, auc 0.9990
epoch 4601, loss 0.0030, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9992, auc 0.9991
epoch 4701, loss 0.0005, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 4801, loss 0.0039, train acc 99.91%, f1 0.9991, precision 0.9988, recall 0.9995, auc 0.9991
epoch 4901, loss 0.0023, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 5001, loss 0.0037, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 5101, loss 0.0032, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 5201, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 5301, loss 0.0034, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5401, loss 0.0051, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 5501, loss 0.0021, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5601, loss 0.0037, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5701, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 5801, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5901, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6001, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 6101, loss 0.0027, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 6201, loss 0.0043, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 6301, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 6401, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9997, auc 0.9995
epoch 6501, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 6601, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 6701, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9997, auc 0.9996
epoch 6801, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 6901, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 7001, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7101, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 7201, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 7301, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7401, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9995, recall 0.9998, auc 0.9997
epoch 7501, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7601, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7701, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 7801, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 7901, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 8001, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 8101, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 8301, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 8401, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 8501, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8601, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8701, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 8801, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 8901, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 9001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 9401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0006, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_4
./test_vehicle0/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9461240310077519

the Fscore is 0.935064935064935

the precision is 0.972972972972973

the recall is 0.9

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_4
----------------------



epoch 1, loss 0.6935, train acc 50.03%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.2948, train acc 88.51%, f1 0.8846, precision 0.8883, recall 0.8809, auc 0.8851
epoch 201, loss 0.1455, train acc 97.50%, f1 0.9750, precision 0.9746, recall 0.9753, auc 0.9750
epoch 301, loss 0.0839, train acc 98.54%, f1 0.9854, precision 0.9857, recall 0.9852, auc 0.9854
epoch 401, loss 0.0607, train acc 98.84%, f1 0.9884, precision 0.9886, recall 0.9883, auc 0.9884
epoch 501, loss 0.0392, train acc 99.08%, f1 0.9908, precision 0.9912, recall 0.9903, auc 0.9908
epoch 601, loss 0.0494, train acc 99.20%, f1 0.9920, precision 0.9926, recall 0.9915, auc 0.9920
epoch 701, loss 0.0194, train acc 99.29%, f1 0.9929, precision 0.9927, recall 0.9930, auc 0.9929
epoch 801, loss 0.0331, train acc 99.36%, f1 0.9936, precision 0.9938, recall 0.9935, auc 0.9936
epoch 901, loss 0.0145, train acc 99.44%, f1 0.9944, precision 0.9941, recall 0.9946, auc 0.9944
epoch 1001, loss 0.0187, train acc 99.49%, f1 0.9949, precision 0.9954, recall 0.9945, auc 0.9949
epoch 1101, loss 0.0103, train acc 99.54%, f1 0.9954, precision 0.9960, recall 0.9948, auc 0.9954
epoch 1201, loss 0.0119, train acc 99.59%, f1 0.9959, precision 0.9961, recall 0.9958, auc 0.9959
epoch 1301, loss 0.0187, train acc 99.62%, f1 0.9962, precision 0.9967, recall 0.9958, auc 0.9962
epoch 1401, loss 0.0095, train acc 99.66%, f1 0.9966, precision 0.9969, recall 0.9963, auc 0.9966
epoch 1501, loss 0.0141, train acc 99.69%, f1 0.9969, precision 0.9971, recall 0.9967, auc 0.9969
epoch 1601, loss 0.0097, train acc 99.71%, f1 0.9971, precision 0.9975, recall 0.9967, auc 0.9971
epoch 1701, loss 0.0071, train acc 99.73%, f1 0.9973, precision 0.9977, recall 0.9969, auc 0.9973
epoch 1801, loss 0.0023, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9974, auc 0.9975
epoch 1901, loss 0.0076, train acc 99.77%, f1 0.9977, precision 0.9979, recall 0.9975, auc 0.9977
epoch 2001, loss 0.0078, train acc 99.77%, f1 0.9977, precision 0.9979, recall 0.9975, auc 0.9977
epoch 2101, loss 0.0082, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 2201, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9980, auc 0.9982
epoch 2301, loss 0.0079, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 2401, loss 0.0104, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 2501, loss 0.0075, train acc 99.83%, f1 0.9983, precision 0.9986, recall 0.9981, auc 0.9983
epoch 2601, loss 0.0056, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9983, auc 0.9985
epoch 2701, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9988, recall 0.9983, auc 0.9985
epoch 2801, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 2901, loss 0.0025, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9984, auc 0.9986
epoch 3001, loss 0.0077, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9985, auc 0.9987
epoch 3101, loss 0.0049, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 3201, loss 0.0053, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 3301, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 3401, loss 0.0059, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 3501, loss 0.0023, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 3601, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 3701, loss 0.0035, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 3801, loss 0.0020, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 3901, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9987, auc 0.9990
epoch 4001, loss 0.0049, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 4101, loss 0.0042, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 4201, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 4301, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 4401, loss 0.0035, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 4501, loss 0.0038, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 4601, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 4701, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 4801, loss 0.0027, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 4901, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5001, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 5101, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 5201, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 5301, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 5401, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 5501, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9997, auc 0.9995
epoch 5601, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 5701, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 5801, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 5901, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 6001, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 6101, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 6201, loss 0.0021, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 6301, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 6401, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 6501, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 6601, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 6701, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 6801, loss 0.0022, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 6901, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7001, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 7101, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 7201, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 7301, loss 0.0017, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 7401, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7501, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 7601, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 7701, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 7801, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 7901, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_4
./test_vehicle0/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.958624031007752

the Fscore is 0.9487179487179489

the precision is 0.9736842105263158

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_4
----------------------



epoch 1, loss 0.6929, train acc 50.16%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.2807, train acc 88.25%, f1 0.8825, precision 0.8797, recall 0.8854, auc 0.8825
epoch 201, loss 0.1449, train acc 97.46%, f1 0.9745, precision 0.9746, recall 0.9744, auc 0.9746
epoch 301, loss 0.0512, train acc 98.56%, f1 0.9856, precision 0.9851, recall 0.9860, auc 0.9856
epoch 401, loss 0.0605, train acc 98.80%, f1 0.9880, precision 0.9881, recall 0.9879, auc 0.9880
epoch 501, loss 0.0577, train acc 99.04%, f1 0.9904, precision 0.9903, recall 0.9905, auc 0.9904
epoch 601, loss 0.0232, train acc 99.18%, f1 0.9918, precision 0.9925, recall 0.9911, auc 0.9918
epoch 701, loss 0.0221, train acc 99.27%, f1 0.9927, precision 0.9929, recall 0.9925, auc 0.9927
epoch 801, loss 0.0155, train acc 99.36%, f1 0.9936, precision 0.9934, recall 0.9938, auc 0.9936
epoch 901, loss 0.0205, train acc 99.43%, f1 0.9942, precision 0.9944, recall 0.9940, auc 0.9943
epoch 1001, loss 0.0231, train acc 99.47%, f1 0.9946, precision 0.9950, recall 0.9943, auc 0.9947
epoch 1101, loss 0.0187, train acc 99.53%, f1 0.9953, precision 0.9952, recall 0.9954, auc 0.9953
epoch 1201, loss 0.0184, train acc 99.57%, f1 0.9957, precision 0.9960, recall 0.9953, auc 0.9957
epoch 1301, loss 0.0129, train acc 99.60%, f1 0.9960, precision 0.9958, recall 0.9962, auc 0.9960
epoch 1401, loss 0.0179, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 1501, loss 0.0107, train acc 99.68%, f1 0.9967, precision 0.9968, recall 0.9967, auc 0.9968
epoch 1601, loss 0.0095, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 1701, loss 0.0094, train acc 99.72%, f1 0.9972, precision 0.9969, recall 0.9975, auc 0.9972
epoch 1801, loss 0.0041, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9975, auc 0.9975
epoch 1901, loss 0.0060, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 2001, loss 0.0114, train acc 99.75%, f1 0.9975, precision 0.9973, recall 0.9978, auc 0.9975
epoch 2101, loss 0.0068, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2201, loss 0.0041, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2301, loss 0.0040, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 2401, loss 0.0073, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2501, loss 0.0024, train acc 99.83%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9983
epoch 2601, loss 0.0031, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9984, auc 0.9981
epoch 2701, loss 0.0051, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 2801, loss 0.0050, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9986, auc 0.9984
epoch 2901, loss 0.0064, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3001, loss 0.0063, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 3101, loss 0.0056, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 3201, loss 0.0057, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9988, auc 0.9987
epoch 3301, loss 0.0026, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9985, auc 0.9987
epoch 3401, loss 0.0040, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 3501, loss 0.0067, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3601, loss 0.0048, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 3701, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9990, auc 0.9989
epoch 3801, loss 0.0063, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9986, auc 0.9989
epoch 3901, loss 0.0075, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 4001, loss 0.0040, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 4101, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 4201, loss 0.0044, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 4301, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 4401, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 4501, loss 0.0024, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4601, loss 0.0036, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 4701, loss 0.0053, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 4801, loss 0.0028, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9992
epoch 4901, loss 0.0033, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_4
./test_vehicle0/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9547480620155038

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_4
----------------------



epoch 1, loss 0.6928, train acc 75.20%, f1 0.7282, precision 0.8018, recall 0.6670, auc 0.7517
epoch 101, loss 0.3486, train acc 87.94%, f1 0.8793, precision 0.8769, recall 0.8816, auc 0.8794
epoch 201, loss 0.1461, train acc 97.39%, f1 0.9738, precision 0.9736, recall 0.9739, auc 0.9739
epoch 301, loss 0.0800, train acc 98.55%, f1 0.9855, precision 0.9850, recall 0.9860, auc 0.9855
epoch 401, loss 0.0707, train acc 98.81%, f1 0.9881, precision 0.9879, recall 0.9882, auc 0.9881
epoch 501, loss 0.0335, train acc 99.00%, f1 0.9900, precision 0.9904, recall 0.9896, auc 0.9900
epoch 601, loss 0.0186, train acc 99.17%, f1 0.9917, precision 0.9916, recall 0.9919, auc 0.9917
epoch 701, loss 0.0327, train acc 99.27%, f1 0.9927, precision 0.9929, recall 0.9925, auc 0.9927
epoch 801, loss 0.0151, train acc 99.35%, f1 0.9935, precision 0.9933, recall 0.9938, auc 0.9935
epoch 901, loss 0.0182, train acc 99.40%, f1 0.9939, precision 0.9937, recall 0.9942, auc 0.9940
epoch 1001, loss 0.0180, train acc 99.46%, f1 0.9946, precision 0.9944, recall 0.9948, auc 0.9946
epoch 1101, loss 0.0118, train acc 99.52%, f1 0.9952, precision 0.9951, recall 0.9952, auc 0.9952
epoch 1201, loss 0.0259, train acc 99.57%, f1 0.9957, precision 0.9956, recall 0.9958, auc 0.9957
epoch 1301, loss 0.0293, train acc 99.61%, f1 0.9961, precision 0.9963, recall 0.9959, auc 0.9961
epoch 1401, loss 0.0180, train acc 99.65%, f1 0.9965, precision 0.9968, recall 0.9961, auc 0.9965
epoch 1501, loss 0.0090, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9966, auc 0.9968
epoch 1601, loss 0.0081, train acc 99.70%, f1 0.9970, precision 0.9974, recall 0.9967, auc 0.9970
epoch 1701, loss 0.0122, train acc 99.72%, f1 0.9972, precision 0.9975, recall 0.9969, auc 0.9972
epoch 1801, loss 0.0041, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9974, auc 0.9975
epoch 1901, loss 0.0051, train acc 99.76%, f1 0.9976, precision 0.9978, recall 0.9975, auc 0.9976
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_4
./test_vehicle0/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9306201550387597

the Fscore is 0.888888888888889

the precision is 0.8780487804878049

the recall is 0.9

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_4
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3982, train acc 84.48%, f1 0.8447, precision 0.8454, recall 0.8440, auc 0.8448
epoch 201, loss 0.2359, train acc 95.27%, f1 0.9526, precision 0.9528, recall 0.9525, auc 0.9527
epoch 301, loss 0.1236, train acc 98.01%, f1 0.9801, precision 0.9801, recall 0.9801, auc 0.9801
epoch 401, loss 0.1048, train acc 98.58%, f1 0.9858, precision 0.9858, recall 0.9858, auc 0.9858
epoch 501, loss 0.0544, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9879, auc 0.9879
epoch 601, loss 0.0276, train acc 98.94%, f1 0.9894, precision 0.9894, recall 0.9895, auc 0.9894
epoch 701, loss 0.0406, train acc 99.13%, f1 0.9913, precision 0.9913, recall 0.9913, auc 0.9913
epoch 801, loss 0.0425, train acc 99.20%, f1 0.9920, precision 0.9920, recall 0.9920, auc 0.9920
epoch 901, loss 0.0271, train acc 99.26%, f1 0.9926, precision 0.9926, recall 0.9926, auc 0.9926
epoch 1001, loss 0.0306, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9934, auc 0.9934
epoch 1101, loss 0.0201, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9939, auc 0.9939
epoch 1201, loss 0.0346, train acc 99.44%, f1 0.9944, precision 0.9944, recall 0.9944, auc 0.9944
epoch 1301, loss 0.0139, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1401, loss 0.0204, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1501, loss 0.0200, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1601, loss 0.0100, train acc 99.60%, f1 0.9960, precision 0.9959, recall 0.9960, auc 0.9960
epoch 1701, loss 0.0138, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1801, loss 0.0159, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 1901, loss 0.0089, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2001, loss 0.0090, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2101, loss 0.0083, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2201, loss 0.0069, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2301, loss 0.0106, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9975, auc 0.9975
epoch 2401, loss 0.0076, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2501, loss 0.0101, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 2601, loss 0.0065, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2701, loss 0.0063, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2801, loss 0.0053, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2901, loss 0.0046, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3001, loss 0.0039, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3101, loss 0.0044, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3201, loss 0.0087, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3301, loss 0.0031, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3401, loss 0.0073, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3501, loss 0.0029, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3601, loss 0.0041, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3701, loss 0.0048, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3801, loss 0.0024, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3901, loss 0.0069, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4001, loss 0.0054, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4101, loss 0.0047, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4201, loss 0.0044, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4301, loss 0.0042, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4401, loss 0.0049, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4501, loss 0.0060, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4601, loss 0.0089, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4701, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 4801, loss 0.0048, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4901, loss 0.0013, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5001, loss 0.0049, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5101, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5201, loss 0.0041, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5301, loss 0.0037, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 5401, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5501, loss 0.0054, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5601, loss 0.0039, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5701, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5801, loss 0.0042, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5901, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6001, loss 0.0012, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 6101, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6201, loss 0.0034, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6301, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6401, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6501, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6601, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6701, loss 0.0005, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6801, loss 0.0023, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6901, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7001, loss 0.0030, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7101, loss 0.0038, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7201, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7301, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7401, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7501, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7601, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7701, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7801, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7901, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8001, loss 0.0038, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8101, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8201, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8301, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8401, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8501, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 8601, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8701, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 8801, loss 0.0020, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8901, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9001, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9101, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9201, loss 0.0020, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9301, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9401, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9501, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9601, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9701, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9801, loss 0.0001, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9901, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10001, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10101, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10201, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10301, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10401, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10501, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10601, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10701, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10801, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10901, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11001, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11101, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_4
./test_vehicle0/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9422480620155038

the Fscore is 0.9230769230769231

the precision is 0.9473684210526315

the recall is 0.9

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_4
----------------------



epoch 1, loss 0.6937, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3859, train acc 84.19%, f1 0.8410, precision 0.8454, recall 0.8367, auc 0.8419
epoch 201, loss 0.1914, train acc 95.32%, f1 0.9531, precision 0.9538, recall 0.9525, auc 0.9532
epoch 301, loss 0.1151, train acc 97.89%, f1 0.9789, precision 0.9789, recall 0.9789, auc 0.9789
epoch 401, loss 0.0689, train acc 98.60%, f1 0.9860, precision 0.9859, recall 0.9861, auc 0.9860
epoch 501, loss 0.0613, train acc 98.72%, f1 0.9872, precision 0.9870, recall 0.9873, auc 0.9872
epoch 601, loss 0.0437, train acc 98.97%, f1 0.9897, precision 0.9896, recall 0.9899, auc 0.9897
epoch 701, loss 0.0283, train acc 99.10%, f1 0.9910, precision 0.9909, recall 0.9911, auc 0.9910
epoch 801, loss 0.0327, train acc 99.21%, f1 0.9921, precision 0.9920, recall 0.9921, auc 0.9921
epoch 901, loss 0.0453, train acc 99.24%, f1 0.9924, precision 0.9923, recall 0.9925, auc 0.9924
epoch 1001, loss 0.0206, train acc 99.33%, f1 0.9933, precision 0.9932, recall 0.9934, auc 0.9933
epoch 1101, loss 0.0242, train acc 99.36%, f1 0.9936, precision 0.9935, recall 0.9936, auc 0.9936
epoch 1201, loss 0.0155, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9942, auc 0.9941
epoch 1301, loss 0.0235, train acc 99.45%, f1 0.9945, precision 0.9944, recall 0.9946, auc 0.9945
epoch 1401, loss 0.0160, train acc 99.52%, f1 0.9952, precision 0.9951, recall 0.9953, auc 0.9952
epoch 1501, loss 0.0189, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9956, auc 0.9955
epoch 1601, loss 0.0103, train acc 99.60%, f1 0.9960, precision 0.9959, recall 0.9960, auc 0.9960
epoch 1701, loss 0.0185, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9963, auc 0.9963
epoch 1801, loss 0.0092, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9965, auc 0.9964
epoch 1901, loss 0.0130, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2001, loss 0.0039, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2101, loss 0.0110, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2201, loss 0.0104, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2301, loss 0.0049, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 2401, loss 0.0146, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0050, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 2601, loss 0.0044, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 2701, loss 0.0049, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2801, loss 0.0028, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 2901, loss 0.0041, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3001, loss 0.0063, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 3101, loss 0.0043, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3201, loss 0.0082, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 3301, loss 0.0056, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3401, loss 0.0048, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3501, loss 0.0019, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3601, loss 0.0037, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3701, loss 0.0022, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3801, loss 0.0070, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 3901, loss 0.0054, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4001, loss 0.0061, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4101, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4201, loss 0.0068, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4301, loss 0.0046, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4401, loss 0.0049, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4501, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4601, loss 0.0018, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4701, loss 0.0046, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4801, loss 0.0035, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4901, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5001, loss 0.0040, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 5101, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5201, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5301, loss 0.0025, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 5401, loss 0.0054, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5501, loss 0.0070, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5601, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5701, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5801, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5901, loss 0.0037, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 6001, loss 0.0030, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 6101, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 6201, loss 0.0007, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6301, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6401, loss 0.0039, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6501, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6601, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6701, loss 0.0032, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6801, loss 0.0027, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6901, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7001, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7101, loss 0.0031, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7201, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7301, loss 0.0028, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7401, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7501, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7601, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 7701, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7801, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7901, loss 0.0038, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 8001, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8101, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8201, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.0025, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8401, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8501, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 8601, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8701, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8801, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8901, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9001, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9101, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9201, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 9301, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9401, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9501, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9601, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9701, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9801, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9901, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10001, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10101, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10201, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10301, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 10401, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10501, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10601, loss 0.0015, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10701, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10801, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10901, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11001, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11101, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_4
./test_vehicle0/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9383720930232557

the Fscore is 0.9113924050632911

the precision is 0.9230769230769231

the recall is 0.9

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_4
----------------------



epoch 1, loss 0.6935, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.4466, train acc 84.10%, f1 0.8410, precision 0.8411, recall 0.8410, auc 0.8410
epoch 201, loss 0.2320, train acc 95.41%, f1 0.9541, precision 0.9541, recall 0.9541, auc 0.9541
epoch 301, loss 0.1314, train acc 97.93%, f1 0.9793, precision 0.9793, recall 0.9793, auc 0.9793
epoch 401, loss 0.0959, train acc 98.57%, f1 0.9857, precision 0.9857, recall 0.9857, auc 0.9857
epoch 501, loss 0.0676, train acc 98.75%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9875
epoch 601, loss 0.0402, train acc 98.98%, f1 0.9898, precision 0.9898, recall 0.9898, auc 0.9898
epoch 701, loss 0.0373, train acc 99.10%, f1 0.9910, precision 0.9910, recall 0.9910, auc 0.9910
epoch 801, loss 0.0235, train acc 99.19%, f1 0.9919, precision 0.9919, recall 0.9919, auc 0.9919
epoch 901, loss 0.0441, train acc 99.28%, f1 0.9928, precision 0.9928, recall 0.9928, auc 0.9928
epoch 1001, loss 0.0340, train acc 99.35%, f1 0.9935, precision 0.9935, recall 0.9935, auc 0.9935
epoch 1101, loss 0.0220, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9939, auc 0.9939
epoch 1201, loss 0.0159, train acc 99.45%, f1 0.9945, precision 0.9945, recall 0.9945, auc 0.9945
epoch 1301, loss 0.0286, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 1401, loss 0.0151, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9953, auc 0.9953
epoch 1501, loss 0.0205, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1601, loss 0.0137, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1701, loss 0.0163, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1801, loss 0.0126, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 1901, loss 0.0072, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2001, loss 0.0137, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2101, loss 0.0120, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2201, loss 0.0057, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2301, loss 0.0055, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2401, loss 0.0137, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2501, loss 0.0046, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2601, loss 0.0057, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2701, loss 0.0048, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2801, loss 0.0054, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2901, loss 0.0070, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3001, loss 0.0047, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3101, loss 0.0043, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3201, loss 0.0071, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3301, loss 0.0065, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3401, loss 0.0044, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3501, loss 0.0057, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3601, loss 0.0047, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3701, loss 0.0052, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3801, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3901, loss 0.0057, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4001, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4101, loss 0.0026, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4201, loss 0.0039, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4301, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4401, loss 0.0067, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4501, loss 0.0025, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4601, loss 0.0035, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4701, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4801, loss 0.0044, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4901, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5001, loss 0.0016, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5101, loss 0.0052, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5201, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5301, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5401, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5501, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5601, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5701, loss 0.0020, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5801, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5901, loss 0.0016, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6001, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6101, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6201, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6301, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6401, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6501, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6601, loss 0.0045, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6701, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6801, loss 0.0002, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6901, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7001, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7101, loss 0.0023, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7201, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7301, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7401, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7501, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7601, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7701, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7801, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7901, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8001, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8101, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8201, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8401, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8501, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8601, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8701, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8801, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8901, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9001, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9101, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9201, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9301, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9401, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9501, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9601, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9701, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9801, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_4
./test_vehicle0/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9422480620155038

the Fscore is 0.9230769230769231

the precision is 0.9473684210526315

the recall is 0.9

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_4
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4086, train acc 83.98%, f1 0.8391, precision 0.8426, recall 0.8356, auc 0.8398
epoch 201, loss 0.2393, train acc 95.30%, f1 0.9530, precision 0.9536, recall 0.9524, auc 0.9530
epoch 301, loss 0.1255, train acc 98.04%, f1 0.9804, precision 0.9803, recall 0.9804, auc 0.9804
epoch 401, loss 0.0910, train acc 98.58%, f1 0.9858, precision 0.9857, recall 0.9860, auc 0.9858
epoch 501, loss 0.0489, train acc 98.81%, f1 0.9881, precision 0.9879, recall 0.9882, auc 0.9881
epoch 601, loss 0.0401, train acc 98.96%, f1 0.9896, precision 0.9895, recall 0.9898, auc 0.9896
epoch 701, loss 0.0466, train acc 99.10%, f1 0.9910, precision 0.9908, recall 0.9911, auc 0.9910
epoch 801, loss 0.0414, train acc 99.21%, f1 0.9921, precision 0.9920, recall 0.9922, auc 0.9921
epoch 901, loss 0.0401, train acc 99.28%, f1 0.9928, precision 0.9928, recall 0.9929, auc 0.9928
epoch 1001, loss 0.0293, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9935, auc 0.9934
epoch 1101, loss 0.0171, train acc 99.38%, f1 0.9938, precision 0.9937, recall 0.9939, auc 0.9938
epoch 1201, loss 0.0392, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9944, auc 0.9943
epoch 1301, loss 0.0189, train acc 99.49%, f1 0.9949, precision 0.9948, recall 0.9950, auc 0.9949
epoch 1401, loss 0.0185, train acc 99.53%, f1 0.9953, precision 0.9952, recall 0.9953, auc 0.9953
epoch 1501, loss 0.0249, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9958, auc 0.9958
epoch 1601, loss 0.0154, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9962, auc 0.9961
epoch 1701, loss 0.0109, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9964, auc 0.9963
epoch 1801, loss 0.0088, train acc 99.67%, f1 0.9967, precision 0.9966, recall 0.9967, auc 0.9967
epoch 1901, loss 0.0126, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9969, auc 0.9969
epoch 2001, loss 0.0105, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2101, loss 0.0132, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9972
epoch 2201, loss 0.0086, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2301, loss 0.0069, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 2401, loss 0.0108, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 2501, loss 0.0065, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9979, auc 0.9978
epoch 2601, loss 0.0047, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 2701, loss 0.0123, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2801, loss 0.0066, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2901, loss 0.0030, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3001, loss 0.0073, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 3101, loss 0.0093, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 3201, loss 0.0075, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3301, loss 0.0055, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 3401, loss 0.0047, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 3501, loss 0.0037, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3601, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3701, loss 0.0079, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3801, loss 0.0051, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3901, loss 0.0052, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4001, loss 0.0049, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 4101, loss 0.0035, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4201, loss 0.0024, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 4301, loss 0.0057, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4401, loss 0.0039, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 4501, loss 0.0071, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4601, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4701, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4801, loss 0.0012, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4901, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5001, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5101, loss 0.0057, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 5201, loss 0.0023, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5301, loss 0.0046, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5401, loss 0.0042, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5501, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 5601, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5701, loss 0.0039, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5801, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5901, loss 0.0027, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6001, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 6101, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6201, loss 0.0039, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6301, loss 0.0033, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6401, loss 0.0003, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6501, loss 0.0033, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6601, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6701, loss 0.0026, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6801, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6901, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7001, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7101, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7201, loss 0.0024, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7301, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7401, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7501, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7601, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7701, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7801, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 7901, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_4
./test_vehicle0/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9508720930232558

the Fscore is 0.925

the precision is 0.925

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_4
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.4182, train acc 83.90%, f1 0.8392, precision 0.8382, recall 0.8403, auc 0.8390
epoch 201, loss 0.2060, train acc 95.41%, f1 0.9541, precision 0.9538, recall 0.9544, auc 0.9541
epoch 301, loss 0.1337, train acc 97.94%, f1 0.9794, precision 0.9794, recall 0.9793, auc 0.9794
epoch 401, loss 0.0667, train acc 98.63%, f1 0.9863, precision 0.9863, recall 0.9863, auc 0.9863
epoch 501, loss 0.0709, train acc 98.79%, f1 0.9879, precision 0.9880, recall 0.9879, auc 0.9879
epoch 601, loss 0.0442, train acc 98.95%, f1 0.9895, precision 0.9896, recall 0.9895, auc 0.9895
epoch 701, loss 0.0489, train acc 99.08%, f1 0.9908, precision 0.9908, recall 0.9907, auc 0.9908
epoch 801, loss 0.0371, train acc 99.18%, f1 0.9918, precision 0.9919, recall 0.9918, auc 0.9918
epoch 901, loss 0.0296, train acc 99.25%, f1 0.9925, precision 0.9926, recall 0.9925, auc 0.9925
epoch 1001, loss 0.0288, train acc 99.32%, f1 0.9932, precision 0.9933, recall 0.9932, auc 0.9932
epoch 1101, loss 0.0279, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9939, auc 0.9939
epoch 1201, loss 0.0180, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9942, auc 0.9943
epoch 1301, loss 0.0194, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9947, auc 0.9947
epoch 1401, loss 0.0209, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1501, loss 0.0230, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1601, loss 0.0189, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 1701, loss 0.0269, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1801, loss 0.0076, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 1901, loss 0.0132, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2001, loss 0.0095, train acc 99.71%, f1 0.9971, precision 0.9972, recall 0.9971, auc 0.9971
epoch 2101, loss 0.0046, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 2201, loss 0.0086, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2301, loss 0.0102, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9976, auc 0.9976
epoch 2401, loss 0.0081, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9976, auc 0.9977
epoch 2501, loss 0.0102, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2601, loss 0.0069, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2701, loss 0.0032, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2801, loss 0.0042, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2901, loss 0.0033, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3001, loss 0.0034, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 3101, loss 0.0047, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3201, loss 0.0061, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3301, loss 0.0048, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3401, loss 0.0059, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3501, loss 0.0034, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3601, loss 0.0035, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3701, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3801, loss 0.0021, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 3901, loss 0.0027, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4001, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4101, loss 0.0029, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4201, loss 0.0050, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4301, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4401, loss 0.0042, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4501, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4601, loss 0.0067, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4701, loss 0.0050, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4801, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 4901, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_4
./test_vehicle0/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9344961240310076

the Fscore is 0.9

the precision is 0.9

the recall is 0.9

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_4
----------------------



epoch 1, loss 0.6934, train acc 50.87%, f1 0.0377, precision 0.9146, recall 0.0193, auc 0.5087
epoch 101, loss 0.4044, train acc 84.53%, f1 0.8453, precision 0.8452, recall 0.8453, auc 0.8453
epoch 201, loss 0.2449, train acc 95.46%, f1 0.9546, precision 0.9546, recall 0.9546, auc 0.9546
epoch 301, loss 0.1057, train acc 97.98%, f1 0.9798, precision 0.9798, recall 0.9798, auc 0.9798
epoch 401, loss 0.0647, train acc 98.50%, f1 0.9850, precision 0.9850, recall 0.9850, auc 0.9850
epoch 501, loss 0.0653, train acc 98.82%, f1 0.9882, precision 0.9882, recall 0.9882, auc 0.9882
epoch 601, loss 0.0482, train acc 98.98%, f1 0.9898, precision 0.9898, recall 0.9898, auc 0.9898
epoch 701, loss 0.0477, train acc 99.11%, f1 0.9911, precision 0.9911, recall 0.9911, auc 0.9911
epoch 801, loss 0.0305, train acc 99.21%, f1 0.9921, precision 0.9921, recall 0.9921, auc 0.9921
epoch 901, loss 0.0341, train acc 99.26%, f1 0.9926, precision 0.9926, recall 0.9926, auc 0.9926
epoch 1001, loss 0.0244, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9933, auc 0.9933
epoch 1101, loss 0.0259, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9939, auc 0.9939
epoch 1201, loss 0.0231, train acc 99.44%, f1 0.9944, precision 0.9944, recall 0.9944, auc 0.9944
epoch 1301, loss 0.0196, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1401, loss 0.0106, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1501, loss 0.0139, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1601, loss 0.0166, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1701, loss 0.0051, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1801, loss 0.0165, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1901, loss 0.0104, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_4
./test_vehicle0/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9469961240310077

the Fscore is 0.9135802469135802

the precision is 0.9024390243902439

the recall is 0.925

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_4
----------------------



epoch 1, loss 0.6936, train acc 49.87%, f1 0.6655, precision 0.4987, recall 1.0000, auc 0.5000
epoch 101, loss 0.4174, train acc 84.20%, f1 0.8419, precision 0.8406, recall 0.8431, auc 0.8420
epoch 201, loss 0.2042, train acc 95.22%, f1 0.9520, precision 0.9525, recall 0.9515, auc 0.9521
epoch 301, loss 0.1102, train acc 97.89%, f1 0.9788, precision 0.9793, recall 0.9784, auc 0.9789
epoch 401, loss 0.0809, train acc 98.60%, f1 0.9859, precision 0.9860, recall 0.9859, auc 0.9860
epoch 501, loss 0.0676, train acc 98.84%, f1 0.9884, precision 0.9887, recall 0.9881, auc 0.9884
epoch 601, loss 0.0564, train acc 99.00%, f1 0.9900, precision 0.9898, recall 0.9902, auc 0.9900
epoch 701, loss 0.0466, train acc 99.09%, f1 0.9909, precision 0.9909, recall 0.9908, auc 0.9909
epoch 801, loss 0.0281, train acc 99.20%, f1 0.9920, precision 0.9921, recall 0.9919, auc 0.9920
epoch 901, loss 0.0295, train acc 99.27%, f1 0.9927, precision 0.9928, recall 0.9926, auc 0.9927
epoch 1001, loss 0.0321, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9932, auc 0.9933
epoch 1101, loss 0.0342, train acc 99.37%, f1 0.9937, precision 0.9937, recall 0.9936, auc 0.9937
epoch 1201, loss 0.0301, train acc 99.43%, f1 0.9942, precision 0.9944, recall 0.9941, auc 0.9943
epoch 1301, loss 0.0202, train acc 99.48%, f1 0.9948, precision 0.9951, recall 0.9945, auc 0.9948
epoch 1401, loss 0.0196, train acc 99.53%, f1 0.9952, precision 0.9955, recall 0.9950, auc 0.9953
epoch 1501, loss 0.0145, train acc 99.56%, f1 0.9956, precision 0.9957, recall 0.9956, auc 0.9956
epoch 1601, loss 0.0209, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 1701, loss 0.0100, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9962, auc 0.9963
epoch 1801, loss 0.0173, train acc 99.67%, f1 0.9967, precision 0.9966, recall 0.9967, auc 0.9967
epoch 1901, loss 0.0148, train acc 99.68%, f1 0.9967, precision 0.9968, recall 0.9967, auc 0.9968
epoch 2001, loss 0.0098, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9972, auc 0.9971
epoch 2101, loss 0.0153, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 2201, loss 0.0122, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2301, loss 0.0074, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9974, auc 0.9975
epoch 2401, loss 0.0043, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9977, auc 0.9978
epoch 2501, loss 0.0078, train acc 99.78%, f1 0.9978, precision 0.9980, recall 0.9976, auc 0.9978
epoch 2601, loss 0.0067, train acc 99.79%, f1 0.9978, precision 0.9980, recall 0.9977, auc 0.9979
epoch 2701, loss 0.0072, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 2801, loss 0.0061, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 2901, loss 0.0041, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 3001, loss 0.0087, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9981, auc 0.9983
epoch 3101, loss 0.0063, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 3201, loss 0.0054, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 3301, loss 0.0070, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 3401, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 3501, loss 0.0032, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 3601, loss 0.0038, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 3701, loss 0.0050, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 3801, loss 0.0025, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 3901, loss 0.0061, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4001, loss 0.0054, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 4101, loss 0.0067, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4201, loss 0.0067, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 4301, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 4401, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 4501, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 4601, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4701, loss 0.0058, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 4801, loss 0.0024, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 4901, loss 0.0045, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 5001, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 5101, loss 0.0033, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 5201, loss 0.0035, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 5301, loss 0.0035, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 5401, loss 0.0033, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9991, auc 0.9993
epoch 5501, loss 0.0053, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 5601, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 5701, loss 0.0021, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 5801, loss 0.0040, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 5901, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6001, loss 0.0051, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 6101, loss 0.0038, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6201, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 6301, loss 0.0011, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6401, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6501, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 6601, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6701, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6801, loss 0.0023, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6901, loss 0.0026, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7001, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 7101, loss 0.0023, train acc 99.96%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7201, loss 0.0017, train acc 99.96%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7301, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7401, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7501, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 7601, loss 0.0025, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 7701, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7801, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7901, loss 0.0021, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8001, loss 0.0021, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8101, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8201, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 8301, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 8401, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 8501, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 8601, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8701, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 8801, loss 0.0018, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8901, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9001, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 9101, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 9201, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9301, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9401, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 9501, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9601, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 9701, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9801, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9901, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10001, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10101, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 10201, loss 0.0006, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10301, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 10401, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_4
./test_vehicle0/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9422480620155038

the Fscore is 0.9230769230769231

the precision is 0.9473684210526315

the recall is 0.9

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_4
----------------------



epoch 1, loss 0.6930, train acc 49.86%, f1 0.6654, precision 0.4986, recall 1.0000, auc 0.5000
epoch 101, loss 0.4426, train acc 84.04%, f1 0.8403, precision 0.8384, recall 0.8422, auc 0.8404
epoch 201, loss 0.2140, train acc 95.25%, f1 0.9525, precision 0.9505, recall 0.9544, auc 0.9525
epoch 301, loss 0.1588, train acc 97.95%, f1 0.9795, precision 0.9782, recall 0.9808, auc 0.9795
epoch 401, loss 0.0829, train acc 98.54%, f1 0.9854, precision 0.9844, recall 0.9864, auc 0.9854
epoch 501, loss 0.0550, train acc 98.77%, f1 0.9876, precision 0.9866, recall 0.9887, auc 0.9877
epoch 601, loss 0.0508, train acc 98.97%, f1 0.9897, precision 0.9890, recall 0.9904, auc 0.9897
epoch 701, loss 0.0498, train acc 99.07%, f1 0.9906, precision 0.9899, recall 0.9914, auc 0.9907
epoch 801, loss 0.0302, train acc 99.19%, f1 0.9918, precision 0.9913, recall 0.9924, auc 0.9919
epoch 901, loss 0.0431, train acc 99.25%, f1 0.9924, precision 0.9919, recall 0.9929, auc 0.9925
epoch 1001, loss 0.0197, train acc 99.34%, f1 0.9934, precision 0.9928, recall 0.9939, auc 0.9934
epoch 1101, loss 0.0235, train acc 99.39%, f1 0.9939, precision 0.9935, recall 0.9943, auc 0.9939
epoch 1201, loss 0.0155, train acc 99.43%, f1 0.9943, precision 0.9939, recall 0.9948, auc 0.9943
epoch 1301, loss 0.0178, train acc 99.46%, f1 0.9946, precision 0.9942, recall 0.9949, auc 0.9946
epoch 1401, loss 0.0182, train acc 99.51%, f1 0.9951, precision 0.9947, recall 0.9955, auc 0.9951
epoch 1501, loss 0.0181, train acc 99.54%, f1 0.9954, precision 0.9952, recall 0.9957, auc 0.9954
epoch 1601, loss 0.0174, train acc 99.60%, f1 0.9960, precision 0.9957, recall 0.9963, auc 0.9960
epoch 1701, loss 0.0119, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9964, auc 0.9962
epoch 1801, loss 0.0125, train acc 99.65%, f1 0.9965, precision 0.9962, recall 0.9969, auc 0.9965
epoch 1901, loss 0.0174, train acc 99.69%, f1 0.9969, precision 0.9965, recall 0.9972, auc 0.9969
epoch 2001, loss 0.0055, train acc 99.71%, f1 0.9971, precision 0.9968, recall 0.9974, auc 0.9971
epoch 2101, loss 0.0166, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9972, auc 0.9971
epoch 2201, loss 0.0092, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 2301, loss 0.0118, train acc 99.74%, f1 0.9974, precision 0.9972, recall 0.9975, auc 0.9974
epoch 2401, loss 0.0087, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 2501, loss 0.0084, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 2601, loss 0.0085, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9981, auc 0.9980
epoch 2701, loss 0.0089, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 2801, loss 0.0079, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 2901, loss 0.0043, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9983, auc 0.9982
epoch 3001, loss 0.0035, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3101, loss 0.0084, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9985, auc 0.9983
epoch 3201, loss 0.0058, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 3301, loss 0.0070, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 3401, loss 0.0041, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 3501, loss 0.0049, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 3601, loss 0.0016, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9988, auc 0.9987
epoch 3701, loss 0.0035, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 3801, loss 0.0030, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9989, auc 0.9987
epoch 3901, loss 0.0055, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 4001, loss 0.0043, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 4101, loss 0.0019, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 4201, loss 0.0020, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 4301, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 4401, loss 0.0010, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 4501, loss 0.0046, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 4601, loss 0.0026, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 4701, loss 0.0043, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 4801, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 4901, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 5001, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 5101, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 5201, loss 0.0009, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5301, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 5401, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9992, auc 0.9991
epoch 5501, loss 0.0023, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 5601, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 5701, loss 0.0055, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 5801, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 5901, loss 0.0023, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 6001, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6101, loss 0.0012, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 6201, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6301, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9995, auc 0.9993
epoch 6401, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 6501, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 6601, loss 0.0005, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 6701, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 6801, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 6901, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 7001, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 7101, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 7201, loss 0.0026, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 7301, loss 0.0030, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9997, auc 0.9995
epoch 7401, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9997, auc 0.9995
epoch 7501, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 7601, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 7701, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 7801, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9997, auc 0.9996
epoch 7901, loss 0.0027, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 8001, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9995, recall 0.9998, auc 0.9997
epoch 8101, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9995, recall 0.9999, auc 0.9997
epoch 8301, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9995, recall 0.9999, auc 0.9997
epoch 8401, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9995, recall 0.9999, auc 0.9997
epoch 8501, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9999, auc 0.9997
epoch 8601, loss 0.0024, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 8701, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9999, auc 0.9997
epoch 8801, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9996, recall 0.9999, auc 0.9998
epoch 8901, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9999, auc 0.9997
epoch 9001, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9996, recall 0.9999, auc 0.9998
epoch 9101, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9996, recall 0.9999, auc 0.9998
epoch 9201, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 9301, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 9401, loss 0.0021, train acc 99.98%, f1 0.9998, precision 0.9997, recall 1.0000, auc 0.9998
epoch 9501, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 9601, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 9701, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 9801, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 9901, loss 0.0017, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10001, loss 0.0015, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10101, loss 0.0015, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10201, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 10501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_4
./test_vehicle0/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9172480620155039

the Fscore is 0.8947368421052632

the precision is 0.9444444444444444

the recall is 0.85

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_4
----------------------



epoch 1, loss 0.6937, train acc 50.26%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4314, train acc 84.09%, f1 0.8389, precision 0.8453, recall 0.8325, auc 0.8409
epoch 201, loss 0.2551, train acc 94.92%, f1 0.9489, precision 0.9492, recall 0.9486, auc 0.9492
epoch 301, loss 0.1167, train acc 98.08%, f1 0.9807, precision 0.9812, recall 0.9802, auc 0.9808
epoch 401, loss 0.0637, train acc 98.52%, f1 0.9851, precision 0.9852, recall 0.9849, auc 0.9852
epoch 501, loss 0.0491, train acc 98.76%, f1 0.9876, precision 0.9873, recall 0.9878, auc 0.9876
epoch 601, loss 0.0463, train acc 98.95%, f1 0.9894, precision 0.9889, recall 0.9900, auc 0.9895
epoch 701, loss 0.0266, train acc 99.10%, f1 0.9910, precision 0.9906, recall 0.9913, auc 0.9910
epoch 801, loss 0.0287, train acc 99.20%, f1 0.9920, precision 0.9917, recall 0.9923, auc 0.9920
epoch 901, loss 0.0380, train acc 99.26%, f1 0.9926, precision 0.9922, recall 0.9929, auc 0.9926
epoch 1001, loss 0.0227, train acc 99.34%, f1 0.9933, precision 0.9930, recall 0.9937, auc 0.9934
epoch 1101, loss 0.0394, train acc 99.39%, f1 0.9939, precision 0.9935, recall 0.9942, auc 0.9939
epoch 1201, loss 0.0264, train acc 99.43%, f1 0.9943, precision 0.9940, recall 0.9946, auc 0.9943
epoch 1301, loss 0.0238, train acc 99.48%, f1 0.9948, precision 0.9946, recall 0.9950, auc 0.9948
epoch 1401, loss 0.0135, train acc 99.51%, f1 0.9950, precision 0.9949, recall 0.9951, auc 0.9951
epoch 1501, loss 0.0145, train acc 99.55%, f1 0.9955, precision 0.9953, recall 0.9958, auc 0.9955
epoch 1601, loss 0.0149, train acc 99.59%, f1 0.9959, precision 0.9956, recall 0.9961, auc 0.9959
epoch 1701, loss 0.0264, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9965, auc 0.9962
epoch 1801, loss 0.0109, train acc 99.64%, f1 0.9964, precision 0.9962, recall 0.9965, auc 0.9964
epoch 1901, loss 0.0117, train acc 99.67%, f1 0.9967, precision 0.9964, recall 0.9970, auc 0.9967
epoch 2001, loss 0.0165, train acc 99.69%, f1 0.9969, precision 0.9967, recall 0.9972, auc 0.9969
epoch 2101, loss 0.0152, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 2201, loss 0.0095, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9973
epoch 2301, loss 0.0164, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 2401, loss 0.0054, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9976, auc 0.9975
epoch 2501, loss 0.0232, train acc 99.76%, f1 0.9975, precision 0.9974, recall 0.9977, auc 0.9976
epoch 2601, loss 0.0086, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9979, auc 0.9978
epoch 2701, loss 0.0090, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 2801, loss 0.0047, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 2901, loss 0.0083, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3001, loss 0.0103, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9981, auc 0.9982
epoch 3101, loss 0.0057, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3201, loss 0.0040, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 3301, loss 0.0105, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 3401, loss 0.0092, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 3501, loss 0.0025, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 3601, loss 0.0024, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 3701, loss 0.0058, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 3801, loss 0.0067, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3901, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4001, loss 0.0087, train acc 99.87%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4101, loss 0.0030, train acc 99.87%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9987
epoch 4201, loss 0.0044, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4301, loss 0.0074, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4401, loss 0.0052, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4501, loss 0.0025, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 4601, loss 0.0069, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4701, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 4801, loss 0.0047, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 4901, loss 0.0034, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 5001, loss 0.0013, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5101, loss 0.0050, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5201, loss 0.0037, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 5301, loss 0.0013, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 5401, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 5501, loss 0.0050, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5601, loss 0.0060, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 5701, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 5801, loss 0.0046, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 5901, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 6001, loss 0.0023, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 6101, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 6201, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 6301, loss 0.0027, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6401, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 6501, loss 0.0032, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6601, loss 0.0033, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 6701, loss 0.0018, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6801, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 6901, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 7001, loss 0.0041, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 7101, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 7201, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7301, loss 0.0033, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7401, loss 0.0033, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7501, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7601, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7701, loss 0.0030, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 7801, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 7901, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 8001, loss 0.0029, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 8101, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 8301, loss 0.0014, train acc 99.96%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9996
epoch 8401, loss 0.0011, train acc 99.96%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9996
epoch 8501, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 8601, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8701, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 8801, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 8901, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 9001, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 9101, loss 0.0025, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 9201, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 9301, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 9401, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 9501, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 9601, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 9701, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 9801, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9901, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_4
./test_vehicle0/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9547480620155038

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_4
----------------------



epoch 1, loss 0.6930, train acc 49.68%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4205, train acc 83.93%, f1 0.8389, precision 0.8464, recall 0.8316, auc 0.8393
epoch 201, loss 0.2098, train acc 95.29%, f1 0.9531, precision 0.9547, recall 0.9516, auc 0.9529
epoch 301, loss 0.1522, train acc 97.89%, f1 0.9791, precision 0.9796, recall 0.9785, auc 0.9789
epoch 401, loss 0.0830, train acc 98.51%, f1 0.9852, precision 0.9854, recall 0.9851, auc 0.9851
epoch 501, loss 0.0583, train acc 98.77%, f1 0.9878, precision 0.9879, recall 0.9876, auc 0.9877
epoch 601, loss 0.0361, train acc 98.94%, f1 0.9895, precision 0.9893, recall 0.9897, auc 0.9894
epoch 701, loss 0.0465, train acc 99.07%, f1 0.9908, precision 0.9907, recall 0.9909, auc 0.9907
epoch 801, loss 0.0241, train acc 99.18%, f1 0.9918, precision 0.9916, recall 0.9921, auc 0.9918
epoch 901, loss 0.0379, train acc 99.26%, f1 0.9927, precision 0.9923, recall 0.9931, auc 0.9926
epoch 1001, loss 0.0210, train acc 99.32%, f1 0.9933, precision 0.9931, recall 0.9935, auc 0.9932
epoch 1101, loss 0.0179, train acc 99.39%, f1 0.9939, precision 0.9936, recall 0.9943, auc 0.9939
epoch 1201, loss 0.0111, train acc 99.43%, f1 0.9943, precision 0.9940, recall 0.9947, auc 0.9943
epoch 1301, loss 0.0315, train acc 99.47%, f1 0.9947, precision 0.9944, recall 0.9951, auc 0.9947
epoch 1401, loss 0.0132, train acc 99.51%, f1 0.9951, precision 0.9947, recall 0.9956, auc 0.9951
epoch 1501, loss 0.0304, train acc 99.56%, f1 0.9957, precision 0.9955, recall 0.9959, auc 0.9956
epoch 1601, loss 0.0269, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9960, auc 0.9959
epoch 1701, loss 0.0113, train acc 99.61%, f1 0.9962, precision 0.9960, recall 0.9963, auc 0.9961
epoch 1801, loss 0.0110, train acc 99.65%, f1 0.9965, precision 0.9963, recall 0.9967, auc 0.9965
epoch 1901, loss 0.0135, train acc 99.67%, f1 0.9968, precision 0.9968, recall 0.9967, auc 0.9967
epoch 2001, loss 0.0189, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 2101, loss 0.0091, train acc 99.72%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9972
epoch 2201, loss 0.0096, train acc 99.74%, f1 0.9974, precision 0.9975, recall 0.9974, auc 0.9974
epoch 2301, loss 0.0058, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9976, auc 0.9974
epoch 2401, loss 0.0049, train acc 99.75%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9975
epoch 2501, loss 0.0124, train acc 99.77%, f1 0.9978, precision 0.9978, recall 0.9977, auc 0.9977
epoch 2601, loss 0.0066, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2701, loss 0.0025, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2801, loss 0.0040, train acc 99.80%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9980
epoch 2901, loss 0.0047, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3001, loss 0.0064, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 3101, loss 0.0049, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 3201, loss 0.0057, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 3301, loss 0.0058, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 3401, loss 0.0074, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 3501, loss 0.0044, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 3601, loss 0.0025, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 3701, loss 0.0055, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9988, auc 0.9987
epoch 3801, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3901, loss 0.0065, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 4001, loss 0.0053, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 4101, loss 0.0057, train acc 99.88%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9988
epoch 4201, loss 0.0024, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 4301, loss 0.0020, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4401, loss 0.0013, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4501, loss 0.0049, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 4601, loss 0.0011, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4701, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 4801, loss 0.0049, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 4901, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 5001, loss 0.0061, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5101, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 5201, loss 0.0045, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 5301, loss 0.0030, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5401, loss 0.0032, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 5501, loss 0.0036, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 5601, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5701, loss 0.0026, train acc 99.92%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9992
epoch 5801, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 5901, loss 0.0013, train acc 99.92%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9992
epoch 6001, loss 0.0048, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 6101, loss 0.0032, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6201, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6301, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6401, loss 0.0083, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6501, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6601, loss 0.0023, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6701, loss 0.0026, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6801, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6901, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 7001, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 7101, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 7201, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 7301, loss 0.0023, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 7401, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 7501, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 7601, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 7701, loss 0.0034, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 7801, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 7901, loss 0.0024, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_4
./test_vehicle0/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9461240310077519

the Fscore is 0.935064935064935

the precision is 0.972972972972973

the recall is 0.9

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_4
----------------------



epoch 1, loss 0.6930, train acc 49.70%, f1 0.6640, precision 0.4970, recall 1.0000, auc 0.5000
epoch 101, loss 0.4115, train acc 84.36%, f1 0.8446, precision 0.8347, recall 0.8547, auc 0.8437
epoch 201, loss 0.2275, train acc 95.00%, f1 0.9496, precision 0.9502, recall 0.9491, auc 0.9499
epoch 301, loss 0.1220, train acc 97.96%, f1 0.9795, precision 0.9801, recall 0.9788, auc 0.9796
epoch 401, loss 0.0608, train acc 98.53%, f1 0.9852, precision 0.9855, recall 0.9848, auc 0.9853
epoch 501, loss 0.0611, train acc 98.75%, f1 0.9875, precision 0.9879, recall 0.9870, auc 0.9875
epoch 601, loss 0.0411, train acc 98.96%, f1 0.9896, precision 0.9899, recall 0.9893, auc 0.9896
epoch 701, loss 0.0468, train acc 99.13%, f1 0.9912, precision 0.9914, recall 0.9910, auc 0.9913
epoch 801, loss 0.0290, train acc 99.20%, f1 0.9920, precision 0.9920, recall 0.9919, auc 0.9920
epoch 901, loss 0.0349, train acc 99.26%, f1 0.9926, precision 0.9924, recall 0.9927, auc 0.9926
epoch 1001, loss 0.0254, train acc 99.32%, f1 0.9932, precision 0.9934, recall 0.9929, auc 0.9932
epoch 1101, loss 0.0353, train acc 99.39%, f1 0.9939, precision 0.9940, recall 0.9937, auc 0.9939
epoch 1201, loss 0.0175, train acc 99.43%, f1 0.9943, precision 0.9942, recall 0.9943, auc 0.9943
epoch 1301, loss 0.0186, train acc 99.47%, f1 0.9946, precision 0.9949, recall 0.9944, auc 0.9947
epoch 1401, loss 0.0071, train acc 99.52%, f1 0.9951, precision 0.9951, recall 0.9952, auc 0.9952
epoch 1501, loss 0.0180, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9957, auc 0.9956
epoch 1601, loss 0.0131, train acc 99.59%, f1 0.9959, precision 0.9958, recall 0.9959, auc 0.9959
epoch 1701, loss 0.0092, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9964, auc 0.9962
epoch 1801, loss 0.0151, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9966, auc 0.9965
epoch 1901, loss 0.0095, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2001, loss 0.0127, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 2101, loss 0.0219, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9970, auc 0.9971
epoch 2201, loss 0.0135, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9972, auc 0.9973
epoch 2301, loss 0.0053, train acc 99.75%, f1 0.9974, precision 0.9976, recall 0.9973, auc 0.9974
epoch 2401, loss 0.0173, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9975, auc 0.9976
epoch 2501, loss 0.0074, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 2601, loss 0.0084, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 2701, loss 0.0063, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 2801, loss 0.0055, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 2901, loss 0.0091, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3001, loss 0.0053, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 3101, loss 0.0107, train acc 99.83%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9983
epoch 3201, loss 0.0065, train acc 99.83%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9983
epoch 3301, loss 0.0034, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 3401, loss 0.0035, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 3501, loss 0.0059, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 3601, loss 0.0036, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 3701, loss 0.0043, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 3801, loss 0.0054, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3901, loss 0.0053, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 4001, loss 0.0057, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 4101, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 4201, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 4301, loss 0.0066, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4401, loss 0.0041, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4501, loss 0.0072, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 4601, loss 0.0020, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4701, loss 0.0013, train acc 99.90%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9990
epoch 4801, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 4901, loss 0.0055, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_4
./test_vehicle0/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9469961240310077

the Fscore is 0.9135802469135802

the precision is 0.9024390243902439

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_4
----------------------



epoch 1, loss 0.6932, train acc 49.90%, f1 0.6657, precision 0.4990, recall 1.0000, auc 0.5000
epoch 101, loss 0.4140, train acc 83.86%, f1 0.8393, precision 0.8335, recall 0.8452, auc 0.8386
epoch 201, loss 0.2179, train acc 95.21%, f1 0.9520, precision 0.9525, recall 0.9515, auc 0.9521
epoch 301, loss 0.1114, train acc 97.90%, f1 0.9789, precision 0.9794, recall 0.9784, auc 0.9790
epoch 401, loss 0.0757, train acc 98.49%, f1 0.9848, precision 0.9853, recall 0.9844, auc 0.9849
epoch 501, loss 0.0538, train acc 98.78%, f1 0.9877, precision 0.9883, recall 0.9872, auc 0.9878
epoch 601, loss 0.0428, train acc 98.94%, f1 0.9894, precision 0.9896, recall 0.9892, auc 0.9894
epoch 701, loss 0.0341, train acc 99.06%, f1 0.9906, precision 0.9909, recall 0.9902, auc 0.9906
epoch 801, loss 0.0396, train acc 99.16%, f1 0.9916, precision 0.9917, recall 0.9914, auc 0.9916
epoch 901, loss 0.0368, train acc 99.24%, f1 0.9924, precision 0.9926, recall 0.9921, auc 0.9924
epoch 1001, loss 0.0216, train acc 99.31%, f1 0.9931, precision 0.9936, recall 0.9927, auc 0.9931
epoch 1101, loss 0.0228, train acc 99.37%, f1 0.9937, precision 0.9940, recall 0.9933, auc 0.9937
epoch 1201, loss 0.0159, train acc 99.41%, f1 0.9941, precision 0.9944, recall 0.9938, auc 0.9941
epoch 1301, loss 0.0295, train acc 99.46%, f1 0.9946, precision 0.9946, recall 0.9945, auc 0.9946
epoch 1401, loss 0.0144, train acc 99.52%, f1 0.9952, precision 0.9953, recall 0.9951, auc 0.9952
epoch 1501, loss 0.0183, train acc 99.56%, f1 0.9956, precision 0.9957, recall 0.9955, auc 0.9956
epoch 1601, loss 0.0187, train acc 99.60%, f1 0.9959, precision 0.9959, recall 0.9960, auc 0.9960
epoch 1701, loss 0.0069, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1801, loss 0.0202, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9966, auc 0.9965
epoch 1901, loss 0.0072, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_4
./test_vehicle0/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9431201550387598

the Fscore is 0.9024390243902439

the precision is 0.8809523809523809

the recall is 0.925

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_4
----------------------



epoch 1, loss 0.6945, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4685, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4102, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3669, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3301, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.2988, train acc 77.55%, f1 0.0952, precision 0.8889, recall 0.0503, auc 0.5242
epoch 601, loss 0.2721, train acc 79.32%, f1 0.2308, precision 0.9130, recall 0.1321, auc 0.5641
epoch 701, loss 0.2491, train acc 83.01%, f1 0.4444, precision 0.9583, recall 0.2893, auc 0.6427
epoch 801, loss 0.2289, train acc 87.59%, f1 0.6529, precision 0.9518, recall 0.4969, auc 0.7446
epoch 901, loss 0.2106, train acc 90.10%, f1 0.7413, precision 0.9600, recall 0.6038, auc 0.7980
epoch 1001, loss 0.1941, train acc 93.35%, f1 0.8410, precision 0.9597, recall 0.7484, auc 0.8694
epoch 1101, loss 0.1794, train acc 94.83%, f1 0.8797, precision 0.9697, recall 0.8050, auc 0.8987
epoch 1201, loss 0.1663, train acc 96.45%, f1 0.9200, precision 0.9787, recall 0.8679, auc 0.9311
epoch 1301, loss 0.1545, train acc 97.93%, f1 0.9548, precision 0.9801, recall 0.9308, auc 0.9625
epoch 1401, loss 0.1439, train acc 98.23%, f1 0.9615, precision 0.9804, recall 0.9434, auc 0.9688
epoch 1501, loss 0.1343, train acc 98.38%, f1 0.9651, precision 0.9744, recall 0.9560, auc 0.9741
epoch 1601, loss 0.1255, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1701, loss 0.1174, train acc 98.52%, f1 0.9686, precision 0.9686, recall 0.9686, auc 0.9795
epoch 1801, loss 0.1100, train acc 98.52%, f1 0.9686, precision 0.9686, recall 0.9686, auc 0.9795
epoch 1901, loss 0.1033, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2001, loss 0.0972, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2101, loss 0.0916, train acc 99.11%, f1 0.9812, precision 0.9752, recall 0.9874, auc 0.9898
epoch 2201, loss 0.0863, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2301, loss 0.0814, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2401, loss 0.0769, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2501, loss 0.0725, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2601, loss 0.0686, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2701, loss 0.0649, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2801, loss 0.0614, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2901, loss 0.0580, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3001, loss 0.0549, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3101, loss 0.0519, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3201, loss 0.0490, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3301, loss 0.0464, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3401, loss 0.0439, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3501, loss 0.0416, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3601, loss 0.0394, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3701, loss 0.0372, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3801, loss 0.0353, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3901, loss 0.0334, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0317, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0300, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0284, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0269, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0255, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0242, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0230, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4701, loss 0.0218, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4801, loss 0.0207, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0197, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0187, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0177, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0169, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0160, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0152, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0145, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0138, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0131, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0124, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0118, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0112, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0107, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0102, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0097, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0092, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0088, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0083, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0079, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0075, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0072, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0068, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0065, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0062, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0059, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0051, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0044, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0042, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0040, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8201, loss 0.0038, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0036, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0033, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0031, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0030, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_normal_20000
normal
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_4
./test_vehicle0/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.9672480620155038

the Fscore is 0.9500000000000001

the precision is 0.95

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_4
----------------------



epoch 1, loss 0.6924, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4680, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4101, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3667, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3299, train acc 76.96%, f1 0.0370, precision 1.0000, recall 0.0189, auc 0.5094
epoch 501, loss 0.2985, train acc 77.55%, f1 0.0952, precision 0.8889, recall 0.0503, auc 0.5242
epoch 601, loss 0.2718, train acc 79.47%, f1 0.2404, precision 0.9167, recall 0.1384, auc 0.5673
epoch 701, loss 0.2488, train acc 83.16%, f1 0.4519, precision 0.9592, recall 0.2956, auc 0.6459
epoch 801, loss 0.2285, train acc 87.44%, f1 0.6473, precision 0.9512, recall 0.4906, auc 0.7414
epoch 901, loss 0.2103, train acc 90.25%, f1 0.7462, precision 0.9604, recall 0.6101, auc 0.8012
epoch 1001, loss 0.1938, train acc 93.35%, f1 0.8410, precision 0.9597, recall 0.7484, auc 0.8694
epoch 1101, loss 0.1792, train acc 94.98%, f1 0.8836, precision 0.9699, recall 0.8113, auc 0.9018
epoch 1201, loss 0.1661, train acc 96.31%, f1 0.9164, precision 0.9786, recall 0.8616, auc 0.9279
epoch 1301, loss 0.1544, train acc 97.64%, f1 0.9481, precision 0.9799, recall 0.9182, auc 0.9562
epoch 1401, loss 0.1438, train acc 98.38%, f1 0.9649, precision 0.9805, recall 0.9497, auc 0.9719
epoch 1501, loss 0.1342, train acc 98.38%, f1 0.9651, precision 0.9744, recall 0.9560, auc 0.9741
epoch 1601, loss 0.1255, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1701, loss 0.1175, train acc 98.38%, f1 0.9653, precision 0.9684, recall 0.9623, auc 0.9763
epoch 1801, loss 0.1103, train acc 98.52%, f1 0.9686, precision 0.9686, recall 0.9686, auc 0.9795
epoch 1901, loss 0.1035, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2001, loss 0.0974, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2101, loss 0.0917, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2201, loss 0.0864, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2301, loss 0.0815, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2401, loss 0.0770, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2501, loss 0.0727, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2601, loss 0.0687, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2701, loss 0.0650, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2801, loss 0.0616, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2901, loss 0.0583, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3001, loss 0.0551, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3101, loss 0.0522, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3201, loss 0.0494, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3301, loss 0.0468, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3401, loss 0.0443, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3501, loss 0.0419, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3601, loss 0.0397, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3701, loss 0.0375, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3801, loss 0.0355, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3901, loss 0.0337, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0319, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0303, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0287, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0273, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0259, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0246, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0232, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4701, loss 0.0220, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4801, loss 0.0209, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4901, loss 0.0198, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 5001, loss 0.0189, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0179, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0170, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0162, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0154, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0146, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0139, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0132, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0126, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0120, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0114, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0108, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0103, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0098, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0093, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0088, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0084, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0080, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0076, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0072, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0069, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0065, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0062, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0059, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0051, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0044, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0036, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0031, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_normal_15000
normal
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_4
./test_vehicle0/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.9672480620155038

the Fscore is 0.9500000000000001

the precision is 0.95

the recall is 0.95

Done
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_4
----------------------



epoch 1, loss 0.7024, train acc 23.49%, f1 0.3804, precision 0.2349, recall 1.0000, auc 0.5000
epoch 101, loss 0.4686, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4099, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3666, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3300, train acc 76.81%, f1 0.0248, precision 1.0000, recall 0.0126, auc 0.5063
epoch 501, loss 0.2990, train acc 77.55%, f1 0.0952, precision 0.8889, recall 0.0503, auc 0.5242
epoch 601, loss 0.2725, train acc 79.17%, f1 0.2210, precision 0.9091, recall 0.1258, auc 0.5610
epoch 701, loss 0.2498, train acc 82.72%, f1 0.4293, precision 0.9565, recall 0.2767, auc 0.6364
epoch 801, loss 0.2298, train acc 87.15%, f1 0.6360, precision 0.9500, recall 0.4780, auc 0.7351
epoch 901, loss 0.2118, train acc 89.96%, f1 0.7364, precision 0.9596, recall 0.5975, auc 0.7949
epoch 1001, loss 0.1954, train acc 93.21%, f1 0.8357, precision 0.9669, recall 0.7358, auc 0.8641
epoch 1101, loss 0.1808, train acc 94.53%, f1 0.8720, precision 0.9692, recall 0.7925, auc 0.8924
epoch 1201, loss 0.1677, train acc 96.16%, f1 0.9133, precision 0.9716, recall 0.8616, auc 0.9270
epoch 1301, loss 0.1560, train acc 97.49%, f1 0.9450, precision 0.9733, recall 0.9182, auc 0.9553
epoch 1401, loss 0.1454, train acc 98.52%, f1 0.9682, precision 0.9806, recall 0.9560, auc 0.9751
epoch 1501, loss 0.1357, train acc 98.52%, f1 0.9682, precision 0.9806, recall 0.9560, auc 0.9751
epoch 1601, loss 0.1270, train acc 98.38%, f1 0.9651, precision 0.9744, recall 0.9560, auc 0.9741
epoch 1701, loss 0.1189, train acc 98.38%, f1 0.9651, precision 0.9744, recall 0.9560, auc 0.9741
epoch 1801, loss 0.1116, train acc 98.67%, f1 0.9716, precision 0.9747, recall 0.9686, auc 0.9804
epoch 1901, loss 0.1047, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2001, loss 0.0985, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2101, loss 0.0928, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2201, loss 0.0876, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2301, loss 0.0827, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2401, loss 0.0780, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2501, loss 0.0737, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2601, loss 0.0697, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2701, loss 0.0661, train acc 99.41%, f1 0.9875, precision 0.9814, recall 0.9937, auc 0.9940
epoch 2801, loss 0.0626, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 2901, loss 0.0593, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3001, loss 0.0561, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3101, loss 0.0531, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3201, loss 0.0503, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3301, loss 0.0475, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3401, loss 0.0449, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3501, loss 0.0426, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3601, loss 0.0403, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3701, loss 0.0382, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3801, loss 0.0362, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3901, loss 0.0343, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0325, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0309, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0293, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0277, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0262, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0249, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0236, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4701, loss 0.0224, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4801, loss 0.0212, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0202, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0191, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0182, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0172, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0163, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0155, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0147, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0140, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0133, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0126, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0120, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0114, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0108, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0102, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0097, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0092, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0088, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0083, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0079, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0075, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0071, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0068, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0064, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0061, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0058, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0055, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0050, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0045, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0043, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0035, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0031, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_normal_10000
normal
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_4
./test_vehicle0/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.9547480620155038

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_4
----------------------



epoch 1, loss 0.6848, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4656, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4084, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3651, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3282, train acc 76.96%, f1 0.0370, precision 1.0000, recall 0.0189, auc 0.5094
epoch 501, loss 0.2969, train acc 77.70%, f1 0.1065, precision 0.9000, recall 0.0566, auc 0.5273
epoch 601, loss 0.2702, train acc 79.47%, f1 0.2404, precision 0.9167, recall 0.1384, auc 0.5673
epoch 701, loss 0.2473, train acc 83.46%, f1 0.4667, precision 0.9608, recall 0.3082, auc 0.6522
epoch 801, loss 0.2271, train acc 88.04%, f1 0.6694, precision 0.9535, recall 0.5157, auc 0.7540
epoch 901, loss 0.2089, train acc 90.25%, f1 0.7481, precision 0.9515, recall 0.6164, auc 0.8033
epoch 1001, loss 0.1926, train acc 93.80%, f1 0.8531, precision 0.9606, recall 0.7673, auc 0.8788
epoch 1101, loss 0.1781, train acc 94.83%, f1 0.8805, precision 0.9627, recall 0.8113, auc 0.9008
epoch 1201, loss 0.1651, train acc 96.60%, f1 0.9241, precision 0.9722, recall 0.8805, auc 0.9364
epoch 1301, loss 0.1535, train acc 98.08%, f1 0.9582, precision 0.9803, recall 0.9371, auc 0.9657
epoch 1401, loss 0.1430, train acc 98.67%, f1 0.9714, precision 0.9808, recall 0.9623, auc 0.9782
epoch 1501, loss 0.1335, train acc 98.38%, f1 0.9651, precision 0.9744, recall 0.9560, auc 0.9741
epoch 1601, loss 0.1249, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1701, loss 0.1170, train acc 98.52%, f1 0.9686, precision 0.9686, recall 0.9686, auc 0.9795
epoch 1801, loss 0.1099, train acc 98.52%, f1 0.9686, precision 0.9686, recall 0.9686, auc 0.9795
epoch 1901, loss 0.1033, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2001, loss 0.0971, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2101, loss 0.0915, train acc 99.11%, f1 0.9814, precision 0.9693, recall 0.9937, auc 0.9920
epoch 2201, loss 0.0863, train acc 99.11%, f1 0.9814, precision 0.9693, recall 0.9937, auc 0.9920
epoch 2301, loss 0.0813, train acc 99.11%, f1 0.9814, precision 0.9693, recall 0.9937, auc 0.9920
epoch 2401, loss 0.0768, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2501, loss 0.0726, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2601, loss 0.0688, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2701, loss 0.0651, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2801, loss 0.0615, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2901, loss 0.0582, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3001, loss 0.0549, train acc 99.56%, f1 0.9907, precision 0.9815, recall 1.0000, auc 0.9971
epoch 3101, loss 0.0520, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3201, loss 0.0492, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3301, loss 0.0464, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3401, loss 0.0439, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3501, loss 0.0416, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3601, loss 0.0394, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3701, loss 0.0373, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3801, loss 0.0354, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3901, loss 0.0335, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0317, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0301, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0285, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0270, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0256, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0243, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0231, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4701, loss 0.0219, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4801, loss 0.0208, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4901, loss 0.0198, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 5001, loss 0.0188, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0178, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0169, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0161, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0153, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0145, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0138, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0131, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0125, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0118, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0112, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0107, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0102, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0097, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0092, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0087, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0083, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0079, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0075, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0071, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0068, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0065, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0061, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0058, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0050, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0043, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_normal_8000
normal
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_4
./test_vehicle0/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.9672480620155038

the Fscore is 0.9500000000000001

the precision is 0.95

the recall is 0.95

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_4
----------------------



epoch 1, loss 0.6856, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4659, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4087, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3654, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3285, train acc 76.96%, f1 0.0370, precision 1.0000, recall 0.0189, auc 0.5094
epoch 501, loss 0.2972, train acc 77.70%, f1 0.1065, precision 0.9000, recall 0.0566, auc 0.5273
epoch 601, loss 0.2705, train acc 79.47%, f1 0.2404, precision 0.9167, recall 0.1384, auc 0.5673
epoch 701, loss 0.2475, train acc 83.46%, f1 0.4667, precision 0.9608, recall 0.3082, auc 0.6522
epoch 801, loss 0.2274, train acc 88.04%, f1 0.6694, precision 0.9535, recall 0.5157, auc 0.7540
epoch 901, loss 0.2093, train acc 90.25%, f1 0.7481, precision 0.9515, recall 0.6164, auc 0.8033
epoch 1001, loss 0.1929, train acc 93.65%, f1 0.8491, precision 0.9603, recall 0.7610, auc 0.8757
epoch 1101, loss 0.1784, train acc 94.83%, f1 0.8805, precision 0.9627, recall 0.8113, auc 0.9008
epoch 1201, loss 0.1654, train acc 96.45%, f1 0.9205, precision 0.9720, recall 0.8742, auc 0.9332
epoch 1301, loss 0.1538, train acc 98.08%, f1 0.9582, precision 0.9803, recall 0.9371, auc 0.9657
epoch 1401, loss 0.1433, train acc 98.52%, f1 0.9682, precision 0.9806, recall 0.9560, auc 0.9751
epoch 1501, loss 0.1337, train acc 98.38%, f1 0.9651, precision 0.9744, recall 0.9560, auc 0.9741
epoch 1601, loss 0.1250, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1701, loss 0.1171, train acc 98.67%, f1 0.9716, precision 0.9747, recall 0.9686, auc 0.9804
epoch 1801, loss 0.1099, train acc 98.52%, f1 0.9686, precision 0.9686, recall 0.9686, auc 0.9795
epoch 1901, loss 0.1030, train acc 98.82%, f1 0.9750, precision 0.9689, recall 0.9811, auc 0.9857
epoch 2001, loss 0.0969, train acc 98.97%, f1 0.9782, precision 0.9691, recall 0.9874, auc 0.9889
epoch 2101, loss 0.0913, train acc 99.11%, f1 0.9814, precision 0.9693, recall 0.9937, auc 0.9920
epoch 2201, loss 0.0860, train acc 99.11%, f1 0.9814, precision 0.9693, recall 0.9937, auc 0.9920
epoch 2301, loss 0.0811, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2401, loss 0.0767, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2501, loss 0.0726, train acc 99.26%, f1 0.9844, precision 0.9753, recall 0.9937, auc 0.9930
epoch 2601, loss 0.0687, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2701, loss 0.0651, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2801, loss 0.0615, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 2901, loss 0.0582, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3001, loss 0.0549, train acc 99.41%, f1 0.9876, precision 0.9755, recall 1.0000, auc 0.9961
epoch 3101, loss 0.0519, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3201, loss 0.0492, train acc 99.70%, f1 0.9938, precision 0.9876, recall 1.0000, auc 0.9981
epoch 3301, loss 0.0464, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3401, loss 0.0439, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3501, loss 0.0415, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3601, loss 0.0393, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3701, loss 0.0372, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3801, loss 0.0352, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3901, loss 0.0333, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0315, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0299, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0283, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4301, loss 0.0269, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4401, loss 0.0255, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4501, loss 0.0242, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4601, loss 0.0229, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4701, loss 0.0218, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4801, loss 0.0207, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0196, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_normal_5000
normal
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_4
./test_vehicle0/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.9547480620155038

the Fscore is 0.9367088607594937

the precision is 0.9487179487179487

the recall is 0.925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_4
----------------------



epoch 1, loss 0.6845, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4652, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4082, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3649, train acc 76.51%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3281, train acc 76.96%, f1 0.0370, precision 1.0000, recall 0.0189, auc 0.5094
epoch 501, loss 0.2967, train acc 77.70%, f1 0.1065, precision 0.9000, recall 0.0566, auc 0.5273
epoch 601, loss 0.2701, train acc 79.47%, f1 0.2404, precision 0.9167, recall 0.1384, auc 0.5673
epoch 701, loss 0.2472, train acc 83.31%, f1 0.4645, precision 0.9423, recall 0.3082, auc 0.6512
epoch 801, loss 0.2271, train acc 88.04%, f1 0.6694, precision 0.9535, recall 0.5157, auc 0.7540
epoch 901, loss 0.2091, train acc 90.25%, f1 0.7481, precision 0.9515, recall 0.6164, auc 0.8033
epoch 1001, loss 0.1928, train acc 93.65%, f1 0.8491, precision 0.9603, recall 0.7610, auc 0.8757
epoch 1101, loss 0.1782, train acc 94.98%, f1 0.8836, precision 0.9699, recall 0.8113, auc 0.9018
epoch 1201, loss 0.1653, train acc 96.45%, f1 0.9205, precision 0.9720, recall 0.8742, auc 0.9332
epoch 1301, loss 0.1537, train acc 98.23%, f1 0.9618, precision 0.9742, recall 0.9497, auc 0.9710
epoch 1401, loss 0.1433, train acc 98.52%, f1 0.9682, precision 0.9806, recall 0.9560, auc 0.9751
epoch 1501, loss 0.1339, train acc 98.52%, f1 0.9684, precision 0.9745, recall 0.9623, auc 0.9773
epoch 1601, loss 0.1253, train acc 98.67%, f1 0.9716, precision 0.9747, recall 0.9686, auc 0.9804
epoch 1701, loss 0.1174, train acc 98.38%, f1 0.9653, precision 0.9684, recall 0.9623, auc 0.9763
epoch 1801, loss 0.1103, train acc 98.52%, f1 0.9686, precision 0.9686, recall 0.9686, auc 0.9795
epoch 1901, loss 0.1037, train acc 98.67%, f1 0.9718, precision 0.9688, recall 0.9748, auc 0.9826
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_4.csv
./test_vehicle0/standlization_data/vehicle0_std_test_4.csv
MLP_normal_2000
normal
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_4
./test_vehicle0/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.9383720930232557

the Fscore is 0.9113924050632911

the precision is 0.9230769230769231

the recall is 0.9

Done
./test_yeast3/standlization_data/yeast3_std_train_4.csv
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_4
----------------------



epoch 1, loss 0.6932, train acc 55.41%, f1 0.6909, precision 0.5287, recall 0.9967, auc 0.5541
epoch 101, loss 0.3817, train acc 97.95%, f1 0.9794, precision 0.9827, recall 0.9761, auc 0.9795
epoch 201, loss 0.1452, train acc 98.57%, f1 0.9857, precision 0.9859, recall 0.9854, auc 0.9857
epoch 301, loss 0.0821, train acc 98.78%, f1 0.9878, precision 0.9877, recall 0.9878, auc 0.9878
epoch 401, loss 0.0585, train acc 98.84%, f1 0.9884, precision 0.9884, recall 0.9884, auc 0.9884
epoch 501, loss 0.0453, train acc 98.88%, f1 0.9888, precision 0.9887, recall 0.9889, auc 0.9888
epoch 601, loss 0.0401, train acc 98.88%, f1 0.9888, precision 0.9887, recall 0.9889, auc 0.9888
epoch 701, loss 0.0368, train acc 98.91%, f1 0.9892, precision 0.9890, recall 0.9893, auc 0.9891
epoch 801, loss 0.0341, train acc 98.94%, f1 0.9894, precision 0.9892, recall 0.9895, auc 0.9894
epoch 901, loss 0.0325, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9895, auc 0.9894
epoch 1001, loss 0.0309, train acc 98.94%, f1 0.9894, precision 0.9892, recall 0.9896, auc 0.9894
epoch 1101, loss 0.0296, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9896, auc 0.9894
epoch 1201, loss 0.0295, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9896, auc 0.9894
epoch 1301, loss 0.0282, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9896, auc 0.9894
epoch 1401, loss 0.0286, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9896, auc 0.9894
epoch 1501, loss 0.0277, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9896, auc 0.9894
epoch 1601, loss 0.0261, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9895, auc 0.9894
epoch 1701, loss 0.0279, train acc 98.94%, f1 0.9894, precision 0.9892, recall 0.9896, auc 0.9894
epoch 1801, loss 0.0276, train acc 98.94%, f1 0.9894, precision 0.9892, recall 0.9896, auc 0.9894
epoch 1901, loss 0.0266, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9896, auc 0.9894
epoch 2001, loss 0.0259, train acc 98.94%, f1 0.9894, precision 0.9893, recall 0.9895, auc 0.9894
epoch 2101, loss 0.0272, train acc 98.95%, f1 0.9895, precision 0.9895, recall 0.9895, auc 0.9895
epoch 2201, loss 0.0269, train acc 98.94%, f1 0.9894, precision 0.9894, recall 0.9895, auc 0.9894
epoch 2301, loss 0.0237, train acc 98.96%, f1 0.9896, precision 0.9895, recall 0.9896, auc 0.9896
epoch 2401, loss 0.0239, train acc 98.95%, f1 0.9895, precision 0.9894, recall 0.9896, auc 0.9895
epoch 2501, loss 0.0259, train acc 98.95%, f1 0.9895, precision 0.9895, recall 0.9896, auc 0.9895
epoch 2601, loss 0.0263, train acc 98.95%, f1 0.9895, precision 0.9894, recall 0.9896, auc 0.9895
epoch 2701, loss 0.0261, train acc 98.95%, f1 0.9895, precision 0.9894, recall 0.9897, auc 0.9895
epoch 2801, loss 0.0258, train acc 98.96%, f1 0.9896, precision 0.9895, recall 0.9897, auc 0.9896
epoch 2901, loss 0.0250, train acc 98.96%, f1 0.9896, precision 0.9895, recall 0.9898, auc 0.9896
epoch 3001, loss 0.0259, train acc 98.96%, f1 0.9896, precision 0.9895, recall 0.9898, auc 0.9896
epoch 3101, loss 0.0259, train acc 98.97%, f1 0.9897, precision 0.9895, recall 0.9898, auc 0.9897
epoch 3201, loss 0.0245, train acc 98.97%, f1 0.9897, precision 0.9896, recall 0.9898, auc 0.9897
epoch 3301, loss 0.0253, train acc 98.97%, f1 0.9897, precision 0.9896, recall 0.9898, auc 0.9897
epoch 3401, loss 0.0250, train acc 98.97%, f1 0.9897, precision 0.9896, recall 0.9898, auc 0.9897
epoch 3501, loss 0.0249, train acc 98.97%, f1 0.9897, precision 0.9897, recall 0.9898, auc 0.9897
epoch 3601, loss 0.0242, train acc 98.97%, f1 0.9897, precision 0.9897, recall 0.9898, auc 0.9897
epoch 3701, loss 0.0247, train acc 98.98%, f1 0.9898, precision 0.9897, recall 0.9899, auc 0.9898
epoch 3801, loss 0.0231, train acc 98.98%, f1 0.9898, precision 0.9898, recall 0.9899, auc 0.9898
epoch 3901, loss 0.0240, train acc 98.98%, f1 0.9898, precision 0.9898, recall 0.9899, auc 0.9898
epoch 4001, loss 0.0232, train acc 98.99%, f1 0.9899, precision 0.9899, recall 0.9899, auc 0.9899
epoch 4101, loss 0.0236, train acc 99.01%, f1 0.9901, precision 0.9900, recall 0.9901, auc 0.9901
epoch 4201, loss 0.0223, train acc 99.01%, f1 0.9901, precision 0.9901, recall 0.9902, auc 0.9901
epoch 4301, loss 0.0212, train acc 99.02%, f1 0.9902, precision 0.9901, recall 0.9903, auc 0.9902
epoch 4401, loss 0.0229, train acc 99.03%, f1 0.9903, precision 0.9903, recall 0.9904, auc 0.9903
epoch 4501, loss 0.0226, train acc 99.04%, f1 0.9904, precision 0.9902, recall 0.9905, auc 0.9904
epoch 4601, loss 0.0224, train acc 99.04%, f1 0.9904, precision 0.9903, recall 0.9906, auc 0.9904
epoch 4701, loss 0.0222, train acc 99.05%, f1 0.9905, precision 0.9903, recall 0.9907, auc 0.9905
epoch 4801, loss 0.0219, train acc 99.06%, f1 0.9906, precision 0.9905, recall 0.9907, auc 0.9906
epoch 4901, loss 0.0211, train acc 99.07%, f1 0.9907, precision 0.9907, recall 0.9908, auc 0.9907
epoch 5001, loss 0.0215, train acc 99.08%, f1 0.9908, precision 0.9908, recall 0.9909, auc 0.9908
epoch 5101, loss 0.0212, train acc 99.10%, f1 0.9910, precision 0.9909, recall 0.9910, auc 0.9910
epoch 5201, loss 0.0198, train acc 99.10%, f1 0.9910, precision 0.9909, recall 0.9912, auc 0.9910
epoch 5301, loss 0.0205, train acc 99.11%, f1 0.9911, precision 0.9909, recall 0.9913, auc 0.9911
epoch 5401, loss 0.0196, train acc 99.12%, f1 0.9912, precision 0.9910, recall 0.9913, auc 0.9912
epoch 5501, loss 0.0197, train acc 99.14%, f1 0.9914, precision 0.9912, recall 0.9915, auc 0.9914
epoch 5601, loss 0.0196, train acc 99.15%, f1 0.9915, precision 0.9914, recall 0.9916, auc 0.9915
epoch 5701, loss 0.0187, train acc 99.17%, f1 0.9917, precision 0.9916, recall 0.9918, auc 0.9917
epoch 5801, loss 0.0187, train acc 99.19%, f1 0.9919, precision 0.9918, recall 0.9920, auc 0.9919
epoch 5901, loss 0.0181, train acc 99.22%, f1 0.9922, precision 0.9922, recall 0.9923, auc 0.9922
epoch 6001, loss 0.0176, train acc 99.26%, f1 0.9926, precision 0.9925, recall 0.9927, auc 0.9926
epoch 6101, loss 0.0169, train acc 99.28%, f1 0.9928, precision 0.9926, recall 0.9929, auc 0.9928
epoch 6201, loss 0.0155, train acc 99.32%, f1 0.9932, precision 0.9931, recall 0.9933, auc 0.9932
epoch 6301, loss 0.0161, train acc 99.34%, f1 0.9934, precision 0.9932, recall 0.9937, auc 0.9934
epoch 6401, loss 0.0146, train acc 99.38%, f1 0.9938, precision 0.9936, recall 0.9940, auc 0.9938
epoch 6501, loss 0.0134, train acc 99.41%, f1 0.9941, precision 0.9938, recall 0.9943, auc 0.9941
epoch 6601, loss 0.0144, train acc 99.43%, f1 0.9943, precision 0.9941, recall 0.9946, auc 0.9943
epoch 6701, loss 0.0139, train acc 99.46%, f1 0.9946, precision 0.9943, recall 0.9948, auc 0.9946
epoch 6801, loss 0.0132, train acc 99.48%, f1 0.9948, precision 0.9947, recall 0.9950, auc 0.9948
epoch 6901, loss 0.0127, train acc 99.50%, f1 0.9950, precision 0.9948, recall 0.9952, auc 0.9950
epoch 7001, loss 0.0122, train acc 99.53%, f1 0.9953, precision 0.9951, recall 0.9954, auc 0.9953
epoch 7101, loss 0.0105, train acc 99.54%, f1 0.9954, precision 0.9953, recall 0.9956, auc 0.9954
epoch 7201, loss 0.0113, train acc 99.57%, f1 0.9957, precision 0.9955, recall 0.9958, auc 0.9957
epoch 7301, loss 0.0109, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9959, auc 0.9958
epoch 7401, loss 0.0105, train acc 99.60%, f1 0.9960, precision 0.9959, recall 0.9961, auc 0.9960
epoch 7501, loss 0.0096, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9962, auc 0.9961
epoch 7601, loss 0.0090, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 7701, loss 0.0094, train acc 99.65%, f1 0.9965, precision 0.9964, recall 0.9965, auc 0.9965
epoch 7801, loss 0.0085, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9967, auc 0.9966
epoch 7901, loss 0.0084, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 8001, loss 0.0074, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 8101, loss 0.0080, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 8201, loss 0.0078, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 8301, loss 0.0066, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 8401, loss 0.0071, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 8501, loss 0.0069, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 8601, loss 0.0067, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 8701, loss 0.0064, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 8801, loss 0.0062, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 8901, loss 0.0059, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 9001, loss 0.0057, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9981, auc 0.9981
epoch 9101, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 9201, loss 0.0050, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 9301, loss 0.0049, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 9401, loss 0.0049, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 9501, loss 0.0046, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 9601, loss 0.0045, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 9701, loss 0.0042, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 9801, loss 0.0042, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 9901, loss 0.0040, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 10001, loss 0.0038, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 10101, loss 0.0037, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 10201, loss 0.0035, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9991, auc 0.9990
epoch 10301, loss 0.0033, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9990
epoch 10401, loss 0.0033, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9992
epoch 10501, loss 0.0032, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9994, auc 0.9992
epoch 10601, loss 0.0031, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 10701, loss 0.0029, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 10801, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 10901, loss 0.0027, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 11001, loss 0.0026, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 11101, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 11201, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 11301, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 11401, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 11501, loss 0.0021, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 11601, loss 0.0021, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 11701, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 11801, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 11901, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12001, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12101, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12201, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12301, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12401, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12501, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12601, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 12701, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 12801, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12901, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13001, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13101, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13201, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13301, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13401, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13501, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 13601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_yeast3/standlization_data/yeast3_std_train_4.csv
./test_yeast3/standlization_data/yeast3_std_test_4.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_4
./test_yeast3/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.8315972222222222

the Fscore is 0.75

the precision is 0.8571428571428571

the recall is 0.6666666666666666

Done
train_mlp_5_2.sh: line 433: 22461 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=yeast3 dataset_index=4 record_index=1 device_id=5 train_method=MLP_concat_Mirror_15000
