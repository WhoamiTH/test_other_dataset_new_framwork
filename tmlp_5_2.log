nohup: ignoring input
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_4
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
Validation loss decreased (inf --> 0.693095).  Saving model ...
Validation loss decreased (0.693095 --> 0.692986).  Saving model ...
Validation loss decreased (0.692986 --> 0.692880).  Saving model ...
Validation loss decreased (0.692880 --> 0.692768).  Saving model ...
Validation loss decreased (0.692768 --> 0.692636).  Saving model ...
Validation loss decreased (0.692636 --> 0.692493).  Saving model ...
Validation loss decreased (0.692493 --> 0.692330).  Saving model ...
Validation loss decreased (0.692330 --> 0.692151).  Saving model ...
Validation loss decreased (0.692151 --> 0.691949).  Saving model ...
Validation loss decreased (0.691949 --> 0.691720).  Saving model ...
Validation loss decreased (0.691720 --> 0.691470).  Saving model ...
Validation loss decreased (0.691470 --> 0.691199).  Saving model ...
Validation loss decreased (0.691199 --> 0.690904).  Saving model ...
Validation loss decreased (0.690904 --> 0.690579).  Saving model ...
Validation loss decreased (0.690579 --> 0.690224).  Saving model ...
Validation loss decreased (0.690224 --> 0.689837).  Saving model ...
Validation loss decreased (0.689837 --> 0.689415).  Saving model ...
Validation loss decreased (0.689415 --> 0.688957).  Saving model ...
Validation loss decreased (0.688957 --> 0.688461).  Saving model ...
Validation loss decreased (0.688461 --> 0.687934).  Saving model ...
Validation loss decreased (0.687934 --> 0.687374).  Saving model ...
Validation loss decreased (0.687374 --> 0.686779).  Saving model ...
Validation loss decreased (0.686779 --> 0.686147).  Saving model ...
Validation loss decreased (0.686147 --> 0.685487).  Saving model ...
Validation loss decreased (0.685487 --> 0.684781).  Saving model ...
Validation loss decreased (0.684781 --> 0.684026).  Saving model ...
Validation loss decreased (0.684026 --> 0.683226).  Saving model ...
Validation loss decreased (0.683226 --> 0.682383).  Saving model ...
Validation loss decreased (0.682383 --> 0.681508).  Saving model ...
Validation loss decreased (0.681508 --> 0.680591).  Saving model ...
Validation loss decreased (0.680591 --> 0.679637).  Saving model ...
Validation loss decreased (0.679637 --> 0.678646).  Saving model ...
Validation loss decreased (0.678646 --> 0.677610).  Saving model ...
Validation loss decreased (0.677610 --> 0.676540).  Saving model ...
Validation loss decreased (0.676540 --> 0.675427).  Saving model ...
Validation loss decreased (0.675427 --> 0.674270).  Saving model ...
Validation loss decreased (0.674270 --> 0.673062).  Saving model ...
Validation loss decreased (0.673062 --> 0.671808).  Saving model ...
Validation loss decreased (0.671808 --> 0.670488).  Saving model ...
Validation loss decreased (0.670488 --> 0.669130).  Saving model ...
Validation loss decreased (0.669130 --> 0.667774).  Saving model ...
Validation loss decreased (0.667774 --> 0.666387).  Saving model ...
Validation loss decreased (0.666387 --> 0.664928).  Saving model ...
Validation loss decreased (0.664928 --> 0.663416).  Saving model ...
Validation loss decreased (0.663416 --> 0.661886).  Saving model ...
Validation loss decreased (0.661886 --> 0.660310).  Saving model ...
Validation loss decreased (0.660310 --> 0.658652).  Saving model ...
Validation loss decreased (0.658652 --> 0.656932).  Saving model ...
Validation loss decreased (0.656932 --> 0.655178).  Saving model ...
Validation loss decreased (0.655178 --> 0.653368).  Saving model ...
Validation loss decreased (0.653368 --> 0.651565).  Saving model ...
Validation loss decreased (0.651565 --> 0.649703).  Saving model ...
Validation loss decreased (0.649703 --> 0.647812).  Saving model ...
Validation loss decreased (0.647812 --> 0.645887).  Saving model ...
Validation loss decreased (0.645887 --> 0.643899).  Saving model ...
Validation loss decreased (0.643899 --> 0.641870).  Saving model ...
Validation loss decreased (0.641870 --> 0.639815).  Saving model ...
Validation loss decreased (0.639815 --> 0.637696).  Saving model ...
Validation loss decreased (0.637696 --> 0.635513).  Saving model ...
Validation loss decreased (0.635513 --> 0.633318).  Saving model ...
Validation loss decreased (0.633318 --> 0.631118).  Saving model ...
Validation loss decreased (0.631118 --> 0.628930).  Saving model ...
Validation loss decreased (0.628930 --> 0.626695).  Saving model ...
Validation loss decreased (0.626695 --> 0.624392).  Saving model ...
Validation loss decreased (0.624392 --> 0.622063).  Saving model ...
Validation loss decreased (0.622063 --> 0.619680).  Saving model ...
Validation loss decreased (0.619680 --> 0.617249).  Saving model ...
Validation loss decreased (0.617249 --> 0.614806).  Saving model ...
Validation loss decreased (0.614806 --> 0.612417).  Saving model ...
Validation loss decreased (0.612417 --> 0.610013).  Saving model ...
Validation loss decreased (0.610013 --> 0.607628).  Saving model ...
Validation loss decreased (0.607628 --> 0.605180).  Saving model ...
Validation loss decreased (0.605180 --> 0.602729).  Saving model ...
Validation loss decreased (0.602729 --> 0.600240).  Saving model ...
Validation loss decreased (0.600240 --> 0.597707).  Saving model ...
Validation loss decreased (0.597707 --> 0.595213).  Saving model ...
Validation loss decreased (0.595213 --> 0.592737).  Saving model ...
Validation loss decreased (0.592737 --> 0.590258).  Saving model ...
Validation loss decreased (0.590258 --> 0.587797).  Saving model ...
Validation loss decreased (0.587797 --> 0.585282).  Saving model ...
Validation loss decreased (0.585282 --> 0.582779).  Saving model ...
Validation loss decreased (0.582779 --> 0.580270).  Saving model ...
Validation loss decreased (0.580270 --> 0.577760).  Saving model ...
Validation loss decreased (0.577760 --> 0.575177).  Saving model ...
Validation loss decreased (0.575177 --> 0.572651).  Saving model ...
Validation loss decreased (0.572651 --> 0.570187).  Saving model ...
Validation loss decreased (0.570187 --> 0.567673).  Saving model ...
Validation loss decreased (0.567673 --> 0.565213).  Saving model ...
Validation loss decreased (0.565213 --> 0.562730).  Saving model ...
Validation loss decreased (0.562730 --> 0.560226).  Saving model ...
Validation loss decreased (0.560226 --> 0.557744).  Saving model ...
Validation loss decreased (0.557744 --> 0.555236).  Saving model ...
Validation loss decreased (0.555236 --> 0.552675).  Saving model ...
Validation loss decreased (0.552675 --> 0.550129).  Saving model ...
Validation loss decreased (0.550129 --> 0.547573).  Saving model ...
Validation loss decreased (0.547573 --> 0.545006).  Saving model ...
Validation loss decreased (0.545006 --> 0.542456).  Saving model ...
Validation loss decreased (0.542456 --> 0.539909).  Saving model ...
Validation loss decreased (0.539909 --> 0.537352).  Saving model ...
Validation loss decreased (0.537352 --> 0.534866).  Saving model ...
epoch 101, loss 0.5271, train acc 82.00%, f1 0.8200, precision 0.8200, recall 0.8200, auc 0.8200
Validation loss decreased (0.534866 --> 0.532373).  Saving model ...
Validation loss decreased (0.532373 --> 0.529890).  Saving model ...
Validation loss decreased (0.529890 --> 0.527440).  Saving model ...
Validation loss decreased (0.527440 --> 0.525027).  Saving model ...
Validation loss decreased (0.525027 --> 0.522610).  Saving model ...
Validation loss decreased (0.522610 --> 0.520178).  Saving model ...
Validation loss decreased (0.520178 --> 0.517802).  Saving model ...
Validation loss decreased (0.517802 --> 0.515428).  Saving model ...
Validation loss decreased (0.515428 --> 0.513083).  Saving model ...
Validation loss decreased (0.513083 --> 0.510789).  Saving model ...
Validation loss decreased (0.510789 --> 0.508546).  Saving model ...
Validation loss decreased (0.508546 --> 0.506340).  Saving model ...
Validation loss decreased (0.506340 --> 0.504114).  Saving model ...
Validation loss decreased (0.504114 --> 0.501892).  Saving model ...
Validation loss decreased (0.501892 --> 0.499729).  Saving model ...
Validation loss decreased (0.499729 --> 0.497555).  Saving model ...
Validation loss decreased (0.497555 --> 0.495402).  Saving model ...
Validation loss decreased (0.495402 --> 0.493328).  Saving model ...
Validation loss decreased (0.493328 --> 0.491210).  Saving model ...
Validation loss decreased (0.491210 --> 0.489178).  Saving model ...
Validation loss decreased (0.489178 --> 0.487154).  Saving model ...
Validation loss decreased (0.487154 --> 0.485049).  Saving model ...
Validation loss decreased (0.485049 --> 0.482932).  Saving model ...
Validation loss decreased (0.482932 --> 0.480834).  Saving model ...
Validation loss decreased (0.480834 --> 0.478854).  Saving model ...
Validation loss decreased (0.478854 --> 0.476909).  Saving model ...
Validation loss decreased (0.476909 --> 0.475000).  Saving model ...
Validation loss decreased (0.475000 --> 0.473127).  Saving model ...
Validation loss decreased (0.473127 --> 0.471234).  Saving model ...
Validation loss decreased (0.471234 --> 0.469295).  Saving model ...
Validation loss decreased (0.469295 --> 0.467337).  Saving model ...
Validation loss decreased (0.467337 --> 0.465363).  Saving model ...
Validation loss decreased (0.465363 --> 0.463406).  Saving model ...
Validation loss decreased (0.463406 --> 0.461405).  Saving model ...
Validation loss decreased (0.461405 --> 0.459423).  Saving model ...
Validation loss decreased (0.459423 --> 0.457467).  Saving model ...
Validation loss decreased (0.457467 --> 0.455490).  Saving model ...
Validation loss decreased (0.455490 --> 0.453547).  Saving model ...
Validation loss decreased (0.453547 --> 0.451621).  Saving model ...
Validation loss decreased (0.451621 --> 0.449705).  Saving model ...
Validation loss decreased (0.449705 --> 0.447807).  Saving model ...
Validation loss decreased (0.447807 --> 0.445978).  Saving model ...
Validation loss decreased (0.445978 --> 0.444200).  Saving model ...
Validation loss decreased (0.444200 --> 0.442416).  Saving model ...
Validation loss decreased (0.442416 --> 0.440722).  Saving model ...
Validation loss decreased (0.440722 --> 0.439023).  Saving model ...
Validation loss decreased (0.439023 --> 0.437331).  Saving model ...
Validation loss decreased (0.437331 --> 0.435624).  Saving model ...
Validation loss decreased (0.435624 --> 0.433946).  Saving model ...
Validation loss decreased (0.433946 --> 0.432302).  Saving model ...
Validation loss decreased (0.432302 --> 0.430721).  Saving model ...
Validation loss decreased (0.430721 --> 0.429208).  Saving model ...
Validation loss decreased (0.429208 --> 0.427716).  Saving model ...
Validation loss decreased (0.427716 --> 0.426233).  Saving model ...
Validation loss decreased (0.426233 --> 0.424740).  Saving model ...
Validation loss decreased (0.424740 --> 0.423261).  Saving model ...
Validation loss decreased (0.423261 --> 0.421827).  Saving model ...
Validation loss decreased (0.421827 --> 0.420360).  Saving model ...
Validation loss decreased (0.420360 --> 0.418893).  Saving model ...
Validation loss decreased (0.418893 --> 0.417473).  Saving model ...
Validation loss decreased (0.417473 --> 0.416027).  Saving model ...
Validation loss decreased (0.416027 --> 0.414621).  Saving model ...
Validation loss decreased (0.414621 --> 0.413178).  Saving model ...
Validation loss decreased (0.413178 --> 0.411804).  Saving model ...
Validation loss decreased (0.411804 --> 0.410446).  Saving model ...
Validation loss decreased (0.410446 --> 0.409061).  Saving model ...
Validation loss decreased (0.409061 --> 0.407629).  Saving model ...
Validation loss decreased (0.407629 --> 0.406244).  Saving model ...
Validation loss decreased (0.406244 --> 0.404833).  Saving model ...
Validation loss decreased (0.404833 --> 0.403423).  Saving model ...
Validation loss decreased (0.403423 --> 0.402069).  Saving model ...
Validation loss decreased (0.402069 --> 0.400728).  Saving model ...
Validation loss decreased (0.400728 --> 0.399445).  Saving model ...
Validation loss decreased (0.399445 --> 0.398174).  Saving model ...
Validation loss decreased (0.398174 --> 0.396934).  Saving model ...
Validation loss decreased (0.396934 --> 0.395734).  Saving model ...
Validation loss decreased (0.395734 --> 0.394619).  Saving model ...
Validation loss decreased (0.394619 --> 0.393501).  Saving model ...
Validation loss decreased (0.393501 --> 0.392380).  Saving model ...
Validation loss decreased (0.392380 --> 0.391275).  Saving model ...
Validation loss decreased (0.391275 --> 0.390176).  Saving model ...
Validation loss decreased (0.390176 --> 0.389049).  Saving model ...
Validation loss decreased (0.389049 --> 0.387948).  Saving model ...
Validation loss decreased (0.387948 --> 0.386873).  Saving model ...
Validation loss decreased (0.386873 --> 0.385751).  Saving model ...
Validation loss decreased (0.385751 --> 0.384637).  Saving model ...
Validation loss decreased (0.384637 --> 0.383539).  Saving model ...
Validation loss decreased (0.383539 --> 0.382409).  Saving model ...
Validation loss decreased (0.382409 --> 0.381261).  Saving model ...
Validation loss decreased (0.381261 --> 0.380171).  Saving model ...
Validation loss decreased (0.380171 --> 0.379082).  Saving model ...
Validation loss decreased (0.379082 --> 0.377951).  Saving model ...
Validation loss decreased (0.377951 --> 0.376847).  Saving model ...
Validation loss decreased (0.376847 --> 0.375714).  Saving model ...
Validation loss decreased (0.375714 --> 0.374555).  Saving model ...
Validation loss decreased (0.374555 --> 0.373427).  Saving model ...
Validation loss decreased (0.373427 --> 0.372396).  Saving model ...
Validation loss decreased (0.372396 --> 0.371317).  Saving model ...
Validation loss decreased (0.371317 --> 0.370277).  Saving model ...
Validation loss decreased (0.370277 --> 0.369258).  Saving model ...
epoch 201, loss 0.4328, train acc 85.75%, f1 0.8571, precision 0.8593, recall 0.8550, auc 0.8575
Validation loss decreased (0.369258 --> 0.368223).  Saving model ...
Validation loss decreased (0.368223 --> 0.367143).  Saving model ...
Validation loss decreased (0.367143 --> 0.366094).  Saving model ...
Validation loss decreased (0.366094 --> 0.365079).  Saving model ...
Validation loss decreased (0.365079 --> 0.364008).  Saving model ...
Validation loss decreased (0.364008 --> 0.362921).  Saving model ...
Validation loss decreased (0.362921 --> 0.361882).  Saving model ...
Validation loss decreased (0.361882 --> 0.360843).  Saving model ...
Validation loss decreased (0.360843 --> 0.359862).  Saving model ...
Validation loss decreased (0.359862 --> 0.358915).  Saving model ...
Validation loss decreased (0.358915 --> 0.357957).  Saving model ...
Validation loss decreased (0.357957 --> 0.357017).  Saving model ...
Validation loss decreased (0.357017 --> 0.356098).  Saving model ...
Validation loss decreased (0.356098 --> 0.355152).  Saving model ...
Validation loss decreased (0.355152 --> 0.354271).  Saving model ...
Validation loss decreased (0.354271 --> 0.353340).  Saving model ...
Validation loss decreased (0.353340 --> 0.352373).  Saving model ...
Validation loss decreased (0.352373 --> 0.351401).  Saving model ...
Validation loss decreased (0.351401 --> 0.350481).  Saving model ...
Validation loss decreased (0.350481 --> 0.349583).  Saving model ...
Validation loss decreased (0.349583 --> 0.348645).  Saving model ...
Validation loss decreased (0.348645 --> 0.347757).  Saving model ...
Validation loss decreased (0.347757 --> 0.346867).  Saving model ...
Validation loss decreased (0.346867 --> 0.345963).  Saving model ...
Validation loss decreased (0.345963 --> 0.345089).  Saving model ...
Validation loss decreased (0.345089 --> 0.344219).  Saving model ...
Validation loss decreased (0.344219 --> 0.343325).  Saving model ...
Validation loss decreased (0.343325 --> 0.342421).  Saving model ...
Validation loss decreased (0.342421 --> 0.341552).  Saving model ...
Validation loss decreased (0.341552 --> 0.340697).  Saving model ...
Validation loss decreased (0.340697 --> 0.339829).  Saving model ...
Validation loss decreased (0.339829 --> 0.338982).  Saving model ...
Validation loss decreased (0.338982 --> 0.338114).  Saving model ...
Validation loss decreased (0.338114 --> 0.337258).  Saving model ...
Validation loss decreased (0.337258 --> 0.336471).  Saving model ...
Validation loss decreased (0.336471 --> 0.335665).  Saving model ...
Validation loss decreased (0.335665 --> 0.334891).  Saving model ...
Validation loss decreased (0.334891 --> 0.334133).  Saving model ...
Validation loss decreased (0.334133 --> 0.333307).  Saving model ...
Validation loss decreased (0.333307 --> 0.332499).  Saving model ...
Validation loss decreased (0.332499 --> 0.331706).  Saving model ...
Validation loss decreased (0.331706 --> 0.331008).  Saving model ...
Validation loss decreased (0.331008 --> 0.330315).  Saving model ...
Validation loss decreased (0.330315 --> 0.329629).  Saving model ...
Validation loss decreased (0.329629 --> 0.328887).  Saving model ...
Validation loss decreased (0.328887 --> 0.328125).  Saving model ...
Validation loss decreased (0.328125 --> 0.327293).  Saving model ...
Validation loss decreased (0.327293 --> 0.326459).  Saving model ...
Validation loss decreased (0.326459 --> 0.325725).  Saving model ...
Validation loss decreased (0.325725 --> 0.325027).  Saving model ...
Validation loss decreased (0.325027 --> 0.324331).  Saving model ...
Validation loss decreased (0.324331 --> 0.323641).  Saving model ...
Validation loss decreased (0.323641 --> 0.322915).  Saving model ...
Validation loss decreased (0.322915 --> 0.322235).  Saving model ...
Validation loss decreased (0.322235 --> 0.321553).  Saving model ...
Validation loss decreased (0.321553 --> 0.320909).  Saving model ...
Validation loss decreased (0.320909 --> 0.320227).  Saving model ...
Validation loss decreased (0.320227 --> 0.319551).  Saving model ...
Validation loss decreased (0.319551 --> 0.318908).  Saving model ...
Validation loss decreased (0.318908 --> 0.318248).  Saving model ...
Validation loss decreased (0.318248 --> 0.317580).  Saving model ...
Validation loss decreased (0.317580 --> 0.316940).  Saving model ...
Validation loss decreased (0.316940 --> 0.316375).  Saving model ...
Validation loss decreased (0.316375 --> 0.315827).  Saving model ...
Validation loss decreased (0.315827 --> 0.315208).  Saving model ...
Validation loss decreased (0.315208 --> 0.314583).  Saving model ...
Validation loss decreased (0.314583 --> 0.313938).  Saving model ...
Validation loss decreased (0.313938 --> 0.313305).  Saving model ...
Validation loss decreased (0.313305 --> 0.312660).  Saving model ...
Validation loss decreased (0.312660 --> 0.312075).  Saving model ...
Validation loss decreased (0.312075 --> 0.311528).  Saving model ...
Validation loss decreased (0.311528 --> 0.311020).  Saving model ...
Validation loss decreased (0.311020 --> 0.310458).  Saving model ...
Validation loss decreased (0.310458 --> 0.309891).  Saving model ...
Validation loss decreased (0.309891 --> 0.309344).  Saving model ...
Validation loss decreased (0.309344 --> 0.308764).  Saving model ...
Validation loss decreased (0.308764 --> 0.308267).  Saving model ...
Validation loss decreased (0.308267 --> 0.307818).  Saving model ...
Validation loss decreased (0.307818 --> 0.307307).  Saving model ...
Validation loss decreased (0.307307 --> 0.306715).  Saving model ...
Validation loss decreased (0.306715 --> 0.306176).  Saving model ...
Validation loss decreased (0.306176 --> 0.305606).  Saving model ...
Validation loss decreased (0.305606 --> 0.305063).  Saving model ...
Validation loss decreased (0.305063 --> 0.304506).  Saving model ...
Validation loss decreased (0.304506 --> 0.304000).  Saving model ...
Validation loss decreased (0.304000 --> 0.303544).  Saving model ...
Validation loss decreased (0.303544 --> 0.303089).  Saving model ...
Validation loss decreased (0.303089 --> 0.302590).  Saving model ...
Validation loss decreased (0.302590 --> 0.302093).  Saving model ...
Validation loss decreased (0.302093 --> 0.301598).  Saving model ...
Validation loss decreased (0.301598 --> 0.301068).  Saving model ...
Validation loss decreased (0.301068 --> 0.300477).  Saving model ...
Validation loss decreased (0.300477 --> 0.299943).  Saving model ...
Validation loss decreased (0.299943 --> 0.299377).  Saving model ...
Validation loss decreased (0.299377 --> 0.298876).  Saving model ...
Validation loss decreased (0.298876 --> 0.298367).  Saving model ...
Validation loss decreased (0.298367 --> 0.297838).  Saving model ...
Validation loss decreased (0.297838 --> 0.297305).  Saving model ...
Validation loss decreased (0.297305 --> 0.296824).  Saving model ...
Validation loss decreased (0.296824 --> 0.296367).  Saving model ...
epoch 301, loss 0.3703, train acc 89.50%, f1 0.8950, precision 0.8950, recall 0.8950, auc 0.8950
Validation loss decreased (0.296367 --> 0.295940).  Saving model ...
Validation loss decreased (0.295940 --> 0.295536).  Saving model ...
Validation loss decreased (0.295536 --> 0.295087).  Saving model ...
Validation loss decreased (0.295087 --> 0.294605).  Saving model ...
Validation loss decreased (0.294605 --> 0.294102).  Saving model ...
Validation loss decreased (0.294102 --> 0.293600).  Saving model ...
Validation loss decreased (0.293600 --> 0.293170).  Saving model ...
Validation loss decreased (0.293170 --> 0.292713).  Saving model ...
Validation loss decreased (0.292713 --> 0.292251).  Saving model ...
Validation loss decreased (0.292251 --> 0.291812).  Saving model ...
Validation loss decreased (0.291812 --> 0.291353).  Saving model ...
Validation loss decreased (0.291353 --> 0.290922).  Saving model ...
Validation loss decreased (0.290922 --> 0.290591).  Saving model ...
Validation loss decreased (0.290591 --> 0.290280).  Saving model ...
Validation loss decreased (0.290280 --> 0.289964).  Saving model ...
Validation loss decreased (0.289964 --> 0.289672).  Saving model ...
Validation loss decreased (0.289672 --> 0.289363).  Saving model ...
Validation loss decreased (0.289363 --> 0.289119).  Saving model ...
Validation loss decreased (0.289119 --> 0.288838).  Saving model ...
Validation loss decreased (0.288838 --> 0.288577).  Saving model ...
Validation loss decreased (0.288577 --> 0.288332).  Saving model ...
Validation loss decreased (0.288332 --> 0.288122).  Saving model ...
Validation loss decreased (0.288122 --> 0.287970).  Saving model ...
Validation loss decreased (0.287970 --> 0.287798).  Saving model ...
Validation loss decreased (0.287798 --> 0.287658).  Saving model ...
Validation loss decreased (0.287658 --> 0.287550).  Saving model ...
Validation loss decreased (0.287550 --> 0.287398).  Saving model ...
Validation loss decreased (0.287398 --> 0.287249).  Saving model ...
Validation loss decreased (0.287249 --> 0.287043).  Saving model ...
Validation loss decreased (0.287043 --> 0.286774).  Saving model ...
Validation loss decreased (0.286774 --> 0.286478).  Saving model ...
Validation loss decreased (0.286478 --> 0.286170).  Saving model ...
Validation loss decreased (0.286170 --> 0.285849).  Saving model ...
Validation loss decreased (0.285849 --> 0.285545).  Saving model ...
Validation loss decreased (0.285545 --> 0.285238).  Saving model ...
Validation loss decreased (0.285238 --> 0.284883).  Saving model ...
Validation loss decreased (0.284883 --> 0.284519).  Saving model ...
Validation loss decreased (0.284519 --> 0.284181).  Saving model ...
Validation loss decreased (0.284181 --> 0.283897).  Saving model ...
Validation loss decreased (0.283897 --> 0.283642).  Saving model ...
Validation loss decreased (0.283642 --> 0.283415).  Saving model ...
Validation loss decreased (0.283415 --> 0.283223).  Saving model ...
Validation loss decreased (0.283223 --> 0.283006).  Saving model ...
Validation loss decreased (0.283006 --> 0.282771).  Saving model ...
Validation loss decreased (0.282771 --> 0.282493).  Saving model ...
Validation loss decreased (0.282493 --> 0.282218).  Saving model ...
Validation loss decreased (0.282218 --> 0.281924).  Saving model ...
Validation loss decreased (0.281924 --> 0.281583).  Saving model ...
Validation loss decreased (0.281583 --> 0.281257).  Saving model ...
Validation loss decreased (0.281257 --> 0.280961).  Saving model ...
Validation loss decreased (0.280961 --> 0.280622).  Saving model ...
Validation loss decreased (0.280622 --> 0.280276).  Saving model ...
Validation loss decreased (0.280276 --> 0.279945).  Saving model ...
Validation loss decreased (0.279945 --> 0.279592).  Saving model ...
Validation loss decreased (0.279592 --> 0.279271).  Saving model ...
Validation loss decreased (0.279271 --> 0.278957).  Saving model ...
Validation loss decreased (0.278957 --> 0.278657).  Saving model ...
Validation loss decreased (0.278657 --> 0.278356).  Saving model ...
Validation loss decreased (0.278356 --> 0.278054).  Saving model ...
Validation loss decreased (0.278054 --> 0.277786).  Saving model ...
Validation loss decreased (0.277786 --> 0.277493).  Saving model ...
Validation loss decreased (0.277493 --> 0.277171).  Saving model ...
Validation loss decreased (0.277171 --> 0.276854).  Saving model ...
Validation loss decreased (0.276854 --> 0.276541).  Saving model ...
Validation loss decreased (0.276541 --> 0.276245).  Saving model ...
Validation loss decreased (0.276245 --> 0.275980).  Saving model ...
Validation loss decreased (0.275980 --> 0.275719).  Saving model ...
Validation loss decreased (0.275719 --> 0.275461).  Saving model ...
Validation loss decreased (0.275461 --> 0.275171).  Saving model ...
Validation loss decreased (0.275171 --> 0.274882).  Saving model ...
Validation loss decreased (0.274882 --> 0.274659).  Saving model ...
Validation loss decreased (0.274659 --> 0.274438).  Saving model ...
Validation loss decreased (0.274438 --> 0.274246).  Saving model ...
Validation loss decreased (0.274246 --> 0.274055).  Saving model ...
Validation loss decreased (0.274055 --> 0.273930).  Saving model ...
Validation loss decreased (0.273930 --> 0.273770).  Saving model ...
Validation loss decreased (0.273770 --> 0.273574).  Saving model ...
Validation loss decreased (0.273574 --> 0.273419).  Saving model ...
Validation loss decreased (0.273419 --> 0.273250).  Saving model ...
Validation loss decreased (0.273250 --> 0.273094).  Saving model ...
Validation loss decreased (0.273094 --> 0.272936).  Saving model ...
Validation loss decreased (0.272936 --> 0.272824).  Saving model ...
Validation loss decreased (0.272824 --> 0.272778).  Saving model ...
Validation loss decreased (0.272778 --> 0.272689).  Saving model ...
Validation loss decreased (0.272689 --> 0.272619).  Saving model ...
Validation loss decreased (0.272619 --> 0.272602).  Saving model ...
Validation loss decreased (0.272602 --> 0.272552).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
Validation loss decreased (0.272552 --> 0.272551).  Saving model ...
Validation loss decreased (0.272551 --> 0.272431).  Saving model ...
Validation loss decreased (0.272431 --> 0.272287).  Saving model ...
Validation loss decreased (0.272287 --> 0.272061).  Saving model ...
epoch 401, loss 0.2659, train acc 90.00%, f1 0.9000, precision 0.9000, recall 0.9000, auc 0.9000
Validation loss decreased (0.272061 --> 0.271787).  Saving model ...
Validation loss decreased (0.271787 --> 0.271550).  Saving model ...
Validation loss decreased (0.271550 --> 0.271329).  Saving model ...
Validation loss decreased (0.271329 --> 0.271177).  Saving model ...
Validation loss decreased (0.271177 --> 0.271017).  Saving model ...
Validation loss decreased (0.271017 --> 0.270834).  Saving model ...
Validation loss decreased (0.270834 --> 0.270649).  Saving model ...
Validation loss decreased (0.270649 --> 0.270486).  Saving model ...
Validation loss decreased (0.270486 --> 0.270350).  Saving model ...
Validation loss decreased (0.270350 --> 0.270165).  Saving model ...
Validation loss decreased (0.270165 --> 0.269937).  Saving model ...
Validation loss decreased (0.269937 --> 0.269738).  Saving model ...
Validation loss decreased (0.269738 --> 0.269534).  Saving model ...
Validation loss decreased (0.269534 --> 0.269384).  Saving model ...
Validation loss decreased (0.269384 --> 0.269287).  Saving model ...
Validation loss decreased (0.269287 --> 0.269158).  Saving model ...
Validation loss decreased (0.269158 --> 0.269056).  Saving model ...
Validation loss decreased (0.269056 --> 0.268979).  Saving model ...
Validation loss decreased (0.268979 --> 0.268905).  Saving model ...
Validation loss decreased (0.268905 --> 0.268836).  Saving model ...
Validation loss decreased (0.268836 --> 0.268731).  Saving model ...
Validation loss decreased (0.268731 --> 0.268589).  Saving model ...
Validation loss decreased (0.268589 --> 0.268477).  Saving model ...
Validation loss decreased (0.268477 --> 0.268365).  Saving model ...
Validation loss decreased (0.268365 --> 0.268271).  Saving model ...
Validation loss decreased (0.268271 --> 0.268202).  Saving model ...
Validation loss decreased (0.268202 --> 0.268098).  Saving model ...
Validation loss decreased (0.268098 --> 0.267974).  Saving model ...
Validation loss decreased (0.267974 --> 0.267841).  Saving model ...
Validation loss decreased (0.267841 --> 0.267670).  Saving model ...
Validation loss decreased (0.267670 --> 0.267500).  Saving model ...
Validation loss decreased (0.267500 --> 0.267303).  Saving model ...
Validation loss decreased (0.267303 --> 0.267125).  Saving model ...
Validation loss decreased (0.267125 --> 0.266950).  Saving model ...
Validation loss decreased (0.266950 --> 0.266741).  Saving model ...
Validation loss decreased (0.266741 --> 0.266549).  Saving model ...
Validation loss decreased (0.266549 --> 0.266304).  Saving model ...
Validation loss decreased (0.266304 --> 0.266038).  Saving model ...
Validation loss decreased (0.266038 --> 0.265760).  Saving model ...
Validation loss decreased (0.265760 --> 0.265483).  Saving model ...
Validation loss decreased (0.265483 --> 0.265203).  Saving model ...
Validation loss decreased (0.265203 --> 0.264971).  Saving model ...
Validation loss decreased (0.264971 --> 0.264749).  Saving model ...
Validation loss decreased (0.264749 --> 0.264614).  Saving model ...
Validation loss decreased (0.264614 --> 0.264492).  Saving model ...
Validation loss decreased (0.264492 --> 0.264346).  Saving model ...
Validation loss decreased (0.264346 --> 0.264257).  Saving model ...
Validation loss decreased (0.264257 --> 0.264182).  Saving model ...
Validation loss decreased (0.264182 --> 0.264069).  Saving model ...
Validation loss decreased (0.264069 --> 0.263931).  Saving model ...
Validation loss decreased (0.263931 --> 0.263764).  Saving model ...
Validation loss decreased (0.263764 --> 0.263603).  Saving model ...
Validation loss decreased (0.263603 --> 0.263466).  Saving model ...
Validation loss decreased (0.263466 --> 0.263357).  Saving model ...
Validation loss decreased (0.263357 --> 0.263273).  Saving model ...
Validation loss decreased (0.263273 --> 0.263216).  Saving model ...
Validation loss decreased (0.263216 --> 0.263188).  Saving model ...
Validation loss decreased (0.263188 --> 0.263128).  Saving model ...
Validation loss decreased (0.263128 --> 0.263072).  Saving model ...
Validation loss decreased (0.263072 --> 0.263019).  Saving model ...
Validation loss decreased (0.263019 --> 0.262997).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
Validation loss decreased (0.262997 --> 0.262927).  Saving model ...
Validation loss decreased (0.262927 --> 0.262816).  Saving model ...
Validation loss decreased (0.262816 --> 0.262705).  Saving model ...
Validation loss decreased (0.262705 --> 0.262629).  Saving model ...
Validation loss decreased (0.262629 --> 0.262509).  Saving model .../home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

Validation loss decreased (0.262509 --> 0.262409).  Saving model ...
Validation loss decreased (0.262409 --> 0.262379).  Saving model ...
Validation loss decreased (0.262379 --> 0.262280).  Saving model ...
Validation loss decreased (0.262280 --> 0.262228).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
Validation loss decreased (0.262228 --> 0.262226).  Saving model ...
Validation loss decreased (0.262226 --> 0.262195).  Saving model ...
Validation loss decreased (0.262195 --> 0.262129).  Saving model ...
Validation loss decreased (0.262129 --> 0.262113).  Saving model ...
Validation loss decreased (0.262113 --> 0.262057).  Saving model ...
Validation loss decreased (0.262057 --> 0.262031).  Saving model ...
Validation loss decreased (0.262031 --> 0.261995).  Saving model ...
Validation loss decreased (0.261995 --> 0.261963).  Saving model ...
Validation loss decreased (0.261963 --> 0.261920).  Saving model ...
Validation loss decreased (0.261920 --> 0.261826).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
Validation loss decreased (0.261826 --> 0.261762).  Saving model ...
EarlyStopping counter: 1 out of 20
epoch 501, loss 0.2851, train acc 89.00%, f1 0.8900, precision 0.8900, recall 0.8900, auc 0.8900
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
Validation loss decreased (0.261762 --> 0.261758).  Saving model ...
Validation loss decreased (0.261758 --> 0.261728).  Saving model ...
Validation loss decreased (0.261728 --> 0.261672).  Saving model ...
Validation loss decreased (0.261672 --> 0.261623).  Saving model ...
Validation loss decreased (0.261623 --> 0.261620).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 529, loss 0.2268, train acc 89.00%, f1 0.8900, precision 0.8900, recall 0.8900, auc 0.8900



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_Mirror_True
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_4
./test_pima/result_MLP_concat_Mirror_True_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6578301886792453

the Fscore is 0.6

the precision is 0.4485981308411215

the recall is 0.9056603773584906

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_4
----------------------



epoch 1, loss 0.6932, train acc 50.05%, f1 0.0019, precision 0.9647, recall 0.0010, auc 0.5005
epoch 101, loss 0.4973, train acc 79.13%, f1 0.7914, precision 0.7912, recall 0.7916, auc 0.7913
epoch 201, loss 0.3461, train acc 82.97%, f1 0.8297, precision 0.8298, recall 0.8296, auc 0.8297
epoch 301, loss 0.3645, train acc 84.57%, f1 0.8457, precision 0.8457, recall 0.8457, auc 0.8457
epoch 401, loss 0.3345, train acc 84.92%, f1 0.8492, precision 0.8490, recall 0.8493, auc 0.8492
epoch 501, loss 0.3490, train acc 85.00%, f1 0.8500, precision 0.8500, recall 0.8500, auc 0.8500
epoch 601, loss 0.3608, train acc 85.05%, f1 0.8505, precision 0.8504, recall 0.8507, auc 0.8505
epoch 701, loss 0.3205, train acc 85.09%, f1 0.8509, precision 0.8509, recall 0.8509, auc 0.8509
epoch 801, loss 0.4495, train acc 85.06%, f1 0.8506, precision 0.8506, recall 0.8507, auc 0.8506
epoch 901, loss 0.3688, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8506, auc 0.8505
epoch 1001, loss 0.3196, train acc 85.08%, f1 0.8508, precision 0.8506, recall 0.8509, auc 0.8508
epoch 1101, loss 0.3986, train acc 85.09%, f1 0.8509, precision 0.8508, recall 0.8511, auc 0.8509
epoch 1201, loss 0.3750, train acc 85.07%, f1 0.8507, precision 0.8505, recall 0.8510, auc 0.8507
epoch 1301, loss 0.2675, train acc 85.01%, f1 0.8502, precision 0.8500, recall 0.8504, auc 0.8501
epoch 1401, loss 0.3111, train acc 85.01%, f1 0.8502, precision 0.8498, recall 0.8506, auc 0.8501
epoch 1501, loss 0.2903, train acc 85.07%, f1 0.8508, precision 0.8502, recall 0.8513, auc 0.8507
epoch 1601, loss 0.3852, train acc 85.08%, f1 0.8509, precision 0.8505, recall 0.8512, auc 0.8508
epoch 1701, loss 0.2974, train acc 85.11%, f1 0.8512, precision 0.8508, recall 0.8516, auc 0.8511
epoch 1801, loss 0.4091, train acc 85.03%, f1 0.8504, precision 0.8499, recall 0.8509, auc 0.8503
epoch 1901, loss 0.2523, train acc 85.08%, f1 0.8509, precision 0.8502, recall 0.8516, auc 0.8508
epoch 2001, loss 0.3838, train acc 85.09%, f1 0.8510, precision 0.8504, recall 0.8517, auc 0.8509
epoch 2101, loss 0.4858, train acc 85.15%, f1 0.8516, precision 0.8510, recall 0.8522, auc 0.8515
epoch 2201, loss 0.3751, train acc 85.11%, f1 0.8512, precision 0.8504, recall 0.8520, auc 0.8511
epoch 2301, loss 0.3139, train acc 85.08%, f1 0.8510, precision 0.8500, recall 0.8521, auc 0.8508
epoch 2401, loss 0.2527, train acc 85.14%, f1 0.8514, precision 0.8509, recall 0.8520, auc 0.8514
epoch 2501, loss 0.2565, train acc 85.21%, f1 0.8522, precision 0.8514, recall 0.8530, auc 0.8521
epoch 2601, loss 0.4585, train acc 85.23%, f1 0.8525, precision 0.8517, recall 0.8532, auc 0.8523
epoch 2701, loss 0.2976, train acc 85.25%, f1 0.8526, precision 0.8520, recall 0.8533, auc 0.8525
epoch 2801, loss 0.3339, train acc 85.31%, f1 0.8532, precision 0.8529, recall 0.8534, auc 0.8531
epoch 2901, loss 0.3442, train acc 85.32%, f1 0.8533, precision 0.8529, recall 0.8538, auc 0.8532
epoch 3001, loss 0.3391, train acc 85.49%, f1 0.8550, precision 0.8543, recall 0.8558, auc 0.8549
epoch 3101, loss 0.3539, train acc 85.54%, f1 0.8554, precision 0.8552, recall 0.8556, auc 0.8554
epoch 3201, loss 0.2876, train acc 85.63%, f1 0.8563, precision 0.8560, recall 0.8566, auc 0.8563
epoch 3301, loss 0.4139, train acc 85.78%, f1 0.8579, precision 0.8573, recall 0.8584, auc 0.8578
epoch 3401, loss 0.3797, train acc 85.93%, f1 0.8594, precision 0.8589, recall 0.8600, auc 0.8593
epoch 3501, loss 0.3122, train acc 86.03%, f1 0.8604, precision 0.8600, recall 0.8608, auc 0.8603
epoch 3601, loss 0.3751, train acc 86.22%, f1 0.8621, precision 0.8624, recall 0.8618, auc 0.8622
epoch 3701, loss 0.2496, train acc 86.29%, f1 0.8630, precision 0.8625, recall 0.8636, auc 0.8629
epoch 3801, loss 0.2874, train acc 86.41%, f1 0.8641, precision 0.8641, recall 0.8641, auc 0.8641
epoch 3901, loss 0.2726, train acc 86.49%, f1 0.8651, precision 0.8639, recall 0.8663, auc 0.8649
epoch 4001, loss 0.2616, train acc 86.62%, f1 0.8662, precision 0.8666, recall 0.8657, auc 0.8662
epoch 4101, loss 0.3737, train acc 86.61%, f1 0.8660, precision 0.8664, recall 0.8657, auc 0.8661
epoch 4201, loss 0.3224, train acc 86.67%, f1 0.8666, precision 0.8673, recall 0.8660, auc 0.8667
epoch 4301, loss 0.3801, train acc 86.78%, f1 0.8677, precision 0.8681, recall 0.8673, auc 0.8678
epoch 4401, loss 0.2305, train acc 86.82%, f1 0.8682, precision 0.8679, recall 0.8686, auc 0.8682
epoch 4501, loss 0.3024, train acc 86.80%, f1 0.8681, precision 0.8679, recall 0.8682, auc 0.8680
epoch 4601, loss 0.2425, train acc 86.89%, f1 0.8689, precision 0.8689, recall 0.8690, auc 0.8689
epoch 4701, loss 0.2055, train acc 86.99%, f1 0.8699, precision 0.8703, recall 0.8694, auc 0.8699
epoch 4801, loss 0.2196, train acc 86.98%, f1 0.8698, precision 0.8696, recall 0.8701, auc 0.8698
epoch 4901, loss 0.3425, train acc 87.00%, f1 0.8701, precision 0.8695, recall 0.8707, auc 0.8700
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_4
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6433962264150943

the Fscore is 0.5875

the precision is 0.4392523364485981

the recall is 0.8867924528301887

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_4
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5219, train acc 79.51%, f1 0.7978, precision 0.7876, recall 0.8082, auc 0.7951
epoch 201, loss 0.3639, train acc 82.62%, f1 0.8262, precision 0.8265, recall 0.8259, auc 0.8262
epoch 301, loss 0.2981, train acc 84.33%, f1 0.8430, precision 0.8448, recall 0.8412, auc 0.8433
epoch 401, loss 0.3059, train acc 84.87%, f1 0.8483, precision 0.8504, recall 0.8462, auc 0.8487
epoch 501, loss 0.4001, train acc 85.00%, f1 0.8496, precision 0.8523, recall 0.8469, auc 0.8500
epoch 601, loss 0.3068, train acc 85.06%, f1 0.8503, precision 0.8520, recall 0.8487, auc 0.8506
epoch 701, loss 0.4051, train acc 85.09%, f1 0.8506, precision 0.8525, recall 0.8486, auc 0.8509
epoch 801, loss 0.3482, train acc 85.05%, f1 0.8502, precision 0.8520, recall 0.8485, auc 0.8505
epoch 901, loss 0.2049, train acc 85.05%, f1 0.8502, precision 0.8519, recall 0.8485, auc 0.8505
epoch 1001, loss 0.3614, train acc 85.07%, f1 0.8503, precision 0.8524, recall 0.8482, auc 0.8507
epoch 1101, loss 0.2820, train acc 85.04%, f1 0.8501, precision 0.8520, recall 0.8481, auc 0.8504
epoch 1201, loss 0.4259, train acc 85.10%, f1 0.8507, precision 0.8522, recall 0.8493, auc 0.8510
epoch 1301, loss 0.3556, train acc 85.03%, f1 0.8500, precision 0.8513, recall 0.8488, auc 0.8503
epoch 1401, loss 0.2877, train acc 85.02%, f1 0.8501, precision 0.8507, recall 0.8494, auc 0.8502
epoch 1501, loss 0.2516, train acc 85.07%, f1 0.8505, precision 0.8518, recall 0.8492, auc 0.8507
epoch 1601, loss 0.2642, train acc 85.04%, f1 0.8503, precision 0.8510, recall 0.8497, auc 0.8504
epoch 1701, loss 0.3481, train acc 85.08%, f1 0.8507, precision 0.8514, recall 0.8500, auc 0.8508
epoch 1801, loss 0.3249, train acc 85.05%, f1 0.8504, precision 0.8513, recall 0.8495, auc 0.8505
epoch 1901, loss 0.2680, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8503, auc 0.8505
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/standlization_data/pima_std_test_4.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_4
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6133962264150943

the Fscore is 0.5662650602409639

the precision is 0.415929203539823

the recall is 0.8867924528301887

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_4.csv
./test_pima/model_MLP_concat_notMirror_True/record_1/MLP_concat_notMirror_True_4
----------------------



Traceback (most recent call last):
  File "./classifier_MLP/train_MLP.py", line 312, in <module>
    transformed_valid_data, transformed_valid_label = transform_data_to_train_form(transform_method, mirror_type, valid_pos_data, valid_neg_data)
  File "./classifier_MLP/train_MLP.py", line 192, in transform_data_to_train_form
    trans_pos_data, trans_pos_label, trans_neg_data, trans_neg_label = handleData_extend_not_mirror(positive_repeat_data, negetive_tile_data)
  File "./classifier_MLP/train_MLP.py", line 154, in handleData_extend_not_mirror
    transfrom_positive_data = transfrom_positive_data[positive_index[0]]
UnboundLocalError: local variable 'transfrom_positive_data' referenced before assignment
