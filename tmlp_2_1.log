nohup: ignoring input
./test_abalone19/standlization_data/abalone19_std_train_1.csv
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_1
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5136, train acc 75.91%, f1 0.7590, precision 0.7592, recall 0.7589, auc 0.7591
epoch 201, loss 0.4321, train acc 79.70%, f1 0.7970, precision 0.7970, recall 0.7970, auc 0.7970
epoch 301, loss 0.3483, train acc 86.20%, f1 0.8620, precision 0.8620, recall 0.8621, auc 0.8620
epoch 401, loss 0.2763, train acc 90.62%, f1 0.9062, precision 0.9061, recall 0.9063, auc 0.9062
epoch 501, loss 0.2364, train acc 92.06%, f1 0.9206, precision 0.9206, recall 0.9207, auc 0.9206
epoch 601, loss 0.2158, train acc 92.65%, f1 0.9265, precision 0.9264, recall 0.9266, auc 0.9265
epoch 701, loss 0.2041, train acc 93.01%, f1 0.9301, precision 0.9300, recall 0.9301, auc 0.9301
epoch 801, loss 0.1969, train acc 93.24%, f1 0.9324, precision 0.9324, recall 0.9325, auc 0.9324
epoch 901, loss 0.1920, train acc 93.40%, f1 0.9340, precision 0.9339, recall 0.9340, auc 0.9340
epoch 1001, loss 0.1877, train acc 93.52%, f1 0.9352, precision 0.9352, recall 0.9352, auc 0.9352
epoch 1101, loss 0.1827, train acc 93.63%, f1 0.9363, precision 0.9363, recall 0.9362, auc 0.9363
epoch 1201, loss 0.1773, train acc 93.75%, f1 0.9375, precision 0.9375, recall 0.9375, auc 0.9375
epoch 1301, loss 0.1721, train acc 93.86%, f1 0.9386, precision 0.9386, recall 0.9386, auc 0.9386
epoch 1401, loss 0.1674, train acc 93.98%, f1 0.9398, precision 0.9397, recall 0.9398, auc 0.9398
epoch 1501, loss 0.1633, train acc 94.10%, f1 0.9410, precision 0.9409, recall 0.9411, auc 0.9410
epoch 1601, loss 0.1597, train acc 94.22%, f1 0.9422, precision 0.9422, recall 0.9422, auc 0.9422
epoch 1701, loss 0.1566, train acc 94.31%, f1 0.9431, precision 0.9430, recall 0.9431, auc 0.9431
epoch 1801, loss 0.1537, train acc 94.36%, f1 0.9436, precision 0.9437, recall 0.9436, auc 0.9436
epoch 1901, loss 0.1509, train acc 94.40%, f1 0.9441, precision 0.9440, recall 0.9441, auc 0.9440
epoch 2001, loss 0.1482, train acc 94.47%, f1 0.9447, precision 0.9447, recall 0.9447, auc 0.9447
epoch 2101, loss 0.1455, train acc 94.52%, f1 0.9452, precision 0.9453, recall 0.9452, auc 0.9452
epoch 2201, loss 0.1426, train acc 94.58%, f1 0.9458, precision 0.9459, recall 0.9457, auc 0.9458
epoch 2301, loss 0.1396, train acc 94.67%, f1 0.9467, precision 0.9469, recall 0.9464, auc 0.9467
epoch 2401, loss 0.1366, train acc 94.73%, f1 0.9473, precision 0.9473, recall 0.9473, auc 0.9473
epoch 2501, loss 0.1336, train acc 94.80%, f1 0.9480, precision 0.9482, recall 0.9478, auc 0.9480
epoch 2601, loss 0.1306, train acc 94.89%, f1 0.9488, precision 0.9491, recall 0.9486, auc 0.9489
epoch 2701, loss 0.1277, train acc 94.99%, f1 0.9498, precision 0.9501, recall 0.9496, auc 0.9499
epoch 2801, loss 0.1248, train acc 95.09%, f1 0.9508, precision 0.9511, recall 0.9506, auc 0.9509
epoch 2901, loss 0.1220, train acc 95.16%, f1 0.9516, precision 0.9517, recall 0.9515, auc 0.9516
epoch 3001, loss 0.1194, train acc 95.23%, f1 0.9523, precision 0.9524, recall 0.9523, auc 0.9523
epoch 3101, loss 0.1169, train acc 95.29%, f1 0.9529, precision 0.9530, recall 0.9528, auc 0.9529
epoch 3201, loss 0.1146, train acc 95.35%, f1 0.9535, precision 0.9536, recall 0.9534, auc 0.9535
epoch 3301, loss 0.1123, train acc 95.42%, f1 0.9542, precision 0.9544, recall 0.9541, auc 0.9542
epoch 3401, loss 0.1102, train acc 95.52%, f1 0.9552, precision 0.9553, recall 0.9551, auc 0.9552
epoch 3501, loss 0.1081, train acc 95.58%, f1 0.9558, precision 0.9559, recall 0.9557, auc 0.9558
epoch 3601, loss 0.1061, train acc 95.62%, f1 0.9562, precision 0.9562, recall 0.9562, auc 0.9562
epoch 3701, loss 0.1042, train acc 95.67%, f1 0.9567, precision 0.9567, recall 0.9568, auc 0.9567
epoch 3801, loss 0.1023, train acc 95.72%, f1 0.9572, precision 0.9572, recall 0.9573, auc 0.9572
epoch 3901, loss 0.1005, train acc 95.79%, f1 0.9579, precision 0.9579, recall 0.9579, auc 0.9579
epoch 4001, loss 0.0987, train acc 95.83%, f1 0.9583, precision 0.9583, recall 0.9584, auc 0.9583
epoch 4101, loss 0.0969, train acc 95.88%, f1 0.9588, precision 0.9587, recall 0.9589, auc 0.9588
epoch 4201, loss 0.0951, train acc 95.93%, f1 0.9593, precision 0.9593, recall 0.9592, auc 0.9593
epoch 4301, loss 0.0934, train acc 96.00%, f1 0.9600, precision 0.9601, recall 0.9598, auc 0.9600
epoch 4401, loss 0.0916, train acc 96.07%, f1 0.9606, precision 0.9608, recall 0.9605, auc 0.9607
epoch 4501, loss 0.0897, train acc 96.13%, f1 0.9613, precision 0.9616, recall 0.9610, auc 0.9613
epoch 4601, loss 0.0879, train acc 96.22%, f1 0.9622, precision 0.9625, recall 0.9619, auc 0.9622
epoch 4701, loss 0.0859, train acc 96.30%, f1 0.9630, precision 0.9635, recall 0.9626, auc 0.9630
epoch 4801, loss 0.0840, train acc 96.39%, f1 0.9639, precision 0.9643, recall 0.9634, auc 0.9639
epoch 4901, loss 0.0820, train acc 96.47%, f1 0.9647, precision 0.9651, recall 0.9643, auc 0.9647
epoch 5001, loss 0.0800, train acc 96.56%, f1 0.9655, precision 0.9660, recall 0.9651, auc 0.9656
epoch 5101, loss 0.0780, train acc 96.63%, f1 0.9663, precision 0.9667, recall 0.9659, auc 0.9663
epoch 5201, loss 0.0759, train acc 96.71%, f1 0.9671, precision 0.9674, recall 0.9668, auc 0.9671
epoch 5301, loss 0.0739, train acc 96.80%, f1 0.9680, precision 0.9683, recall 0.9676, auc 0.9680
epoch 5401, loss 0.0718, train acc 96.89%, f1 0.9689, precision 0.9692, recall 0.9686, auc 0.9689
epoch 5501, loss 0.0697, train acc 96.99%, f1 0.9699, precision 0.9703, recall 0.9694, auc 0.9699
epoch 5601, loss 0.0676, train acc 97.08%, f1 0.9707, precision 0.9713, recall 0.9702, auc 0.9708
epoch 5701, loss 0.0654, train acc 97.17%, f1 0.9716, precision 0.9723, recall 0.9710, auc 0.9717
epoch 5801, loss 0.0631, train acc 97.27%, f1 0.9726, precision 0.9732, recall 0.9721, auc 0.9727
epoch 5901, loss 0.0607, train acc 97.40%, f1 0.9740, precision 0.9746, recall 0.9733, auc 0.9740
epoch 6001, loss 0.0584, train acc 97.52%, f1 0.9752, precision 0.9759, recall 0.9744, auc 0.9752
epoch 6101, loss 0.0562, train acc 97.64%, f1 0.9764, precision 0.9771, recall 0.9757, auc 0.9764
epoch 6201, loss 0.0540, train acc 97.74%, f1 0.9774, precision 0.9781, recall 0.9768, auc 0.9774
epoch 6301, loss 0.0520, train acc 97.86%, f1 0.9786, precision 0.9794, recall 0.9778, auc 0.9786
epoch 6401, loss 0.0500, train acc 97.98%, f1 0.9798, precision 0.9805, recall 0.9790, auc 0.9798
epoch 6501, loss 0.0482, train acc 98.05%, f1 0.9805, precision 0.9812, recall 0.9799, auc 0.9805
epoch 6601, loss 0.0463, train acc 98.14%, f1 0.9814, precision 0.9819, recall 0.9809, auc 0.9814
epoch 6701, loss 0.0446, train acc 98.24%, f1 0.9824, precision 0.9830, recall 0.9818, auc 0.9824
epoch 6801, loss 0.0429, train acc 98.33%, f1 0.9833, precision 0.9839, recall 0.9827, auc 0.9833
epoch 6901, loss 0.0411, train acc 98.42%, f1 0.9841, precision 0.9847, recall 0.9836, auc 0.9842
epoch 7001, loss 0.0395, train acc 98.50%, f1 0.9850, precision 0.9854, recall 0.9846, auc 0.9850
epoch 7101, loss 0.0381, train acc 98.58%, f1 0.9858, precision 0.9861, recall 0.9854, auc 0.9858
epoch 7201, loss 0.0367, train acc 98.64%, f1 0.9864, precision 0.9866, recall 0.9862, auc 0.9864
epoch 7301, loss 0.0354, train acc 98.71%, f1 0.9871, precision 0.9871, recall 0.9871, auc 0.9871
epoch 7401, loss 0.0341, train acc 98.78%, f1 0.9878, precision 0.9877, recall 0.9880, auc 0.9878
epoch 7501, loss 0.0330, train acc 98.83%, f1 0.9883, precision 0.9879, recall 0.9887, auc 0.9883
epoch 7601, loss 0.0319, train acc 98.89%, f1 0.9889, precision 0.9885, recall 0.9893, auc 0.9889
epoch 7701, loss 0.0308, train acc 98.93%, f1 0.9893, precision 0.9889, recall 0.9896, auc 0.9893
epoch 7801, loss 0.0298, train acc 98.98%, f1 0.9898, precision 0.9894, recall 0.9901, auc 0.9898
epoch 7901, loss 0.0288, train acc 99.03%, f1 0.9903, precision 0.9900, recall 0.9907, auc 0.9903
epoch 8001, loss 0.0279, train acc 99.07%, f1 0.9907, precision 0.9903, recall 0.9912, auc 0.9907
epoch 8101, loss 0.0270, train acc 99.12%, f1 0.9912, precision 0.9907, recall 0.9917, auc 0.9912
epoch 8201, loss 0.0262, train acc 99.15%, f1 0.9915, precision 0.9910, recall 0.9920, auc 0.9915
epoch 8301, loss 0.0254, train acc 99.19%, f1 0.9919, precision 0.9913, recall 0.9924, auc 0.9919
epoch 8401, loss 0.0246, train acc 99.22%, f1 0.9922, precision 0.9917, recall 0.9927, auc 0.9922
epoch 8501, loss 0.0238, train acc 99.25%, f1 0.9925, precision 0.9921, recall 0.9929, auc 0.9925
epoch 8601, loss 0.0231, train acc 99.28%, f1 0.9928, precision 0.9925, recall 0.9932, auc 0.9928
epoch 8701, loss 0.0224, train acc 99.31%, f1 0.9931, precision 0.9927, recall 0.9935, auc 0.9931
epoch 8801, loss 0.0218, train acc 99.33%, f1 0.9933, precision 0.9931, recall 0.9936, auc 0.9933
epoch 8901, loss 0.0211, train acc 99.35%, f1 0.9935, precision 0.9933, recall 0.9938, auc 0.9935
epoch 9001, loss 0.0205, train acc 99.37%, f1 0.9937, precision 0.9935, recall 0.9939, auc 0.9937
epoch 9101, loss 0.0199, train acc 99.39%, f1 0.9939, precision 0.9937, recall 0.9941, auc 0.9939
epoch 9201, loss 0.0193, train acc 99.41%, f1 0.9941, precision 0.9938, recall 0.9943, auc 0.9941
epoch 9301, loss 0.0188, train acc 99.43%, f1 0.9943, precision 0.9941, recall 0.9945, auc 0.9943
epoch 9401, loss 0.0182, train acc 99.45%, f1 0.9945, precision 0.9943, recall 0.9946, auc 0.9945
epoch 9501, loss 0.0176, train acc 99.46%, f1 0.9946, precision 0.9944, recall 0.9948, auc 0.9946
epoch 9601, loss 0.0170, train acc 99.49%, f1 0.9949, precision 0.9947, recall 0.9951, auc 0.9949
epoch 9701, loss 0.0164, train acc 99.51%, f1 0.9951, precision 0.9949, recall 0.9953, auc 0.9951
epoch 9801, loss 0.0158, train acc 99.53%, f1 0.9953, precision 0.9951, recall 0.9955, auc 0.9953
epoch 9901, loss 0.0152, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9957, auc 0.9956
epoch 10001, loss 0.0147, train acc 99.59%, f1 0.9959, precision 0.9958, recall 0.9959, auc 0.9959
epoch 10101, loss 0.0141, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 10201, loss 0.0136, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9963, auc 0.9962
epoch 10301, loss 0.0132, train acc 99.64%, f1 0.9964, precision 0.9963, recall 0.9965, auc 0.9964
epoch 10401, loss 0.0127, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9966, auc 0.9965
epoch 10501, loss 0.0123, train acc 99.67%, f1 0.9967, precision 0.9966, recall 0.9967, auc 0.9967
epoch 10601, loss 0.0118, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9969, auc 0.9969
epoch 10701, loss 0.0114, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 10801, loss 0.0110, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 10901, loss 0.0106, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9974, auc 0.9974
epoch 11001, loss 0.0102, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9975, auc 0.9975
epoch 11101, loss 0.0098, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 11201, loss 0.0095, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9977, auc 0.9976
epoch 11301, loss 0.0091, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9979, auc 0.9978
epoch 11401, loss 0.0088, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9981, auc 0.9979
epoch 11501, loss 0.0084, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9982, auc 0.9980
epoch 11601, loss 0.0081, train acc 99.82%, f1 0.9982, precision 0.9979, recall 0.9984, auc 0.9982
epoch 11701, loss 0.0077, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 11801, loss 0.0074, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 11901, loss 0.0070, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 12001, loss 0.0067, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 12101, loss 0.0063, train acc 99.88%, f1 0.9988, precision 0.9985, recall 0.9990, auc 0.9988
epoch 12201, loss 0.0060, train acc 99.89%, f1 0.9989, precision 0.9986, recall 0.9991, auc 0.9989
epoch 12301, loss 0.0057, train acc 99.90%, f1 0.9990, precision 0.9987, recall 0.9993, auc 0.9990
epoch 12401, loss 0.0054, train acc 99.91%, f1 0.9991, precision 0.9988, recall 0.9994, auc 0.9991
epoch 12501, loss 0.0051, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9994, auc 0.9991
epoch 12601, loss 0.0049, train acc 99.93%, f1 0.9993, precision 0.9990, recall 0.9995, auc 0.9993
epoch 12701, loss 0.0047, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9995, auc 0.9993
epoch 12801, loss 0.0044, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9996, auc 0.9994
epoch 12901, loss 0.0042, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 13001, loss 0.0040, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 13101, loss 0.0038, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9997, auc 0.9996
epoch 13201, loss 0.0037, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 13301, loss 0.0035, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 13401, loss 0.0033, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 13501, loss 0.0032, train acc 99.97%, f1 0.9997, precision 0.9995, recall 0.9998, auc 0.9997
epoch 13601, loss 0.0030, train acc 99.97%, f1 0.9997, precision 0.9995, recall 0.9998, auc 0.9997
epoch 13701, loss 0.0029, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 13801, loss 0.0028, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 13901, loss 0.0026, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9999, auc 0.9997
epoch 14001, loss 0.0025, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 14101, loss 0.0024, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 14201, loss 0.0023, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 14301, loss 0.0022, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 14401, loss 0.0021, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 14501, loss 0.0020, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 14601, loss 0.0019, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 14701, loss 0.0018, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 14801, loss 0.0018, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 14901, loss 0.0017, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 15001, loss 0.0016, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 15101, loss 0.0015, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 15201, loss 0.0015, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 15301, loss 0.0014, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 15401, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15501, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 15601, loss 0.0012, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0012, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_1.csv
./test_abalone19/standlization_data/abalone19_std_test_1.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_1
./test_abalone19/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.49578313253012046

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
./test_abalone19/standlization_data/abalone19_std_train_1.csv
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_1
----------------------



epoch 1, loss 0.6940, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5165, train acc 75.91%, f1 0.7597, precision 0.7579, recall 0.7615, auc 0.7591
epoch 201, loss 0.4331, train acc 79.67%, f1 0.7958, precision 0.7993, recall 0.7924, auc 0.7967
epoch 301, loss 0.3492, train acc 86.16%, f1 0.8615, precision 0.8625, recall 0.8604, auc 0.8616
epoch 401, loss 0.2770, train acc 90.60%, f1 0.9059, precision 0.9062, recall 0.9057, auc 0.9060
epoch 501, loss 0.2369, train acc 92.05%, f1 0.9204, precision 0.9209, recall 0.9199, auc 0.9205
epoch 601, loss 0.2161, train acc 92.64%, f1 0.9264, precision 0.9272, recall 0.9256, auc 0.9264
epoch 701, loss 0.2043, train acc 93.01%, f1 0.9300, precision 0.9309, recall 0.9292, auc 0.9301
epoch 801, loss 0.1970, train acc 93.24%, f1 0.9323, precision 0.9333, recall 0.9313, auc 0.9324
epoch 901, loss 0.1921, train acc 93.40%, f1 0.9339, precision 0.9348, recall 0.9330, auc 0.9340
epoch 1001, loss 0.1876, train acc 93.51%, f1 0.9351, precision 0.9359, recall 0.9343, auc 0.9351
epoch 1101, loss 0.1825, train acc 93.61%, f1 0.9360, precision 0.9368, recall 0.9353, auc 0.9361
epoch 1201, loss 0.1773, train acc 93.73%, f1 0.9373, precision 0.9379, recall 0.9367, auc 0.9373
epoch 1301, loss 0.1722, train acc 93.84%, f1 0.9384, precision 0.9390, recall 0.9378, auc 0.9384
epoch 1401, loss 0.1675, train acc 93.96%, f1 0.9395, precision 0.9401, recall 0.9390, auc 0.9396
epoch 1501, loss 0.1633, train acc 94.07%, f1 0.9406, precision 0.9413, recall 0.9400, auc 0.9407
epoch 1601, loss 0.1596, train acc 94.20%, f1 0.9419, precision 0.9425, recall 0.9413, auc 0.9420
epoch 1701, loss 0.1562, train acc 94.28%, f1 0.9427, precision 0.9433, recall 0.9422, auc 0.9428
epoch 1801, loss 0.1531, train acc 94.35%, f1 0.9435, precision 0.9440, recall 0.9430, auc 0.9435
epoch 1901, loss 0.1501, train acc 94.41%, f1 0.9441, precision 0.9446, recall 0.9435, auc 0.9441
epoch 2001, loss 0.1472, train acc 94.47%, f1 0.9446, precision 0.9452, recall 0.9441, auc 0.9447
epoch 2101, loss 0.1443, train acc 94.52%, f1 0.9451, precision 0.9457, recall 0.9445, auc 0.9452
epoch 2201, loss 0.1414, train acc 94.59%, f1 0.9458, precision 0.9464, recall 0.9453, auc 0.9459
epoch 2301, loss 0.1384, train acc 94.65%, f1 0.9465, precision 0.9471, recall 0.9459, auc 0.9465
epoch 2401, loss 0.1355, train acc 94.71%, f1 0.9471, precision 0.9477, recall 0.9464, auc 0.9471
epoch 2501, loss 0.1326, train acc 94.79%, f1 0.9479, precision 0.9484, recall 0.9474, auc 0.9479
epoch 2601, loss 0.1298, train acc 94.87%, f1 0.9487, precision 0.9491, recall 0.9483, auc 0.9487
epoch 2701, loss 0.1269, train acc 94.97%, f1 0.9496, precision 0.9501, recall 0.9492, auc 0.9497
epoch 2801, loss 0.1242, train acc 95.05%, f1 0.9505, precision 0.9508, recall 0.9502, auc 0.9505
epoch 2901, loss 0.1215, train acc 95.13%, f1 0.9513, precision 0.9517, recall 0.9510, auc 0.9513
epoch 3001, loss 0.1190, train acc 95.22%, f1 0.9522, precision 0.9525, recall 0.9518, auc 0.9522
epoch 3101, loss 0.1166, train acc 95.30%, f1 0.9530, precision 0.9534, recall 0.9525, auc 0.9530
epoch 3201, loss 0.1143, train acc 95.35%, f1 0.9535, precision 0.9539, recall 0.9531, auc 0.9535
epoch 3301, loss 0.1122, train acc 95.42%, f1 0.9542, precision 0.9545, recall 0.9538, auc 0.9542
epoch 3401, loss 0.1101, train acc 95.49%, f1 0.9549, precision 0.9552, recall 0.9546, auc 0.9549
epoch 3501, loss 0.1081, train acc 95.55%, f1 0.9555, precision 0.9558, recall 0.9552, auc 0.9555
epoch 3601, loss 0.1062, train acc 95.60%, f1 0.9560, precision 0.9563, recall 0.9557, auc 0.9560
epoch 3701, loss 0.1044, train acc 95.66%, f1 0.9566, precision 0.9568, recall 0.9563, auc 0.9566
epoch 3801, loss 0.1026, train acc 95.72%, f1 0.9572, precision 0.9573, recall 0.9571, auc 0.9572
epoch 3901, loss 0.1008, train acc 95.78%, f1 0.9578, precision 0.9578, recall 0.9578, auc 0.9578
epoch 4001, loss 0.0991, train acc 95.84%, f1 0.9584, precision 0.9584, recall 0.9584, auc 0.9584
epoch 4101, loss 0.0973, train acc 95.88%, f1 0.9588, precision 0.9587, recall 0.9589, auc 0.9588
epoch 4201, loss 0.0956, train acc 95.94%, f1 0.9594, precision 0.9594, recall 0.9593, auc 0.9594
epoch 4301, loss 0.0938, train acc 96.00%, f1 0.9600, precision 0.9601, recall 0.9599, auc 0.9600
epoch 4401, loss 0.0920, train acc 96.06%, f1 0.9606, precision 0.9606, recall 0.9607, auc 0.9606
epoch 4501, loss 0.0902, train acc 96.12%, f1 0.9612, precision 0.9611, recall 0.9614, auc 0.9612
epoch 4601, loss 0.0883, train acc 96.19%, f1 0.9619, precision 0.9617, recall 0.9621, auc 0.9619
epoch 4701, loss 0.0864, train acc 96.28%, f1 0.9628, precision 0.9625, recall 0.9631, auc 0.9628
epoch 4801, loss 0.0844, train acc 96.37%, f1 0.9637, precision 0.9633, recall 0.9641, auc 0.9637
epoch 4901, loss 0.0822, train acc 96.45%, f1 0.9646, precision 0.9640, recall 0.9652, auc 0.9645
epoch 5001, loss 0.0800, train acc 96.56%, f1 0.9656, precision 0.9649, recall 0.9664, auc 0.9656
epoch 5101, loss 0.0776, train acc 96.66%, f1 0.9666, precision 0.9659, recall 0.9673, auc 0.9666
epoch 5201, loss 0.0753, train acc 96.75%, f1 0.9676, precision 0.9670, recall 0.9682, auc 0.9675
epoch 5301, loss 0.0731, train acc 96.86%, f1 0.9686, precision 0.9681, recall 0.9691, auc 0.9686
epoch 5401, loss 0.0708, train acc 96.96%, f1 0.9697, precision 0.9690, recall 0.9703, auc 0.9696
epoch 5501, loss 0.0687, train acc 97.07%, f1 0.9708, precision 0.9703, recall 0.9712, auc 0.9707
epoch 5601, loss 0.0665, train acc 97.17%, f1 0.9717, precision 0.9712, recall 0.9723, auc 0.9717
epoch 5701, loss 0.0644, train acc 97.27%, f1 0.9727, precision 0.9722, recall 0.9732, auc 0.9727
epoch 5801, loss 0.0624, train acc 97.35%, f1 0.9735, precision 0.9731, recall 0.9740, auc 0.9735
epoch 5901, loss 0.0603, train acc 97.47%, f1 0.9747, precision 0.9742, recall 0.9752, auc 0.9747
epoch 6001, loss 0.0582, train acc 97.56%, f1 0.9757, precision 0.9752, recall 0.9761, auc 0.9756
epoch 6101, loss 0.0562, train acc 97.68%, f1 0.9769, precision 0.9763, recall 0.9774, auc 0.9768
epoch 6201, loss 0.0542, train acc 97.79%, f1 0.9780, precision 0.9776, recall 0.9783, auc 0.9779
epoch 6301, loss 0.0523, train acc 97.90%, f1 0.9790, precision 0.9786, recall 0.9794, auc 0.9790
epoch 6401, loss 0.0504, train acc 98.00%, f1 0.9800, precision 0.9798, recall 0.9801, auc 0.9800
epoch 6501, loss 0.0486, train acc 98.09%, f1 0.9809, precision 0.9808, recall 0.9810, auc 0.9809
epoch 6601, loss 0.0468, train acc 98.17%, f1 0.9817, precision 0.9818, recall 0.9816, auc 0.9817
epoch 6701, loss 0.0451, train acc 98.24%, f1 0.9824, precision 0.9826, recall 0.9822, auc 0.9824
epoch 6801, loss 0.0435, train acc 98.33%, f1 0.9833, precision 0.9837, recall 0.9830, auc 0.9833
epoch 6901, loss 0.0419, train acc 98.42%, f1 0.9842, precision 0.9847, recall 0.9837, auc 0.9842
epoch 7001, loss 0.0404, train acc 98.50%, f1 0.9850, precision 0.9855, recall 0.9845, auc 0.9850
epoch 7101, loss 0.0389, train acc 98.57%, f1 0.9857, precision 0.9863, recall 0.9850, auc 0.9857
epoch 7201, loss 0.0375, train acc 98.64%, f1 0.9864, precision 0.9871, recall 0.9858, auc 0.9864
epoch 7301, loss 0.0362, train acc 98.70%, f1 0.9870, precision 0.9878, recall 0.9862, auc 0.9870
epoch 7401, loss 0.0349, train acc 98.75%, f1 0.9875, precision 0.9885, recall 0.9866, auc 0.9875
epoch 7501, loss 0.0337, train acc 98.81%, f1 0.9881, precision 0.9891, recall 0.9870, auc 0.9881
epoch 7601, loss 0.0325, train acc 98.86%, f1 0.9886, precision 0.9897, recall 0.9876, auc 0.9886
epoch 7701, loss 0.0314, train acc 98.92%, f1 0.9892, precision 0.9902, recall 0.9881, auc 0.9892
epoch 7801, loss 0.0303, train acc 98.97%, f1 0.9897, precision 0.9908, recall 0.9886, auc 0.9897
epoch 7901, loss 0.0292, train acc 99.03%, f1 0.9903, precision 0.9913, recall 0.9893, auc 0.9903
epoch 8001, loss 0.0281, train acc 99.08%, f1 0.9908, precision 0.9918, recall 0.9898, auc 0.9908
epoch 8101, loss 0.0270, train acc 99.13%, f1 0.9913, precision 0.9925, recall 0.9901, auc 0.9913/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0259, train acc 99.18%, f1 0.9918, precision 0.9930, recall 0.9905, auc 0.9918
epoch 8301, loss 0.0249, train acc 99.23%, f1 0.9923, precision 0.9935, recall 0.9912, auc 0.9923
epoch 8401, loss 0.0239, train acc 99.27%, f1 0.9927, precision 0.9938, recall 0.9917, auc 0.9927
epoch 8501, loss 0.0229, train acc 99.32%, f1 0.9932, precision 0.9942, recall 0.9922, auc 0.9932
epoch 8601, loss 0.0220, train acc 99.36%, f1 0.9936, precision 0.9945, recall 0.9926, auc 0.9936
epoch 8701, loss 0.0212, train acc 99.39%, f1 0.9939, precision 0.9949, recall 0.9929, auc 0.9939
epoch 8801, loss 0.0203, train acc 99.43%, f1 0.9943, precision 0.9952, recall 0.9934, auc 0.9943
epoch 8901, loss 0.0195, train acc 99.46%, f1 0.9946, precision 0.9954, recall 0.9937, auc 0.9946
epoch 9001, loss 0.0187, train acc 99.49%, f1 0.9949, precision 0.9957, recall 0.9940, auc 0.9949
epoch 9101, loss 0.0179, train acc 99.51%, f1 0.9951, precision 0.9959, recall 0.9944, auc 0.9951
epoch 9201, loss 0.0170, train acc 99.54%, f1 0.9954, precision 0.9961, recall 0.9947, auc 0.9954
epoch 9301, loss 0.0163, train acc 99.56%, f1 0.9956, precision 0.9963, recall 0.9950, auc 0.9956
epoch 9401, loss 0.0155, train acc 99.59%, f1 0.9959, precision 0.9965, recall 0.9953, auc 0.9959
epoch 9501, loss 0.0149, train acc 99.61%, f1 0.9961, precision 0.9967, recall 0.9956, auc 0.9961
epoch 9601, loss 0.0143, train acc 99.64%, f1 0.9964, precision 0.9970, recall 0.9957, auc 0.9964
epoch 9701, loss 0.0137, train acc 99.65%, f1 0.9965, precision 0.9971, recall 0.9960, auc 0.9965
epoch 9801, loss 0.0131, train acc 99.67%, f1 0.9967, precision 0.9973, recall 0.9961, auc 0.9967
epoch 9901, loss 0.0126, train acc 99.69%, f1 0.9969, precision 0.9975, recall 0.9963, auc 0.9969
epoch 10001, loss 0.0120, train acc 99.71%, f1 0.9971, precision 0.9977, recall 0.9965, auc 0.9971
epoch 10101, loss 0.0115, train acc 99.72%, f1 0.9972, precision 0.9978, recall 0.9965, auc 0.9972
epoch 10201, loss 0.0111, train acc 99.73%, f1 0.9973, precision 0.9980, recall 0.9967, auc 0.9973
epoch 10301, loss 0.0106, train acc 99.74%, f1 0.9974, precision 0.9980, recall 0.9968, auc 0.9974
epoch 10401, loss 0.0101, train acc 99.76%, f1 0.9976, precision 0.9980, recall 0.9971, auc 0.9976
epoch 10501, loss 0.0097, train acc 99.78%, f1 0.9978, precision 0.9982, recall 0.9973, auc 0.9978
epoch 10601, loss 0.0093, train acc 99.79%, f1 0.9979, precision 0.9983, recall 0.9975, auc 0.9979
epoch 10701, loss 0.0089, train acc 99.81%, f1 0.9981, precision 0.9985, recall 0.9977, auc 0.9981
epoch 10801, loss 0.0085, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9979, auc 0.9982
epoch 10901, loss 0.0082, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9981, auc 0.9984
epoch 11001, loss 0.0078, train acc 99.84%, f1 0.9984, precision 0.9987, recall 0.9982, auc 0.9984
epoch 11101, loss 0.0075, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9983, auc 0.9985
epoch 11201, loss 0.0072, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9984, auc 0.9986
epoch 11301, loss 0.0069, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9985, auc 0.9987
epoch 11401, loss 0.0067, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 11501, loss 0.0064, train acc 99.88%, f1 0.9988, precision 0.9991, recall 0.9986, auc 0.9988
epoch 11601, loss 0.0062, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9987, auc 0.9989
epoch 11701, loss 0.0059, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9988, auc 0.9990
epoch 11801, loss 0.0057, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9989, auc 0.9990
epoch 11901, loss 0.0055, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9989, auc 0.9991
epoch 12001, loss 0.0053, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9990, auc 0.9992
epoch 12101, loss 0.0051, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 12201, loss 0.0049, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9992
epoch 12301, loss 0.0047, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9991, auc 0.9993
epoch 12401, loss 0.0046, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 12501, loss 0.0044, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 12601, loss 0.0042, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 12701, loss 0.0041, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9993, auc 0.9994
epoch 12801, loss 0.0039, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9993, auc 0.9994
epoch 12901, loss 0.0038, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 13001, loss 0.0037, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 13101, loss 0.0035, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9994, auc 0.9996
epoch 13201, loss 0.0034, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 13301, loss 0.0033, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9995, auc 0.9996
epoch 13401, loss 0.0032, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9995, auc 0.9997
epoch 13501, loss 0.0030, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 13601, loss 0.0029, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 13701, loss 0.0028, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9996, auc 0.9997
epoch 13801, loss 0.0027, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9996, auc 0.9997
epoch 13901, loss 0.0026, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9996, auc 0.9997
epoch 14001, loss 0.0025, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9996, auc 0.9998
epoch 14101, loss 0.0024, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 14201, loss 0.0023, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 14301, loss 0.0022, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 14401, loss 0.0022, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 14501, loss 0.0021, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 14601, loss 0.0020, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9997, auc 0.9998
epoch 14701, loss 0.0019, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9997, auc 0.9998
epoch 14801, loss 0.0019, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9997, auc 0.9998
epoch 14901, loss 0.0018, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_1.csv
./test_abalone19/standlization_data/abalone19_std_test_1.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_1
./test_abalone19/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.696987951807229

the Fscore is 0.3333333333333333

the precision is 0.2857142857142857

the recall is 0.4

Done
./test_abalone19/standlization_data/abalone19_std_train_1.csv
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_1
----------------------



epoch 1, loss 0.6930, train acc 54.45%, f1 0.6846, precision 0.5236, recall 0.9886, auc 0.5445
epoch 101, loss 0.5139, train acc 75.85%, f1 0.7622, precision 0.7509, recall 0.7738, auc 0.7585
epoch 201, loss 0.4327, train acc 79.65%, f1 0.7952, precision 0.8002, recall 0.7904, auc 0.7965
epoch 301, loss 0.3493, train acc 86.17%, f1 0.8614, precision 0.8630, recall 0.8599, auc 0.8617
epoch 401, loss 0.2774, train acc 90.60%, f1 0.9060, precision 0.9065, recall 0.9055, auc 0.9060
epoch 501, loss 0.2372, train acc 92.04%, f1 0.9203, precision 0.9214, recall 0.9193, auc 0.9204
epoch 601, loss 0.2164, train acc 92.65%, f1 0.9263, precision 0.9280, recall 0.9246, auc 0.9265
epoch 701, loss 0.2045, train acc 93.00%, f1 0.9298, precision 0.9317, recall 0.9281, auc 0.9300
epoch 801, loss 0.1971, train acc 93.25%, f1 0.9323, precision 0.9344, recall 0.9303, auc 0.9325
epoch 901, loss 0.1919, train acc 93.40%, f1 0.9339, precision 0.9357, recall 0.9320, auc 0.9340
epoch 1001, loss 0.1872, train acc 93.52%, f1 0.9351, precision 0.9368, recall 0.9334, auc 0.9352
epoch 1101, loss 0.1820, train acc 93.63%, f1 0.9362, precision 0.9379, recall 0.9346, auc 0.9363
epoch 1201, loss 0.1768, train acc 93.74%, f1 0.9373, precision 0.9389, recall 0.9357, auc 0.9374
epoch 1301, loss 0.1718, train acc 93.85%, f1 0.9384, precision 0.9398, recall 0.9370, auc 0.9385
epoch 1401, loss 0.1672, train acc 93.96%, f1 0.9395, precision 0.9409, recall 0.9381, auc 0.9396
epoch 1501, loss 0.1631, train acc 94.09%, f1 0.9408, precision 0.9423, recall 0.9392, auc 0.9409
epoch 1601, loss 0.1593, train acc 94.21%, f1 0.9420, precision 0.9434, recall 0.9405, auc 0.9421
epoch 1701, loss 0.1560, train acc 94.29%, f1 0.9428, precision 0.9442, recall 0.9414, auc 0.9429
epoch 1801, loss 0.1528, train acc 94.36%, f1 0.9435, precision 0.9448, recall 0.9422, auc 0.9436
epoch 1901, loss 0.1499, train acc 94.42%, f1 0.9441, precision 0.9454, recall 0.9428, auc 0.9442
epoch 2001, loss 0.1470, train acc 94.46%, f1 0.9445, precision 0.9458, recall 0.9433, auc 0.9446
epoch 2101, loss 0.1442, train acc 94.51%, f1 0.9450, precision 0.9462, recall 0.9439, auc 0.9451
epoch 2201, loss 0.1415, train acc 94.57%, f1 0.9456, precision 0.9466, recall 0.9446, auc 0.9457
epoch 2301, loss 0.1387, train acc 94.63%, f1 0.9463, precision 0.9470, recall 0.9455, auc 0.9463
epoch 2401, loss 0.1359, train acc 94.70%, f1 0.9469, precision 0.9476, recall 0.9463, auc 0.9470
epoch 2501, loss 0.1332, train acc 94.77%, f1 0.9477, precision 0.9481, recall 0.9473, auc 0.9477
epoch 2601, loss 0.1305, train acc 94.83%, f1 0.9483, precision 0.9485, recall 0.9480, auc 0.9483
epoch 2701, loss 0.1280, train acc 94.89%, f1 0.9489, precision 0.9491, recall 0.9487, auc 0.9489
epoch 2801, loss 0.1256, train acc 94.96%, f1 0.9496, precision 0.9496, recall 0.9496, auc 0.9496
epoch 2901, loss 0.1232, train acc 95.02%, f1 0.9502, precision 0.9501, recall 0.9502, auc 0.9502
epoch 3001, loss 0.1209, train acc 95.08%, f1 0.9508, precision 0.9509, recall 0.9507, auc 0.9508
epoch 3101, loss 0.1185, train acc 95.17%, f1 0.9517, precision 0.9520, recall 0.9514, auc 0.9517
epoch 3201, loss 0.1162, train acc 95.25%, f1 0.9525, precision 0.9528, recall 0.9521, auc 0.9525
epoch 3301, loss 0.1139, train acc 95.33%, f1 0.9533, precision 0.9538, recall 0.9529, auc 0.9533
epoch 3401, loss 0.1117, train acc 95.42%, f1 0.9542, precision 0.9547, recall 0.9537, auc 0.9542
epoch 3501, loss 0.1096, train acc 95.47%, f1 0.9547, precision 0.9551, recall 0.9543, auc 0.9547
epoch 3601, loss 0.1075, train acc 95.56%, f1 0.9556, precision 0.9560, recall 0.9552, auc 0.9556
epoch 3701, loss 0.1056, train acc 95.61%, f1 0.9561, precision 0.9565, recall 0.9557, auc 0.9561
epoch 3801, loss 0.1036, train acc 95.67%, f1 0.9567, precision 0.9569, recall 0.9564, auc 0.9567
epoch 3901, loss 0.1017, train acc 95.72%, f1 0.9572, precision 0.9575, recall 0.9569, auc 0.9572
epoch 4001, loss 0.0999, train acc 95.79%, f1 0.9579, precision 0.9583, recall 0.9575, auc 0.9579
epoch 4101, loss 0.0981, train acc 95.85%, f1 0.9585, precision 0.9590, recall 0.9580, auc 0.9585
epoch 4201, loss 0.0963, train acc 95.92%, f1 0.9592, precision 0.9596, recall 0.9588, auc 0.9592
epoch 4301, loss 0.0945, train acc 95.97%, f1 0.9596, precision 0.9601, recall 0.9592, auc 0.9597
epoch 4401, loss 0.0927, train acc 96.03%, f1 0.9603, precision 0.9608, recall 0.9597, auc 0.9603
epoch 4501, loss 0.0910, train acc 96.09%, f1 0.9609, precision 0.9615, recall 0.9603, auc 0.9609
epoch 4601, loss 0.0892, train acc 96.15%, f1 0.9615, precision 0.9620, recall 0.9610, auc 0.9615
epoch 4701, loss 0.0874, train acc 96.25%, f1 0.9625, precision 0.9629, recall 0.9620, auc 0.9625
epoch 4801, loss 0.0856, train acc 96.31%, f1 0.9631, precision 0.9635, recall 0.9628, auc 0.9631
epoch 4901, loss 0.0838, train acc 96.39%, f1 0.9639, precision 0.9642, recall 0.9635, auc 0.9639
epoch 5001, loss 0.0820, train acc 96.46%, f1 0.9646, precision 0.9649, recall 0.9642, auc 0.9646
epoch 5101, loss 0.0802, train acc 96.54%, f1 0.9654, precision 0.9657, recall 0.9650, auc 0.9654
epoch 5201, loss 0.0783, train acc 96.61%, f1 0.9661, precision 0.9664, recall 0.9659, auc 0.9661
epoch 5301, loss 0.0764, train acc 96.69%, f1 0.9669, precision 0.9670, recall 0.9667, auc 0.9669
epoch 5401, loss 0.0744, train acc 96.77%, f1 0.9677, precision 0.9679, recall 0.9676, auc 0.9677
epoch 5501, loss 0.0724, train acc 96.88%, f1 0.9688, precision 0.9688, recall 0.9687, auc 0.9688
epoch 5601, loss 0.0703, train acc 96.99%, f1 0.9699, precision 0.9699, recall 0.9700, auc 0.9699
epoch 5701, loss 0.0682, train acc 97.08%, f1 0.9708, precision 0.9709, recall 0.9707, auc 0.9708
epoch 5801, loss 0.0661, train acc 97.19%, f1 0.9719, precision 0.9721, recall 0.9717, auc 0.9719
epoch 5901, loss 0.0640, train acc 97.30%, f1 0.9730, precision 0.9732, recall 0.9728, auc 0.9730
epoch 6001, loss 0.0618, train acc 97.41%, f1 0.9741, precision 0.9744, recall 0.9737, auc 0.9741
epoch 6101, loss 0.0598, train acc 97.51%, f1 0.9751, precision 0.9754, recall 0.9748, auc 0.9751
epoch 6201, loss 0.0577, train acc 97.63%, f1 0.9763, precision 0.9766, recall 0.9760, auc 0.9763
epoch 6301, loss 0.0556, train acc 97.73%, f1 0.9773, precision 0.9776, recall 0.9770, auc 0.9773
epoch 6401, loss 0.0536, train acc 97.86%, f1 0.9786, precision 0.9788, recall 0.9783, auc 0.9786
epoch 6501, loss 0.0516, train acc 97.95%, f1 0.9795, precision 0.9798, recall 0.9791, auc 0.9795
epoch 6601, loss 0.0496, train acc 98.05%, f1 0.9805, precision 0.9808, recall 0.9801, auc 0.9805
epoch 6701, loss 0.0477, train acc 98.13%, f1 0.9813, precision 0.9817, recall 0.9810, auc 0.9813
epoch 6801, loss 0.0458, train acc 98.23%, f1 0.9823, precision 0.9826, recall 0.9820, auc 0.9823
epoch 6901, loss 0.0440, train acc 98.33%, f1 0.9833, precision 0.9835, recall 0.9831, auc 0.9833
epoch 7001, loss 0.0423, train acc 98.41%, f1 0.9841, precision 0.9844, recall 0.9838, auc 0.9841
epoch 7101, loss 0.0406, train acc 98.49%, f1 0.9849, precision 0.9852, recall 0.9846, auc 0.9849
epoch 7201, loss 0.0389, train acc 98.57%, f1 0.9857, precision 0.9859, recall 0.9855, auc 0.9857
epoch 7301, loss 0.0373, train acc 98.64%, f1 0.9864, precision 0.9866, recall 0.9863, auc 0.9864
epoch 7401, loss 0.0357, train acc 98.72%, f1 0.9872, precision 0.9874, recall 0.9869, auc 0.9872
epoch 7501, loss 0.0342, train acc 98.81%, f1 0.9881, precision 0.9883, recall 0.9878, auc 0.9881
epoch 7601, loss 0.0327, train acc 98.88%, f1 0.9888, precision 0.9890, recall 0.9886, auc 0.9888
epoch 7701, loss 0.0313, train acc 98.95%, f1 0.9894, precision 0.9897, recall 0.9892, auc 0.9895
epoch 7801, loss 0.0299, train acc 99.01%, f1 0.9901, precision 0.9903, recall 0.9898, auc 0.9901
epoch 7901, loss 0.0285, train acc 99.07%, f1 0.9907, precision 0.9909, recall 0.9904, auc 0.9907
epoch 8001, loss 0.0273, train acc 99.14%, f1 0.9914, precision 0.9916, recall 0.9912, auc 0.9914
epoch 8101, loss 0.0260, train acc 99.20%, f1 0.9920, precision 0.9923, recall 0.9917, auc 0.9920/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0248, train acc 99.26%, f1 0.9926, precision 0.9929, recall 0.9923, auc 0.9926
epoch 8301, loss 0.0237, train acc 99.32%, f1 0.9932, precision 0.9935, recall 0.9929, auc 0.9932
epoch 8401, loss 0.0226, train acc 99.37%, f1 0.9937, precision 0.9940, recall 0.9935, auc 0.9937
epoch 8501, loss 0.0216, train acc 99.42%, f1 0.9942, precision 0.9945, recall 0.9939, auc 0.9942
epoch 8601, loss 0.0206, train acc 99.44%, f1 0.9944, precision 0.9947, recall 0.9941, auc 0.9944
epoch 8701, loss 0.0197, train acc 99.48%, f1 0.9948, precision 0.9950, recall 0.9945, auc 0.9948
epoch 8801, loss 0.0188, train acc 99.51%, f1 0.9951, precision 0.9954, recall 0.9949, auc 0.9951
epoch 8901, loss 0.0180, train acc 99.54%, f1 0.9954, precision 0.9957, recall 0.9952, auc 0.9954
epoch 9001, loss 0.0172, train acc 99.57%, f1 0.9957, precision 0.9959, recall 0.9955, auc 0.9957
epoch 9101, loss 0.0165, train acc 99.59%, f1 0.9959, precision 0.9962, recall 0.9957, auc 0.9959
epoch 9201, loss 0.0158, train acc 99.61%, f1 0.9961, precision 0.9964, recall 0.9959, auc 0.9961
epoch 9301, loss 0.0152, train acc 99.63%, f1 0.9963, precision 0.9966, recall 0.9960, auc 0.9963
epoch 9401, loss 0.0145, train acc 99.66%, f1 0.9966, precision 0.9969, recall 0.9963, auc 0.9966
epoch 9501, loss 0.0139, train acc 99.69%, f1 0.9969, precision 0.9972, recall 0.9965, auc 0.9969
epoch 9601, loss 0.0133, train acc 99.71%, f1 0.9971, precision 0.9975, recall 0.9967, auc 0.9971
epoch 9701, loss 0.0128, train acc 99.73%, f1 0.9973, precision 0.9977, recall 0.9969, auc 0.9973
epoch 9801, loss 0.0122, train acc 99.76%, f1 0.9976, precision 0.9980, recall 0.9971, auc 0.9976
epoch 9901, loss 0.0117, train acc 99.77%, f1 0.9977, precision 0.9983, recall 0.9972, auc 0.9977
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_1.csv
./test_abalone19/standlization_data/abalone19_std_test_1.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_1
./test_abalone19/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6921686746987952

the Fscore is 0.2

the precision is 0.13333333333333333

the recall is 0.4

Done
train_mlp_2_1.sh: line 27: 19347 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=abalone19 dataset_index=1 record_index=1 device_id=2 train_method=MLP_concat_Mirror_8000
