nohup: ignoring input
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_5
----------------------



epoch 1, loss 0.6934, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5299, train acc 79.20%, f1 0.7910, precision 0.7947, recall 0.7874, auc 0.7920
epoch 201, loss 0.4246, train acc 82.07%, f1 0.8208, precision 0.8206, recall 0.8209, auc 0.8207
epoch 301, loss 0.4733, train acc 83.53%, f1 0.8354, precision 0.8351, recall 0.8357, auc 0.8353
epoch 401, loss 0.4670, train acc 83.98%, f1 0.8400, precision 0.8391, recall 0.8409, auc 0.8398
epoch 501, loss 0.3800, train acc 84.11%, f1 0.8413, precision 0.8407, recall 0.8418, auc 0.8411
epoch 601, loss 0.3682, train acc 84.09%, f1 0.8410, precision 0.8405, recall 0.8414, auc 0.8409
epoch 701, loss 0.4381, train acc 84.19%, f1 0.8421, precision 0.8414, recall 0.8427, auc 0.8419
epoch 801, loss 0.3070, train acc 84.21%, f1 0.8421, precision 0.8418, recall 0.8424, auc 0.8421
epoch 901, loss 0.2923, train acc 84.22%, f1 0.8423, precision 0.8420, recall 0.8426, auc 0.8422
epoch 1001, loss 0.2450, train acc 84.26%, f1 0.8426, precision 0.8424, recall 0.8428, auc 0.8426
epoch 1101, loss 0.4392, train acc 84.25%, f1 0.8425, precision 0.8423, recall 0.8428, auc 0.8425
epoch 1201, loss 0.2802, train acc 84.22%, f1 0.8423, precision 0.8420, recall 0.8426, auc 0.8422
epoch 1301, loss 0.3439, train acc 84.21%, f1 0.8421, precision 0.8418, recall 0.8424, auc 0.8421
epoch 1401, loss 0.3733, train acc 84.23%, f1 0.8423, precision 0.8422, recall 0.8424, auc 0.8423
epoch 1501, loss 0.2592, train acc 84.16%, f1 0.8417, precision 0.8414, recall 0.8420, auc 0.8416
epoch 1601, loss 0.3978, train acc 84.24%, f1 0.8424, precision 0.8421, recall 0.8428, auc 0.8424
epoch 1701, loss 0.3038, train acc 84.21%, f1 0.8421, precision 0.8418, recall 0.8424, auc 0.8421
epoch 1801, loss 0.4582, train acc 84.23%, f1 0.8423, precision 0.8422, recall 0.8425, auc 0.8423
epoch 1901, loss 0.2759, train acc 84.15%, f1 0.8415, precision 0.8414, recall 0.8416, auc 0.8415
epoch 2001, loss 0.2935, train acc 84.29%, f1 0.8429, precision 0.8426, recall 0.8433, auc 0.8429
epoch 2101, loss 0.3536, train acc 84.31%, f1 0.8431, precision 0.8428, recall 0.8434, auc 0.8431
epoch 2201, loss 0.4395, train acc 84.30%, f1 0.8430, precision 0.8427, recall 0.8433, auc 0.8430
epoch 2301, loss 0.2505, train acc 84.37%, f1 0.8437, precision 0.8435, recall 0.8439, auc 0.8437
epoch 2401, loss 0.3037, train acc 84.41%, f1 0.8442, precision 0.8439, recall 0.8444, auc 0.8441
epoch 2501, loss 0.2666, train acc 84.45%, f1 0.8445, precision 0.8443, recall 0.8447, auc 0.8445
epoch 2601, loss 0.3547, train acc 84.58%, f1 0.8458, precision 0.8457, recall 0.8458, auc 0.8458
epoch 2701, loss 0.3736, train acc 84.62%, f1 0.8462, precision 0.8461, recall 0.8463, auc 0.8462
epoch 2801, loss 0.3627, train acc 84.72%, f1 0.8472, precision 0.8472, recall 0.8472, auc 0.8472
epoch 2901, loss 0.2982, train acc 84.87%, f1 0.8488, precision 0.8483, recall 0.8492, auc 0.8487
epoch 3001, loss 0.3439, train acc 84.91%, f1 0.8491, precision 0.8490, recall 0.8493, auc 0.8491
epoch 3101, loss 0.3550, train acc 85.02%, f1 0.8502, precision 0.8502, recall 0.8501, auc 0.8502
epoch 3201, loss 0.4513, train acc 85.14%, f1 0.8514, precision 0.8513, recall 0.8516, auc 0.8514
epoch 3301, loss 0.3623, train acc 85.28%, f1 0.8527, precision 0.8530, recall 0.8524, auc 0.8528
epoch 3401, loss 0.4038, train acc 85.37%, f1 0.8537, precision 0.8536, recall 0.8537, auc 0.8537
epoch 3501, loss 0.2863, train acc 85.52%, f1 0.8552, precision 0.8550, recall 0.8555, auc 0.8552
epoch 3601, loss 0.4007, train acc 85.67%, f1 0.8567, precision 0.8568, recall 0.8565, auc 0.8567
epoch 3701, loss 0.3049, train acc 85.75%, f1 0.8575, precision 0.8575, recall 0.8575, auc 0.8575
epoch 3801, loss 0.2333, train acc 85.89%, f1 0.8589, precision 0.8584, recall 0.8595, auc 0.8589
epoch 3901, loss 0.3238, train acc 85.96%, f1 0.8596, precision 0.8594, recall 0.8598, auc 0.8596
epoch 4001, loss 0.3425, train acc 86.08%, f1 0.8608, precision 0.8610, recall 0.8605, auc 0.8608
epoch 4101, loss 0.2959, train acc 86.08%, f1 0.8608, precision 0.8609, recall 0.8608, auc 0.8608
epoch 4201, loss 0.3765, train acc 86.28%, f1 0.8629, precision 0.8627, recall 0.8630, auc 0.8628
epoch 4301, loss 0.2784, train acc 86.32%, f1 0.8631, precision 0.8632, recall 0.8630, auc 0.8632
epoch 4401, loss 0.3389, train acc 86.38%, f1 0.8638, precision 0.8638, recall 0.8638, auc 0.8638
epoch 4501, loss 0.2360, train acc 86.51%, f1 0.8651, precision 0.8651, recall 0.8652, auc 0.8651
epoch 4601, loss 0.2663, train acc 86.54%, f1 0.8654, precision 0.8653, recall 0.8654, auc 0.8654
epoch 4701, loss 0.2906, train acc 86.63%, f1 0.8663, precision 0.8658, recall 0.8668, auc 0.8663
epoch 4801, loss 0.2137, train acc 86.66%, f1 0.8666, precision 0.8667, recall 0.8666, auc 0.8666
epoch 4901, loss 0.2739, train acc 86.73%, f1 0.8674, precision 0.8668, recall 0.8680, auc 0.8673
epoch 5001, loss 0.2476, train acc 86.82%, f1 0.8682, precision 0.8682, recall 0.8682, auc 0.8682
epoch 5101, loss 0.2437, train acc 86.87%, f1 0.8687, precision 0.8690, recall 0.8684, auc 0.8687
epoch 5201, loss 0.2003, train acc 86.96%, f1 0.8697, precision 0.8691, recall 0.8702, auc 0.8696
epoch 5301, loss 0.3455, train acc 86.99%, f1 0.8699, precision 0.8698, recall 0.8700, auc 0.8699
epoch 5401, loss 0.3841, train acc 87.04%, f1 0.8704, precision 0.8701, recall 0.8707, auc 0.8704
epoch 5501, loss 0.3036, train acc 87.12%, f1 0.8713, precision 0.8706, recall 0.8720, auc 0.8712
epoch 5601, loss 0.3752, train acc 87.17%, f1 0.8717, precision 0.8718, recall 0.8716, auc 0.8717
epoch 5701, loss 0.2569, train acc 87.22%, f1 0.8723, precision 0.8714, recall 0.8732, auc 0.8722
epoch 5801, loss 0.3450, train acc 87.31%, f1 0.8732, precision 0.8722, recall 0.8743, auc 0.8731
epoch 5901, loss 0.2997, train acc 87.29%, f1 0.8730, precision 0.8726, recall 0.8733, auc 0.8729
epoch 6001, loss 0.2901, train acc 87.36%, f1 0.8738, precision 0.8727, recall 0.8748, auc 0.8736
epoch 6101, loss 0.2179, train acc 87.38%, f1 0.8739, precision 0.8733, recall 0.8745, auc 0.8738
epoch 6201, loss 0.3779, train acc 87.43%, f1 0.8743, precision 0.8743, recall 0.8742, auc 0.8743
epoch 6301, loss 0.3351, train acc 87.51%, f1 0.8751, precision 0.8747, recall 0.8755, auc 0.8751
epoch 6401, loss 0.2998, train acc 87.53%, f1 0.8754, precision 0.8745, recall 0.8763, auc 0.8753
epoch 6501, loss 0.3628, train acc 87.59%, f1 0.8759, precision 0.8758, recall 0.8760, auc 0.8759
epoch 6601, loss 0.2898, train acc 87.64%, f1 0.8765, precision 0.8758, recall 0.8773, auc 0.8764
epoch 6701, loss 0.3216, train acc 87.67%, f1 0.8768, precision 0.8760, recall 0.8776, auc 0.8767
epoch 6801, loss 0.2634, train acc 87.68%, f1 0.8768, precision 0.8764, recall 0.8772, auc 0.8768
epoch 6901, loss 0.2698, train acc 87.77%, f1 0.8777, precision 0.8782, recall 0.8771, auc 0.8777
epoch 7001, loss 0.2693, train acc 87.76%, f1 0.8777, precision 0.8771, recall 0.8783, auc 0.8776
epoch 7101, loss 0.3107, train acc 87.83%, f1 0.8783, precision 0.8778, recall 0.8789, auc 0.8783
epoch 7201, loss 0.3112, train acc 87.89%, f1 0.8789, precision 0.8788, recall 0.8791, auc 0.8789
epoch 7301, loss 0.2831, train acc 87.95%, f1 0.8795, precision 0.8794, recall 0.8795, auc 0.8795
epoch 7401, loss 0.1881, train acc 87.88%, f1 0.8788, precision 0.8792, recall 0.8784, auc 0.8788
epoch 7501, loss 0.2718, train acc 87.94%, f1 0.8794, precision 0.8797, recall 0.8791, auc 0.8794
epoch 7601, loss 0.2907, train acc 88.03%, f1 0.8804, precision 0.8802, recall 0.8805, auc 0.8803
epoch 7701, loss 0.1556, train acc 88.07%, f1 0.8808, precision 0.8799, recall 0.8817, auc 0.8807
epoch 7801, loss 0.2033, train acc 88.08%, f1 0.8810, precision 0.8795, recall 0.8824, auc 0.8808
epoch 7901, loss 0.3375, train acc 88.14%, f1 0.8815, precision 0.8806, recall 0.8823, auc 0.8814
epoch 8001, loss 0.3259, train acc 88.19%, f1 0.8821, precision 0.8810, recall 0.8831, auc 0.8819
epoch 8101, loss 0.2326, train acc 88.22%, f1 0.8823, precision 0.8820, recall 0.8825, auc 0.8822
epoch 8201, loss 0.3155, train acc 88.22%, f1 0.8821, precision 0.8823, recall 0.8820, auc 0.8822
epoch 8301, loss 0.2425, train acc 88.28%, f1 0.8828, precision 0.8825, recall 0.8831, auc 0.8828
epoch 8401, loss 0.2332, train acc 88.37%, f1 0.8836, precision 0.8840, recall 0.8833, auc 0.8837
epoch 8501, loss 0.3029, train acc 88.38%, f1 0.8837, precision 0.8840, recall 0.8835, auc 0.8837
epoch 8601, loss 0.1596, train acc 88.40%, f1 0.8842, precision 0.8828, recall 0.8857, auc 0.8840
epoch 8701, loss 0.2971, train acc 88.49%, f1 0.8851, precision 0.8836, recall 0.8866, auc 0.8849
epoch 8801, loss 0.3303, train acc 88.51%, f1 0.8851, precision 0.8852, recall 0.8850, auc 0.8851
epoch 8901, loss 0.2207, train acc 88.62%, f1 0.8862, precision 0.8864, recall 0.8860, auc 0.8862
epoch 9001, loss 0.3432, train acc 88.63%, f1 0.8865, precision 0.8852, recall 0.8879, auc 0.8863
epoch 9101, loss 0.2179, train acc 88.76%, f1 0.8877, precision 0.8874, recall 0.8880, auc 0.8876
epoch 9201, loss 0.2421, train acc 88.79%, f1 0.8880, precision 0.8876, recall 0.8883, auc 0.8879
epoch 9301, loss 0.2420, train acc 88.78%, f1 0.8879, precision 0.8872, recall 0.8886, auc 0.8878
epoch 9401, loss 0.2999, train acc 88.89%, f1 0.8890, precision 0.8885, recall 0.8894, auc 0.8889
epoch 9501, loss 0.2087, train acc 88.93%, f1 0.8893, precision 0.8893, recall 0.8893, auc 0.8893
epoch 9601, loss 0.2857, train acc 88.97%, f1 0.8898, precision 0.8893, recall 0.8903, auc 0.8897
epoch 9701, loss 0.3006, train acc 89.00%, f1 0.8900, precision 0.8903, recall 0.8896, auc 0.8900
epoch 9801, loss 0.3316, train acc 89.06%, f1 0.8907, precision 0.8901, recall 0.8913, auc 0.8906
epoch 9901, loss 0.1251, train acc 89.14%, f1 0.8914, precision 0.8914, recall 0.8913, auc 0.8914
epoch 10001, loss 0.2834, train acc 89.19%, f1 0.8919, precision 0.8920, recall 0.8919, auc 0.8919
epoch 10101, loss 0.1945, train acc 89.25%, f1 0.8924, precision 0.8934, recall 0.8915, auc 0.8925
epoch 10201, loss 0.1854, train acc 89.27%, f1 0.8926, precision 0.8929, recall 0.8923, auc 0.8927
epoch 10301, loss 0.2550, train acc 89.35%, f1 0.8935, precision 0.8932, recall 0.8938, auc 0.8935
epoch 10401, loss 0.2404, train acc 89.32%, f1 0.8933, precision 0.8930, recall 0.8936, auc 0.8932
epoch 10501, loss 0.3260, train acc 89.45%, f1 0.8945, precision 0.8949, recall 0.8941, auc 0.8945
epoch 10601, loss 0.2590, train acc 89.46%, f1 0.8946, precision 0.8947, recall 0.8946, auc 0.8946
epoch 10701, loss 0.1858, train acc 89.55%, f1 0.8955, precision 0.8951, recall 0.8960, auc 0.8955
epoch 10801, loss 0.1738, train acc 89.58%, f1 0.8957, precision 0.8964, recall 0.8950, auc 0.8958
epoch 10901, loss 0.2589, train acc 89.65%, f1 0.8964, precision 0.8974, recall 0.8953, auc 0.8965
epoch 11001, loss 0.2125, train acc 89.72%, f1 0.8971, precision 0.8976, recall 0.8966, auc 0.8972
epoch 11101, loss 0.2387, train acc 89.74%, f1 0.8973, precision 0.8983, recall 0.8964, auc 0.8974
epoch 11201, loss 0.2842, train acc 89.82%, f1 0.8983, precision 0.8969, recall 0.8998, auc 0.8982
epoch 11301, loss 0.3051, train acc 89.85%, f1 0.8986, precision 0.8980, recall 0.8992, auc 0.8985
epoch 11401, loss 0.2066, train acc 89.90%, f1 0.8991, precision 0.8983, recall 0.8999, auc 0.8990
epoch 11501, loss 0.3415, train acc 89.89%, f1 0.8987, precision 0.8998, recall 0.8976, auc 0.8989
epoch 11601, loss 0.2164, train acc 89.93%, f1 0.8993, precision 0.8993, recall 0.8994, auc 0.8993
epoch 11701, loss 0.2694, train acc 90.02%, f1 0.9004, precision 0.8985, recall 0.9024, auc 0.9002
epoch 11801, loss 0.2407, train acc 90.09%, f1 0.9008, precision 0.9012, recall 0.9005, auc 0.9009
epoch 11901, loss 0.2942, train acc 90.09%, f1 0.9010, precision 0.9008, recall 0.9011, auc 0.9009
epoch 12001, loss 0.2377, train acc 90.13%, f1 0.9014, precision 0.9007, recall 0.9021, auc 0.9013
epoch 12101, loss 0.2090, train acc 90.15%, f1 0.9017, precision 0.9003, recall 0.9031, auc 0.9015
epoch 12201, loss 0.2640, train acc 90.25%, f1 0.9025, precision 0.9019, recall 0.9032, auc 0.9025
epoch 12301, loss 0.1990, train acc 90.28%, f1 0.9030, precision 0.9012, recall 0.9048, auc 0.9028
epoch 12401, loss 0.2576, train acc 90.35%, f1 0.9036, precision 0.9023, recall 0.9049, auc 0.9035
epoch 12501, loss 0.2384, train acc 90.31%, f1 0.9032, precision 0.9024, recall 0.9040, auc 0.9031
epoch 12601, loss 0.1120, train acc 90.42%, f1 0.9042, precision 0.9047, recall 0.9036, auc 0.9042
epoch 12701, loss 0.2373, train acc 90.46%, f1 0.9046, precision 0.9047, recall 0.9046, auc 0.9046
epoch 12801, loss 0.2290, train acc 90.56%, f1 0.9055, precision 0.9066, recall 0.9044, auc 0.9056
epoch 12901, loss 0.2570, train acc 90.55%, f1 0.9056, precision 0.9048, recall 0.9063, auc 0.9055
epoch 13001, loss 0.1719, train acc 90.58%, f1 0.9058, precision 0.9059, recall 0.9057, auc 0.9058
epoch 13101, loss 0.1784, train acc 90.63%, f1 0.9063, precision 0.9061, recall 0.9065, auc 0.9063
epoch 13201, loss 0.1675, train acc 90.69%, f1 0.9069, precision 0.9073, recall 0.9065, auc 0.9069
epoch 13301, loss 0.2340, train acc 90.68%, f1 0.9069, precision 0.9058, recall 0.9080, auc 0.9068
epoch 13401, loss 0.2199, train acc 90.73%, f1 0.9072, precision 0.9076, recall 0.9068, auc 0.9073
epoch 13501, loss 0.1517, train acc 90.76%, f1 0.9076, precision 0.9071, recall 0.9082, auc 0.9076
epoch 13601, loss 0.1597, train acc 90.83%, f1 0.9085, precision 0.9074, recall 0.9095, auc 0.9083
epoch 13701, loss 0.2791, train acc 90.88%, f1 0.9088, precision 0.9090, recall 0.9086, auc 0.9088
epoch 13801, loss 0.2175, train acc 90.91%, f1 0.9090, precision 0.9102, recall 0.9078, auc 0.9091
epoch 13901, loss 0.2861, train acc 90.95%, f1 0.9095, precision 0.9092, recall 0.9098, auc 0.9095
epoch 14001, loss 0.2114, train acc 90.94%, f1 0.9091, precision 0.9115, recall 0.9067, auc 0.9094
epoch 14101, loss 0.1790, train acc 91.03%, f1 0.9101, precision 0.9114, recall 0.9088, auc 0.9103
epoch 14201, loss 0.2964, train acc 91.04%, f1 0.9104, precision 0.9101, recall 0.9107, auc 0.9104
epoch 14301, loss 0.2116, train acc 91.13%, f1 0.9113, precision 0.9115, recall 0.9110, auc 0.9113
epoch 14401, loss 0.2749, train acc 91.12%, f1 0.9113, precision 0.9110, recall 0.9116, auc 0.9113
epoch 14501, loss 0.1564, train acc 91.10%, f1 0.9111, precision 0.9099, recall 0.9124, auc 0.9110
epoch 14601, loss 0.2510, train acc 91.16%, f1 0.9116, precision 0.9116, recall 0.9116, auc 0.9116
epoch 14701, loss 0.2194, train acc 91.22%, f1 0.9123, precision 0.9119, recall 0.9127, auc 0.9122
epoch 14801, loss 0.2426, train acc 91.23%, f1 0.9123, precision 0.9123, recall 0.9123, auc 0.9123
epoch 14901, loss 0.2766, train acc 91.27%, f1 0.9128, precision 0.9113, recall 0.9144, auc 0.9127
epoch 15001, loss 0.2337, train acc 91.30%, f1 0.9131, precision 0.9121, recall 0.9141, auc 0.9130
epoch 15101, loss 0.1844, train acc 91.34%, f1 0.9135, precision 0.9128, recall 0.9142, auc 0.9134
epoch 15201, loss 0.2878, train acc 91.39%, f1 0.9141, precision 0.9125, recall 0.9156, auc 0.9139
epoch 15301, loss 0.1539, train acc 91.39%, f1 0.9139, precision 0.9139, recall 0.9139, auc 0.9139
epoch 15401, loss 0.2406, train acc 91.44%, f1 0.9144, precision 0.9145, recall 0.9143, auc 0.9144
epoch 15501, loss 0.1575, train acc 91.47%, f1 0.9146, precision 0.9151, recall 0.9142, auc 0.9147
epoch 15601, loss 0.2442, train acc 91.51%, f1 0.9153, precision 0.9132, recall 0.9174, auc 0.9151
epoch 15701, loss 0.2812, train acc 91.49%, f1 0.9150, precision 0.9137, recall 0.9163, auc 0.9149
epoch 15801, loss 0.2217, train acc 91.55%, f1 0.9156, precision 0.9144, recall 0.9168, auc 0.9155
epoch 15901, loss 0.1866, train acc 91.58%, f1 0.9159, precision 0.9146, recall 0.9171, auc 0.9158
epoch 16001, loss 0.1724, train acc 91.61%, f1 0.9162, precision 0.9150, recall 0.9174, auc 0.9161
epoch 16101, loss 0.2023, train acc 91.65%, f1 0.9165, precision 0.9172, recall 0.9158, auc 0.9165
epoch 16201, loss 0.1817, train acc 91.74%, f1 0.9175, precision 0.9163, recall 0.9187, auc 0.9174
epoch 16301, loss 0.1798, train acc 91.74%, f1 0.9175, precision 0.9164, recall 0.9185, auc 0.9174
epoch 16401, loss 0.2267, train acc 91.77%, f1 0.9179, precision 0.9163, recall 0.9195, auc 0.9177
epoch 16501, loss 0.1497, train acc 91.82%, f1 0.9182, precision 0.9187, recall 0.9176, auc 0.9182/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.2126, train acc 91.84%, f1 0.9184, precision 0.9180, recall 0.9188, auc 0.9184
epoch 16701, loss 0.2038, train acc 91.88%, f1 0.9190, precision 0.9169, recall 0.9210, auc 0.9188
epoch 16801, loss 0.2242, train acc 91.90%, f1 0.9192, precision 0.9175, recall 0.9208, auc 0.9190
epoch 16901, loss 0.2271, train acc 91.95%, f1 0.9196, precision 0.9183, recall 0.9209, auc 0.9195
epoch 17001, loss 0.2069, train acc 91.98%, f1 0.9199, precision 0.9184, recall 0.9215, auc 0.9198
epoch 17101, loss 0.2214, train acc 91.97%, f1 0.9197, precision 0.9192, recall 0.9203, auc 0.9197
epoch 17201, loss 0.2361, train acc 92.00%, f1 0.9202, precision 0.9180, recall 0.9224, auc 0.9200
epoch 17301, loss 0.2474, train acc 92.10%, f1 0.9211, precision 0.9208, recall 0.9213, auc 0.9210
epoch 17401, loss 0.1587, train acc 92.06%, f1 0.9206, precision 0.9204, recall 0.9209, auc 0.9206
epoch 17501, loss 0.1381, train acc 92.12%, f1 0.9212, precision 0.9214, recall 0.9209, auc 0.9212
epoch 17601, loss 0.2017, train acc 92.17%, f1 0.9218, precision 0.9210, recall 0.9225, auc 0.9217
epoch 17701, loss 0.1501, train acc 92.23%, f1 0.9223, precision 0.9220, recall 0.9226, auc 0.9223
epoch 17801, loss 0.2308, train acc 92.27%, f1 0.9227, precision 0.9221, recall 0.9234, auc 0.9227
epoch 17901, loss 0.2445, train acc 92.28%, f1 0.9229, precision 0.9225, recall 0.9232, auc 0.9228
epoch 18001, loss 0.1430, train acc 92.36%, f1 0.9237, precision 0.9220, recall 0.9255, auc 0.9236
epoch 18101, loss 0.2976, train acc 92.39%, f1 0.9239, precision 0.9235, recall 0.9244, auc 0.9239
epoch 18201, loss 0.1364, train acc 92.37%, f1 0.9238, precision 0.9219, recall 0.9257, auc 0.9237
epoch 18301, loss 0.2674, train acc 92.38%, f1 0.9240, precision 0.9224, recall 0.9256, auc 0.9238
epoch 18401, loss 0.1297, train acc 92.47%, f1 0.9247, precision 0.9249, recall 0.9245, auc 0.9247
epoch 18501, loss 0.1382, train acc 92.51%, f1 0.9253, precision 0.9232, recall 0.9274, auc 0.9251
epoch 18601, loss 0.2338, train acc 92.55%, f1 0.9256, precision 0.9244, recall 0.9269, auc 0.9255
epoch 18701, loss 0.2173, train acc 92.60%, f1 0.9260, precision 0.9258, recall 0.9262, auc 0.9260
epoch 18801, loss 0.2202, train acc 92.64%, f1 0.9264, precision 0.9270, recall 0.9258, auc 0.9264
epoch 18901, loss 0.1587, train acc 92.66%, f1 0.9267, precision 0.9257, recall 0.9277, auc 0.9266
epoch 19001, loss 0.1558, train acc 92.65%, f1 0.9265, precision 0.9273, recall 0.9256, auc 0.9265
epoch 19101, loss 0.2077, train acc 92.65%, f1 0.9266, precision 0.9259, recall 0.9273, auc 0.9265
epoch 19201, loss 0.1599, train acc 92.72%, f1 0.9273, precision 0.9263, recall 0.9282, auc 0.9272
epoch 19301, loss 0.1493, train acc 92.73%, f1 0.9273, precision 0.9268, recall 0.9277, auc 0.9272
epoch 19401, loss 0.1216, train acc 92.81%, f1 0.9281, precision 0.9276, recall 0.9286, auc 0.9281
epoch 19501, loss 0.2653, train acc 92.85%, f1 0.9286, precision 0.9278, recall 0.9293, auc 0.9285
epoch 19601, loss 0.2525, train acc 92.87%, f1 0.9288, precision 0.9279, recall 0.9297, auc 0.9287
epoch 19701, loss 0.1244, train acc 92.89%, f1 0.9291, precision 0.9269, recall 0.9313, auc 0.9289
epoch 19801, loss 0.1195, train acc 92.95%, f1 0.9295, precision 0.9304, recall 0.9285, auc 0.9295
epoch 19901, loss 0.1295, train acc 92.92%, f1 0.9293, precision 0.9286, recall 0.9300, auc 0.9292
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_5
./test_pima/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6845283018867925

the Fscore is 0.6164383561643836

the precision is 0.4838709677419355

the recall is 0.8490566037735849

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_5
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5694, train acc 79.71%, f1 0.7971, precision 0.7971, recall 0.7971, auc 0.7971
epoch 201, loss 0.4084, train acc 82.08%, f1 0.8209, precision 0.8207, recall 0.8210, auc 0.8208
epoch 301, loss 0.3383, train acc 83.41%, f1 0.8340, precision 0.8341, recall 0.8340, auc 0.8341
epoch 401, loss 0.4567, train acc 83.94%, f1 0.8394, precision 0.8394, recall 0.8394, auc 0.8394
epoch 501, loss 0.3421, train acc 84.10%, f1 0.8410, precision 0.8410, recall 0.8410, auc 0.8410
epoch 601, loss 0.3132, train acc 84.18%, f1 0.8418, precision 0.8419, recall 0.8418, auc 0.8418
epoch 701, loss 0.3507, train acc 84.26%, f1 0.8426, precision 0.8426, recall 0.8426, auc 0.8426
epoch 801, loss 0.3735, train acc 84.27%, f1 0.8427, precision 0.8427, recall 0.8427, auc 0.8427
epoch 901, loss 0.4232, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8422, auc 0.8422
epoch 1001, loss 0.3974, train acc 84.20%, f1 0.8420, precision 0.8418, recall 0.8422, auc 0.8420
epoch 1101, loss 0.3852, train acc 84.19%, f1 0.8419, precision 0.8418, recall 0.8420, auc 0.8419
epoch 1201, loss 0.3296, train acc 84.25%, f1 0.8425, precision 0.8423, recall 0.8427, auc 0.8425
epoch 1301, loss 0.3379, train acc 84.15%, f1 0.8416, precision 0.8412, recall 0.8419, auc 0.8415
epoch 1401, loss 0.3429, train acc 84.24%, f1 0.8424, precision 0.8422, recall 0.8427, auc 0.8424
epoch 1501, loss 0.2892, train acc 84.25%, f1 0.8426, precision 0.8422, recall 0.8430, auc 0.8425
epoch 1601, loss 0.5117, train acc 84.22%, f1 0.8423, precision 0.8419, recall 0.8426, auc 0.8422
epoch 1701, loss 0.2349, train acc 84.23%, f1 0.8423, precision 0.8420, recall 0.8427, auc 0.8423
epoch 1801, loss 0.3637, train acc 84.24%, f1 0.8425, precision 0.8420, recall 0.8429, auc 0.8424
epoch 1901, loss 0.3915, train acc 84.23%, f1 0.8424, precision 0.8419, recall 0.8429, auc 0.8423
epoch 2001, loss 0.2911, train acc 84.24%, f1 0.8426, precision 0.8417, recall 0.8436, auc 0.8424
epoch 2101, loss 0.2579, train acc 84.24%, f1 0.8425, precision 0.8417, recall 0.8433, auc 0.8424
epoch 2201, loss 0.3617, train acc 84.26%, f1 0.8428, precision 0.8420, recall 0.8435, auc 0.8426
epoch 2301, loss 0.5229, train acc 84.27%, f1 0.8428, precision 0.8420, recall 0.8437, auc 0.8427
epoch 2401, loss 0.4159, train acc 84.31%, f1 0.8433, precision 0.8424, recall 0.8441, auc 0.8431
epoch 2501, loss 0.3870, train acc 84.32%, f1 0.8433, precision 0.8424, recall 0.8443, auc 0.8432
epoch 2601, loss 0.4650, train acc 84.48%, f1 0.8451, precision 0.8437, recall 0.8464, auc 0.8448
epoch 2701, loss 0.3700, train acc 84.57%, f1 0.8459, precision 0.8449, recall 0.8468, auc 0.8457
epoch 2801, loss 0.2984, train acc 84.62%, f1 0.8462, precision 0.8462, recall 0.8462, auc 0.8462
epoch 2901, loss 0.3964, train acc 84.70%, f1 0.8471, precision 0.8465, recall 0.8478, auc 0.8470
epoch 3001, loss 0.3803, train acc 84.79%, f1 0.8480, precision 0.8472, recall 0.8488, auc 0.8479
epoch 3101, loss 0.4474, train acc 84.85%, f1 0.8484, precision 0.8486, recall 0.8482, auc 0.8485
epoch 3201, loss 0.2497, train acc 85.02%, f1 0.8503, precision 0.8499, recall 0.8507, auc 0.8502
epoch 3301, loss 0.3912, train acc 85.10%, f1 0.8511, precision 0.8508, recall 0.8514, auc 0.8510
epoch 3401, loss 0.2084, train acc 85.18%, f1 0.8518, precision 0.8518, recall 0.8519, auc 0.8518
epoch 3501, loss 0.2464, train acc 85.35%, f1 0.8536, precision 0.8531, recall 0.8542, auc 0.8535
epoch 3601, loss 0.2940, train acc 85.51%, f1 0.8551, precision 0.8548, recall 0.8555, auc 0.8551
epoch 3701, loss 0.4114, train acc 85.59%, f1 0.8558, precision 0.8565, recall 0.8551, auc 0.8559
epoch 3801, loss 0.3199, train acc 85.66%, f1 0.8567, precision 0.8563, recall 0.8571, auc 0.8566
epoch 3901, loss 0.3429, train acc 85.76%, f1 0.8576, precision 0.8578, recall 0.8574, auc 0.8576
epoch 4001, loss 0.3308, train acc 85.90%, f1 0.8591, precision 0.8587, recall 0.8595, auc 0.8590
epoch 4101, loss 0.3199, train acc 86.04%, f1 0.8606, precision 0.8596, recall 0.8615, auc 0.8604
epoch 4201, loss 0.3687, train acc 86.14%, f1 0.8615, precision 0.8610, recall 0.8621, auc 0.8614
epoch 4301, loss 0.2135, train acc 86.23%, f1 0.8623, precision 0.8626, recall 0.8620, auc 0.8623
epoch 4401, loss 0.2743, train acc 86.32%, f1 0.8630, precision 0.8641, recall 0.8619, auc 0.8632
epoch 4501, loss 0.3320, train acc 86.44%, f1 0.8644, precision 0.8643, recall 0.8645, auc 0.8644
epoch 4601, loss 0.3125, train acc 86.48%, f1 0.8648, precision 0.8647, recall 0.8649, auc 0.8648
epoch 4701, loss 0.2862, train acc 86.48%, f1 0.8648, precision 0.8653, recall 0.8643, auc 0.8648
epoch 4801, loss 0.2510, train acc 86.59%, f1 0.8658, precision 0.8670, recall 0.8646, auc 0.8659
epoch 4901, loss 0.2130, train acc 86.64%, f1 0.8663, precision 0.8665, recall 0.8662, auc 0.8664
epoch 5001, loss 0.2039, train acc 86.67%, f1 0.8667, precision 0.8668, recall 0.8665, auc 0.8667
epoch 5101, loss 0.2781, train acc 86.75%, f1 0.8674, precision 0.8676, recall 0.8672, auc 0.8675
epoch 5201, loss 0.2625, train acc 86.83%, f1 0.8683, precision 0.8683, recall 0.8683, auc 0.8683
epoch 5301, loss 0.2707, train acc 86.94%, f1 0.8693, precision 0.8696, recall 0.8691, auc 0.8694
epoch 5401, loss 0.4110, train acc 87.00%, f1 0.8699, precision 0.8705, recall 0.8694, auc 0.8700
epoch 5501, loss 0.2727, train acc 87.08%, f1 0.8708, precision 0.8708, recall 0.8708, auc 0.8708
epoch 5601, loss 0.4056, train acc 87.14%, f1 0.8713, precision 0.8718, recall 0.8709, auc 0.8714
epoch 5701, loss 0.2419, train acc 87.18%, f1 0.8717, precision 0.8721, recall 0.8713, auc 0.8718
epoch 5801, loss 0.3314, train acc 87.23%, f1 0.8722, precision 0.8729, recall 0.8714, auc 0.8723
epoch 5901, loss 0.3313, train acc 87.29%, f1 0.8728, precision 0.8735, recall 0.8720, auc 0.8729
epoch 6001, loss 0.2878, train acc 87.27%, f1 0.8726, precision 0.8731, recall 0.8721, auc 0.8727
epoch 6101, loss 0.3273, train acc 87.39%, f1 0.8739, precision 0.8742, recall 0.8736, auc 0.8739
epoch 6201, loss 0.2694, train acc 87.45%, f1 0.8745, precision 0.8744, recall 0.8746, auc 0.8745
epoch 6301, loss 0.3281, train acc 87.51%, f1 0.8750, precision 0.8754, recall 0.8746, auc 0.8751
epoch 6401, loss 0.2428, train acc 87.55%, f1 0.8754, precision 0.8762, recall 0.8746, auc 0.8755
epoch 6501, loss 0.2400, train acc 87.58%, f1 0.8757, precision 0.8766, recall 0.8747, auc 0.8758
epoch 6601, loss 0.3519, train acc 87.66%, f1 0.8766, precision 0.8768, recall 0.8764, auc 0.8766
epoch 6701, loss 0.3393, train acc 87.64%, f1 0.8764, precision 0.8767, recall 0.8761, auc 0.8764
epoch 6801, loss 0.3043, train acc 87.76%, f1 0.8774, precision 0.8788, recall 0.8760, auc 0.8776
epoch 6901, loss 0.1731, train acc 87.83%, f1 0.8782, precision 0.8787, recall 0.8777, auc 0.8783
epoch 7001, loss 0.2858, train acc 87.86%, f1 0.8784, precision 0.8793, recall 0.8776, auc 0.8786
epoch 7101, loss 0.3732, train acc 87.92%, f1 0.8792, precision 0.8797, recall 0.8786, auc 0.8792
epoch 7201, loss 0.2589, train acc 87.93%, f1 0.8792, precision 0.8797, recall 0.8788, auc 0.8793
epoch 7301, loss 0.2902, train acc 88.00%, f1 0.8800, precision 0.8805, recall 0.8794, auc 0.8800
epoch 7401, loss 0.2603, train acc 87.99%, f1 0.8799, precision 0.8798, recall 0.8800, auc 0.8799
epoch 7501, loss 0.2421, train acc 88.11%, f1 0.8809, precision 0.8819, recall 0.8799, auc 0.8811
epoch 7601, loss 0.3024, train acc 88.15%, f1 0.8815, precision 0.8820, recall 0.8810, auc 0.8815
epoch 7701, loss 0.2651, train acc 88.23%, f1 0.8822, precision 0.8827, recall 0.8817, auc 0.8823
epoch 7801, loss 0.3164, train acc 88.26%, f1 0.8825, precision 0.8834, recall 0.8816, auc 0.8826
epoch 7901, loss 0.3278, train acc 88.34%, f1 0.8834, precision 0.8836, recall 0.8832, auc 0.8834
epoch 8001, loss 0.2233, train acc 88.36%, f1 0.8834, precision 0.8843, recall 0.8826, auc 0.8836
epoch 8101, loss 0.2889, train acc 88.42%, f1 0.8842, precision 0.8839, recall 0.8845, auc 0.8842
epoch 8201, loss 0.2854, train acc 88.46%, f1 0.8846, precision 0.8852, recall 0.8839, auc 0.8846/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.1638, train acc 88.53%, f1 0.8853, precision 0.8854, recall 0.8852, auc 0.8853
epoch 8401, loss 0.2316, train acc 88.61%, f1 0.8861, precision 0.8865, recall 0.8856, auc 0.8861
epoch 8501, loss 0.2898, train acc 88.65%, f1 0.8865, precision 0.8870, recall 0.8859, auc 0.8865
epoch 8601, loss 0.4042, train acc 88.73%, f1 0.8871, precision 0.8889, recall 0.8853, auc 0.8873
epoch 8701, loss 0.3007, train acc 88.77%, f1 0.8876, precision 0.8883, recall 0.8868, auc 0.8877
epoch 8801, loss 0.2151, train acc 88.84%, f1 0.8883, precision 0.8892, recall 0.8873, auc 0.8884
epoch 8901, loss 0.3032, train acc 88.85%, f1 0.8885, precision 0.8881, recall 0.8889, auc 0.8885
epoch 9001, loss 0.3229, train acc 88.98%, f1 0.8896, precision 0.8909, recall 0.8883, auc 0.8898
epoch 9101, loss 0.3936, train acc 89.00%, f1 0.8897, precision 0.8917, recall 0.8878, auc 0.8900
epoch 9201, loss 0.2449, train acc 89.08%, f1 0.8908, precision 0.8909, recall 0.8907, auc 0.8908
epoch 9301, loss 0.3030, train acc 89.11%, f1 0.8910, precision 0.8917, recall 0.8903, auc 0.8911
epoch 9401, loss 0.2209, train acc 89.16%, f1 0.8917, precision 0.8910, recall 0.8924, auc 0.8916
epoch 9501, loss 0.2180, train acc 89.20%, f1 0.8921, precision 0.8916, recall 0.8925, auc 0.8920
epoch 9601, loss 0.3212, train acc 89.32%, f1 0.8930, precision 0.8945, recall 0.8916, auc 0.8932
epoch 9701, loss 0.2349, train acc 89.35%, f1 0.8935, precision 0.8939, recall 0.8930, auc 0.8935
epoch 9801, loss 0.2471, train acc 89.33%, f1 0.8932, precision 0.8942, recall 0.8922, auc 0.8933
epoch 9901, loss 0.2532, train acc 89.42%, f1 0.8941, precision 0.8948, recall 0.8935, auc 0.8942
epoch 10001, loss 0.2165, train acc 89.50%, f1 0.8947, precision 0.8967, recall 0.8928, auc 0.8950
epoch 10101, loss 0.2297, train acc 89.49%, f1 0.8948, precision 0.8955, recall 0.8941, auc 0.8949
epoch 10201, loss 0.2147, train acc 89.54%, f1 0.8953, precision 0.8961, recall 0.8944, auc 0.8954
epoch 10301, loss 0.2866, train acc 89.65%, f1 0.8965, precision 0.8962, recall 0.8968, auc 0.8965
epoch 10401, loss 0.3982, train acc 89.66%, f1 0.8966, precision 0.8964, recall 0.8969, auc 0.8966
epoch 10501, loss 0.2429, train acc 89.71%, f1 0.8969, precision 0.8982, recall 0.8957, auc 0.8971
epoch 10601, loss 0.1905, train acc 89.76%, f1 0.8976, precision 0.8980, recall 0.8972, auc 0.8976
epoch 10701, loss 0.1837, train acc 89.77%, f1 0.8978, precision 0.8972, recall 0.8984, auc 0.8977
epoch 10801, loss 0.2784, train acc 89.83%, f1 0.8983, precision 0.8982, recall 0.8985, auc 0.8983
epoch 10901, loss 0.2873, train acc 89.87%, f1 0.8988, precision 0.8983, recall 0.8993, auc 0.8987
epoch 11001, loss 0.3246, train acc 89.94%, f1 0.8993, precision 0.8997, recall 0.8990, auc 0.8994
epoch 11101, loss 0.1442, train acc 89.97%, f1 0.8996, precision 0.9003, recall 0.8988, auc 0.8997
epoch 11201, loss 0.1614, train acc 89.99%, f1 0.8999, precision 0.9002, recall 0.8996, auc 0.8999
epoch 11301, loss 0.2270, train acc 90.12%, f1 0.9011, precision 0.9015, recall 0.9007, auc 0.9012
epoch 11401, loss 0.1688, train acc 90.12%, f1 0.9012, precision 0.9012, recall 0.9013, auc 0.9012
epoch 11501, loss 0.2060, train acc 90.20%, f1 0.9020, precision 0.9019, recall 0.9021, auc 0.9020
epoch 11601, loss 0.3124, train acc 90.28%, f1 0.9028, precision 0.9028, recall 0.9029, auc 0.9028
epoch 11701, loss 0.1865, train acc 90.26%, f1 0.9027, precision 0.9017, recall 0.9037, auc 0.9026
epoch 11801, loss 0.2120, train acc 90.32%, f1 0.9030, precision 0.9046, recall 0.9013, auc 0.9032
epoch 11901, loss 0.2393, train acc 90.38%, f1 0.9039, precision 0.9033, recall 0.9045, auc 0.9038
epoch 12001, loss 0.2058, train acc 90.46%, f1 0.9046, precision 0.9051, recall 0.9040, auc 0.9046
epoch 12101, loss 0.1636, train acc 90.46%, f1 0.9045, precision 0.9055, recall 0.9035, auc 0.9046
epoch 12201, loss 0.2704, train acc 90.49%, f1 0.9050, precision 0.9047, recall 0.9052, auc 0.9049
epoch 12301, loss 0.2186, train acc 90.52%, f1 0.9051, precision 0.9062, recall 0.9041, auc 0.9052
epoch 12401, loss 0.2305, train acc 90.63%, f1 0.9062, precision 0.9071, recall 0.9053, auc 0.9063
epoch 12501, loss 0.1924, train acc 90.67%, f1 0.9067, precision 0.9069, recall 0.9065, auc 0.9067
epoch 12601, loss 0.2561, train acc 90.69%, f1 0.9070, precision 0.9059, recall 0.9081, auc 0.9069
epoch 12701, loss 0.2120, train acc 90.75%, f1 0.9075, precision 0.9071, recall 0.9079, auc 0.9075
epoch 12801, loss 0.2785, train acc 90.73%, f1 0.9073, precision 0.9077, recall 0.9069, auc 0.9073
epoch 12901, loss 0.1723, train acc 90.77%, f1 0.9076, precision 0.9082, recall 0.9070, auc 0.9077
epoch 13001, loss 0.1281, train acc 90.88%, f1 0.9087, precision 0.9093, recall 0.9081, auc 0.9088
epoch 13101, loss 0.1933, train acc 90.88%, f1 0.9089, precision 0.9075, recall 0.9103, auc 0.9088
epoch 13201, loss 0.2338, train acc 90.92%, f1 0.9093, precision 0.9084, recall 0.9102, auc 0.9093
epoch 13301, loss 0.2137, train acc 90.96%, f1 0.9096, precision 0.9100, recall 0.9092, auc 0.9096
epoch 13401, loss 0.2835, train acc 90.97%, f1 0.9096, precision 0.9102, recall 0.9091, auc 0.9097
epoch 13501, loss 0.2354, train acc 91.03%, f1 0.9102, precision 0.9109, recall 0.9095, auc 0.9103
epoch 13601, loss 0.2034, train acc 91.08%, f1 0.9109, precision 0.9097, recall 0.9121, auc 0.9108
epoch 13701, loss 0.1577, train acc 91.09%, f1 0.9108, precision 0.9113, recall 0.9104, auc 0.9109
epoch 13801, loss 0.3022, train acc 91.17%, f1 0.9118, precision 0.9109, recall 0.9128, auc 0.9117
epoch 13901, loss 0.2474, train acc 91.20%, f1 0.9119, precision 0.9123, recall 0.9116, auc 0.9120
epoch 14001, loss 0.2647, train acc 91.26%, f1 0.9126, precision 0.9126, recall 0.9126, auc 0.9126
epoch 14101, loss 0.2173, train acc 91.30%, f1 0.9129, precision 0.9138, recall 0.9120, auc 0.9130
epoch 14201, loss 0.1335, train acc 91.33%, f1 0.9133, precision 0.9137, recall 0.9129, auc 0.9133
epoch 14301, loss 0.1515, train acc 91.37%, f1 0.9137, precision 0.9136, recall 0.9137, auc 0.9137
epoch 14401, loss 0.2161, train acc 91.37%, f1 0.9137, precision 0.9129, recall 0.9145, auc 0.9137
epoch 14501, loss 0.1535, train acc 91.44%, f1 0.9144, precision 0.9144, recall 0.9144, auc 0.9144
epoch 14601, loss 0.2579, train acc 91.48%, f1 0.9147, precision 0.9158, recall 0.9136, auc 0.9148
epoch 14701, loss 0.2170, train acc 91.49%, f1 0.9149, precision 0.9145, recall 0.9154, auc 0.9149
epoch 14801, loss 0.2062, train acc 91.54%, f1 0.9153, precision 0.9160, recall 0.9146, auc 0.9154
epoch 14901, loss 0.1513, train acc 91.55%, f1 0.9154, precision 0.9161, recall 0.9148, auc 0.9155
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_5
./test_pima/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6722641509433962

the Fscore is 0.6125

the precision is 0.45794392523364486

the recall is 0.9245283018867925

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_5
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5617, train acc 78.87%, f1 0.7863, precision 0.7955, recall 0.7773, auc 0.7887
epoch 201, loss 0.3984, train acc 81.85%, f1 0.8187, precision 0.8178, recall 0.8196, auc 0.8185
epoch 301, loss 0.4165, train acc 83.40%, f1 0.8344, precision 0.8321, recall 0.8367, auc 0.8340
epoch 401, loss 0.3920, train acc 83.97%, f1 0.8399, precision 0.8387, recall 0.8410, auc 0.8397
epoch 501, loss 0.2195, train acc 84.14%, f1 0.8418, precision 0.8397, recall 0.8440, auc 0.8414
epoch 601, loss 0.3444, train acc 84.21%, f1 0.8424, precision 0.8407, recall 0.8442, auc 0.8421
epoch 701, loss 0.3829, train acc 84.17%, f1 0.8421, precision 0.8396, recall 0.8447, auc 0.8417
epoch 801, loss 0.5261, train acc 84.23%, f1 0.8426, precision 0.8409, recall 0.8444, auc 0.8423
epoch 901, loss 0.2849, train acc 84.20%, f1 0.8423, precision 0.8409, recall 0.8437, auc 0.8420
epoch 1001, loss 0.3177, train acc 84.21%, f1 0.8424, precision 0.8410, recall 0.8439, auc 0.8421
epoch 1101, loss 0.4245, train acc 84.19%, f1 0.8422, precision 0.8406, recall 0.8438, auc 0.8419
epoch 1201, loss 0.3974, train acc 84.25%, f1 0.8427, precision 0.8417, recall 0.8437, auc 0.8425
epoch 1301, loss 0.3369, train acc 84.19%, f1 0.8420, precision 0.8416, recall 0.8423, auc 0.8419
epoch 1401, loss 0.3490, train acc 84.22%, f1 0.8423, precision 0.8417, recall 0.8429, auc 0.8422
epoch 1501, loss 0.3913, train acc 84.19%, f1 0.8419, precision 0.8417, recall 0.8421, auc 0.8419
epoch 1601, loss 0.4395, train acc 84.21%, f1 0.8422, precision 0.8414, recall 0.8430, auc 0.8421
epoch 1701, loss 0.3007, train acc 84.22%, f1 0.8423, precision 0.8419, recall 0.8426, auc 0.8422
epoch 1801, loss 0.3273, train acc 84.27%, f1 0.8427, precision 0.8427, recall 0.8427, auc 0.8427
epoch 1901, loss 0.3489, train acc 84.21%, f1 0.8422, precision 0.8417, recall 0.8427, auc 0.8421
epoch 2001, loss 0.4029, train acc 84.24%, f1 0.8425, precision 0.8421, recall 0.8428, auc 0.8424
epoch 2101, loss 0.4123, train acc 84.32%, f1 0.8433, precision 0.8424, recall 0.8443, auc 0.8432
epoch 2201, loss 0.1832, train acc 84.34%, f1 0.8435, precision 0.8427, recall 0.8444, auc 0.8434
epoch 2301, loss 0.3128, train acc 84.37%, f1 0.8438, precision 0.8432, recall 0.8443, auc 0.8437
epoch 2401, loss 0.4494, train acc 84.43%, f1 0.8445, precision 0.8437, recall 0.8452, auc 0.8443
epoch 2501, loss 0.3675, train acc 84.42%, f1 0.8442, precision 0.8441, recall 0.8443, auc 0.8442
epoch 2601, loss 0.3057, train acc 84.54%, f1 0.8455, precision 0.8451, recall 0.8459, auc 0.8454
epoch 2701, loss 0.2768, train acc 84.57%, f1 0.8457, precision 0.8456, recall 0.8458, auc 0.8457
epoch 2801, loss 0.2665, train acc 84.75%, f1 0.8476, precision 0.8469, recall 0.8483, auc 0.8475
epoch 2901, loss 0.3582, train acc 84.86%, f1 0.8487, precision 0.8482, recall 0.8492, auc 0.8486
epoch 3001, loss 0.3326, train acc 84.92%, f1 0.8492, precision 0.8492, recall 0.8493, auc 0.8492
epoch 3101, loss 0.2791, train acc 84.98%, f1 0.8497, precision 0.8500, recall 0.8495, auc 0.8498
epoch 3201, loss 0.3876, train acc 85.11%, f1 0.8510, precision 0.8513, recall 0.8508, auc 0.8511
epoch 3301, loss 0.3671, train acc 85.23%, f1 0.8522, precision 0.8527, recall 0.8516, auc 0.8523
epoch 3401, loss 0.3611, train acc 85.34%, f1 0.8534, precision 0.8536, recall 0.8531, auc 0.8534
epoch 3501, loss 0.2889, train acc 85.47%, f1 0.8547, precision 0.8548, recall 0.8546, auc 0.8547
epoch 3601, loss 0.4173, train acc 85.61%, f1 0.8560, precision 0.8564, recall 0.8555, auc 0.8561
epoch 3701, loss 0.3658, train acc 85.70%, f1 0.8569, precision 0.8574, recall 0.8564, auc 0.8570
epoch 3801, loss 0.2330, train acc 85.83%, f1 0.8583, precision 0.8587, recall 0.8578, auc 0.8583
epoch 3901, loss 0.2310, train acc 85.94%, f1 0.8594, precision 0.8595, recall 0.8592, auc 0.8594
epoch 4001, loss 0.3205, train acc 86.00%, f1 0.8601, precision 0.8595, recall 0.8608, auc 0.8600
epoch 4101, loss 0.3315, train acc 86.13%, f1 0.8612, precision 0.8617, recall 0.8607, auc 0.8613
epoch 4201, loss 0.2617, train acc 86.22%, f1 0.8622, precision 0.8623, recall 0.8620, auc 0.8622
epoch 4301, loss 0.2998, train acc 86.32%, f1 0.8631, precision 0.8634, recall 0.8629, auc 0.8632
epoch 4401, loss 0.3379, train acc 86.42%, f1 0.8641, precision 0.8645, recall 0.8637, auc 0.8642
epoch 4501, loss 0.2838, train acc 86.46%, f1 0.8647, precision 0.8643, recall 0.8650, auc 0.8646
epoch 4601, loss 0.3213, train acc 86.53%, f1 0.8653, precision 0.8654, recall 0.8651, auc 0.8653
epoch 4701, loss 0.3191, train acc 86.59%, f1 0.8659, precision 0.8657, recall 0.8661, auc 0.8659
epoch 4801, loss 0.2683, train acc 86.65%, f1 0.8664, precision 0.8671, recall 0.8657, auc 0.8665
epoch 4901, loss 0.3381, train acc 86.71%, f1 0.8670, precision 0.8677, recall 0.8662, auc 0.8671
epoch 5001, loss 0.2211, train acc 86.78%, f1 0.8676, precision 0.8687, recall 0.8665, auc 0.8678
epoch 5101, loss 0.3652, train acc 86.84%, f1 0.8682, precision 0.8697, recall 0.8667, auc 0.8684
epoch 5201, loss 0.2616, train acc 86.94%, f1 0.8693, precision 0.8701, recall 0.8684, auc 0.8694
epoch 5301, loss 0.2520, train acc 86.98%, f1 0.8697, precision 0.8698, recall 0.8696, auc 0.8698
epoch 5401, loss 0.2972, train acc 87.05%, f1 0.8704, precision 0.8707, recall 0.8701, auc 0.8705
epoch 5501, loss 0.2926, train acc 87.06%, f1 0.8704, precision 0.8715, recall 0.8694, auc 0.8706
epoch 5601, loss 0.3792, train acc 87.14%, f1 0.8714, precision 0.8719, recall 0.8708, auc 0.8714
epoch 5701, loss 0.3274, train acc 87.23%, f1 0.8722, precision 0.8724, recall 0.8720, auc 0.8723
epoch 5801, loss 0.2771, train acc 87.24%, f1 0.8722, precision 0.8740, recall 0.8703, auc 0.8724
epoch 5901, loss 0.2990, train acc 87.36%, f1 0.8736, precision 0.8734, recall 0.8738, auc 0.8736
epoch 6001, loss 0.4513, train acc 87.37%, f1 0.8738, precision 0.8734, recall 0.8741, auc 0.8737
epoch 6101, loss 0.3699, train acc 87.50%, f1 0.8748, precision 0.8758, recall 0.8738, auc 0.8750
epoch 6201, loss 0.2675, train acc 87.54%, f1 0.8753, precision 0.8756, recall 0.8751, auc 0.8754
epoch 6301, loss 0.2636, train acc 87.60%, f1 0.8758, precision 0.8774, recall 0.8741, auc 0.8760
epoch 6401, loss 0.2104, train acc 87.61%, f1 0.8759, precision 0.8778, recall 0.8739, auc 0.8761
epoch 6501, loss 0.2763, train acc 87.67%, f1 0.8765, precision 0.8775, recall 0.8756, auc 0.8767
epoch 6601, loss 0.2161, train acc 87.68%, f1 0.8767, precision 0.8775, recall 0.8759, auc 0.8768
epoch 6701, loss 0.3551, train acc 87.79%, f1 0.8779, precision 0.8776, recall 0.8782, auc 0.8779
epoch 6801, loss 0.2785, train acc 87.81%, f1 0.8779, precision 0.8792, recall 0.8765, auc 0.8781
epoch 6901, loss 0.2260, train acc 87.88%, f1 0.8786, precision 0.8799, recall 0.8774, auc 0.8788
epoch 7001, loss 0.3582, train acc 87.95%, f1 0.8793, precision 0.8808, recall 0.8778, auc 0.8795
epoch 7101, loss 0.2772, train acc 87.90%, f1 0.8788, precision 0.8801, recall 0.8776, auc 0.8790
epoch 7201, loss 0.2396, train acc 88.06%, f1 0.8805, precision 0.8809, recall 0.8802, auc 0.8806
epoch 7301, loss 0.2495, train acc 88.06%, f1 0.8807, precision 0.8806, recall 0.8807, auc 0.8806
epoch 7401, loss 0.3114, train acc 88.15%, f1 0.8813, precision 0.8824, recall 0.8803, auc 0.8815
epoch 7501, loss 0.2341, train acc 88.18%, f1 0.8819, precision 0.8813, recall 0.8825, auc 0.8818
epoch 7601, loss 0.3185, train acc 88.26%, f1 0.8824, precision 0.8842, recall 0.8806, auc 0.8826
epoch 7701, loss 0.3064, train acc 88.34%, f1 0.8833, precision 0.8840, recall 0.8826, auc 0.8834
epoch 7801, loss 0.2922, train acc 88.37%, f1 0.8835, precision 0.8849, recall 0.8822, auc 0.8837
epoch 7901, loss 0.2701, train acc 88.42%, f1 0.8840, precision 0.8853, recall 0.8827, auc 0.8842
epoch 8001, loss 0.2964, train acc 88.41%, f1 0.8841, precision 0.8842, recall 0.8839, auc 0.8841
epoch 8101, loss 0.2911, train acc 88.50%, f1 0.8851, precision 0.8848, recall 0.8853, auc 0.8850
epoch 8201, loss 0.2364, train acc 88.60%, f1 0.8859, precision 0.8867, recall 0.8851, auc 0.8860/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2325, train acc 88.65%, f1 0.8864, precision 0.8873, recall 0.8855, auc 0.8865
epoch 8401, loss 0.3573, train acc 88.68%, f1 0.8867, precision 0.8873, recall 0.8861, auc 0.8868
epoch 8501, loss 0.2546, train acc 88.76%, f1 0.8874, precision 0.8892, recall 0.8856, auc 0.8876
epoch 8601, loss 0.2492, train acc 88.80%, f1 0.8880, precision 0.8881, recall 0.8879, auc 0.8880
epoch 8701, loss 0.2671, train acc 88.91%, f1 0.8890, precision 0.8902, recall 0.8877, auc 0.8891
epoch 8801, loss 0.2779, train acc 88.89%, f1 0.8888, precision 0.8900, recall 0.8876, auc 0.8889
epoch 8901, loss 0.1977, train acc 89.02%, f1 0.8902, precision 0.8904, recall 0.8900, auc 0.8902
epoch 9001, loss 0.2997, train acc 89.01%, f1 0.8902, precision 0.8894, recall 0.8909, auc 0.8901
epoch 9101, loss 0.3703, train acc 89.09%, f1 0.8908, precision 0.8918, recall 0.8898, auc 0.8909
epoch 9201, loss 0.2914, train acc 89.13%, f1 0.8914, precision 0.8910, recall 0.8917, auc 0.8913
epoch 9301, loss 0.2562, train acc 89.19%, f1 0.8917, precision 0.8932, recall 0.8902, auc 0.8919
epoch 9401, loss 0.2067, train acc 89.23%, f1 0.8921, precision 0.8933, recall 0.8910, auc 0.8923
epoch 9501, loss 0.2330, train acc 89.28%, f1 0.8928, precision 0.8932, recall 0.8923, auc 0.8928
epoch 9601, loss 0.3072, train acc 89.36%, f1 0.8936, precision 0.8931, recall 0.8942, auc 0.8936
epoch 9701, loss 0.2884, train acc 89.39%, f1 0.8939, precision 0.8936, recall 0.8942, auc 0.8939
epoch 9801, loss 0.2533, train acc 89.49%, f1 0.8948, precision 0.8956, recall 0.8940, auc 0.8949
epoch 9901, loss 0.2483, train acc 89.49%, f1 0.8947, precision 0.8966, recall 0.8927, auc 0.8949
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_5
./test_pima/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6522641509433962

the Fscore is 0.5975609756097561

the precision is 0.44144144144144143

the recall is 0.9245283018867925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_5
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5461, train acc 79.14%, f1 0.7914, precision 0.7913, recall 0.7915, auc 0.7914
epoch 201, loss 0.4182, train acc 81.99%, f1 0.8199, precision 0.8200, recall 0.8198, auc 0.8199
epoch 301, loss 0.3585, train acc 83.49%, f1 0.8349, precision 0.8350, recall 0.8348, auc 0.8349
epoch 401, loss 0.2959, train acc 83.94%, f1 0.8394, precision 0.8395, recall 0.8394, auc 0.8394
epoch 501, loss 0.3430, train acc 84.18%, f1 0.8418, precision 0.8419, recall 0.8418, auc 0.8418
epoch 601, loss 0.2992, train acc 84.29%, f1 0.8429, precision 0.8430, recall 0.8428, auc 0.8429
epoch 701, loss 0.2806, train acc 84.21%, f1 0.8421, precision 0.8421, recall 0.8421, auc 0.8421
epoch 801, loss 0.4364, train acc 84.18%, f1 0.8418, precision 0.8418, recall 0.8417, auc 0.8418
epoch 901, loss 0.3321, train acc 84.15%, f1 0.8415, precision 0.8415, recall 0.8414, auc 0.8415
epoch 1001, loss 0.3127, train acc 84.15%, f1 0.8415, precision 0.8416, recall 0.8415, auc 0.8415
epoch 1101, loss 0.3380, train acc 84.20%, f1 0.8420, precision 0.8421, recall 0.8419, auc 0.8420
epoch 1201, loss 0.3738, train acc 84.16%, f1 0.8415, precision 0.8416, recall 0.8414, auc 0.8416
epoch 1301, loss 0.4481, train acc 84.27%, f1 0.8427, precision 0.8428, recall 0.8426, auc 0.8427
epoch 1401, loss 0.3330, train acc 84.19%, f1 0.8419, precision 0.8420, recall 0.8418, auc 0.8419
epoch 1501, loss 0.3555, train acc 84.24%, f1 0.8424, precision 0.8425, recall 0.8423, auc 0.8424
epoch 1601, loss 0.4190, train acc 84.24%, f1 0.8424, precision 0.8425, recall 0.8422, auc 0.8424
epoch 1701, loss 0.3843, train acc 84.24%, f1 0.8423, precision 0.8424, recall 0.8422, auc 0.8424
epoch 1801, loss 0.4901, train acc 84.14%, f1 0.8414, precision 0.8414, recall 0.8414, auc 0.8414
epoch 1901, loss 0.3690, train acc 84.20%, f1 0.8420, precision 0.8420, recall 0.8420, auc 0.8420
epoch 2001, loss 0.4101, train acc 84.30%, f1 0.8430, precision 0.8431, recall 0.8428, auc 0.8430
epoch 2101, loss 0.3975, train acc 84.29%, f1 0.8429, precision 0.8430, recall 0.8429, auc 0.8429
epoch 2201, loss 0.3368, train acc 84.25%, f1 0.8424, precision 0.8426, recall 0.8423, auc 0.8425
epoch 2301, loss 0.4330, train acc 84.28%, f1 0.8428, precision 0.8428, recall 0.8428, auc 0.8428
epoch 2401, loss 0.3946, train acc 84.39%, f1 0.8438, precision 0.8440, recall 0.8437, auc 0.8439
epoch 2501, loss 0.2526, train acc 84.41%, f1 0.8441, precision 0.8439, recall 0.8443, auc 0.8441
epoch 2601, loss 0.3018, train acc 84.46%, f1 0.8446, precision 0.8447, recall 0.8446, auc 0.8446
epoch 2701, loss 0.4663, train acc 84.52%, f1 0.8453, precision 0.8450, recall 0.8455, auc 0.8452
epoch 2801, loss 0.3461, train acc 84.66%, f1 0.8467, precision 0.8462, recall 0.8473, auc 0.8466
epoch 2901, loss 0.3233, train acc 84.69%, f1 0.8470, precision 0.8468, recall 0.8472, auc 0.8469
epoch 3001, loss 0.3352, train acc 84.81%, f1 0.8481, precision 0.8482, recall 0.8480, auc 0.8481
epoch 3101, loss 0.3637, train acc 84.91%, f1 0.8490, precision 0.8493, recall 0.8487, auc 0.8491
epoch 3201, loss 0.3361, train acc 85.03%, f1 0.8502, precision 0.8508, recall 0.8497, auc 0.8503
epoch 3301, loss 0.2698, train acc 85.17%, f1 0.8516, precision 0.8520, recall 0.8511, auc 0.8517
epoch 3401, loss 0.3296, train acc 85.29%, f1 0.8529, precision 0.8532, recall 0.8525, auc 0.8529
epoch 3501, loss 0.3737, train acc 85.43%, f1 0.8542, precision 0.8545, recall 0.8540, auc 0.8543
epoch 3601, loss 0.3144, train acc 85.55%, f1 0.8555, precision 0.8555, recall 0.8556, auc 0.8555
epoch 3701, loss 0.4529, train acc 85.63%, f1 0.8563, precision 0.8564, recall 0.8561, auc 0.8563
epoch 3801, loss 0.2671, train acc 85.72%, f1 0.8571, precision 0.8574, recall 0.8569, auc 0.8572
epoch 3901, loss 0.4041, train acc 85.91%, f1 0.8591, precision 0.8591, recall 0.8590, auc 0.8591
epoch 4001, loss 0.2268, train acc 85.98%, f1 0.8596, precision 0.8606, recall 0.8586, auc 0.8598
epoch 4101, loss 0.3525, train acc 86.08%, f1 0.8607, precision 0.8611, recall 0.8604, auc 0.8608
epoch 4201, loss 0.3236, train acc 86.16%, f1 0.8615, precision 0.8621, recall 0.8609, auc 0.8616
epoch 4301, loss 0.3002, train acc 86.25%, f1 0.8624, precision 0.8630, recall 0.8617, auc 0.8625
epoch 4401, loss 0.3121, train acc 86.31%, f1 0.8630, precision 0.8635, recall 0.8625, auc 0.8631
epoch 4501, loss 0.2703, train acc 86.44%, f1 0.8643, precision 0.8652, recall 0.8633, auc 0.8644
epoch 4601, loss 0.3697, train acc 86.46%, f1 0.8646, precision 0.8650, recall 0.8641, auc 0.8646
epoch 4701, loss 0.3451, train acc 86.53%, f1 0.8653, precision 0.8653, recall 0.8652, auc 0.8653
epoch 4801, loss 0.2579, train acc 86.58%, f1 0.8657, precision 0.8665, recall 0.8649, auc 0.8658
epoch 4901, loss 0.2654, train acc 86.63%, f1 0.8665, precision 0.8658, recall 0.8672, auc 0.8663
epoch 5001, loss 0.3013, train acc 86.73%, f1 0.8673, precision 0.8677, recall 0.8668, auc 0.8673
epoch 5101, loss 0.2514, train acc 86.78%, f1 0.8678, precision 0.8677, recall 0.8679, auc 0.8678
epoch 5201, loss 0.1641, train acc 86.78%, f1 0.8678, precision 0.8677, recall 0.8679, auc 0.8678
epoch 5301, loss 0.3504, train acc 86.87%, f1 0.8688, precision 0.8687, recall 0.8688, auc 0.8687
epoch 5401, loss 0.3846, train acc 86.95%, f1 0.8694, precision 0.8703, recall 0.8685, auc 0.8695
epoch 5501, loss 0.2985, train acc 87.07%, f1 0.8709, precision 0.8700, recall 0.8718, auc 0.8707
epoch 5601, loss 0.3574, train acc 87.07%, f1 0.8706, precision 0.8709, recall 0.8703, auc 0.8707
epoch 5701, loss 0.2335, train acc 87.15%, f1 0.8714, precision 0.8719, recall 0.8709, auc 0.8715
epoch 5801, loss 0.2836, train acc 87.22%, f1 0.8722, precision 0.8721, recall 0.8724, auc 0.8722
epoch 5901, loss 0.2373, train acc 87.27%, f1 0.8728, precision 0.8726, recall 0.8730, auc 0.8727
epoch 6001, loss 0.2478, train acc 87.33%, f1 0.8733, precision 0.8732, recall 0.8734, auc 0.8733
epoch 6101, loss 0.2495, train acc 87.36%, f1 0.8737, precision 0.8731, recall 0.8742, auc 0.8736
epoch 6201, loss 0.2794, train acc 87.46%, f1 0.8745, precision 0.8751, recall 0.8738, auc 0.8746
epoch 6301, loss 0.4854, train acc 87.46%, f1 0.8746, precision 0.8744, recall 0.8749, auc 0.8746
epoch 6401, loss 0.4717, train acc 87.56%, f1 0.8755, precision 0.8761, recall 0.8749, auc 0.8756
epoch 6501, loss 0.3009, train acc 87.59%, f1 0.8760, precision 0.8754, recall 0.8765, auc 0.8759
epoch 6601, loss 0.3132, train acc 87.63%, f1 0.8764, precision 0.8759, recall 0.8770, auc 0.8763
epoch 6701, loss 0.2604, train acc 87.71%, f1 0.8771, precision 0.8775, recall 0.8767, auc 0.8771
epoch 6801, loss 0.2517, train acc 87.69%, f1 0.8769, precision 0.8773, recall 0.8765, auc 0.8769
epoch 6901, loss 0.2687, train acc 87.71%, f1 0.8771, precision 0.8772, recall 0.8771, auc 0.8771
epoch 7001, loss 0.3169, train acc 87.82%, f1 0.8784, precision 0.8770, recall 0.8798, auc 0.8782
epoch 7101, loss 0.2757, train acc 87.75%, f1 0.8775, precision 0.8779, recall 0.8770, auc 0.8775
epoch 7201, loss 0.2736, train acc 87.86%, f1 0.8786, precision 0.8785, recall 0.8787, auc 0.8786
epoch 7301, loss 0.2589, train acc 87.93%, f1 0.8794, precision 0.8785, recall 0.8803, auc 0.8793
epoch 7401, loss 0.3256, train acc 87.95%, f1 0.8795, precision 0.8797, recall 0.8793, auc 0.8795
epoch 7501, loss 0.1121, train acc 88.00%, f1 0.8801, precision 0.8794, recall 0.8808, auc 0.8800
epoch 7601, loss 0.2796, train acc 88.10%, f1 0.8809, precision 0.8815, recall 0.8803, auc 0.8810
epoch 7701, loss 0.2503, train acc 88.14%, f1 0.8816, precision 0.8805, recall 0.8827, auc 0.8814
epoch 7801, loss 0.2672, train acc 88.17%, f1 0.8816, precision 0.8823, recall 0.8809, auc 0.8817
epoch 7901, loss 0.2929, train acc 88.20%, f1 0.8821, precision 0.8816, recall 0.8825, auc 0.8820
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_5
./test_pima/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6661320754716982

the Fscore is 0.6107784431137725

the precision is 0.4473684210526316

the recall is 0.9622641509433962

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_5
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5396, train acc 79.23%, f1 0.7918, precision 0.7936, recall 0.7900, auc 0.7923
epoch 201, loss 0.4093, train acc 81.95%, f1 0.8195, precision 0.8196, recall 0.8195, auc 0.8195
epoch 301, loss 0.3239, train acc 83.53%, f1 0.8354, precision 0.8349, recall 0.8359, auc 0.8353
epoch 401, loss 0.5510, train acc 83.97%, f1 0.8398, precision 0.8393, recall 0.8404, auc 0.8397
epoch 501, loss 0.4195, train acc 84.18%, f1 0.8419, precision 0.8414, recall 0.8424, auc 0.8418
epoch 601, loss 0.2704, train acc 84.14%, f1 0.8414, precision 0.8413, recall 0.8415, auc 0.8414
epoch 701, loss 0.3469, train acc 84.18%, f1 0.8418, precision 0.8416, recall 0.8421, auc 0.8418
epoch 801, loss 0.3244, train acc 84.18%, f1 0.8418, precision 0.8416, recall 0.8421, auc 0.8418
epoch 901, loss 0.3125, train acc 84.13%, f1 0.8413, precision 0.8411, recall 0.8415, auc 0.8413
epoch 1001, loss 0.4175, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8422, auc 0.8422
epoch 1101, loss 0.2211, train acc 84.14%, f1 0.8414, precision 0.8413, recall 0.8416, auc 0.8414
epoch 1201, loss 0.4080, train acc 84.17%, f1 0.8417, precision 0.8415, recall 0.8420, auc 0.8417
epoch 1301, loss 0.4140, train acc 84.21%, f1 0.8421, precision 0.8420, recall 0.8423, auc 0.8421
epoch 1401, loss 0.4177, train acc 84.20%, f1 0.8420, precision 0.8418, recall 0.8423, auc 0.8420
epoch 1501, loss 0.4387, train acc 84.26%, f1 0.8426, precision 0.8425, recall 0.8427, auc 0.8426
epoch 1601, loss 0.3683, train acc 84.21%, f1 0.8422, precision 0.8419, recall 0.8424, auc 0.8421
epoch 1701, loss 0.4871, train acc 84.19%, f1 0.8420, precision 0.8418, recall 0.8422, auc 0.8419
epoch 1801, loss 0.4019, train acc 84.15%, f1 0.8415, precision 0.8411, recall 0.8419, auc 0.8415
epoch 1901, loss 0.3365, train acc 84.20%, f1 0.8420, precision 0.8419, recall 0.8422, auc 0.8420
epoch 2001, loss 0.3824, train acc 84.20%, f1 0.8421, precision 0.8418, recall 0.8424, auc 0.8420
epoch 2101, loss 0.3685, train acc 84.23%, f1 0.8424, precision 0.8421, recall 0.8427, auc 0.8423
epoch 2201, loss 0.4143, train acc 84.28%, f1 0.8427, precision 0.8429, recall 0.8426, auc 0.8428
epoch 2301, loss 0.2779, train acc 84.29%, f1 0.8430, precision 0.8427, recall 0.8432, auc 0.8429
epoch 2401, loss 0.3566, train acc 84.40%, f1 0.8441, precision 0.8437, recall 0.8445, auc 0.8440
epoch 2501, loss 0.3541, train acc 84.42%, f1 0.8441, precision 0.8442, recall 0.8441, auc 0.8442
epoch 2601, loss 0.4749, train acc 84.51%, f1 0.8451, precision 0.8451, recall 0.8451, auc 0.8451
epoch 2701, loss 0.3347, train acc 84.53%, f1 0.8452, precision 0.8456, recall 0.8448, auc 0.8453
epoch 2801, loss 0.3576, train acc 84.48%, f1 0.8447, precision 0.8450, recall 0.8445, auc 0.8448
epoch 2901, loss 0.3161, train acc 84.73%, f1 0.8472, precision 0.8478, recall 0.8467, auc 0.8473
epoch 3001, loss 0.3550, train acc 84.80%, f1 0.8479, precision 0.8481, recall 0.8478, auc 0.8480
epoch 3101, loss 0.2969, train acc 84.96%, f1 0.8497, precision 0.8495, recall 0.8498, auc 0.8496
epoch 3201, loss 0.3238, train acc 85.03%, f1 0.8503, precision 0.8503, recall 0.8502, auc 0.8503
epoch 3301, loss 0.3810, train acc 85.17%, f1 0.8517, precision 0.8519, recall 0.8515, auc 0.8517
epoch 3401, loss 0.4292, train acc 85.28%, f1 0.8527, precision 0.8528, recall 0.8527, auc 0.8528
epoch 3501, loss 0.3903, train acc 85.43%, f1 0.8541, precision 0.8554, recall 0.8528, auc 0.8543
epoch 3601, loss 0.3474, train acc 85.58%, f1 0.8558, precision 0.8553, recall 0.8564, auc 0.8558
epoch 3701, loss 0.2417, train acc 85.60%, f1 0.8560, precision 0.8561, recall 0.8559, auc 0.8560
epoch 3801, loss 0.3776, train acc 85.76%, f1 0.8575, precision 0.8580, recall 0.8570, auc 0.8576
epoch 3901, loss 0.2949, train acc 85.88%, f1 0.8587, precision 0.8595, recall 0.8579, auc 0.8588
epoch 4001, loss 0.3081, train acc 85.98%, f1 0.8598, precision 0.8599, recall 0.8598, auc 0.8598
epoch 4101, loss 0.3158, train acc 86.06%, f1 0.8606, precision 0.8608, recall 0.8603, auc 0.8606
epoch 4201, loss 0.3345, train acc 86.13%, f1 0.8612, precision 0.8618, recall 0.8606, auc 0.8613
epoch 4301, loss 0.2255, train acc 86.20%, f1 0.8621, precision 0.8618, recall 0.8623, auc 0.8620
epoch 4401, loss 0.3909, train acc 86.31%, f1 0.8630, precision 0.8637, recall 0.8622, auc 0.8631
epoch 4501, loss 0.3009, train acc 86.41%, f1 0.8640, precision 0.8649, recall 0.8630, auc 0.8641
epoch 4601, loss 0.3318, train acc 86.53%, f1 0.8652, precision 0.8657, recall 0.8647, auc 0.8653
epoch 4701, loss 0.3498, train acc 86.55%, f1 0.8654, precision 0.8661, recall 0.8647, auc 0.8655
epoch 4801, loss 0.3093, train acc 86.60%, f1 0.8659, precision 0.8666, recall 0.8652, auc 0.8660
epoch 4901, loss 0.3549, train acc 86.70%, f1 0.8670, precision 0.8668, recall 0.8672, auc 0.8670
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_5
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6561320754716982

the Fscore is 0.6035502958579881

the precision is 0.4396551724137931

the recall is 0.9622641509433962

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_5
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5329, train acc 78.67%, f1 0.7864, precision 0.7875, recall 0.7852, auc 0.7867
epoch 201, loss 0.3855, train acc 81.83%, f1 0.8183, precision 0.8182, recall 0.8185, auc 0.8183
epoch 301, loss 0.4486, train acc 83.46%, f1 0.8347, precision 0.8344, recall 0.8349, auc 0.8346
epoch 401, loss 0.3542, train acc 83.94%, f1 0.8394, precision 0.8393, recall 0.8395, auc 0.8394
epoch 501, loss 0.2827, train acc 84.10%, f1 0.8411, precision 0.8408, recall 0.8413, auc 0.8410
epoch 601, loss 0.3742, train acc 84.20%, f1 0.8420, precision 0.8420, recall 0.8421, auc 0.8420
epoch 701, loss 0.3569, train acc 84.17%, f1 0.8417, precision 0.8417, recall 0.8418, auc 0.8417
epoch 801, loss 0.2850, train acc 84.25%, f1 0.8425, precision 0.8425, recall 0.8425, auc 0.8425
epoch 901, loss 0.4927, train acc 84.25%, f1 0.8425, precision 0.8425, recall 0.8425, auc 0.8425
epoch 1001, loss 0.4338, train acc 84.08%, f1 0.8409, precision 0.8408, recall 0.8409, auc 0.8408
epoch 1101, loss 0.2883, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8423, auc 0.8423
epoch 1201, loss 0.2350, train acc 84.27%, f1 0.8427, precision 0.8428, recall 0.8426, auc 0.8427
epoch 1301, loss 0.4810, train acc 84.21%, f1 0.8421, precision 0.8422, recall 0.8420, auc 0.8421
epoch 1401, loss 0.3193, train acc 84.22%, f1 0.8421, precision 0.8422, recall 0.8420, auc 0.8422
epoch 1501, loss 0.3874, train acc 84.17%, f1 0.8417, precision 0.8419, recall 0.8416, auc 0.8417
epoch 1601, loss 0.2789, train acc 84.25%, f1 0.8425, precision 0.8426, recall 0.8423, auc 0.8425
epoch 1701, loss 0.2558, train acc 84.17%, f1 0.8417, precision 0.8419, recall 0.8414, auc 0.8417
epoch 1801, loss 0.3802, train acc 84.23%, f1 0.8422, precision 0.8425, recall 0.8420, auc 0.8423
epoch 1901, loss 0.3479, train acc 84.18%, f1 0.8418, precision 0.8420, recall 0.8416, auc 0.8418
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_5
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.605566037735849

the Fscore is 0.5714285714285714

the precision is 0.40310077519379844

the recall is 0.9811320754716981

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_5
----------------------



epoch 1, loss 0.6937, train acc 50.18%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5305, train acc 78.63%, f1 0.7769, precision 0.8093, recall 0.7470, auc 0.7862
epoch 201, loss 0.5032, train acc 81.91%, f1 0.8179, precision 0.8205, recall 0.8153, auc 0.8191
epoch 301, loss 0.3933, train acc 83.37%, f1 0.8335, precision 0.8316, recall 0.8354, auc 0.8337
epoch 401, loss 0.4071, train acc 83.95%, f1 0.8395, precision 0.8367, recall 0.8422, auc 0.8395
epoch 501, loss 0.4024, train acc 84.04%, f1 0.8415, precision 0.8328, recall 0.8503, auc 0.8404
epoch 601, loss 0.4213, train acc 84.14%, f1 0.8423, precision 0.8346, recall 0.8502, auc 0.8414
epoch 701, loss 0.3291, train acc 84.18%, f1 0.8423, precision 0.8366, recall 0.8481, auc 0.8418
epoch 801, loss 0.2948, train acc 84.08%, f1 0.8420, precision 0.8328, recall 0.8515, auc 0.8409
epoch 901, loss 0.3543, train acc 84.17%, f1 0.8424, precision 0.8359, recall 0.8489, auc 0.8417
epoch 1001, loss 0.3236, train acc 84.21%, f1 0.8423, precision 0.8384, recall 0.8462, auc 0.8421
epoch 1101, loss 0.4183, train acc 84.23%, f1 0.8430, precision 0.8367, recall 0.8494, auc 0.8424
epoch 1201, loss 0.2426, train acc 84.22%, f1 0.8429, precision 0.8360, recall 0.8499, auc 0.8422
epoch 1301, loss 0.3799, train acc 84.27%, f1 0.8423, precision 0.8412, recall 0.8434, auc 0.8427
epoch 1401, loss 0.4903, train acc 84.27%, f1 0.8428, precision 0.8393, recall 0.8462, auc 0.8427
epoch 1501, loss 0.3534, train acc 84.18%, f1 0.8420, precision 0.8382, recall 0.8458, auc 0.8419
epoch 1601, loss 0.2680, train acc 84.21%, f1 0.8427, precision 0.8365, recall 0.8490, auc 0.8421
epoch 1701, loss 0.3413, train acc 84.23%, f1 0.8425, precision 0.8385, recall 0.8465, auc 0.8424
epoch 1801, loss 0.4296, train acc 84.26%, f1 0.8425, precision 0.8401, recall 0.8449, auc 0.8426
epoch 1901, loss 0.2952, train acc 84.24%, f1 0.8428, precision 0.8376, recall 0.8481, auc 0.8424
epoch 2001, loss 0.4986, train acc 84.17%, f1 0.8412, precision 0.8409, recall 0.8415, auc 0.8417
epoch 2101, loss 0.3318, train acc 84.21%, f1 0.8420, precision 0.8395, recall 0.8445, auc 0.8421
epoch 2201, loss 0.3898, train acc 84.15%, f1 0.8418, precision 0.8372, recall 0.8465, auc 0.8415
epoch 2301, loss 0.3221, train acc 84.21%, f1 0.8425, precision 0.8376, recall 0.8475, auc 0.8422
epoch 2401, loss 0.3838, train acc 84.24%, f1 0.8428, precision 0.8378, recall 0.8479, auc 0.8424
epoch 2501, loss 0.3442, train acc 84.29%, f1 0.8422, precision 0.8433, recall 0.8410, auc 0.8429
epoch 2601, loss 0.3650, train acc 84.28%, f1 0.8431, precision 0.8386, recall 0.8475, auc 0.8428
epoch 2701, loss 0.3173, train acc 84.27%, f1 0.8426, precision 0.8398, recall 0.8455, auc 0.8427
epoch 2801, loss 0.3363, train acc 84.28%, f1 0.8430, precision 0.8389, recall 0.8471, auc 0.8428
epoch 2901, loss 0.3527, train acc 84.26%, f1 0.8430, precision 0.8381, recall 0.8479, auc 0.8426
epoch 3001, loss 0.4102, train acc 84.29%, f1 0.8428, precision 0.8405, recall 0.8451, auc 0.8429
epoch 3101, loss 0.4276, train acc 84.33%, f1 0.8437, precision 0.8386, recall 0.8488, auc 0.8433
epoch 3201, loss 0.3383, train acc 84.38%, f1 0.8444, precision 0.8380, recall 0.8510, auc 0.8438
epoch 3301, loss 0.4201, train acc 84.47%, f1 0.8446, precision 0.8421, recall 0.8470, auc 0.8447
epoch 3401, loss 0.3363, train acc 84.43%, f1 0.8439, precision 0.8433, recall 0.8444, auc 0.8443
epoch 3501, loss 0.3522, train acc 84.51%, f1 0.8449, precision 0.8426, recall 0.8473, auc 0.8451
epoch 3601, loss 0.4233, train acc 84.56%, f1 0.8450, precision 0.8452, recall 0.8447, auc 0.8456
epoch 3701, loss 0.3148, train acc 84.58%, f1 0.8452, precision 0.8454, recall 0.8450, auc 0.8458
epoch 3801, loss 0.3756, train acc 84.71%, f1 0.8471, precision 0.8440, recall 0.8502, auc 0.8471
epoch 3901, loss 0.3602, train acc 84.82%, f1 0.8475, precision 0.8483, recall 0.8468, auc 0.8482
epoch 4001, loss 0.2656, train acc 84.85%, f1 0.8483, precision 0.8467, recall 0.8498, auc 0.8485
epoch 4101, loss 0.2539, train acc 85.04%, f1 0.8503, precision 0.8478, recall 0.8528, auc 0.8504
epoch 4201, loss 0.3640, train acc 85.12%, f1 0.8507, precision 0.8504, recall 0.8510, auc 0.8512
epoch 4301, loss 0.2937, train acc 85.18%, f1 0.8512, precision 0.8515, recall 0.8510, auc 0.8518
epoch 4401, loss 0.2380, train acc 85.32%, f1 0.8528, precision 0.8522, recall 0.8534, auc 0.8532
epoch 4501, loss 0.3282, train acc 85.39%, f1 0.8537, precision 0.8520, recall 0.8553, auc 0.8539
epoch 4601, loss 0.3929, train acc 85.53%, f1 0.8549, precision 0.8543, recall 0.8555, auc 0.8553
epoch 4701, loss 0.3071, train acc 85.65%, f1 0.8559, precision 0.8562, recall 0.8556, auc 0.8565
epoch 4801, loss 0.3029, train acc 85.75%, f1 0.8568, precision 0.8582, recall 0.8554, auc 0.8575
epoch 4901, loss 0.3972, train acc 85.91%, f1 0.8584, precision 0.8597, recall 0.8571, auc 0.8591
epoch 5001, loss 0.3144, train acc 85.97%, f1 0.8595, precision 0.8575, recall 0.8614, auc 0.8597
epoch 5101, loss 0.3164, train acc 86.04%, f1 0.8601, precision 0.8589, recall 0.8613, auc 0.8604
epoch 5201, loss 0.3500, train acc 86.20%, f1 0.8617, precision 0.8604, recall 0.8629, auc 0.8620
epoch 5301, loss 0.3480, train acc 86.35%, f1 0.8629, precision 0.8635, recall 0.8623, auc 0.8635
epoch 5401, loss 0.3823, train acc 86.41%, f1 0.8639, precision 0.8618, recall 0.8661, auc 0.8641
epoch 5501, loss 0.2866, train acc 86.50%, f1 0.8645, precision 0.8642, recall 0.8649, auc 0.8650
epoch 5601, loss 0.2590, train acc 86.57%, f1 0.8655, precision 0.8637, recall 0.8672, auc 0.8657
epoch 5701, loss 0.3268, train acc 86.76%, f1 0.8672, precision 0.8665, recall 0.8680, auc 0.8676
epoch 5801, loss 0.2925, train acc 86.86%, f1 0.8678, precision 0.8697, recall 0.8660, auc 0.8686
epoch 5901, loss 0.3204, train acc 86.85%, f1 0.8681, precision 0.8677, recall 0.8686, auc 0.8685
epoch 6001, loss 0.3255, train acc 86.94%, f1 0.8688, precision 0.8698, recall 0.8679, auc 0.8694
epoch 6101, loss 0.3859, train acc 87.02%, f1 0.8701, precision 0.8677, recall 0.8725, auc 0.8702
epoch 6201, loss 0.3489, train acc 87.13%, f1 0.8711, precision 0.8698, recall 0.8723, auc 0.8714
epoch 6301, loss 0.2809, train acc 87.18%, f1 0.8714, precision 0.8709, recall 0.8719, auc 0.8718
epoch 6401, loss 0.2135, train acc 87.28%, f1 0.8718, precision 0.8757, recall 0.8679, auc 0.8728
epoch 6501, loss 0.3612, train acc 87.32%, f1 0.8726, precision 0.8732, recall 0.8721, auc 0.8731
epoch 6601, loss 0.2767, train acc 87.37%, f1 0.8734, precision 0.8721, recall 0.8747, auc 0.8737
epoch 6701, loss 0.2978, train acc 87.56%, f1 0.8749, precision 0.8765, recall 0.8733, auc 0.8756
epoch 6801, loss 0.2398, train acc 87.58%, f1 0.8754, precision 0.8751, recall 0.8757, auc 0.8758
epoch 6901, loss 0.2352, train acc 87.64%, f1 0.8764, precision 0.8736, recall 0.8792, auc 0.8764
epoch 7001, loss 0.2624, train acc 87.74%, f1 0.8772, precision 0.8755, recall 0.8789, auc 0.8774
epoch 7101, loss 0.4040, train acc 87.75%, f1 0.8771, precision 0.8771, recall 0.8771, auc 0.8775
epoch 7201, loss 0.2241, train acc 87.79%, f1 0.8777, precision 0.8763, recall 0.8790, auc 0.8779
epoch 7301, loss 0.1749, train acc 87.83%, f1 0.8780, precision 0.8775, recall 0.8784, auc 0.8783
epoch 7401, loss 0.1954, train acc 87.94%, f1 0.8792, precision 0.8774, recall 0.8810, auc 0.8794
epoch 7501, loss 0.2736, train acc 87.95%, f1 0.8793, precision 0.8773, recall 0.8813, auc 0.8795
epoch 7601, loss 0.2808, train acc 88.01%, f1 0.8800, precision 0.8782, recall 0.8818, auc 0.8801
epoch 7701, loss 0.4007, train acc 88.08%, f1 0.8803, precision 0.8805, recall 0.8802, auc 0.8808
epoch 7801, loss 0.3553, train acc 88.13%, f1 0.8810, precision 0.8802, recall 0.8818, auc 0.8813
epoch 7901, loss 0.2723, train acc 88.16%, f1 0.8815, precision 0.8791, recall 0.8839, auc 0.8816
epoch 8001, loss 0.2277, train acc 88.19%, f1 0.8815, precision 0.8813, recall 0.8818, auc 0.8819
epoch 8101, loss 0.1943, train acc 88.24%, f1 0.8818, precision 0.8828, recall 0.8808, auc 0.8824
epoch 8201, loss 0.2813, train acc 88.28%, f1 0.8825, precision 0.8813, recall 0.8837, auc 0.8828
epoch 8301, loss 0.3140, train acc 88.35%, f1 0.8830, precision 0.8837, recall 0.8823, auc 0.8835
epoch 8401, loss 0.2449, train acc 88.35%, f1 0.8834, precision 0.8816, recall 0.8851, auc 0.8836
epoch 8501, loss 0.2533, train acc 88.41%, f1 0.8839, precision 0.8824, recall 0.8854, auc 0.8841
epoch 8601, loss 0.2141, train acc 88.45%, f1 0.8841, precision 0.8844, recall 0.8838, auc 0.8845
epoch 8701, loss 0.1995, train acc 88.48%, f1 0.8843, precision 0.8848, recall 0.8838, auc 0.8848
epoch 8801, loss 0.2413, train acc 88.51%, f1 0.8849, precision 0.8831, recall 0.8867, auc 0.8851
epoch 8901, loss 0.3257, train acc 88.60%, f1 0.8858, precision 0.8844, recall 0.8871, auc 0.8860
epoch 9001, loss 0.2657, train acc 88.63%, f1 0.8859, precision 0.8855, recall 0.8863, auc 0.8863
epoch 9101, loss 0.2642, train acc 88.71%, f1 0.8866, precision 0.8879, recall 0.8852, auc 0.8871
epoch 9201, loss 0.2512, train acc 88.72%, f1 0.8873, precision 0.8838, recall 0.8907, auc 0.8872
epoch 9301, loss 0.1743, train acc 88.76%, f1 0.8871, precision 0.8878, recall 0.8863, auc 0.8876
epoch 9401, loss 0.2978, train acc 88.86%, f1 0.8879, precision 0.8903, recall 0.8855, auc 0.8886
epoch 9501, loss 0.2676, train acc 88.84%, f1 0.8880, precision 0.8881, recall 0.8879, auc 0.8884
epoch 9601, loss 0.2229, train acc 88.93%, f1 0.8892, precision 0.8870, recall 0.8914, auc 0.8893
epoch 9701, loss 0.2580, train acc 88.94%, f1 0.8894, precision 0.8861, recall 0.8926, auc 0.8894
epoch 9801, loss 0.1699, train acc 89.00%, f1 0.8896, precision 0.8902, recall 0.8890, auc 0.8900
epoch 9901, loss 0.2606, train acc 89.07%, f1 0.8903, precision 0.8909, recall 0.8896, auc 0.8907
epoch 10001, loss 0.2778, train acc 89.04%, f1 0.8901, precision 0.8896, recall 0.8905, auc 0.8904
epoch 10101, loss 0.2556, train acc 89.12%, f1 0.8910, precision 0.8896, recall 0.8924, auc 0.8912
epoch 10201, loss 0.3627, train acc 89.16%, f1 0.8914, precision 0.8902, recall 0.8925, auc 0.8916
epoch 10301, loss 0.2370, train acc 89.16%, f1 0.8913, precision 0.8902, recall 0.8925, auc 0.8916
epoch 10401, loss 0.2672, train acc 89.29%, f1 0.8925, precision 0.8931, recall 0.8918, auc 0.8929
epoch 10501, loss 0.2991, train acc 89.33%, f1 0.8929, precision 0.8932, recall 0.8927, auc 0.8933
epoch 10601, loss 0.2587, train acc 89.40%, f1 0.8935, precision 0.8945, recall 0.8925, auc 0.8940
epoch 10701, loss 0.2206, train acc 89.36%, f1 0.8932, precision 0.8931, recall 0.8933, auc 0.8936
epoch 10801, loss 0.2564, train acc 89.49%, f1 0.8944, precision 0.8949, recall 0.8940, auc 0.8949
epoch 10901, loss 0.2967, train acc 89.52%, f1 0.8949, precision 0.8945, recall 0.8953, auc 0.8952
epoch 11001, loss 0.1916, train acc 89.52%, f1 0.8948, precision 0.8947, recall 0.8950, auc 0.8952
epoch 11101, loss 0.2468, train acc 89.58%, f1 0.8956, precision 0.8943, recall 0.8970, auc 0.8958
epoch 11201, loss 0.2630, train acc 89.64%, f1 0.8961, precision 0.8950, recall 0.8972, auc 0.8964
epoch 11301, loss 0.2926, train acc 89.72%, f1 0.8970, precision 0.8956, recall 0.8983, auc 0.8972
epoch 11401, loss 0.1723, train acc 89.66%, f1 0.8964, precision 0.8951, recall 0.8977, auc 0.8966
epoch 11501, loss 0.2515, train acc 89.75%, f1 0.8970, precision 0.8976, recall 0.8964, auc 0.8974
epoch 11601, loss 0.2233, train acc 89.79%, f1 0.8974, precision 0.8982, recall 0.8966, auc 0.8979
epoch 11701, loss 0.2206, train acc 89.89%, f1 0.8986, precision 0.8981, recall 0.8992, auc 0.8989
epoch 11801, loss 0.2350, train acc 89.88%, f1 0.8984, precision 0.8990, recall 0.8978, auc 0.8988
epoch 11901, loss 0.1740, train acc 89.91%, f1 0.8987, precision 0.8989, recall 0.8985, auc 0.8991
epoch 12001, loss 0.3507, train acc 89.97%, f1 0.8992, precision 0.9007, recall 0.8976, auc 0.8997
epoch 12101, loss 0.3191, train acc 89.95%, f1 0.8993, precision 0.8977, recall 0.9009, auc 0.8995
epoch 12201, loss 0.2404, train acc 89.99%, f1 0.8995, precision 0.8996, recall 0.8995, auc 0.8999
epoch 12301, loss 0.2027, train acc 90.08%, f1 0.9003, precision 0.9015, recall 0.8990, auc 0.9008
epoch 12401, loss 0.2963, train acc 90.12%, f1 0.9008, precision 0.9013, recall 0.9003, auc 0.9012
epoch 12501, loss 0.2731, train acc 90.18%, f1 0.9013, precision 0.9031, recall 0.8994, auc 0.9018
epoch 12601, loss 0.2169, train acc 90.19%, f1 0.9015, precision 0.9018, recall 0.9013, auc 0.9019
epoch 12701, loss 0.2392, train acc 90.23%, f1 0.9018, precision 0.9033, recall 0.9004, auc 0.9023
epoch 12801, loss 0.2299, train acc 90.23%, f1 0.9022, precision 0.9005, recall 0.9039, auc 0.9024
epoch 12901, loss 0.1705, train acc 90.32%, f1 0.9026, precision 0.9049, recall 0.9004, auc 0.9032
epoch 13001, loss 0.1431, train acc 90.34%, f1 0.9032, precision 0.9025, recall 0.9038, auc 0.9034
epoch 13101, loss 0.1355, train acc 90.40%, f1 0.9038, precision 0.9024, recall 0.9052, auc 0.9040
epoch 13201, loss 0.3542, train acc 90.42%, f1 0.9039, precision 0.9042, recall 0.9035, auc 0.9042
epoch 13301, loss 0.2114, train acc 90.48%, f1 0.9042, precision 0.9065, recall 0.9020, auc 0.9048
epoch 13401, loss 0.2281, train acc 90.56%, f1 0.9053, precision 0.9051, recall 0.9054, auc 0.9056
epoch 13501, loss 0.2261, train acc 90.59%, f1 0.9055, precision 0.9062, recall 0.9048, auc 0.9059
epoch 13601, loss 0.2811, train acc 90.63%, f1 0.9061, precision 0.9049, recall 0.9073, auc 0.9063
epoch 13701, loss 0.2866, train acc 90.63%, f1 0.9060, precision 0.9053, recall 0.9068, auc 0.9063
epoch 13801, loss 0.1749, train acc 90.63%, f1 0.9058, precision 0.9075, recall 0.9042, auc 0.9063
epoch 13901, loss 0.1846, train acc 90.70%, f1 0.9065, precision 0.9079, recall 0.9051, auc 0.9070
epoch 14001, loss 0.1451, train acc 90.68%, f1 0.9062, precision 0.9089, recall 0.9036, auc 0.9068
epoch 14101, loss 0.2348, train acc 90.77%, f1 0.9073, precision 0.9081, recall 0.9065, auc 0.9077
epoch 14201, loss 0.2193, train acc 90.78%, f1 0.9075, precision 0.9079, recall 0.9070, auc 0.9078
epoch 14301, loss 0.2459, train acc 90.89%, f1 0.9085, precision 0.9096, recall 0.9073, auc 0.9089
epoch 14401, loss 0.1995, train acc 90.90%, f1 0.9086, precision 0.9094, recall 0.9078, auc 0.9090
epoch 14501, loss 0.1987, train acc 90.88%, f1 0.9084, precision 0.9096, recall 0.9072, auc 0.9088
epoch 14601, loss 0.2083, train acc 90.94%, f1 0.9092, precision 0.9080, recall 0.9104, auc 0.9094
epoch 14701, loss 0.2620, train acc 90.96%, f1 0.9094, precision 0.9081, recall 0.9107, auc 0.9096
epoch 14801, loss 0.1710, train acc 90.98%, f1 0.9094, precision 0.9099, recall 0.9089, auc 0.9098
epoch 14901, loss 0.3019, train acc 90.98%, f1 0.9093, precision 0.9110, recall 0.9076, auc 0.9098
epoch 15001, loss 0.2461, train acc 91.07%, f1 0.9107, precision 0.9077, recall 0.9137, auc 0.9107
epoch 15101, loss 0.2123, train acc 91.14%, f1 0.9108, precision 0.9137, recall 0.9080, auc 0.9114
epoch 15201, loss 0.2209, train acc 91.11%, f1 0.9108, precision 0.9104, recall 0.9112, auc 0.9111
epoch 15301, loss 0.2241, train acc 91.18%, f1 0.9114, precision 0.9121, recall 0.9108, auc 0.9118
epoch 15401, loss 0.2305, train acc 91.23%, f1 0.9119, precision 0.9123, recall 0.9116, auc 0.9123
epoch 15501, loss 0.1818, train acc 91.25%, f1 0.9123, precision 0.9113, recall 0.9132, auc 0.9125
epoch 15601, loss 0.1726, train acc 91.28%, f1 0.9125, precision 0.9119, recall 0.9131, auc 0.9128
epoch 15701, loss 0.1364, train acc 91.32%, f1 0.9129, precision 0.9132, recall 0.9125, auc 0.9132
epoch 15801, loss 0.1739, train acc 91.39%, f1 0.9137, precision 0.9132, recall 0.9141, auc 0.9139
epoch 15901, loss 0.2045, train acc 91.40%, f1 0.9137, precision 0.9140, recall 0.9134, auc 0.9140
epoch 16001, loss 0.1970, train acc 91.41%, f1 0.9137, precision 0.9144, recall 0.9130, auc 0.9141
epoch 16101, loss 0.2576, train acc 91.47%, f1 0.9142, precision 0.9164, recall 0.9121, auc 0.9147
epoch 16201, loss 0.2738, train acc 91.51%, f1 0.9151, precision 0.9121, recall 0.9181, auc 0.9151
epoch 16301, loss 0.2375, train acc 91.52%, f1 0.9149, precision 0.9151, recall 0.9146, auc 0.9152
epoch 16401, loss 0.2118, train acc 91.47%, f1 0.9146, precision 0.9125, recall 0.9167, auc 0.9147
epoch 16501, loss 0.2305, train acc 91.55%, f1 0.9154, precision 0.9135, recall 0.9173, auc 0.9155/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.2574, train acc 91.60%, f1 0.9156, precision 0.9171, recall 0.9142, auc 0.9160
epoch 16701, loss 0.1606, train acc 91.60%, f1 0.9159, precision 0.9140, recall 0.9178, auc 0.9161
epoch 16801, loss 0.2387, train acc 91.66%, f1 0.9162, precision 0.9169, recall 0.9155, auc 0.9166
epoch 16901, loss 0.2896, train acc 91.66%, f1 0.9163, precision 0.9170, recall 0.9156, auc 0.9166
epoch 17001, loss 0.1713, train acc 91.70%, f1 0.9166, precision 0.9168, recall 0.9165, auc 0.9170
epoch 17101, loss 0.2268, train acc 91.76%, f1 0.9173, precision 0.9167, recall 0.9180, auc 0.9176
epoch 17201, loss 0.2370, train acc 91.82%, f1 0.9177, precision 0.9201, recall 0.9153, auc 0.9182
epoch 17301, loss 0.1454, train acc 91.79%, f1 0.9177, precision 0.9165, recall 0.9189, auc 0.9179
epoch 17401, loss 0.1873, train acc 91.83%, f1 0.9181, precision 0.9165, recall 0.9197, auc 0.9183
epoch 17501, loss 0.2099, train acc 91.85%, f1 0.9184, precision 0.9169, recall 0.9198, auc 0.9185
epoch 17601, loss 0.1759, train acc 91.84%, f1 0.9183, precision 0.9162, recall 0.9204, auc 0.9184
epoch 17701, loss 0.1259, train acc 91.89%, f1 0.9184, precision 0.9212, recall 0.9156, auc 0.9189
epoch 17801, loss 0.2275, train acc 91.93%, f1 0.9192, precision 0.9170, recall 0.9214, auc 0.9193
epoch 17901, loss 0.1542, train acc 92.00%, f1 0.9198, precision 0.9186, recall 0.9210, auc 0.9200
epoch 18001, loss 0.1341, train acc 91.98%, f1 0.9198, precision 0.9173, recall 0.9222, auc 0.9198
epoch 18101, loss 0.1453, train acc 91.99%, f1 0.9197, precision 0.9184, recall 0.9210, auc 0.9199
epoch 18201, loss 0.1731, train acc 92.04%, f1 0.9203, precision 0.9184, recall 0.9222, auc 0.9204
epoch 18301, loss 0.1638, train acc 92.06%, f1 0.9205, precision 0.9179, recall 0.9232, auc 0.9206
epoch 18401, loss 0.1992, train acc 92.05%, f1 0.9203, precision 0.9200, recall 0.9206, auc 0.9205
epoch 18501, loss 0.1955, train acc 92.12%, f1 0.9210, precision 0.9196, recall 0.9223, auc 0.9212
epoch 18601, loss 0.1241, train acc 92.11%, f1 0.9209, precision 0.9197, recall 0.9221, auc 0.9211
epoch 18701, loss 0.1976, train acc 92.11%, f1 0.9210, precision 0.9184, recall 0.9236, auc 0.9211
epoch 18801, loss 0.1279, train acc 92.20%, f1 0.9219, precision 0.9199, recall 0.9238, auc 0.9220
epoch 18901, loss 0.1588, train acc 92.24%, f1 0.9224, precision 0.9187, recall 0.9262, auc 0.9224
epoch 19001, loss 0.1729, train acc 92.24%, f1 0.9222, precision 0.9220, recall 0.9223, auc 0.9224
epoch 19101, loss 0.2912, train acc 92.28%, f1 0.9225, precision 0.9222, recall 0.9228, auc 0.9228
epoch 19201, loss 0.1928, train acc 92.27%, f1 0.9229, precision 0.9183, recall 0.9275, auc 0.9228
epoch 19301, loss 0.1550, train acc 92.31%, f1 0.9230, precision 0.9209, recall 0.9251, auc 0.9231
epoch 19401, loss 0.1819, train acc 92.39%, f1 0.9239, precision 0.9202, recall 0.9276, auc 0.9239
epoch 19501, loss 0.1854, train acc 92.41%, f1 0.9240, precision 0.9221, recall 0.9258, auc 0.9241
epoch 19601, loss 0.2290, train acc 92.39%, f1 0.9238, precision 0.9208, recall 0.9269, auc 0.9239
epoch 19701, loss 0.1703, train acc 92.44%, f1 0.9243, precision 0.9223, recall 0.9263, auc 0.9244
epoch 19801, loss 0.1426, train acc 92.48%, f1 0.9248, precision 0.9220, recall 0.9276, auc 0.9249
epoch 19901, loss 0.2361, train acc 92.48%, f1 0.9247, precision 0.9229, recall 0.9264, auc 0.9248
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_5
./test_pima/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.7078301886792453

the Fscore is 0.64

the precision is 0.4948453608247423

the recall is 0.9056603773584906

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_5
----------------------



epoch 1, loss 0.6946, train acc 49.98%, f1 0.6665, precision 0.4998, recall 1.0000, auc 0.5000
epoch 101, loss 0.5649, train acc 79.09%, f1 0.7853, precision 0.8066, recall 0.7652, auc 0.7909
epoch 201, loss 0.4615, train acc 81.91%, f1 0.8193, precision 0.8183, recall 0.8202, auc 0.8191
epoch 301, loss 0.3880, train acc 83.38%, f1 0.8341, precision 0.8326, recall 0.8355, auc 0.8338
epoch 401, loss 0.3614, train acc 83.91%, f1 0.8397, precision 0.8360, recall 0.8434, auc 0.8391
epoch 501, loss 0.2843, train acc 84.09%, f1 0.8415, precision 0.8380, recall 0.8451, auc 0.8409
epoch 601, loss 0.3953, train acc 84.19%, f1 0.8426, precision 0.8385, recall 0.8467, auc 0.8419
epoch 701, loss 0.3809, train acc 84.23%, f1 0.8427, precision 0.8404, recall 0.8449, auc 0.8423
epoch 801, loss 0.3260, train acc 84.18%, f1 0.8422, precision 0.8401, recall 0.8442, auc 0.8418
epoch 901, loss 0.2776, train acc 84.29%, f1 0.8436, precision 0.8393, recall 0.8479, auc 0.8429
epoch 1001, loss 0.4719, train acc 84.22%, f1 0.8423, precision 0.8415, recall 0.8431, auc 0.8422
epoch 1101, loss 0.3064, train acc 84.22%, f1 0.8429, precision 0.8385, recall 0.8474, auc 0.8422
epoch 1201, loss 0.4347, train acc 84.22%, f1 0.8419, precision 0.8430, recall 0.8408, auc 0.8422
epoch 1301, loss 0.3282, train acc 84.20%, f1 0.8421, precision 0.8408, recall 0.8435, auc 0.8420
epoch 1401, loss 0.5737, train acc 84.25%, f1 0.8426, precision 0.8419, recall 0.8433, auc 0.8425
epoch 1501, loss 0.3642, train acc 84.17%, f1 0.8415, precision 0.8423, recall 0.8407, auc 0.8417
epoch 1601, loss 0.4874, train acc 84.21%, f1 0.8425, precision 0.8402, recall 0.8447, auc 0.8421
epoch 1701, loss 0.4703, train acc 84.19%, f1 0.8425, precision 0.8388, recall 0.8462, auc 0.8419
epoch 1801, loss 0.3595, train acc 84.25%, f1 0.8425, precision 0.8421, recall 0.8428, auc 0.8425
epoch 1901, loss 0.4053, train acc 84.28%, f1 0.8431, precision 0.8410, recall 0.8452, auc 0.8428
epoch 2001, loss 0.3456, train acc 84.22%, f1 0.8428, precision 0.8392, recall 0.8464, auc 0.8422
epoch 2101, loss 0.4281, train acc 84.18%, f1 0.8423, precision 0.8393, recall 0.8452, auc 0.8418
epoch 2201, loss 0.4433, train acc 84.26%, f1 0.8429, precision 0.8411, recall 0.8447, auc 0.8426
epoch 2301, loss 0.2374, train acc 84.28%, f1 0.8430, precision 0.8416, recall 0.8443, auc 0.8428
epoch 2401, loss 0.3011, train acc 84.31%, f1 0.8436, precision 0.8408, recall 0.8465, auc 0.8431
epoch 2501, loss 0.3151, train acc 84.32%, f1 0.8434, precision 0.8417, recall 0.8452, auc 0.8432
epoch 2601, loss 0.3245, train acc 84.33%, f1 0.8437, precision 0.8413, recall 0.8461, auc 0.8434
epoch 2701, loss 0.3402, train acc 84.35%, f1 0.8434, precision 0.8435, recall 0.8433, auc 0.8435
epoch 2801, loss 0.2558, train acc 84.42%, f1 0.8436, precision 0.8463, recall 0.8410, auc 0.8442
epoch 2901, loss 0.5164, train acc 84.42%, f1 0.8447, precision 0.8417, recall 0.8477, auc 0.8442
epoch 3001, loss 0.3602, train acc 84.40%, f1 0.8443, precision 0.8424, recall 0.8462, auc 0.8440
epoch 3101, loss 0.2119, train acc 84.41%, f1 0.8437, precision 0.8455, recall 0.8420, auc 0.8441
epoch 3201, loss 0.4625, train acc 84.48%, f1 0.8449, precision 0.8438, recall 0.8459, auc 0.8448
epoch 3301, loss 0.3096, train acc 84.54%, f1 0.8458, precision 0.8433, recall 0.8483, auc 0.8454
epoch 3401, loss 0.4149, train acc 84.60%, f1 0.8460, precision 0.8459, recall 0.8460, auc 0.8460
epoch 3501, loss 0.2974, train acc 84.66%, f1 0.8465, precision 0.8468, recall 0.8462, auc 0.8466
epoch 3601, loss 0.3885, train acc 84.82%, f1 0.8488, precision 0.8450, recall 0.8526, auc 0.8482
epoch 3701, loss 0.3389, train acc 84.68%, f1 0.8472, precision 0.8447, recall 0.8497, auc 0.8468
epoch 3801, loss 0.2924, train acc 84.81%, f1 0.8482, precision 0.8473, recall 0.8491, auc 0.8481
epoch 3901, loss 0.4080, train acc 85.02%, f1 0.8510, precision 0.8461, recall 0.8560, auc 0.8502
epoch 4001, loss 0.2675, train acc 85.08%, f1 0.8507, precision 0.8506, recall 0.8507, auc 0.8508
epoch 4101, loss 0.4465, train acc 85.15%, f1 0.8516, precision 0.8503, recall 0.8529, auc 0.8515
epoch 4201, loss 0.2718, train acc 85.23%, f1 0.8527, precision 0.8498, recall 0.8556, auc 0.8523
epoch 4301, loss 0.2914, train acc 85.29%, f1 0.8532, precision 0.8509, recall 0.8554, auc 0.8529
epoch 4401, loss 0.2963, train acc 85.40%, f1 0.8542, precision 0.8525, recall 0.8558, auc 0.8540
epoch 4501, loss 0.3969, train acc 85.46%, f1 0.8551, precision 0.8517, recall 0.8586, auc 0.8546
epoch 4601, loss 0.2674, train acc 85.57%, f1 0.8559, precision 0.8544, recall 0.8574, auc 0.8557
epoch 4701, loss 0.4567, train acc 85.69%, f1 0.8568, precision 0.8570, recall 0.8566, auc 0.8569
epoch 4801, loss 0.4127, train acc 85.82%, f1 0.8583, precision 0.8575, recall 0.8591, auc 0.8582
epoch 4901, loss 0.2678, train acc 85.87%, f1 0.8589, precision 0.8572, recall 0.8607, auc 0.8587
epoch 5001, loss 0.3717, train acc 86.02%, f1 0.8599, precision 0.8616, recall 0.8582, auc 0.8602
epoch 5101, loss 0.3064, train acc 86.15%, f1 0.8615, precision 0.8612, recall 0.8618, auc 0.8615
epoch 5201, loss 0.2544, train acc 86.27%, f1 0.8626, precision 0.8627, recall 0.8625, auc 0.8627
epoch 5301, loss 0.2759, train acc 86.33%, f1 0.8637, precision 0.8610, recall 0.8664, auc 0.8634
epoch 5401, loss 0.3317, train acc 86.43%, f1 0.8650, precision 0.8601, recall 0.8700, auc 0.8643
epoch 5501, loss 0.3435, train acc 86.60%, f1 0.8661, precision 0.8649, recall 0.8673, auc 0.8660
epoch 5601, loss 0.3341, train acc 86.65%, f1 0.8658, precision 0.8699, recall 0.8618, auc 0.8665
epoch 5701, loss 0.3112, train acc 86.78%, f1 0.8679, precision 0.8668, recall 0.8690, auc 0.8678
epoch 5801, loss 0.1807, train acc 86.85%, f1 0.8686, precision 0.8675, recall 0.8698, auc 0.8685
epoch 5901, loss 0.3336, train acc 87.00%, f1 0.8700, precision 0.8701, recall 0.8698, auc 0.8700
epoch 6001, loss 0.3765, train acc 87.10%, f1 0.8709, precision 0.8717, recall 0.8701, auc 0.8710
epoch 6101, loss 0.2577, train acc 87.09%, f1 0.8713, precision 0.8680, recall 0.8747, auc 0.8709
epoch 6201, loss 0.3245, train acc 87.20%, f1 0.8721, precision 0.8709, recall 0.8734, auc 0.8720
epoch 6301, loss 0.2452, train acc 87.27%, f1 0.8726, precision 0.8731, recall 0.8722, auc 0.8727
epoch 6401, loss 0.3263, train acc 87.41%, f1 0.8741, precision 0.8736, recall 0.8747, auc 0.8741
epoch 6501, loss 0.3553, train acc 87.51%, f1 0.8754, precision 0.8729, recall 0.8778, auc 0.8751
epoch 6601, loss 0.3709, train acc 87.61%, f1 0.8763, precision 0.8744, recall 0.8783, auc 0.8761
epoch 6701, loss 0.3146, train acc 87.67%, f1 0.8765, precision 0.8774, recall 0.8757, auc 0.8767
epoch 6801, loss 0.3266, train acc 87.71%, f1 0.8770, precision 0.8771, recall 0.8769, auc 0.8771
epoch 6901, loss 0.3526, train acc 87.82%, f1 0.8784, precision 0.8767, recall 0.8802, auc 0.8782
epoch 7001, loss 0.3827, train acc 87.92%, f1 0.8797, precision 0.8756, recall 0.8838, auc 0.8792
epoch 7101, loss 0.3172, train acc 87.95%, f1 0.8796, precision 0.8785, recall 0.8806, auc 0.8795
epoch 7201, loss 0.3785, train acc 87.98%, f1 0.8801, precision 0.8779, recall 0.8823, auc 0.8798
epoch 7301, loss 0.2585, train acc 88.00%, f1 0.8803, precision 0.8783, recall 0.8822, auc 0.8800
epoch 7401, loss 0.2912, train acc 88.11%, f1 0.8811, precision 0.8805, recall 0.8816, auc 0.8811
epoch 7501, loss 0.2900, train acc 88.16%, f1 0.8813, precision 0.8830, recall 0.8796, auc 0.8816
epoch 7601, loss 0.2070, train acc 88.19%, f1 0.8822, precision 0.8800, recall 0.8844, auc 0.8819
epoch 7701, loss 0.2494, train acc 88.25%, f1 0.8826, precision 0.8816, recall 0.8836, auc 0.8825
epoch 7801, loss 0.2654, train acc 88.31%, f1 0.8827, precision 0.8850, recall 0.8805, auc 0.8831
epoch 7901, loss 0.3015, train acc 88.41%, f1 0.8840, precision 0.8844, recall 0.8835, auc 0.8841
epoch 8001, loss 0.3010, train acc 88.37%, f1 0.8834, precision 0.8859, recall 0.8808, auc 0.8837
epoch 8101, loss 0.3144, train acc 88.45%, f1 0.8845, precision 0.8844, recall 0.8845, auc 0.8845
epoch 8201, loss 0.3499, train acc 88.49%, f1 0.8850, precision 0.8835, recall 0.8866, auc 0.8849/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.3694, train acc 88.57%, f1 0.8858, precision 0.8848, recall 0.8867, auc 0.8857
epoch 8401, loss 0.3751, train acc 88.63%, f1 0.8866, precision 0.8840, recall 0.8891, auc 0.8863
epoch 8501, loss 0.3082, train acc 88.62%, f1 0.8863, precision 0.8854, recall 0.8872, auc 0.8862
epoch 8601, loss 0.3296, train acc 88.69%, f1 0.8866, precision 0.8884, recall 0.8848, auc 0.8869
epoch 8701, loss 0.1958, train acc 88.70%, f1 0.8870, precision 0.8864, recall 0.8876, auc 0.8870
epoch 8801, loss 0.3953, train acc 88.78%, f1 0.8873, precision 0.8903, recall 0.8844, auc 0.8878
epoch 8901, loss 0.2339, train acc 88.86%, f1 0.8887, precision 0.8874, recall 0.8901, auc 0.8886
epoch 9001, loss 0.2403, train acc 88.94%, f1 0.8894, precision 0.8896, recall 0.8892, auc 0.8894
epoch 9101, loss 0.1907, train acc 88.99%, f1 0.8900, precision 0.8884, recall 0.8917, auc 0.8899
epoch 9201, loss 0.2250, train acc 89.00%, f1 0.8899, precision 0.8899, recall 0.8899, auc 0.8900
epoch 9301, loss 0.3148, train acc 89.05%, f1 0.8906, precision 0.8895, recall 0.8917, auc 0.8905
epoch 9401, loss 0.1410, train acc 89.03%, f1 0.8901, precision 0.8913, recall 0.8890, auc 0.8903
epoch 9501, loss 0.2931, train acc 89.13%, f1 0.8916, precision 0.8888, recall 0.8944, auc 0.8913
epoch 9601, loss 0.3002, train acc 89.21%, f1 0.8924, precision 0.8890, recall 0.8958, auc 0.8921
epoch 9701, loss 0.2882, train acc 89.21%, f1 0.8921, precision 0.8921, recall 0.8921, auc 0.8921
epoch 9801, loss 0.3704, train acc 89.21%, f1 0.8924, precision 0.8894, recall 0.8955, auc 0.8921
epoch 9901, loss 0.2650, train acc 89.32%, f1 0.8932, precision 0.8930, recall 0.8933, auc 0.8932
epoch 10001, loss 0.2450, train acc 89.38%, f1 0.8937, precision 0.8944, recall 0.8929, auc 0.8938
epoch 10101, loss 0.1866, train acc 89.43%, f1 0.8942, precision 0.8950, recall 0.8934, auc 0.8943
epoch 10201, loss 0.2907, train acc 89.44%, f1 0.8943, precision 0.8952, recall 0.8934, auc 0.8944
epoch 10301, loss 0.2346, train acc 89.52%, f1 0.8952, precision 0.8949, recall 0.8955, auc 0.8952
epoch 10401, loss 0.1478, train acc 89.53%, f1 0.8954, precision 0.8939, recall 0.8970, auc 0.8953
epoch 10501, loss 0.3108, train acc 89.57%, f1 0.8956, precision 0.8961, recall 0.8951, auc 0.8957
epoch 10601, loss 0.3186, train acc 89.72%, f1 0.8972, precision 0.8972, recall 0.8972, auc 0.8972
epoch 10701, loss 0.2624, train acc 89.68%, f1 0.8969, precision 0.8951, recall 0.8988, auc 0.8968
epoch 10801, loss 0.3440, train acc 89.78%, f1 0.8972, precision 0.9014, recall 0.8931, auc 0.8978
epoch 10901, loss 0.3004, train acc 89.84%, f1 0.8985, precision 0.8975, recall 0.8995, auc 0.8984
epoch 11001, loss 0.2063, train acc 89.87%, f1 0.8986, precision 0.8987, recall 0.8985, auc 0.8987
epoch 11101, loss 0.2817, train acc 89.90%, f1 0.8991, precision 0.8978, recall 0.9004, auc 0.8990
epoch 11201, loss 0.2357, train acc 89.99%, f1 0.8998, precision 0.9000, recall 0.8996, auc 0.8999
epoch 11301, loss 0.2268, train acc 90.02%, f1 0.9001, precision 0.9008, recall 0.8994, auc 0.9002
epoch 11401, loss 0.2628, train acc 90.07%, f1 0.9008, precision 0.8994, recall 0.9022, auc 0.9007
epoch 11501, loss 0.2442, train acc 90.08%, f1 0.9007, precision 0.9015, recall 0.8999, auc 0.9008
epoch 11601, loss 0.2221, train acc 90.18%, f1 0.9018, precision 0.9015, recall 0.9022, auc 0.9018
epoch 11701, loss 0.2540, train acc 90.24%, f1 0.9024, precision 0.9018, recall 0.9031, auc 0.9024
epoch 11801, loss 0.2695, train acc 90.27%, f1 0.9025, precision 0.9033, recall 0.9018, auc 0.9027
epoch 11901, loss 0.2318, train acc 90.33%, f1 0.9032, precision 0.9034, recall 0.9030, auc 0.9033
epoch 12001, loss 0.2217, train acc 90.28%, f1 0.9024, precision 0.9053, recall 0.8995, auc 0.9028
epoch 12101, loss 0.2810, train acc 90.35%, f1 0.9035, precision 0.9028, recall 0.9043, auc 0.9035
epoch 12201, loss 0.1200, train acc 90.42%, f1 0.9041, precision 0.9039, recall 0.9044, auc 0.9042
epoch 12301, loss 0.2149, train acc 90.42%, f1 0.9042, precision 0.9045, recall 0.9038, auc 0.9042
epoch 12401, loss 0.2245, train acc 90.47%, f1 0.9046, precision 0.9055, recall 0.9036, auc 0.9047
epoch 12501, loss 0.2760, train acc 90.57%, f1 0.9055, precision 0.9065, recall 0.9045, auc 0.9057
epoch 12601, loss 0.1707, train acc 90.61%, f1 0.9060, precision 0.9070, recall 0.9050, auc 0.9061
epoch 12701, loss 0.2489, train acc 90.65%, f1 0.9068, precision 0.9038, recall 0.9098, auc 0.9065
epoch 12801, loss 0.2813, train acc 90.74%, f1 0.9074, precision 0.9070, recall 0.9077, auc 0.9074
epoch 12901, loss 0.2774, train acc 90.72%, f1 0.9071, precision 0.9079, recall 0.9064, auc 0.9072
epoch 13001, loss 0.2754, train acc 90.77%, f1 0.9079, precision 0.9055, recall 0.9102, auc 0.9077
epoch 13101, loss 0.2713, train acc 90.77%, f1 0.9077, precision 0.9074, recall 0.9080, auc 0.9077
epoch 13201, loss 0.2245, train acc 90.82%, f1 0.9083, precision 0.9074, recall 0.9092, auc 0.9082
epoch 13301, loss 0.1443, train acc 90.94%, f1 0.9093, precision 0.9097, recall 0.9089, auc 0.9094
epoch 13401, loss 0.1985, train acc 90.90%, f1 0.9087, precision 0.9109, recall 0.9065, auc 0.9090
epoch 13501, loss 0.2201, train acc 90.97%, f1 0.9095, precision 0.9109, recall 0.9081, auc 0.9097
epoch 13601, loss 0.2174, train acc 90.94%, f1 0.9094, precision 0.9097, recall 0.9090, auc 0.9094
epoch 13701, loss 0.2263, train acc 91.10%, f1 0.9107, precision 0.9130, recall 0.9084, auc 0.9110
epoch 13801, loss 0.2815, train acc 91.10%, f1 0.9107, precision 0.9132, recall 0.9083, auc 0.9110
epoch 13901, loss 0.2016, train acc 91.15%, f1 0.9116, precision 0.9100, recall 0.9133, auc 0.9115
epoch 14001, loss 0.1887, train acc 91.13%, f1 0.9113, precision 0.9113, recall 0.9113, auc 0.9113
epoch 14101, loss 0.3048, train acc 91.18%, f1 0.9118, precision 0.9119, recall 0.9117, auc 0.9118
epoch 14201, loss 0.2302, train acc 91.20%, f1 0.9120, precision 0.9114, recall 0.9127, auc 0.9120
epoch 14301, loss 0.2017, train acc 91.31%, f1 0.9132, precision 0.9118, recall 0.9146, auc 0.9131
epoch 14401, loss 0.2392, train acc 91.29%, f1 0.9127, precision 0.9145, recall 0.9110, auc 0.9129
epoch 14501, loss 0.2076, train acc 91.36%, f1 0.9136, precision 0.9133, recall 0.9139, auc 0.9136
epoch 14601, loss 0.2436, train acc 91.34%, f1 0.9131, precision 0.9158, recall 0.9104, auc 0.9134
epoch 14701, loss 0.2899, train acc 91.43%, f1 0.9145, precision 0.9119, recall 0.9171, auc 0.9143
epoch 14801, loss 0.2082, train acc 91.43%, f1 0.9145, precision 0.9120, recall 0.9170, auc 0.9143
epoch 14901, loss 0.1905, train acc 91.48%, f1 0.9147, precision 0.9149, recall 0.9145, auc 0.9148
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_5
./test_pima/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6933962264150944

the Fscore is 0.6266666666666666

the precision is 0.4845360824742268

the recall is 0.8867924528301887

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_5
----------------------



epoch 1, loss 0.6932, train acc 50.20%, f1 0.6685, precision 0.5020, recall 1.0000, auc 0.5000
epoch 101, loss 0.5554, train acc 78.85%, f1 0.7897, precision 0.7883, recall 0.7912, auc 0.7885
epoch 201, loss 0.5136, train acc 81.98%, f1 0.8201, precision 0.8220, recall 0.8181, auc 0.8198
epoch 301, loss 0.4027, train acc 83.36%, f1 0.8349, precision 0.8317, recall 0.8381, auc 0.8336
epoch 401, loss 0.2801, train acc 83.97%, f1 0.8405, precision 0.8401, recall 0.8408, auc 0.8397
epoch 501, loss 0.3822, train acc 84.08%, f1 0.8411, precision 0.8426, recall 0.8396, auc 0.8408
epoch 601, loss 0.2377, train acc 84.18%, f1 0.8425, precision 0.8420, recall 0.8431, auc 0.8418
epoch 701, loss 0.4095, train acc 84.26%, f1 0.8432, precision 0.8436, recall 0.8428, auc 0.8426
epoch 801, loss 0.3483, train acc 84.22%, f1 0.8429, precision 0.8425, recall 0.8432, auc 0.8421
epoch 901, loss 0.4663, train acc 84.07%, f1 0.8410, precision 0.8426, recall 0.8395, auc 0.8407
epoch 1001, loss 0.4732, train acc 84.20%, f1 0.8421, precision 0.8453, recall 0.8388, auc 0.8420
epoch 1101, loss 0.4680, train acc 84.25%, f1 0.8436, precision 0.8414, recall 0.8458, auc 0.8425
epoch 1201, loss 0.3994, train acc 84.20%, f1 0.8423, precision 0.8443, recall 0.8403, auc 0.8420
epoch 1301, loss 0.2804, train acc 84.24%, f1 0.8430, precision 0.8432, recall 0.8428, auc 0.8424
epoch 1401, loss 0.4097, train acc 84.23%, f1 0.8430, precision 0.8428, recall 0.8432, auc 0.8423
epoch 1501, loss 0.2752, train acc 84.25%, f1 0.8434, precision 0.8420, recall 0.8449, auc 0.8425
epoch 1601, loss 0.5207, train acc 84.20%, f1 0.8426, precision 0.8428, recall 0.8424, auc 0.8420
epoch 1701, loss 0.4028, train acc 84.22%, f1 0.8428, precision 0.8434, recall 0.8422, auc 0.8422
epoch 1801, loss 0.5279, train acc 84.25%, f1 0.8432, precision 0.8431, recall 0.8433, auc 0.8425
epoch 1901, loss 0.4033, train acc 84.29%, f1 0.8436, precision 0.8430, recall 0.8443, auc 0.8429
epoch 2001, loss 0.2459, train acc 84.18%, f1 0.8424, precision 0.8430, recall 0.8417, auc 0.8418
epoch 2101, loss 0.2741, train acc 84.28%, f1 0.8431, precision 0.8445, recall 0.8418, auc 0.8428
epoch 2201, loss 0.3475, train acc 84.25%, f1 0.8428, precision 0.8448, recall 0.8407, auc 0.8425
epoch 2301, loss 0.2914, train acc 84.26%, f1 0.8434, precision 0.8428, recall 0.8440, auc 0.8426
epoch 2401, loss 0.3845, train acc 84.17%, f1 0.8423, precision 0.8425, recall 0.8421, auc 0.8417
epoch 2501, loss 0.4134, train acc 84.24%, f1 0.8433, precision 0.8419, recall 0.8447, auc 0.8424
epoch 2601, loss 0.3249, train acc 84.25%, f1 0.8430, precision 0.8438, recall 0.8423, auc 0.8425
epoch 2701, loss 0.3362, train acc 84.30%, f1 0.8440, precision 0.8416, recall 0.8465, auc 0.8430
epoch 2801, loss 0.2255, train acc 84.31%, f1 0.8434, precision 0.8452, recall 0.8416, auc 0.8431
epoch 2901, loss 0.3660, train acc 84.30%, f1 0.8437, precision 0.8433, recall 0.8441, auc 0.8430
epoch 3001, loss 0.3504, train acc 84.33%, f1 0.8439, precision 0.8442, recall 0.8435, auc 0.8433
epoch 3101, loss 0.3917, train acc 84.33%, f1 0.8444, precision 0.8420, recall 0.8468, auc 0.8433
epoch 3201, loss 0.3437, train acc 84.36%, f1 0.8443, precision 0.8438, recall 0.8448, auc 0.8436
epoch 3301, loss 0.3908, train acc 84.39%, f1 0.8436, precision 0.8487, recall 0.8386, auc 0.8439
epoch 3401, loss 0.5122, train acc 84.46%, f1 0.8455, precision 0.8439, recall 0.8471, auc 0.8446
epoch 3501, loss 0.3252, train acc 84.48%, f1 0.8450, precision 0.8472, recall 0.8427, auc 0.8448
epoch 3601, loss 0.4517, train acc 84.53%, f1 0.8457, precision 0.8467, recall 0.8448, auc 0.8453
epoch 3701, loss 0.2823, train acc 84.60%, f1 0.8457, precision 0.8509, recall 0.8405, auc 0.8460
epoch 3801, loss 0.3336, train acc 84.68%, f1 0.8473, precision 0.8480, recall 0.8467, auc 0.8468
epoch 3901, loss 0.3073, train acc 84.71%, f1 0.8483, precision 0.8450, recall 0.8516, auc 0.8471
epoch 4001, loss 0.3958, train acc 84.83%, f1 0.8489, precision 0.8490, recall 0.8489, auc 0.8483
epoch 4101, loss 0.3706, train acc 84.93%, f1 0.8490, precision 0.8540, recall 0.8441, auc 0.8493
epoch 4201, loss 0.3563, train acc 84.93%, f1 0.8503, precision 0.8483, recall 0.8523, auc 0.8493
epoch 4301, loss 0.2821, train acc 85.11%, f1 0.8520, precision 0.8505, recall 0.8535, auc 0.8511
epoch 4401, loss 0.2927, train acc 85.21%, f1 0.8526, precision 0.8530, recall 0.8523, auc 0.8521
epoch 4501, loss 0.3736, train acc 85.31%, f1 0.8539, precision 0.8526, recall 0.8553, auc 0.8531
epoch 4601, loss 0.3517, train acc 85.37%, f1 0.8541, precision 0.8553, recall 0.8529, auc 0.8537
epoch 4701, loss 0.2628, train acc 85.48%, f1 0.8551, precision 0.8565, recall 0.8538, auc 0.8548
epoch 4801, loss 0.2350, train acc 85.56%, f1 0.8563, precision 0.8557, recall 0.8569, auc 0.8556
epoch 4901, loss 0.3144, train acc 85.68%, f1 0.8568, precision 0.8602, recall 0.8535, auc 0.8568
epoch 5001, loss 0.3304, train acc 85.83%, f1 0.8587, precision 0.8595, recall 0.8579, auc 0.8583
epoch 5101, loss 0.3282, train acc 85.97%, f1 0.8598, precision 0.8625, recall 0.8571, auc 0.8597
epoch 5201, loss 0.2505, train acc 86.08%, f1 0.8609, precision 0.8638, recall 0.8580, auc 0.8608
epoch 5301, loss 0.2873, train acc 86.17%, f1 0.8617, precision 0.8651, recall 0.8583, auc 0.8617
epoch 5401, loss 0.3473, train acc 86.30%, f1 0.8632, precision 0.8654, recall 0.8611, auc 0.8630
epoch 5501, loss 0.3020, train acc 86.35%, f1 0.8636, precision 0.8664, recall 0.8608, auc 0.8635
epoch 5601, loss 0.4165, train acc 86.46%, f1 0.8651, precision 0.8653, recall 0.8650, auc 0.8646
epoch 5701, loss 0.3229, train acc 86.57%, f1 0.8653, precision 0.8715, recall 0.8592, auc 0.8657
epoch 5801, loss 0.3673, train acc 86.72%, f1 0.8675, precision 0.8694, recall 0.8656, auc 0.8672
epoch 5901, loss 0.2945, train acc 86.78%, f1 0.8680, precision 0.8698, recall 0.8663, auc 0.8678
epoch 6001, loss 0.2400, train acc 86.82%, f1 0.8683, precision 0.8712, recall 0.8654, auc 0.8682
epoch 6101, loss 0.3429, train acc 86.89%, f1 0.8696, precision 0.8688, recall 0.8703, auc 0.8689
epoch 6201, loss 0.2573, train acc 87.05%, f1 0.8706, precision 0.8735, recall 0.8677, auc 0.8705
epoch 6301, loss 0.2929, train acc 87.10%, f1 0.8714, precision 0.8718, recall 0.8711, auc 0.8710
epoch 6401, loss 0.2527, train acc 87.15%, f1 0.8721, precision 0.8718, recall 0.8724, auc 0.8715
epoch 6501, loss 0.2054, train acc 87.28%, f1 0.8729, precision 0.8762, recall 0.8696, auc 0.8728
epoch 6601, loss 0.2914, train acc 87.32%, f1 0.8740, precision 0.8721, recall 0.8759, auc 0.8732
epoch 6701, loss 0.1955, train acc 87.36%, f1 0.8743, precision 0.8727, recall 0.8759, auc 0.8735
epoch 6801, loss 0.3199, train acc 87.39%, f1 0.8739, precision 0.8774, recall 0.8705, auc 0.8739
epoch 6901, loss 0.1900, train acc 87.52%, f1 0.8749, precision 0.8805, recall 0.8693, auc 0.8752
epoch 7001, loss 0.3309, train acc 87.57%, f1 0.8761, precision 0.8769, recall 0.8754, auc 0.8757
epoch 7101, loss 0.2289, train acc 87.63%, f1 0.8768, precision 0.8772, recall 0.8764, auc 0.8763
epoch 7201, loss 0.2847, train acc 87.71%, f1 0.8772, precision 0.8800, recall 0.8746, auc 0.8771
epoch 7301, loss 0.2295, train acc 87.75%, f1 0.8777, precision 0.8793, recall 0.8762, auc 0.8775
epoch 7401, loss 0.2786, train acc 87.80%, f1 0.8785, precision 0.8780, recall 0.8790, auc 0.8779
epoch 7501, loss 0.3148, train acc 87.81%, f1 0.8789, precision 0.8771, recall 0.8806, auc 0.8781
epoch 7601, loss 0.2718, train acc 87.91%, f1 0.8797, precision 0.8792, recall 0.8801, auc 0.8791
epoch 7701, loss 0.3322, train acc 87.89%, f1 0.8794, precision 0.8792, recall 0.8797, auc 0.8789
epoch 7801, loss 0.4020, train acc 87.97%, f1 0.8801, precision 0.8808, recall 0.8794, auc 0.8797
epoch 7901, loss 0.3120, train acc 87.96%, f1 0.8800, precision 0.8812, recall 0.8787, auc 0.8796
epoch 8001, loss 0.3062, train acc 88.06%, f1 0.8809, precision 0.8824, recall 0.8793, auc 0.8806
epoch 8101, loss 0.2321, train acc 88.11%, f1 0.8815, precision 0.8823, recall 0.8806, auc 0.8811
epoch 8201, loss 0.3622, train acc 88.17%, f1 0.8821, precision 0.8828, recall 0.8813, auc 0.8817/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.2900, train acc 88.17%, f1 0.8821, precision 0.8824, recall 0.8819, auc 0.8817
epoch 8401, loss 0.2476, train acc 88.27%, f1 0.8831, precision 0.8833, recall 0.8829, auc 0.8827
epoch 8501, loss 0.2558, train acc 88.28%, f1 0.8832, precision 0.8839, recall 0.8824, auc 0.8828
epoch 8601, loss 0.3009, train acc 88.32%, f1 0.8834, precision 0.8852, recall 0.8816, auc 0.8832
epoch 8701, loss 0.2668, train acc 88.35%, f1 0.8836, precision 0.8865, recall 0.8808, auc 0.8835
epoch 8801, loss 0.3539, train acc 88.41%, f1 0.8847, precision 0.8836, recall 0.8858, auc 0.8841
epoch 8901, loss 0.2612, train acc 88.42%, f1 0.8846, precision 0.8850, recall 0.8843, auc 0.8842
epoch 9001, loss 0.2483, train acc 88.47%, f1 0.8852, precision 0.8849, recall 0.8856, auc 0.8847
epoch 9101, loss 0.2038, train acc 88.49%, f1 0.8854, precision 0.8853, recall 0.8855, auc 0.8849
epoch 9201, loss 0.3600, train acc 88.56%, f1 0.8860, precision 0.8863, recall 0.8857, auc 0.8856
epoch 9301, loss 0.2930, train acc 88.61%, f1 0.8866, precision 0.8861, recall 0.8872, auc 0.8861
epoch 9401, loss 0.2592, train acc 88.66%, f1 0.8872, precision 0.8858, recall 0.8887, auc 0.8866
epoch 9501, loss 0.1919, train acc 88.72%, f1 0.8877, precision 0.8873, recall 0.8881, auc 0.8872
epoch 9601, loss 0.2289, train acc 88.71%, f1 0.8874, precision 0.8882, recall 0.8867, auc 0.8871
epoch 9701, loss 0.2141, train acc 88.75%, f1 0.8876, precision 0.8910, recall 0.8841, auc 0.8876
epoch 9801, loss 0.3206, train acc 88.86%, f1 0.8893, precision 0.8872, recall 0.8914, auc 0.8886
epoch 9901, loss 0.3048, train acc 88.88%, f1 0.8893, precision 0.8892, recall 0.8893, auc 0.8888
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_5
./test_pima/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6861320754716982

the Fscore is 0.6257668711656441

the precision is 0.4636363636363636

the recall is 0.9622641509433962

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_5
----------------------



epoch 1, loss 0.6931, train acc 50.71%, f1 0.0344, precision 0.8638, recall 0.0175, auc 0.5074
epoch 101, loss 0.5856, train acc 78.90%, f1 0.7896, precision 0.7880, recall 0.7912, auc 0.7890
epoch 201, loss 0.3905, train acc 81.97%, f1 0.8206, precision 0.8167, recall 0.8246, auc 0.8196
epoch 301, loss 0.5451, train acc 83.47%, f1 0.8348, precision 0.8347, recall 0.8349, auc 0.8347
epoch 401, loss 0.4819, train acc 84.00%, f1 0.8402, precision 0.8395, recall 0.8409, auc 0.8400
epoch 501, loss 0.4620, train acc 84.08%, f1 0.8412, precision 0.8400, recall 0.8424, auc 0.8408
epoch 601, loss 0.3621, train acc 84.16%, f1 0.8417, precision 0.8416, recall 0.8419, auc 0.8416
epoch 701, loss 0.3143, train acc 84.18%, f1 0.8420, precision 0.8415, recall 0.8424, auc 0.8418
epoch 801, loss 0.3435, train acc 84.20%, f1 0.8424, precision 0.8409, recall 0.8440, auc 0.8420
epoch 901, loss 0.3802, train acc 84.23%, f1 0.8429, precision 0.8400, recall 0.8458, auc 0.8423
epoch 1001, loss 0.4474, train acc 84.21%, f1 0.8420, precision 0.8430, recall 0.8411, auc 0.8421
epoch 1101, loss 0.3603, train acc 84.29%, f1 0.8427, precision 0.8441, recall 0.8414, auc 0.8429
epoch 1201, loss 0.3269, train acc 84.27%, f1 0.8429, precision 0.8422, recall 0.8436, auc 0.8427
epoch 1301, loss 0.4279, train acc 84.19%, f1 0.8421, precision 0.8415, recall 0.8428, auc 0.8419
epoch 1401, loss 0.3093, train acc 84.14%, f1 0.8426, precision 0.8367, recall 0.8486, auc 0.8414
epoch 1501, loss 0.3013, train acc 84.28%, f1 0.8434, precision 0.8407, recall 0.8460, auc 0.8428
epoch 1601, loss 0.3766, train acc 84.25%, f1 0.8428, precision 0.8417, recall 0.8438, auc 0.8425
epoch 1701, loss 0.3026, train acc 84.28%, f1 0.8438, precision 0.8393, recall 0.8483, auc 0.8428
epoch 1801, loss 0.2771, train acc 84.19%, f1 0.8421, precision 0.8415, recall 0.8427, auc 0.8419
epoch 1901, loss 0.3675, train acc 84.27%, f1 0.8427, precision 0.8434, recall 0.8420, auc 0.8427
epoch 2001, loss 0.2985, train acc 84.25%, f1 0.8433, precision 0.8398, recall 0.8468, auc 0.8425
epoch 2101, loss 0.4125, train acc 84.24%, f1 0.8427, precision 0.8413, recall 0.8441, auc 0.8424
epoch 2201, loss 0.2510, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8422, auc 0.8423
epoch 2301, loss 0.3696, train acc 84.20%, f1 0.8423, precision 0.8413, recall 0.8434, auc 0.8420
epoch 2401, loss 0.2808, train acc 84.29%, f1 0.8431, precision 0.8425, recall 0.8437, auc 0.8429
epoch 2501, loss 0.3724, train acc 84.28%, f1 0.8431, precision 0.8421, recall 0.8440, auc 0.8428
epoch 2601, loss 0.3864, train acc 84.29%, f1 0.8427, precision 0.8444, recall 0.8410, auc 0.8429
epoch 2701, loss 0.2930, train acc 84.33%, f1 0.8436, precision 0.8424, recall 0.8448, auc 0.8433
epoch 2801, loss 0.2897, train acc 84.32%, f1 0.8435, precision 0.8425, recall 0.8445, auc 0.8432
epoch 2901, loss 0.2724, train acc 84.37%, f1 0.8442, precision 0.8420, recall 0.8465, auc 0.8437
epoch 3001, loss 0.3644, train acc 84.40%, f1 0.8448, precision 0.8412, recall 0.8484, auc 0.8440
epoch 3101, loss 0.2823, train acc 84.41%, f1 0.8440, precision 0.8447, recall 0.8434, auc 0.8441
epoch 3201, loss 0.3162, train acc 84.45%, f1 0.8449, precision 0.8434, recall 0.8463, auc 0.8445
epoch 3301, loss 0.4200, train acc 84.51%, f1 0.8450, precision 0.8459, recall 0.8441, auc 0.8451
epoch 3401, loss 0.3954, train acc 84.49%, f1 0.8452, precision 0.8444, recall 0.8459, auc 0.8449
epoch 3501, loss 0.3927, train acc 84.67%, f1 0.8468, precision 0.8466, recall 0.8469, auc 0.8467
epoch 3601, loss 0.3804, train acc 84.67%, f1 0.8472, precision 0.8452, recall 0.8491, auc 0.8467
epoch 3701, loss 0.3234, train acc 84.81%, f1 0.8483, precision 0.8481, recall 0.8485, auc 0.8481
epoch 3801, loss 0.2921, train acc 84.89%, f1 0.8492, precision 0.8476, recall 0.8509, auc 0.8489
epoch 3901, loss 0.4854, train acc 84.89%, f1 0.8489, precision 0.8495, recall 0.8483, auc 0.8489
epoch 4001, loss 0.3256, train acc 85.13%, f1 0.8516, precision 0.8507, recall 0.8524, auc 0.8513
epoch 4101, loss 0.3528, train acc 85.18%, f1 0.8517, precision 0.8529, recall 0.8505, auc 0.8518
epoch 4201, loss 0.4076, train acc 85.20%, f1 0.8518, precision 0.8532, recall 0.8504, auc 0.8520
epoch 4301, loss 0.3300, train acc 85.35%, f1 0.8542, precision 0.8510, recall 0.8573, auc 0.8535
epoch 4401, loss 0.2992, train acc 85.43%, f1 0.8547, precision 0.8532, recall 0.8561, auc 0.8543
epoch 4501, loss 0.3306, train acc 85.57%, f1 0.8560, precision 0.8547, recall 0.8573, auc 0.8557
epoch 4601, loss 0.2694, train acc 85.74%, f1 0.8574, precision 0.8581, recall 0.8567, auc 0.8574
epoch 4701, loss 0.3630, train acc 85.84%, f1 0.8584, precision 0.8593, recall 0.8575, auc 0.8584
epoch 4801, loss 0.3104, train acc 85.98%, f1 0.8597, precision 0.8605, recall 0.8590, auc 0.8598
epoch 4901, loss 0.4216, train acc 86.01%, f1 0.8603, precision 0.8592, recall 0.8614, auc 0.8601
epoch 5001, loss 0.3689, train acc 86.20%, f1 0.8621, precision 0.8621, recall 0.8621, auc 0.8620
epoch 5101, loss 0.3014, train acc 86.24%, f1 0.8626, precision 0.8622, recall 0.8630, auc 0.8624
epoch 5201, loss 0.2549, train acc 86.39%, f1 0.8643, precision 0.8625, recall 0.8660, auc 0.8639
epoch 5301, loss 0.3481, train acc 86.53%, f1 0.8655, precision 0.8651, recall 0.8658, auc 0.8653
epoch 5401, loss 0.3100, train acc 86.63%, f1 0.8663, precision 0.8668, recall 0.8658, auc 0.8663
epoch 5501, loss 0.3019, train acc 86.75%, f1 0.8680, precision 0.8653, recall 0.8707, auc 0.8675
epoch 5601, loss 0.3678, train acc 86.84%, f1 0.8684, precision 0.8692, recall 0.8676, auc 0.8684
epoch 5701, loss 0.3920, train acc 86.94%, f1 0.8695, precision 0.8693, recall 0.8697, auc 0.8694
epoch 5801, loss 0.3311, train acc 87.00%, f1 0.8697, precision 0.8720, recall 0.8674, auc 0.8700
epoch 5901, loss 0.3845, train acc 87.06%, f1 0.8708, precision 0.8699, recall 0.8716, auc 0.8706
epoch 6001, loss 0.2793, train acc 87.22%, f1 0.8722, precision 0.8729, recall 0.8714, auc 0.8722
epoch 6101, loss 0.3236, train acc 87.29%, f1 0.8729, precision 0.8730, recall 0.8729, auc 0.8729
epoch 6201, loss 0.2895, train acc 87.31%, f1 0.8735, precision 0.8710, recall 0.8760, auc 0.8731
epoch 6301, loss 0.2653, train acc 87.41%, f1 0.8745, precision 0.8721, recall 0.8769, auc 0.8741
epoch 6401, loss 0.2188, train acc 87.38%, f1 0.8738, precision 0.8740, recall 0.8736, auc 0.8738
epoch 6501, loss 0.2109, train acc 87.57%, f1 0.8759, precision 0.8752, recall 0.8767, auc 0.8757
epoch 6601, loss 0.3952, train acc 87.60%, f1 0.8763, precision 0.8747, recall 0.8780, auc 0.8760
epoch 6701, loss 0.3602, train acc 87.67%, f1 0.8769, precision 0.8758, recall 0.8779, auc 0.8767
epoch 6801, loss 0.2620, train acc 87.73%, f1 0.8779, precision 0.8746, recall 0.8812, auc 0.8773
epoch 6901, loss 0.3374, train acc 87.76%, f1 0.8779, precision 0.8764, recall 0.8793, auc 0.8776
epoch 7001, loss 0.2035, train acc 87.86%, f1 0.8789, precision 0.8773, recall 0.8806, auc 0.8786
epoch 7101, loss 0.1957, train acc 87.84%, f1 0.8785, precision 0.8779, recall 0.8792, auc 0.8784
epoch 7201, loss 0.3552, train acc 87.93%, f1 0.8796, precision 0.8778, recall 0.8814, auc 0.8793
epoch 7301, loss 0.2526, train acc 88.00%, f1 0.8802, precision 0.8794, recall 0.8810, auc 0.8800
epoch 7401, loss 0.4104, train acc 88.06%, f1 0.8805, precision 0.8816, recall 0.8794, auc 0.8806
epoch 7501, loss 0.3748, train acc 88.14%, f1 0.8818, precision 0.8792, recall 0.8845, auc 0.8814
epoch 7601, loss 0.3725, train acc 88.20%, f1 0.8819, precision 0.8833, recall 0.8804, auc 0.8820
epoch 7701, loss 0.2800, train acc 88.23%, f1 0.8822, precision 0.8835, recall 0.8810, auc 0.8823
epoch 7801, loss 0.2649, train acc 88.29%, f1 0.8829, precision 0.8833, recall 0.8825, auc 0.8829
epoch 7901, loss 0.2609, train acc 88.36%, f1 0.8837, precision 0.8829, recall 0.8846, auc 0.8836
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_5
./test_pima/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.7016981132075472

the Fscore is 0.6369426751592356

the precision is 0.4807692307692308

the recall is 0.9433962264150944

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_5
----------------------



epoch 1, loss 0.6931, train acc 49.93%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5888, train acc 79.10%, f1 0.7869, precision 0.8039, recall 0.7705, auc 0.7910
epoch 201, loss 0.5112, train acc 82.06%, f1 0.8209, precision 0.8204, recall 0.8214, auc 0.8206
epoch 301, loss 0.3337, train acc 83.63%, f1 0.8368, precision 0.8351, recall 0.8385, auc 0.8363
epoch 401, loss 0.3453, train acc 83.91%, f1 0.8397, precision 0.8377, recall 0.8416, auc 0.8391
epoch 501, loss 0.3968, train acc 84.12%, f1 0.8419, precision 0.8391, recall 0.8446, auc 0.8411
epoch 601, loss 0.3018, train acc 84.18%, f1 0.8426, precision 0.8391, recall 0.8462, auc 0.8418
epoch 701, loss 0.2830, train acc 84.17%, f1 0.8423, precision 0.8399, recall 0.8447, auc 0.8416
epoch 801, loss 0.3771, train acc 84.18%, f1 0.8425, precision 0.8397, recall 0.8454, auc 0.8418
epoch 901, loss 0.3746, train acc 84.17%, f1 0.8427, precision 0.8388, recall 0.8466, auc 0.8417
epoch 1001, loss 0.3371, train acc 84.18%, f1 0.8425, precision 0.8400, recall 0.8450, auc 0.8418
epoch 1101, loss 0.3781, train acc 84.19%, f1 0.8421, precision 0.8421, recall 0.8421, auc 0.8419
epoch 1201, loss 0.3894, train acc 84.22%, f1 0.8427, precision 0.8415, recall 0.8438, auc 0.8422
epoch 1301, loss 0.1731, train acc 84.26%, f1 0.8428, precision 0.8432, recall 0.8423, auc 0.8426
epoch 1401, loss 0.3283, train acc 84.23%, f1 0.8430, precision 0.8404, recall 0.8456, auc 0.8423
epoch 1501, loss 0.3407, train acc 84.20%, f1 0.8426, precision 0.8405, recall 0.8448, auc 0.8420
epoch 1601, loss 0.4491, train acc 84.19%, f1 0.8422, precision 0.8418, recall 0.8426, auc 0.8419
epoch 1701, loss 0.3140, train acc 84.24%, f1 0.8430, precision 0.8412, recall 0.8447, auc 0.8424
epoch 1801, loss 0.4354, train acc 84.26%, f1 0.8424, precision 0.8445, recall 0.8403, auc 0.8426
epoch 1901, loss 0.3087, train acc 84.29%, f1 0.8435, precision 0.8413, recall 0.8457, auc 0.8429
epoch 2001, loss 0.2692, train acc 84.22%, f1 0.8426, precision 0.8416, recall 0.8436, auc 0.8422
epoch 2101, loss 0.4375, train acc 84.20%, f1 0.8427, precision 0.8403, recall 0.8451, auc 0.8420
epoch 2201, loss 0.3693, train acc 84.25%, f1 0.8430, precision 0.8416, recall 0.8444, auc 0.8425
epoch 2301, loss 0.4932, train acc 84.21%, f1 0.8427, precision 0.8406, recall 0.8449, auc 0.8421
epoch 2401, loss 0.3327, train acc 84.27%, f1 0.8430, precision 0.8425, recall 0.8435, auc 0.8427
epoch 2501, loss 0.3655, train acc 84.30%, f1 0.8433, precision 0.8425, recall 0.8441, auc 0.8430
epoch 2601, loss 0.3248, train acc 84.30%, f1 0.8434, precision 0.8421, recall 0.8447, auc 0.8430
epoch 2701, loss 0.2911, train acc 84.32%, f1 0.8433, precision 0.8438, recall 0.8427, auc 0.8432
epoch 2801, loss 0.2995, train acc 84.33%, f1 0.8434, precision 0.8438, recall 0.8430, auc 0.8433
epoch 2901, loss 0.3303, train acc 84.40%, f1 0.8442, precision 0.8445, recall 0.8439, auc 0.8440
epoch 3001, loss 0.4224, train acc 84.39%, f1 0.8439, precision 0.8449, recall 0.8429, auc 0.8439
epoch 3101, loss 0.3724, train acc 84.42%, f1 0.8442, precision 0.8454, recall 0.8430, auc 0.8442
epoch 3201, loss 0.3063, train acc 84.51%, f1 0.8455, precision 0.8447, recall 0.8463, auc 0.8451
epoch 3301, loss 0.4925, train acc 84.59%, f1 0.8454, precision 0.8489, recall 0.8420, auc 0.8459
epoch 3401, loss 0.4552, train acc 84.64%, f1 0.8467, precision 0.8464, recall 0.8470, auc 0.8464
epoch 3501, loss 0.2894, train acc 84.70%, f1 0.8472, precision 0.8473, recall 0.8472, auc 0.8470
epoch 3601, loss 0.3964, train acc 84.79%, f1 0.8483, precision 0.8472, recall 0.8495, auc 0.8479
epoch 3701, loss 0.4225, train acc 84.89%, f1 0.8492, precision 0.8485, recall 0.8499, auc 0.8489
epoch 3801, loss 0.2716, train acc 84.85%, f1 0.8486, precision 0.8493, recall 0.8479, auc 0.8485
epoch 3901, loss 0.3481, train acc 85.02%, f1 0.8502, precision 0.8513, recall 0.8490, auc 0.8502
epoch 4001, loss 0.4538, train acc 85.10%, f1 0.8506, precision 0.8538, recall 0.8475, auc 0.8510
epoch 4101, loss 0.4802, train acc 85.18%, f1 0.8523, precision 0.8509, recall 0.8536, auc 0.8518
epoch 4201, loss 0.2145, train acc 85.26%, f1 0.8529, precision 0.8527, recall 0.8531, auc 0.8526
epoch 4301, loss 0.4306, train acc 85.27%, f1 0.8524, precision 0.8555, recall 0.8492, auc 0.8527
epoch 4401, loss 0.3280, train acc 85.37%, f1 0.8539, precision 0.8536, recall 0.8542, auc 0.8537
epoch 4501, loss 0.3989, train acc 85.53%, f1 0.8557, precision 0.8545, recall 0.8569, auc 0.8553
epoch 4601, loss 0.2792, train acc 85.61%, f1 0.8565, precision 0.8552, recall 0.8579, auc 0.8561
epoch 4701, loss 0.4397, train acc 85.70%, f1 0.8573, precision 0.8568, recall 0.8577, auc 0.8570
epoch 4801, loss 0.2767, train acc 85.80%, f1 0.8580, precision 0.8593, recall 0.8567, auc 0.8580
epoch 4901, loss 0.2489, train acc 85.88%, f1 0.8587, precision 0.8603, recall 0.8571, auc 0.8588
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_5
./test_pima/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6950000000000001

the Fscore is 0.6347305389221557

the precision is 0.4649122807017544

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_5
----------------------



epoch 1, loss 0.6931, train acc 49.94%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5798, train acc 78.68%, f1 0.7888, precision 0.7825, recall 0.7952, auc 0.7868
epoch 201, loss 0.3892, train acc 82.02%, f1 0.8206, precision 0.8198, recall 0.8213, auc 0.8202
epoch 301, loss 0.4305, train acc 83.45%, f1 0.8342, precision 0.8368, recall 0.8317, auc 0.8345
epoch 401, loss 0.3110, train acc 83.93%, f1 0.8396, precision 0.8389, recall 0.8402, auc 0.8393
epoch 501, loss 0.3249, train acc 84.08%, f1 0.8407, precision 0.8421, recall 0.8392, auc 0.8408
epoch 601, loss 0.4125, train acc 84.17%, f1 0.8416, precision 0.8432, recall 0.8400, auc 0.8417
epoch 701, loss 0.4195, train acc 84.17%, f1 0.8419, precision 0.8417, recall 0.8422, auc 0.8417
epoch 801, loss 0.3406, train acc 84.17%, f1 0.8414, precision 0.8438, recall 0.8390, auc 0.8417
epoch 901, loss 0.3344, train acc 84.21%, f1 0.8417, precision 0.8447, recall 0.8387, auc 0.8421
epoch 1001, loss 0.3260, train acc 84.22%, f1 0.8426, precision 0.8412, recall 0.8440, auc 0.8422
epoch 1101, loss 0.3512, train acc 84.22%, f1 0.8417, precision 0.8450, recall 0.8385, auc 0.8422
epoch 1201, loss 0.2671, train acc 84.25%, f1 0.8422, precision 0.8448, recall 0.8395, auc 0.8425
epoch 1301, loss 0.4441, train acc 84.21%, f1 0.8414, precision 0.8463, recall 0.8365, auc 0.8421
epoch 1401, loss 0.2960, train acc 84.21%, f1 0.8414, precision 0.8459, recall 0.8370, auc 0.8421
epoch 1501, loss 0.4762, train acc 84.28%, f1 0.8426, precision 0.8444, recall 0.8408, auc 0.8428
epoch 1601, loss 0.3344, train acc 84.16%, f1 0.8415, precision 0.8429, recall 0.8401, auc 0.8416
epoch 1701, loss 0.2619, train acc 84.18%, f1 0.8415, precision 0.8442, recall 0.8388, auc 0.8418
epoch 1801, loss 0.4403, train acc 84.15%, f1 0.8419, precision 0.8410, recall 0.8427, auc 0.8415
epoch 1901, loss 0.2813, train acc 84.27%, f1 0.8427, precision 0.8437, recall 0.8417, auc 0.8427
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_5
./test_pima/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.595566037735849

the Fscore is 0.5652173913043478

the precision is 0.3969465648854962

the recall is 0.9811320754716981

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_5
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5879, train acc 78.59%, f1 0.7759, precision 0.8138, recall 0.7413, auc 0.7859
epoch 201, loss 0.4644, train acc 80.88%, f1 0.8076, precision 0.8127, recall 0.8026, auc 0.8088
epoch 301, loss 0.4340, train acc 82.48%, f1 0.8249, precision 0.8244, recall 0.8253, auc 0.8248
epoch 401, loss 0.4692, train acc 83.46%, f1 0.8352, precision 0.8323, recall 0.8380, auc 0.8346
epoch 501, loss 0.4222, train acc 83.84%, f1 0.8390, precision 0.8358, recall 0.8423, auc 0.8384
epoch 601, loss 0.3541, train acc 84.07%, f1 0.8414, precision 0.8380, recall 0.8448, auc 0.8407
epoch 701, loss 0.3264, train acc 84.11%, f1 0.8418, precision 0.8384, recall 0.8452, auc 0.8411
epoch 801, loss 0.3973, train acc 84.18%, f1 0.8423, precision 0.8396, recall 0.8449, auc 0.8418
epoch 901, loss 0.3962, train acc 84.17%, f1 0.8422, precision 0.8395, recall 0.8449, auc 0.8417
epoch 1001, loss 0.4685, train acc 84.19%, f1 0.8423, precision 0.8400, recall 0.8447, auc 0.8419
epoch 1101, loss 0.3319, train acc 84.23%, f1 0.8426, precision 0.8407, recall 0.8446, auc 0.8423
epoch 1201, loss 0.3516, train acc 84.23%, f1 0.8425, precision 0.8411, recall 0.8439, auc 0.8423
epoch 1301, loss 0.2102, train acc 84.26%, f1 0.8429, precision 0.8412, recall 0.8445, auc 0.8426
epoch 1401, loss 0.3041, train acc 84.26%, f1 0.8429, precision 0.8413, recall 0.8445, auc 0.8426
epoch 1501, loss 0.4013, train acc 84.24%, f1 0.8427, precision 0.8411, recall 0.8444, auc 0.8424
epoch 1601, loss 0.3870, train acc 84.16%, f1 0.8418, precision 0.8408, recall 0.8427, auc 0.8416
epoch 1701, loss 0.2395, train acc 84.15%, f1 0.8416, precision 0.8411, recall 0.8422, auc 0.8415
epoch 1801, loss 0.3098, train acc 84.21%, f1 0.8422, precision 0.8415, recall 0.8430, auc 0.8421
epoch 1901, loss 0.4886, train acc 84.20%, f1 0.8421, precision 0.8415, recall 0.8428, auc 0.8420
epoch 2001, loss 0.3851, train acc 84.24%, f1 0.8424, precision 0.8424, recall 0.8424, auc 0.8424
epoch 2101, loss 0.3079, train acc 84.24%, f1 0.8425, precision 0.8420, recall 0.8429, auc 0.8424
epoch 2201, loss 0.5128, train acc 84.23%, f1 0.8423, precision 0.8426, recall 0.8419, auc 0.8423
epoch 2301, loss 0.4153, train acc 84.22%, f1 0.8422, precision 0.8423, recall 0.8421, auc 0.8422
epoch 2401, loss 0.3933, train acc 84.23%, f1 0.8423, precision 0.8425, recall 0.8422, auc 0.8423
epoch 2501, loss 0.3354, train acc 84.26%, f1 0.8425, precision 0.8427, recall 0.8423, auc 0.8426
epoch 2601, loss 0.3792, train acc 84.26%, f1 0.8427, precision 0.8419, recall 0.8436, auc 0.8426
epoch 2701, loss 0.3015, train acc 84.19%, f1 0.8420, precision 0.8416, recall 0.8423, auc 0.8419
epoch 2801, loss 0.5582, train acc 84.19%, f1 0.8419, precision 0.8419, recall 0.8420, auc 0.8419
epoch 2901, loss 0.3353, train acc 84.24%, f1 0.8424, precision 0.8426, recall 0.8422, auc 0.8424
epoch 3001, loss 0.3112, train acc 84.25%, f1 0.8425, precision 0.8425, recall 0.8426, auc 0.8425
epoch 3101, loss 0.2506, train acc 84.24%, f1 0.8426, precision 0.8419, recall 0.8433, auc 0.8424
epoch 3201, loss 0.3864, train acc 84.25%, f1 0.8425, precision 0.8422, recall 0.8428, auc 0.8425
epoch 3301, loss 0.3599, train acc 84.25%, f1 0.8426, precision 0.8424, recall 0.8427, auc 0.8425
epoch 3401, loss 0.3086, train acc 84.26%, f1 0.8427, precision 0.8423, recall 0.8432, auc 0.8426
epoch 3501, loss 0.3402, train acc 84.26%, f1 0.8426, precision 0.8429, recall 0.8423, auc 0.8426
epoch 3601, loss 0.3581, train acc 84.29%, f1 0.8430, precision 0.8424, recall 0.8436, auc 0.8429
epoch 3701, loss 0.2375, train acc 84.30%, f1 0.8430, precision 0.8429, recall 0.8431, auc 0.8430
epoch 3801, loss 0.2966, train acc 84.29%, f1 0.8429, precision 0.8427, recall 0.8432, auc 0.8429
epoch 3901, loss 0.3636, train acc 84.28%, f1 0.8428, precision 0.8426, recall 0.8430, auc 0.8428
epoch 4001, loss 0.2845, train acc 84.25%, f1 0.8426, precision 0.8424, recall 0.8427, auc 0.8425
epoch 4101, loss 0.3521, train acc 84.30%, f1 0.8431, precision 0.8426, recall 0.8436, auc 0.8430
epoch 4201, loss 0.2230, train acc 84.28%, f1 0.8429, precision 0.8424, recall 0.8434, auc 0.8428
epoch 4301, loss 0.2429, train acc 84.29%, f1 0.8429, precision 0.8427, recall 0.8430, auc 0.8429
epoch 4401, loss 0.4888, train acc 84.29%, f1 0.8429, precision 0.8429, recall 0.8430, auc 0.8429
epoch 4501, loss 0.4531, train acc 84.33%, f1 0.8434, precision 0.8431, recall 0.8436, auc 0.8433
epoch 4601, loss 0.3282, train acc 84.33%, f1 0.8434, precision 0.8431, recall 0.8437, auc 0.8433
epoch 4701, loss 0.3575, train acc 84.32%, f1 0.8433, precision 0.8431, recall 0.8435, auc 0.8432
epoch 4801, loss 0.4306, train acc 84.39%, f1 0.8440, precision 0.8436, recall 0.8444, auc 0.8439
epoch 4901, loss 0.3399, train acc 84.39%, f1 0.8439, precision 0.8439, recall 0.8439, auc 0.8439
epoch 5001, loss 0.3568, train acc 84.43%, f1 0.8443, precision 0.8440, recall 0.8446, auc 0.8443
epoch 5101, loss 0.2346, train acc 84.45%, f1 0.8445, precision 0.8443, recall 0.8447, auc 0.8445
epoch 5201, loss 0.3121, train acc 84.48%, f1 0.8449, precision 0.8447, recall 0.8450, auc 0.8448
epoch 5301, loss 0.3352, train acc 84.49%, f1 0.8449, precision 0.8446, recall 0.8452, auc 0.8449
epoch 5401, loss 0.3557, train acc 84.54%, f1 0.8454, precision 0.8454, recall 0.8454, auc 0.8454
epoch 5501, loss 0.2785, train acc 84.51%, f1 0.8452, precision 0.8450, recall 0.8453, auc 0.8451
epoch 5601, loss 0.4535, train acc 84.54%, f1 0.8454, precision 0.8452, recall 0.8456, auc 0.8454
epoch 5701, loss 0.2896, train acc 84.56%, f1 0.8456, precision 0.8455, recall 0.8457, auc 0.8456
epoch 5801, loss 0.3516, train acc 84.61%, f1 0.8461, precision 0.8461, recall 0.8460, auc 0.8461
epoch 5901, loss 0.3108, train acc 84.61%, f1 0.8461, precision 0.8462, recall 0.8459, auc 0.8461
epoch 6001, loss 0.4104, train acc 84.62%, f1 0.8462, precision 0.8463, recall 0.8461, auc 0.8462
epoch 6101, loss 0.4406, train acc 84.67%, f1 0.8467, precision 0.8466, recall 0.8468, auc 0.8467
epoch 6201, loss 0.3083, train acc 84.69%, f1 0.8469, precision 0.8469, recall 0.8469, auc 0.8469
epoch 6301, loss 0.4072, train acc 84.70%, f1 0.8470, precision 0.8470, recall 0.8470, auc 0.8470
epoch 6401, loss 0.4451, train acc 84.70%, f1 0.8470, precision 0.8470, recall 0.8471, auc 0.8470
epoch 6501, loss 0.3775, train acc 84.70%, f1 0.8470, precision 0.8471, recall 0.8469, auc 0.8470
epoch 6601, loss 0.3732, train acc 84.70%, f1 0.8470, precision 0.8471, recall 0.8470, auc 0.8470
epoch 6701, loss 0.2393, train acc 84.78%, f1 0.8478, precision 0.8478, recall 0.8477, auc 0.8478
epoch 6801, loss 0.3832, train acc 84.80%, f1 0.8480, precision 0.8480, recall 0.8480, auc 0.8480
epoch 6901, loss 0.3269, train acc 84.80%, f1 0.8480, precision 0.8481, recall 0.8479, auc 0.8480
epoch 7001, loss 0.3049, train acc 84.83%, f1 0.8483, precision 0.8483, recall 0.8484, auc 0.8483
epoch 7101, loss 0.3362, train acc 84.89%, f1 0.8489, precision 0.8489, recall 0.8489, auc 0.8489
epoch 7201, loss 0.3777, train acc 84.90%, f1 0.8490, precision 0.8491, recall 0.8489, auc 0.8490
epoch 7301, loss 0.2777, train acc 84.91%, f1 0.8491, precision 0.8492, recall 0.8490, auc 0.8491
epoch 7401, loss 0.2754, train acc 84.93%, f1 0.8493, precision 0.8492, recall 0.8494, auc 0.8493
epoch 7501, loss 0.3373, train acc 84.97%, f1 0.8497, precision 0.8497, recall 0.8496, auc 0.8497
epoch 7601, loss 0.2428, train acc 84.97%, f1 0.8498, precision 0.8497, recall 0.8498, auc 0.8497
epoch 7701, loss 0.2128, train acc 84.99%, f1 0.8499, precision 0.8498, recall 0.8499, auc 0.8499
epoch 7801, loss 0.4246, train acc 85.06%, f1 0.8506, precision 0.8506, recall 0.8507, auc 0.8506
epoch 7901, loss 0.2992, train acc 85.10%, f1 0.8510, precision 0.8509, recall 0.8512, auc 0.8510
epoch 8001, loss 0.2587, train acc 85.11%, f1 0.8511, precision 0.8511, recall 0.8510, auc 0.8511
epoch 8101, loss 0.3648, train acc 85.03%, f1 0.8503, precision 0.8504, recall 0.8503, auc 0.8503
epoch 8201, loss 0.2814, train acc 85.12%, f1 0.8511, precision 0.8512, recall 0.8511, auc 0.8512
epoch 8301, loss 0.2988, train acc 85.12%, f1 0.8512, precision 0.8512, recall 0.8511, auc 0.8512
epoch 8401, loss 0.3249, train acc 85.14%, f1 0.8514, precision 0.8515, recall 0.8513, auc 0.8514
epoch 8501, loss 0.3184, train acc 85.15%, f1 0.8514, precision 0.8515, recall 0.8513, auc 0.8515
epoch 8601, loss 0.4234, train acc 85.19%, f1 0.8519, precision 0.8520, recall 0.8519, auc 0.8519
epoch 8701, loss 0.3715, train acc 85.15%, f1 0.8515, precision 0.8515, recall 0.8516, auc 0.8515
epoch 8801, loss 0.2902, train acc 85.22%, f1 0.8522, precision 0.8522, recall 0.8522, auc 0.8522
epoch 8901, loss 0.3386, train acc 85.27%, f1 0.8527, precision 0.8527, recall 0.8527, auc 0.8527
epoch 9001, loss 0.2892, train acc 85.26%, f1 0.8526, precision 0.8526, recall 0.8526, auc 0.8526
epoch 9101, loss 0.2719, train acc 85.28%, f1 0.8528, precision 0.8528, recall 0.8527, auc 0.8528
epoch 9201, loss 0.3049, train acc 85.26%, f1 0.8526, precision 0.8526, recall 0.8526, auc 0.8526
epoch 9301, loss 0.2330, train acc 85.27%, f1 0.8527, precision 0.8527, recall 0.8527, auc 0.8527
epoch 9401, loss 0.3975, train acc 85.27%, f1 0.8527, precision 0.8527, recall 0.8527, auc 0.8527
epoch 9501, loss 0.4305, train acc 85.31%, f1 0.8531, precision 0.8530, recall 0.8531, auc 0.8531
epoch 9601, loss 0.1778, train acc 85.34%, f1 0.8534, precision 0.8533, recall 0.8534, auc 0.8534
epoch 9701, loss 0.3104, train acc 85.28%, f1 0.8528, precision 0.8527, recall 0.8528, auc 0.8528
epoch 9801, loss 0.3959, train acc 85.35%, f1 0.8535, precision 0.8535, recall 0.8536, auc 0.8535
epoch 9901, loss 0.3503, train acc 85.37%, f1 0.8537, precision 0.8538, recall 0.8537, auc 0.8537
epoch 10001, loss 0.3330, train acc 85.35%, f1 0.8536, precision 0.8535, recall 0.8536, auc 0.8535
epoch 10101, loss 0.3965, train acc 85.37%, f1 0.8537, precision 0.8536, recall 0.8538, auc 0.8537
epoch 10201, loss 0.2696, train acc 85.42%, f1 0.8542, precision 0.8542, recall 0.8542, auc 0.8542
epoch 10301, loss 0.3649, train acc 85.41%, f1 0.8541, precision 0.8541, recall 0.8541, auc 0.8541
epoch 10401, loss 0.3167, train acc 85.47%, f1 0.8547, precision 0.8548, recall 0.8547, auc 0.8547
epoch 10501, loss 0.3324, train acc 85.39%, f1 0.8539, precision 0.8540, recall 0.8538, auc 0.8539
epoch 10601, loss 0.3458, train acc 85.42%, f1 0.8542, precision 0.8542, recall 0.8542, auc 0.8542
epoch 10701, loss 0.3049, train acc 85.42%, f1 0.8542, precision 0.8542, recall 0.8541, auc 0.8542
epoch 10801, loss 0.3157, train acc 85.42%, f1 0.8542, precision 0.8542, recall 0.8543, auc 0.8542
epoch 10901, loss 0.2952, train acc 85.41%, f1 0.8541, precision 0.8541, recall 0.8540, auc 0.8541
epoch 11001, loss 0.3724, train acc 85.47%, f1 0.8547, precision 0.8546, recall 0.8548, auc 0.8547
epoch 11101, loss 0.3793, train acc 85.48%, f1 0.8548, precision 0.8547, recall 0.8548, auc 0.8548
epoch 11201, loss 0.3759, train acc 85.51%, f1 0.8551, precision 0.8551, recall 0.8551, auc 0.8551
epoch 11301, loss 0.2809, train acc 85.44%, f1 0.8544, precision 0.8545, recall 0.8543, auc 0.8544
epoch 11401, loss 0.2710, train acc 85.49%, f1 0.8549, precision 0.8549, recall 0.8548, auc 0.8549
epoch 11501, loss 0.2377, train acc 85.51%, f1 0.8551, precision 0.8551, recall 0.8550, auc 0.8551
epoch 11601, loss 0.3246, train acc 85.49%, f1 0.8549, precision 0.8549, recall 0.8548, auc 0.8549
epoch 11701, loss 0.2730, train acc 85.54%, f1 0.8554, precision 0.8554, recall 0.8553, auc 0.8554
epoch 11801, loss 0.4447, train acc 85.55%, f1 0.8555, precision 0.8556, recall 0.8555, auc 0.8555
epoch 11901, loss 0.3286, train acc 85.55%, f1 0.8555, precision 0.8556, recall 0.8554, auc 0.8555
epoch 12001, loss 0.3178, train acc 85.54%, f1 0.8554, precision 0.8554, recall 0.8554, auc 0.8554
epoch 12101, loss 0.2878, train acc 85.57%, f1 0.8557, precision 0.8557, recall 0.8557, auc 0.8557
epoch 12201, loss 0.3990, train acc 85.58%, f1 0.8558, precision 0.8558, recall 0.8557, auc 0.8558
epoch 12301, loss 0.3839, train acc 85.57%, f1 0.8557, precision 0.8557, recall 0.8557, auc 0.8557
epoch 12401, loss 0.3211, train acc 85.56%, f1 0.8556, precision 0.8556, recall 0.8557, auc 0.8556
epoch 12501, loss 0.3868, train acc 85.64%, f1 0.8564, precision 0.8564, recall 0.8563, auc 0.8564
epoch 12601, loss 0.3007, train acc 85.63%, f1 0.8563, precision 0.8564, recall 0.8563, auc 0.8563
epoch 12701, loss 0.4408, train acc 85.61%, f1 0.8561, precision 0.8561, recall 0.8560, auc 0.8561
epoch 12801, loss 0.3095, train acc 85.67%, f1 0.8567, precision 0.8567, recall 0.8567, auc 0.8567
epoch 12901, loss 0.2651, train acc 85.61%, f1 0.8560, precision 0.8561, recall 0.8560, auc 0.8561
epoch 13001, loss 0.3722, train acc 85.66%, f1 0.8566, precision 0.8566, recall 0.8566, auc 0.8566
epoch 13101, loss 0.3027, train acc 85.63%, f1 0.8563, precision 0.8564, recall 0.8562, auc 0.8563
epoch 13201, loss 0.2818, train acc 85.64%, f1 0.8564, precision 0.8564, recall 0.8565, auc 0.8564
epoch 13301, loss 0.3717, train acc 85.69%, f1 0.8569, precision 0.8569, recall 0.8569, auc 0.8569
epoch 13401, loss 0.3199, train acc 85.63%, f1 0.8563, precision 0.8563, recall 0.8562, auc 0.8563
epoch 13501, loss 0.4183, train acc 85.66%, f1 0.8566, precision 0.8566, recall 0.8565, auc 0.8566
epoch 13601, loss 0.3330, train acc 85.67%, f1 0.8567, precision 0.8567, recall 0.8567, auc 0.8567
epoch 13701, loss 0.2723, train acc 85.69%, f1 0.8569, precision 0.8569, recall 0.8570, auc 0.8569
epoch 13801, loss 0.2841, train acc 85.68%, f1 0.8568, precision 0.8567, recall 0.8568, auc 0.8568
epoch 13901, loss 0.2841, train acc 85.67%, f1 0.8567, precision 0.8567, recall 0.8566, auc 0.8567
epoch 14001, loss 0.3470, train acc 85.69%, f1 0.8569, precision 0.8569, recall 0.8569, auc 0.8569
epoch 14101, loss 0.2954, train acc 85.72%, f1 0.8572, precision 0.8572, recall 0.8572, auc 0.8572
epoch 14201, loss 0.3637, train acc 85.70%, f1 0.8570, precision 0.8570, recall 0.8570, auc 0.8570
epoch 14301, loss 0.2669, train acc 85.71%, f1 0.8571, precision 0.8571, recall 0.8571, auc 0.8571
epoch 14401, loss 0.3888, train acc 85.71%, f1 0.8571, precision 0.8571, recall 0.8571, auc 0.8571
epoch 14501, loss 0.3252, train acc 85.70%, f1 0.8570, precision 0.8571, recall 0.8569, auc 0.8570
epoch 14601, loss 0.3376, train acc 85.77%, f1 0.8576, precision 0.8577, recall 0.8576, auc 0.8577
epoch 14701, loss 0.3879, train acc 85.78%, f1 0.8578, precision 0.8578, recall 0.8578, auc 0.8578
epoch 14801, loss 0.3119, train acc 85.75%, f1 0.8575, precision 0.8575, recall 0.8575, auc 0.8575
epoch 14901, loss 0.3849, train acc 85.74%, f1 0.8574, precision 0.8574, recall 0.8574, auc 0.8574
epoch 15001, loss 0.2414, train acc 85.75%, f1 0.8575, precision 0.8575, recall 0.8574, auc 0.8575
epoch 15101, loss 0.3534, train acc 85.79%, f1 0.8579, precision 0.8579, recall 0.8579, auc 0.8579
epoch 15201, loss 0.3227, train acc 85.79%, f1 0.8579, precision 0.8579, recall 0.8579, auc 0.8579
epoch 15301, loss 0.3938, train acc 85.78%, f1 0.8578, precision 0.8578, recall 0.8578, auc 0.8578
epoch 15401, loss 0.3602, train acc 85.83%, f1 0.8583, precision 0.8583, recall 0.8583, auc 0.8583
epoch 15501, loss 0.4199, train acc 85.78%, f1 0.8578, precision 0.8578, recall 0.8579, auc 0.8578
epoch 15601, loss 0.2650, train acc 85.76%, f1 0.8576, precision 0.8576, recall 0.8575, auc 0.8576
epoch 15701, loss 0.3255, train acc 85.81%, f1 0.8581, precision 0.8580, recall 0.8581, auc 0.8581
epoch 15801, loss 0.2612, train acc 85.77%, f1 0.8577, precision 0.8576, recall 0.8577, auc 0.8577
epoch 15901, loss 0.3240, train acc 85.81%, f1 0.8581, precision 0.8581, recall 0.8582, auc 0.8581
epoch 16001, loss 0.2758, train acc 85.82%, f1 0.8582, precision 0.8582, recall 0.8582, auc 0.8582
epoch 16101, loss 0.2372, train acc 85.85%, f1 0.8585, precision 0.8585, recall 0.8585, auc 0.8585
epoch 16201, loss 0.3342, train acc 85.80%, f1 0.8580, precision 0.8581, recall 0.8580, auc 0.8580
epoch 16301, loss 0.3531, train acc 85.90%, f1 0.8590, precision 0.8590, recall 0.8590, auc 0.8590
epoch 16401, loss 0.4031, train acc 85.87%, f1 0.8587, precision 0.8587, recall 0.8587, auc 0.8587
epoch 16501, loss 0.3050, train acc 85.84%, f1 0.8584, precision 0.8585, recall 0.8583, auc 0.8584/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.3504, train acc 85.82%, f1 0.8582, precision 0.8582, recall 0.8583, auc 0.8582
epoch 16701, loss 0.2416, train acc 85.81%, f1 0.8581, precision 0.8581, recall 0.8581, auc 0.8581
epoch 16801, loss 0.3827, train acc 85.91%, f1 0.8591, precision 0.8591, recall 0.8591, auc 0.8591
epoch 16901, loss 0.3377, train acc 85.92%, f1 0.8592, precision 0.8593, recall 0.8592, auc 0.8592
epoch 17001, loss 0.3304, train acc 85.94%, f1 0.8594, precision 0.8594, recall 0.8594, auc 0.8594
epoch 17101, loss 0.2814, train acc 85.93%, f1 0.8593, precision 0.8593, recall 0.8593, auc 0.8593
epoch 17201, loss 0.2853, train acc 85.90%, f1 0.8590, precision 0.8590, recall 0.8590, auc 0.8590
epoch 17301, loss 0.2843, train acc 85.92%, f1 0.8592, precision 0.8592, recall 0.8592, auc 0.8592
epoch 17401, loss 0.3281, train acc 85.87%, f1 0.8587, precision 0.8587, recall 0.8587, auc 0.8587
epoch 17501, loss 0.2474, train acc 85.86%, f1 0.8586, precision 0.8587, recall 0.8586, auc 0.8586
epoch 17601, loss 0.3255, train acc 85.95%, f1 0.8595, precision 0.8595, recall 0.8595, auc 0.8595
epoch 17701, loss 0.2696, train acc 85.90%, f1 0.8590, precision 0.8590, recall 0.8590, auc 0.8590
epoch 17801, loss 0.3235, train acc 85.91%, f1 0.8591, precision 0.8592, recall 0.8591, auc 0.8591
epoch 17901, loss 0.3430, train acc 85.88%, f1 0.8588, precision 0.8588, recall 0.8588, auc 0.8588
epoch 18001, loss 0.4625, train acc 85.98%, f1 0.8598, precision 0.8598, recall 0.8598, auc 0.8598
epoch 18101, loss 0.2797, train acc 85.97%, f1 0.8597, precision 0.8597, recall 0.8596, auc 0.8597
epoch 18201, loss 0.3087, train acc 85.94%, f1 0.8594, precision 0.8594, recall 0.8594, auc 0.8594
epoch 18301, loss 0.3795, train acc 85.96%, f1 0.8596, precision 0.8597, recall 0.8596, auc 0.8596
epoch 18401, loss 0.2498, train acc 85.94%, f1 0.8594, precision 0.8595, recall 0.8594, auc 0.8594
epoch 18501, loss 0.3560, train acc 85.96%, f1 0.8596, precision 0.8597, recall 0.8596, auc 0.8596
epoch 18601, loss 0.2916, train acc 86.00%, f1 0.8600, precision 0.8601, recall 0.8599, auc 0.8600
epoch 18701, loss 0.2656, train acc 86.02%, f1 0.8603, precision 0.8602, recall 0.8603, auc 0.8603
epoch 18801, loss 0.3576, train acc 85.94%, f1 0.8594, precision 0.8594, recall 0.8594, auc 0.8594
epoch 18901, loss 0.2178, train acc 85.97%, f1 0.8597, precision 0.8597, recall 0.8597, auc 0.8597
epoch 19001, loss 0.3257, train acc 86.01%, f1 0.8601, precision 0.8600, recall 0.8601, auc 0.8601
epoch 19101, loss 0.3680, train acc 85.99%, f1 0.8599, precision 0.8599, recall 0.8599, auc 0.8599
epoch 19201, loss 0.3256, train acc 86.02%, f1 0.8602, precision 0.8602, recall 0.8602, auc 0.8602
epoch 19301, loss 0.2553, train acc 86.03%, f1 0.8603, precision 0.8603, recall 0.8603, auc 0.8603
epoch 19401, loss 0.3883, train acc 85.95%, f1 0.8594, precision 0.8595, recall 0.8594, auc 0.8595
epoch 19501, loss 0.2692, train acc 86.00%, f1 0.8600, precision 0.8600, recall 0.8600, auc 0.8600
epoch 19601, loss 0.3743, train acc 86.02%, f1 0.8602, precision 0.8602, recall 0.8601, auc 0.8602
epoch 19701, loss 0.2709, train acc 86.00%, f1 0.8600, precision 0.8600, recall 0.8600, auc 0.8600
epoch 19801, loss 0.3378, train acc 86.03%, f1 0.8603, precision 0.8603, recall 0.8604, auc 0.8603
epoch 19901, loss 0.3470, train acc 86.07%, f1 0.8607, precision 0.8607, recall 0.8607, auc 0.8607
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_5
./test_pima/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6605660377358491

the Fscore is 0.608187134502924

the precision is 0.4406779661016949

the recall is 0.9811320754716981

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_5
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6035, train acc 78.34%, f1 0.7715, precision 0.8166, recall 0.7310, auc 0.7834
epoch 201, loss 0.4970, train acc 81.07%, f1 0.8094, precision 0.8148, recall 0.8041, auc 0.8107
epoch 301, loss 0.3764, train acc 82.66%, f1 0.8266, precision 0.8264, recall 0.8268, auc 0.8266
epoch 401, loss 0.4272, train acc 83.50%, f1 0.8354, precision 0.8334, recall 0.8375, auc 0.8350
epoch 501, loss 0.3907, train acc 83.89%, f1 0.8395, precision 0.8360, recall 0.8430, auc 0.8389
epoch 601, loss 0.5115, train acc 84.08%, f1 0.8415, precision 0.8379, recall 0.8450, auc 0.8408
epoch 701, loss 0.4180, train acc 84.19%, f1 0.8424, precision 0.8395, recall 0.8453, auc 0.8419
epoch 801, loss 0.3626, train acc 84.28%, f1 0.8434, precision 0.8404, recall 0.8465, auc 0.8428
epoch 901, loss 0.2515, train acc 84.21%, f1 0.8428, precision 0.8388, recall 0.8470, auc 0.8421
epoch 1001, loss 0.3928, train acc 84.17%, f1 0.8421, precision 0.8400, recall 0.8443, auc 0.8417
epoch 1101, loss 0.3274, train acc 84.14%, f1 0.8419, precision 0.8391, recall 0.8448, auc 0.8414
epoch 1201, loss 0.3330, train acc 84.21%, f1 0.8425, precision 0.8405, recall 0.8445, auc 0.8421
epoch 1301, loss 0.4358, train acc 84.24%, f1 0.8427, precision 0.8411, recall 0.8443, auc 0.8424
epoch 1401, loss 0.3807, train acc 84.24%, f1 0.8427, precision 0.8410, recall 0.8445, auc 0.8424
epoch 1501, loss 0.4485, train acc 84.19%, f1 0.8423, precision 0.8404, recall 0.8441, auc 0.8419
epoch 1601, loss 0.4648, train acc 84.18%, f1 0.8420, precision 0.8412, recall 0.8427, auc 0.8418
epoch 1701, loss 0.3223, train acc 84.20%, f1 0.8422, precision 0.8411, recall 0.8433, auc 0.8420
epoch 1801, loss 0.3484, train acc 84.19%, f1 0.8420, precision 0.8415, recall 0.8425, auc 0.8419
epoch 1901, loss 0.4112, train acc 84.25%, f1 0.8426, precision 0.8423, recall 0.8430, auc 0.8425
epoch 2001, loss 0.3922, train acc 84.25%, f1 0.8426, precision 0.8423, recall 0.8429, auc 0.8425
epoch 2101, loss 0.2649, train acc 84.24%, f1 0.8424, precision 0.8424, recall 0.8424, auc 0.8424
epoch 2201, loss 0.2947, train acc 84.30%, f1 0.8431, precision 0.8426, recall 0.8435, auc 0.8430
epoch 2301, loss 0.2288, train acc 84.27%, f1 0.8428, precision 0.8424, recall 0.8432, auc 0.8427
epoch 2401, loss 0.2902, train acc 84.27%, f1 0.8428, precision 0.8422, recall 0.8433, auc 0.8427
epoch 2501, loss 0.4488, train acc 84.21%, f1 0.8421, precision 0.8422, recall 0.8420, auc 0.8421
epoch 2601, loss 0.3982, train acc 84.22%, f1 0.8423, precision 0.8420, recall 0.8426, auc 0.8422
epoch 2701, loss 0.4348, train acc 84.18%, f1 0.8418, precision 0.8419, recall 0.8417, auc 0.8418
epoch 2801, loss 0.2539, train acc 84.21%, f1 0.8420, precision 0.8424, recall 0.8416, auc 0.8421
epoch 2901, loss 0.3076, train acc 84.25%, f1 0.8425, precision 0.8427, recall 0.8423, auc 0.8425
epoch 3001, loss 0.3656, train acc 84.26%, f1 0.8426, precision 0.8428, recall 0.8423, auc 0.8426
epoch 3101, loss 0.4743, train acc 84.24%, f1 0.8425, precision 0.8420, recall 0.8430, auc 0.8424
epoch 3201, loss 0.2819, train acc 84.24%, f1 0.8424, precision 0.8425, recall 0.8422, auc 0.8424
epoch 3301, loss 0.4036, train acc 84.29%, f1 0.8429, precision 0.8429, recall 0.8429, auc 0.8429
epoch 3401, loss 0.3764, train acc 84.26%, f1 0.8426, precision 0.8423, recall 0.8430, auc 0.8426
epoch 3501, loss 0.3196, train acc 84.26%, f1 0.8426, precision 0.8427, recall 0.8425, auc 0.8426
epoch 3601, loss 0.2336, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8422, auc 0.8422
epoch 3701, loss 0.3365, train acc 84.24%, f1 0.8424, precision 0.8423, recall 0.8425, auc 0.8424
epoch 3801, loss 0.4054, train acc 84.25%, f1 0.8425, precision 0.8426, recall 0.8425, auc 0.8425
epoch 3901, loss 0.3057, train acc 84.31%, f1 0.8432, precision 0.8429, recall 0.8435, auc 0.8431
epoch 4001, loss 0.4261, train acc 84.28%, f1 0.8428, precision 0.8430, recall 0.8426, auc 0.8428
epoch 4101, loss 0.3893, train acc 84.26%, f1 0.8425, precision 0.8427, recall 0.8423, auc 0.8426
epoch 4201, loss 0.3284, train acc 84.32%, f1 0.8432, precision 0.8430, recall 0.8434, auc 0.8432
epoch 4301, loss 0.2871, train acc 84.39%, f1 0.8439, precision 0.8440, recall 0.8438, auc 0.8439
epoch 4401, loss 0.2786, train acc 84.33%, f1 0.8433, precision 0.8432, recall 0.8435, auc 0.8433
epoch 4501, loss 0.4347, train acc 84.42%, f1 0.8442, precision 0.8440, recall 0.8443, auc 0.8442
epoch 4601, loss 0.2398, train acc 84.35%, f1 0.8435, precision 0.8435, recall 0.8434, auc 0.8435
epoch 4701, loss 0.3161, train acc 84.39%, f1 0.8439, precision 0.8438, recall 0.8440, auc 0.8439
epoch 4801, loss 0.4236, train acc 84.42%, f1 0.8442, precision 0.8441, recall 0.8443, auc 0.8442
epoch 4901, loss 0.2757, train acc 84.42%, f1 0.8442, precision 0.8441, recall 0.8443, auc 0.8442
epoch 5001, loss 0.3324, train acc 84.41%, f1 0.8440, precision 0.8442, recall 0.8439, auc 0.8441
epoch 5101, loss 0.4095, train acc 84.44%, f1 0.8443, precision 0.8445, recall 0.8442, auc 0.8444
epoch 5201, loss 0.4158, train acc 84.47%, f1 0.8448, precision 0.8446, recall 0.8449, auc 0.8447
epoch 5301, loss 0.3792, train acc 84.51%, f1 0.8451, precision 0.8452, recall 0.8450, auc 0.8451
epoch 5401, loss 0.3260, train acc 84.56%, f1 0.8456, precision 0.8457, recall 0.8454, auc 0.8456
epoch 5501, loss 0.3949, train acc 84.57%, f1 0.8457, precision 0.8457, recall 0.8457, auc 0.8457
epoch 5601, loss 0.3521, train acc 84.58%, f1 0.8458, precision 0.8459, recall 0.8457, auc 0.8458
epoch 5701, loss 0.2891, train acc 84.57%, f1 0.8457, precision 0.8457, recall 0.8456, auc 0.8457
epoch 5801, loss 0.2874, train acc 84.57%, f1 0.8457, precision 0.8458, recall 0.8457, auc 0.8457
epoch 5901, loss 0.3020, train acc 84.64%, f1 0.8464, precision 0.8462, recall 0.8467, auc 0.8464
epoch 6001, loss 0.3156, train acc 84.68%, f1 0.8468, precision 0.8468, recall 0.8468, auc 0.8468
epoch 6101, loss 0.2840, train acc 84.69%, f1 0.8469, precision 0.8470, recall 0.8467, auc 0.8469
epoch 6201, loss 0.4900, train acc 84.72%, f1 0.8472, precision 0.8471, recall 0.8473, auc 0.8472
epoch 6301, loss 0.4967, train acc 84.81%, f1 0.8482, precision 0.8480, recall 0.8483, auc 0.8481
epoch 6401, loss 0.3482, train acc 84.82%, f1 0.8482, precision 0.8482, recall 0.8482, auc 0.8482
epoch 6501, loss 0.3448, train acc 84.85%, f1 0.8485, precision 0.8484, recall 0.8487, auc 0.8485
epoch 6601, loss 0.3777, train acc 84.88%, f1 0.8488, precision 0.8487, recall 0.8488, auc 0.8488
epoch 6701, loss 0.3091, train acc 84.89%, f1 0.8489, precision 0.8489, recall 0.8489, auc 0.8489
epoch 6801, loss 0.4416, train acc 84.87%, f1 0.8488, precision 0.8486, recall 0.8489, auc 0.8487
epoch 6901, loss 0.3596, train acc 84.88%, f1 0.8487, precision 0.8490, recall 0.8484, auc 0.8488
epoch 7001, loss 0.3854, train acc 84.97%, f1 0.8497, precision 0.8498, recall 0.8496, auc 0.8497
epoch 7101, loss 0.2527, train acc 84.92%, f1 0.8492, precision 0.8494, recall 0.8490, auc 0.8492
epoch 7201, loss 0.3397, train acc 84.98%, f1 0.8497, precision 0.8499, recall 0.8496, auc 0.8498
epoch 7301, loss 0.2426, train acc 85.02%, f1 0.8501, precision 0.8503, recall 0.8500, auc 0.8502
epoch 7401, loss 0.3251, train acc 85.07%, f1 0.8507, precision 0.8507, recall 0.8507, auc 0.8507
epoch 7501, loss 0.3202, train acc 85.06%, f1 0.8506, precision 0.8507, recall 0.8505, auc 0.8506
epoch 7601, loss 0.2665, train acc 85.05%, f1 0.8505, precision 0.8505, recall 0.8505, auc 0.8505
epoch 7701, loss 0.3612, train acc 85.15%, f1 0.8515, precision 0.8515, recall 0.8516, auc 0.8515
epoch 7801, loss 0.3636, train acc 85.12%, f1 0.8512, precision 0.8512, recall 0.8512, auc 0.8512
epoch 7901, loss 0.3820, train acc 85.14%, f1 0.8514, precision 0.8514, recall 0.8515, auc 0.8514
epoch 8001, loss 0.2624, train acc 85.16%, f1 0.8516, precision 0.8518, recall 0.8514, auc 0.8516
epoch 8101, loss 0.4213, train acc 85.15%, f1 0.8515, precision 0.8515, recall 0.8514, auc 0.8515
epoch 8201, loss 0.3312, train acc 85.13%, f1 0.8512, precision 0.8513, recall 0.8511, auc 0.8513/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.3420, train acc 85.16%, f1 0.8516, precision 0.8516, recall 0.8517, auc 0.8516
epoch 8401, loss 0.2817, train acc 85.18%, f1 0.8518, precision 0.8518, recall 0.8517, auc 0.8518
epoch 8501, loss 0.3362, train acc 85.25%, f1 0.8525, precision 0.8525, recall 0.8526, auc 0.8525
epoch 8601, loss 0.4207, train acc 85.18%, f1 0.8518, precision 0.8518, recall 0.8517, auc 0.8518
epoch 8701, loss 0.4091, train acc 85.20%, f1 0.8521, precision 0.8519, recall 0.8522, auc 0.8520
epoch 8801, loss 0.2875, train acc 85.24%, f1 0.8524, precision 0.8524, recall 0.8525, auc 0.8524
epoch 8901, loss 0.2878, train acc 85.25%, f1 0.8525, precision 0.8526, recall 0.8523, auc 0.8525
epoch 9001, loss 0.3635, train acc 85.28%, f1 0.8528, precision 0.8529, recall 0.8527, auc 0.8528
epoch 9101, loss 0.3823, train acc 85.27%, f1 0.8527, precision 0.8526, recall 0.8527, auc 0.8527
epoch 9201, loss 0.2235, train acc 85.31%, f1 0.8531, precision 0.8531, recall 0.8531, auc 0.8531
epoch 9301, loss 0.4484, train acc 85.35%, f1 0.8535, precision 0.8534, recall 0.8536, auc 0.8535
epoch 9401, loss 0.3408, train acc 85.33%, f1 0.8533, precision 0.8533, recall 0.8534, auc 0.8533
epoch 9501, loss 0.2910, train acc 85.37%, f1 0.8537, precision 0.8537, recall 0.8537, auc 0.8537
epoch 9601, loss 0.2894, train acc 85.40%, f1 0.8540, precision 0.8540, recall 0.8540, auc 0.8540
epoch 9701, loss 0.3278, train acc 85.41%, f1 0.8541, precision 0.8541, recall 0.8542, auc 0.8541
epoch 9801, loss 0.4133, train acc 85.34%, f1 0.8534, precision 0.8534, recall 0.8534, auc 0.8534
epoch 9901, loss 0.2496, train acc 85.42%, f1 0.8542, precision 0.8542, recall 0.8541, auc 0.8542
epoch 10001, loss 0.3743, train acc 85.39%, f1 0.8539, precision 0.8539, recall 0.8540, auc 0.8539
epoch 10101, loss 0.2590, train acc 85.39%, f1 0.8539, precision 0.8539, recall 0.8538, auc 0.8539
epoch 10201, loss 0.3151, train acc 85.44%, f1 0.8544, precision 0.8544, recall 0.8545, auc 0.8544
epoch 10301, loss 0.3503, train acc 85.44%, f1 0.8544, precision 0.8545, recall 0.8544, auc 0.8544
epoch 10401, loss 0.2911, train acc 85.42%, f1 0.8542, precision 0.8541, recall 0.8543, auc 0.8542
epoch 10501, loss 0.3869, train acc 85.45%, f1 0.8545, precision 0.8546, recall 0.8545, auc 0.8545
epoch 10601, loss 0.3442, train acc 85.46%, f1 0.8546, precision 0.8546, recall 0.8546, auc 0.8546
epoch 10701, loss 0.3088, train acc 85.46%, f1 0.8545, precision 0.8546, recall 0.8545, auc 0.8546
epoch 10801, loss 0.2676, train acc 85.45%, f1 0.8545, precision 0.8544, recall 0.8545, auc 0.8545
epoch 10901, loss 0.3466, train acc 85.38%, f1 0.8538, precision 0.8538, recall 0.8539, auc 0.8538
epoch 11001, loss 0.2326, train acc 85.47%, f1 0.8547, precision 0.8546, recall 0.8548, auc 0.8547
epoch 11101, loss 0.2752, train acc 85.49%, f1 0.8549, precision 0.8550, recall 0.8548, auc 0.8549
epoch 11201, loss 0.3114, train acc 85.44%, f1 0.8544, precision 0.8544, recall 0.8545, auc 0.8544
epoch 11301, loss 0.3051, train acc 85.54%, f1 0.8554, precision 0.8555, recall 0.8553, auc 0.8554
epoch 11401, loss 0.3201, train acc 85.49%, f1 0.8549, precision 0.8550, recall 0.8549, auc 0.8549
epoch 11501, loss 0.4009, train acc 85.50%, f1 0.8550, precision 0.8550, recall 0.8550, auc 0.8550
epoch 11601, loss 0.2359, train acc 85.49%, f1 0.8549, precision 0.8549, recall 0.8549, auc 0.8549
epoch 11701, loss 0.2846, train acc 85.50%, f1 0.8550, precision 0.8550, recall 0.8550, auc 0.8550
epoch 11801, loss 0.3400, train acc 85.48%, f1 0.8548, precision 0.8548, recall 0.8548, auc 0.8548
epoch 11901, loss 0.2733, train acc 85.52%, f1 0.8551, precision 0.8552, recall 0.8551, auc 0.8552
epoch 12001, loss 0.2741, train acc 85.48%, f1 0.8549, precision 0.8548, recall 0.8549, auc 0.8548
epoch 12101, loss 0.3185, train acc 85.58%, f1 0.8559, precision 0.8558, recall 0.8559, auc 0.8558
epoch 12201, loss 0.3599, train acc 85.57%, f1 0.8557, precision 0.8557, recall 0.8556, auc 0.8557
epoch 12301, loss 0.4154, train acc 85.56%, f1 0.8556, precision 0.8556, recall 0.8557, auc 0.8556
epoch 12401, loss 0.3547, train acc 85.56%, f1 0.8556, precision 0.8554, recall 0.8557, auc 0.8556
epoch 12501, loss 0.3352, train acc 85.57%, f1 0.8557, precision 0.8557, recall 0.8557, auc 0.8557
epoch 12601, loss 0.3851, train acc 85.61%, f1 0.8561, precision 0.8561, recall 0.8561, auc 0.8561
epoch 12701, loss 0.3987, train acc 85.58%, f1 0.8558, precision 0.8557, recall 0.8558, auc 0.8558
epoch 12801, loss 0.4085, train acc 85.55%, f1 0.8555, precision 0.8555, recall 0.8555, auc 0.8555
epoch 12901, loss 0.3608, train acc 85.60%, f1 0.8560, precision 0.8560, recall 0.8560, auc 0.8560
epoch 13001, loss 0.3420, train acc 85.52%, f1 0.8552, precision 0.8551, recall 0.8552, auc 0.8552
epoch 13101, loss 0.4310, train acc 85.55%, f1 0.8555, precision 0.8555, recall 0.8555, auc 0.8555
epoch 13201, loss 0.2789, train acc 85.59%, f1 0.8559, precision 0.8559, recall 0.8559, auc 0.8559
epoch 13301, loss 0.3525, train acc 85.55%, f1 0.8555, precision 0.8555, recall 0.8555, auc 0.8555
epoch 13401, loss 0.1897, train acc 85.60%, f1 0.8560, precision 0.8561, recall 0.8560, auc 0.8560
epoch 13501, loss 0.2740, train acc 85.62%, f1 0.8562, precision 0.8563, recall 0.8560, auc 0.8562
epoch 13601, loss 0.3638, train acc 85.60%, f1 0.8560, precision 0.8560, recall 0.8560, auc 0.8560
epoch 13701, loss 0.2640, train acc 85.65%, f1 0.8564, precision 0.8566, recall 0.8563, auc 0.8565
epoch 13801, loss 0.2106, train acc 85.59%, f1 0.8559, precision 0.8559, recall 0.8558, auc 0.8559
epoch 13901, loss 0.3062, train acc 85.63%, f1 0.8563, precision 0.8564, recall 0.8561, auc 0.8563
epoch 14001, loss 0.2671, train acc 85.64%, f1 0.8564, precision 0.8564, recall 0.8564, auc 0.8564
epoch 14101, loss 0.3282, train acc 85.65%, f1 0.8565, precision 0.8565, recall 0.8564, auc 0.8565
epoch 14201, loss 0.2707, train acc 85.68%, f1 0.8568, precision 0.8569, recall 0.8567, auc 0.8568
epoch 14301, loss 0.2464, train acc 85.68%, f1 0.8568, precision 0.8569, recall 0.8568, auc 0.8568
epoch 14401, loss 0.3102, train acc 85.66%, f1 0.8566, precision 0.8567, recall 0.8566, auc 0.8566
epoch 14501, loss 0.2583, train acc 85.68%, f1 0.8568, precision 0.8568, recall 0.8567, auc 0.8568
epoch 14601, loss 0.4088, train acc 85.66%, f1 0.8566, precision 0.8566, recall 0.8567, auc 0.8566
epoch 14701, loss 0.3104, train acc 85.67%, f1 0.8567, precision 0.8568, recall 0.8566, auc 0.8567
epoch 14801, loss 0.2312, train acc 85.64%, f1 0.8564, precision 0.8566, recall 0.8562, auc 0.8564
epoch 14901, loss 0.2684, train acc 85.68%, f1 0.8568, precision 0.8568, recall 0.8567, auc 0.8568
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_5
./test_pima/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.65

the Fscore is 0.6022727272727273

the precision is 0.43089430894308944

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_5
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6159, train acc 78.41%, f1 0.7736, precision 0.8132, recall 0.7377, auc 0.7841
epoch 201, loss 0.4779, train acc 81.10%, f1 0.8097, precision 0.8155, recall 0.8039, auc 0.8110
epoch 301, loss 0.4766, train acc 82.45%, f1 0.8248, precision 0.8236, recall 0.8260, auc 0.8245
epoch 401, loss 0.4606, train acc 83.34%, f1 0.8338, precision 0.8319, recall 0.8357, auc 0.8334
epoch 501, loss 0.5004, train acc 83.86%, f1 0.8391, precision 0.8364, recall 0.8419, auc 0.8386
epoch 601, loss 0.3919, train acc 84.03%, f1 0.8409, precision 0.8375, recall 0.8443, auc 0.8403
epoch 701, loss 0.4203, train acc 84.14%, f1 0.8422, precision 0.8382, recall 0.8462, auc 0.8414
epoch 801, loss 0.3690, train acc 84.18%, f1 0.8424, precision 0.8396, recall 0.8451, auc 0.8418
epoch 901, loss 0.4817, train acc 84.21%, f1 0.8428, precision 0.8393, recall 0.8464, auc 0.8421
epoch 1001, loss 0.3118, train acc 84.24%, f1 0.8428, precision 0.8404, recall 0.8453, auc 0.8424
epoch 1101, loss 0.2927, train acc 84.19%, f1 0.8424, precision 0.8399, recall 0.8449, auc 0.8419
epoch 1201, loss 0.4157, train acc 84.24%, f1 0.8429, precision 0.8402, recall 0.8455, auc 0.8424
epoch 1301, loss 0.3857, train acc 84.23%, f1 0.8426, precision 0.8411, recall 0.8441, auc 0.8423
epoch 1401, loss 0.3506, train acc 84.27%, f1 0.8430, precision 0.8417, recall 0.8442, auc 0.8427
epoch 1501, loss 0.3390, train acc 84.22%, f1 0.8423, precision 0.8417, recall 0.8429, auc 0.8422
epoch 1601, loss 0.2169, train acc 84.21%, f1 0.8423, precision 0.8414, recall 0.8432, auc 0.8421
epoch 1701, loss 0.3546, train acc 84.22%, f1 0.8424, precision 0.8413, recall 0.8435, auc 0.8422
epoch 1801, loss 0.3133, train acc 84.20%, f1 0.8422, precision 0.8415, recall 0.8429, auc 0.8420
epoch 1901, loss 0.4531, train acc 84.23%, f1 0.8426, precision 0.8414, recall 0.8438, auc 0.8423
epoch 2001, loss 0.3075, train acc 84.23%, f1 0.8424, precision 0.8421, recall 0.8427, auc 0.8423
epoch 2101, loss 0.3327, train acc 84.28%, f1 0.8428, precision 0.8425, recall 0.8432, auc 0.8428
epoch 2201, loss 0.3868, train acc 84.25%, f1 0.8425, precision 0.8423, recall 0.8427, auc 0.8425
epoch 2301, loss 0.3510, train acc 84.20%, f1 0.8420, precision 0.8418, recall 0.8422, auc 0.8420
epoch 2401, loss 0.3737, train acc 84.24%, f1 0.8423, precision 0.8428, recall 0.8418, auc 0.8424
epoch 2501, loss 0.4278, train acc 84.28%, f1 0.8429, precision 0.8424, recall 0.8433, auc 0.8428
epoch 2601, loss 0.3334, train acc 84.24%, f1 0.8424, precision 0.8421, recall 0.8428, auc 0.8424
epoch 2701, loss 0.3309, train acc 84.26%, f1 0.8425, precision 0.8427, recall 0.8424, auc 0.8426
epoch 2801, loss 0.3310, train acc 84.24%, f1 0.8424, precision 0.8424, recall 0.8423, auc 0.8424
epoch 2901, loss 0.4125, train acc 84.22%, f1 0.8422, precision 0.8423, recall 0.8420, auc 0.8422
epoch 3001, loss 0.3595, train acc 84.24%, f1 0.8423, precision 0.8425, recall 0.8422, auc 0.8424
epoch 3101, loss 0.3206, train acc 84.24%, f1 0.8424, precision 0.8423, recall 0.8426, auc 0.8424
epoch 3201, loss 0.2349, train acc 84.26%, f1 0.8425, precision 0.8428, recall 0.8422, auc 0.8426
epoch 3301, loss 0.2469, train acc 84.26%, f1 0.8426, precision 0.8426, recall 0.8425, auc 0.8426
epoch 3401, loss 0.2371, train acc 84.22%, f1 0.8422, precision 0.8421, recall 0.8423, auc 0.8422
epoch 3501, loss 0.2753, train acc 84.31%, f1 0.8431, precision 0.8431, recall 0.8430, auc 0.8431
epoch 3601, loss 0.2847, train acc 84.30%, f1 0.8430, precision 0.8429, recall 0.8430, auc 0.8430
epoch 3701, loss 0.2980, train acc 84.31%, f1 0.8431, precision 0.8431, recall 0.8431, auc 0.8431
epoch 3801, loss 0.3094, train acc 84.28%, f1 0.8427, precision 0.8429, recall 0.8426, auc 0.8428
epoch 3901, loss 0.3722, train acc 84.29%, f1 0.8429, precision 0.8429, recall 0.8429, auc 0.8429
epoch 4001, loss 0.3428, train acc 84.28%, f1 0.8428, precision 0.8429, recall 0.8427, auc 0.8428
epoch 4101, loss 0.3834, train acc 84.32%, f1 0.8431, precision 0.8435, recall 0.8428, auc 0.8432
epoch 4201, loss 0.3244, train acc 84.34%, f1 0.8434, precision 0.8436, recall 0.8431, auc 0.8434
epoch 4301, loss 0.3180, train acc 84.33%, f1 0.8433, precision 0.8434, recall 0.8432, auc 0.8433
epoch 4401, loss 0.2922, train acc 84.32%, f1 0.8432, precision 0.8432, recall 0.8432, auc 0.8432
epoch 4501, loss 0.3813, train acc 84.39%, f1 0.8439, precision 0.8439, recall 0.8438, auc 0.8439
epoch 4601, loss 0.3054, train acc 84.38%, f1 0.8438, precision 0.8438, recall 0.8438, auc 0.8438
epoch 4701, loss 0.3335, train acc 84.34%, f1 0.8434, precision 0.8435, recall 0.8433, auc 0.8434
epoch 4801, loss 0.2875, train acc 84.47%, f1 0.8448, precision 0.8446, recall 0.8449, auc 0.8447
epoch 4901, loss 0.2657, train acc 84.42%, f1 0.8442, precision 0.8443, recall 0.8441, auc 0.8442
epoch 5001, loss 0.3995, train acc 84.46%, f1 0.8446, precision 0.8447, recall 0.8445, auc 0.8446
epoch 5101, loss 0.4125, train acc 84.53%, f1 0.8453, precision 0.8453, recall 0.8453, auc 0.8453
epoch 5201, loss 0.2974, train acc 84.56%, f1 0.8455, precision 0.8456, recall 0.8455, auc 0.8456
epoch 5301, loss 0.4408, train acc 84.59%, f1 0.8459, precision 0.8459, recall 0.8459, auc 0.8459
epoch 5401, loss 0.4108, train acc 84.59%, f1 0.8460, precision 0.8458, recall 0.8461, auc 0.8459
epoch 5501, loss 0.2968, train acc 84.62%, f1 0.8462, precision 0.8464, recall 0.8460, auc 0.8462
epoch 5601, loss 0.3781, train acc 84.62%, f1 0.8461, precision 0.8462, recall 0.8461, auc 0.8462
epoch 5701, loss 0.3598, train acc 84.70%, f1 0.8470, precision 0.8470, recall 0.8470, auc 0.8470
epoch 5801, loss 0.4976, train acc 84.72%, f1 0.8472, precision 0.8472, recall 0.8472, auc 0.8472
epoch 5901, loss 0.3716, train acc 84.75%, f1 0.8475, precision 0.8477, recall 0.8473, auc 0.8475
epoch 6001, loss 0.2213, train acc 84.78%, f1 0.8478, precision 0.8479, recall 0.8477, auc 0.8478
epoch 6101, loss 0.3100, train acc 84.78%, f1 0.8478, precision 0.8479, recall 0.8477, auc 0.8478
epoch 6201, loss 0.2435, train acc 84.79%, f1 0.8479, precision 0.8480, recall 0.8478, auc 0.8479
epoch 6301, loss 0.4080, train acc 84.79%, f1 0.8479, precision 0.8478, recall 0.8481, auc 0.8479
epoch 6401, loss 0.3667, train acc 84.82%, f1 0.8482, precision 0.8481, recall 0.8482, auc 0.8482
epoch 6501, loss 0.2838, train acc 84.85%, f1 0.8485, precision 0.8486, recall 0.8484, auc 0.8485
epoch 6601, loss 0.3833, train acc 84.86%, f1 0.8486, precision 0.8486, recall 0.8486, auc 0.8486
epoch 6701, loss 0.4265, train acc 84.89%, f1 0.8489, precision 0.8489, recall 0.8489, auc 0.8489
epoch 6801, loss 0.4085, train acc 84.90%, f1 0.8491, precision 0.8488, recall 0.8494, auc 0.8490
epoch 6901, loss 0.3484, train acc 84.93%, f1 0.8493, precision 0.8492, recall 0.8493, auc 0.8493
epoch 7001, loss 0.2654, train acc 84.93%, f1 0.8493, precision 0.8493, recall 0.8494, auc 0.8493
epoch 7101, loss 0.4013, train acc 84.92%, f1 0.8492, precision 0.8493, recall 0.8492, auc 0.8492
epoch 7201, loss 0.2940, train acc 84.96%, f1 0.8496, precision 0.8497, recall 0.8494, auc 0.8496
epoch 7301, loss 0.3880, train acc 85.00%, f1 0.8500, precision 0.8499, recall 0.8502, auc 0.8500
epoch 7401, loss 0.2800, train acc 84.98%, f1 0.8497, precision 0.8499, recall 0.8496, auc 0.8498
epoch 7501, loss 0.3124, train acc 85.03%, f1 0.8503, precision 0.8503, recall 0.8503, auc 0.8503
epoch 7601, loss 0.2564, train acc 85.06%, f1 0.8507, precision 0.8503, recall 0.8510, auc 0.8506
epoch 7701, loss 0.4407, train acc 85.05%, f1 0.8504, precision 0.8506, recall 0.8503, auc 0.8505
epoch 7801, loss 0.3368, train acc 85.00%, f1 0.8500, precision 0.8501, recall 0.8499, auc 0.8500
epoch 7901, loss 0.2556, train acc 85.06%, f1 0.8505, precision 0.8506, recall 0.8504, auc 0.8506
epoch 8001, loss 0.2750, train acc 85.06%, f1 0.8506, precision 0.8505, recall 0.8506, auc 0.8506
epoch 8101, loss 0.4488, train acc 85.14%, f1 0.8514, precision 0.8515, recall 0.8513, auc 0.8514
epoch 8201, loss 0.2729, train acc 85.15%, f1 0.8515, precision 0.8515, recall 0.8515, auc 0.8515/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2423, train acc 85.17%, f1 0.8517, precision 0.8516, recall 0.8518, auc 0.8517
epoch 8401, loss 0.3514, train acc 85.14%, f1 0.8514, precision 0.8516, recall 0.8512, auc 0.8514
epoch 8501, loss 0.3757, train acc 85.16%, f1 0.8516, precision 0.8515, recall 0.8516, auc 0.8516
epoch 8601, loss 0.3479, train acc 85.17%, f1 0.8517, precision 0.8518, recall 0.8516, auc 0.8517
epoch 8701, loss 0.3605, train acc 85.17%, f1 0.8517, precision 0.8518, recall 0.8517, auc 0.8517
epoch 8801, loss 0.3116, train acc 85.23%, f1 0.8523, precision 0.8522, recall 0.8524, auc 0.8523
epoch 8901, loss 0.4785, train acc 85.21%, f1 0.8521, precision 0.8521, recall 0.8522, auc 0.8521
epoch 9001, loss 0.3998, train acc 85.21%, f1 0.8521, precision 0.8522, recall 0.8520, auc 0.8521
epoch 9101, loss 0.3873, train acc 85.27%, f1 0.8527, precision 0.8528, recall 0.8525, auc 0.8527
epoch 9201, loss 0.3236, train acc 85.23%, f1 0.8523, precision 0.8522, recall 0.8524, auc 0.8523
epoch 9301, loss 0.2943, train acc 85.25%, f1 0.8525, precision 0.8525, recall 0.8525, auc 0.8525
epoch 9401, loss 0.3201, train acc 85.26%, f1 0.8526, precision 0.8525, recall 0.8527, auc 0.8526
epoch 9501, loss 0.2711, train acc 85.28%, f1 0.8528, precision 0.8528, recall 0.8529, auc 0.8528
epoch 9601, loss 0.2787, train acc 85.28%, f1 0.8528, precision 0.8526, recall 0.8530, auc 0.8528
epoch 9701, loss 0.3647, train acc 85.31%, f1 0.8531, precision 0.8530, recall 0.8533, auc 0.8531
epoch 9801, loss 0.2558, train acc 85.34%, f1 0.8534, precision 0.8534, recall 0.8535, auc 0.8534
epoch 9901, loss 0.3150, train acc 85.35%, f1 0.8535, precision 0.8534, recall 0.8536, auc 0.8535
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_5
./test_pima/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.661132075471698

the Fscore is 0.6071428571428571

the precision is 0.4434782608695652

the recall is 0.9622641509433962

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_5
----------------------



epoch 1, loss 0.6933, train acc 46.10%, f1 0.2389, precision 0.4064, recall 0.1692, auc 0.4610
epoch 101, loss 0.6213, train acc 78.20%, f1 0.7682, precision 0.8200, recall 0.7225, auc 0.7820
epoch 201, loss 0.4868, train acc 80.97%, f1 0.8083, precision 0.8144, recall 0.8022, auc 0.8097
epoch 301, loss 0.5818, train acc 82.47%, f1 0.8252, precision 0.8231, recall 0.8273, auc 0.8247
epoch 401, loss 0.4038, train acc 83.52%, f1 0.8358, precision 0.8329, recall 0.8387, auc 0.8352
epoch 501, loss 0.3768, train acc 83.88%, f1 0.8396, precision 0.8354, recall 0.8438, auc 0.8388
epoch 601, loss 0.3565, train acc 84.07%, f1 0.8416, precision 0.8369, recall 0.8465, auc 0.8407
epoch 701, loss 0.3283, train acc 84.13%, f1 0.8420, precision 0.8381, recall 0.8459, auc 0.8413
epoch 801, loss 0.4441, train acc 84.16%, f1 0.8425, precision 0.8380, recall 0.8470, auc 0.8416
epoch 901, loss 0.3326, train acc 84.20%, f1 0.8426, precision 0.8394, recall 0.8459, auc 0.8420
epoch 1001, loss 0.2404, train acc 84.24%, f1 0.8430, precision 0.8396, recall 0.8465, auc 0.8424
epoch 1101, loss 0.3424, train acc 84.22%, f1 0.8428, precision 0.8400, recall 0.8455, auc 0.8422
epoch 1201, loss 0.3984, train acc 84.23%, f1 0.8427, precision 0.8407, recall 0.8447, auc 0.8423
epoch 1301, loss 0.3360, train acc 84.24%, f1 0.8428, precision 0.8406, recall 0.8451, auc 0.8424
epoch 1401, loss 0.4448, train acc 84.27%, f1 0.8430, precision 0.8415, recall 0.8444, auc 0.8427
epoch 1501, loss 0.4587, train acc 84.28%, f1 0.8430, precision 0.8418, recall 0.8442, auc 0.8428
epoch 1601, loss 0.3304, train acc 84.27%, f1 0.8429, precision 0.8416, recall 0.8442, auc 0.8427
epoch 1701, loss 0.3458, train acc 84.28%, f1 0.8429, precision 0.8425, recall 0.8432, auc 0.8428
epoch 1801, loss 0.3220, train acc 84.27%, f1 0.8429, precision 0.8418, recall 0.8440, auc 0.8427
epoch 1901, loss 0.3262, train acc 84.26%, f1 0.8426, precision 0.8424, recall 0.8429, auc 0.8426
epoch 2001, loss 0.3285, train acc 84.23%, f1 0.8423, precision 0.8418, recall 0.8429, auc 0.8423
epoch 2101, loss 0.5042, train acc 84.22%, f1 0.8422, precision 0.8419, recall 0.8426, auc 0.8422
epoch 2201, loss 0.4310, train acc 84.26%, f1 0.8427, precision 0.8421, recall 0.8432, auc 0.8426
epoch 2301, loss 0.3684, train acc 84.26%, f1 0.8427, precision 0.8423, recall 0.8430, auc 0.8426
epoch 2401, loss 0.3743, train acc 84.28%, f1 0.8429, precision 0.8428, recall 0.8430, auc 0.8428
epoch 2501, loss 0.3164, train acc 84.28%, f1 0.8428, precision 0.8426, recall 0.8430, auc 0.8428
epoch 2601, loss 0.3459, train acc 84.23%, f1 0.8424, precision 0.8417, recall 0.8432, auc 0.8423
epoch 2701, loss 0.3242, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8423, auc 0.8422
epoch 2801, loss 0.2147, train acc 84.22%, f1 0.8422, precision 0.8420, recall 0.8424, auc 0.8422
epoch 2901, loss 0.3531, train acc 84.21%, f1 0.8421, precision 0.8421, recall 0.8420, auc 0.8421
epoch 3001, loss 0.3267, train acc 84.26%, f1 0.8426, precision 0.8427, recall 0.8425, auc 0.8426
epoch 3101, loss 0.2607, train acc 84.26%, f1 0.8427, precision 0.8422, recall 0.8431, auc 0.8426
epoch 3201, loss 0.2907, train acc 84.29%, f1 0.8429, precision 0.8428, recall 0.8431, auc 0.8429
epoch 3301, loss 0.2589, train acc 84.25%, f1 0.8425, precision 0.8422, recall 0.8428, auc 0.8425
epoch 3401, loss 0.4267, train acc 84.24%, f1 0.8423, precision 0.8424, recall 0.8423, auc 0.8424
epoch 3501, loss 0.3298, train acc 84.22%, f1 0.8422, precision 0.8419, recall 0.8425, auc 0.8422
epoch 3601, loss 0.4138, train acc 84.24%, f1 0.8425, precision 0.8421, recall 0.8428, auc 0.8424
epoch 3701, loss 0.4776, train acc 84.28%, f1 0.8429, precision 0.8425, recall 0.8433, auc 0.8428
epoch 3801, loss 0.3476, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8422, auc 0.8422
epoch 3901, loss 0.4107, train acc 84.27%, f1 0.8427, precision 0.8426, recall 0.8429, auc 0.8427
epoch 4001, loss 0.2647, train acc 84.32%, f1 0.8433, precision 0.8430, recall 0.8436, auc 0.8432
epoch 4101, loss 0.2999, train acc 84.28%, f1 0.8429, precision 0.8423, recall 0.8436, auc 0.8428
epoch 4201, loss 0.2648, train acc 84.28%, f1 0.8429, precision 0.8427, recall 0.8430, auc 0.8428
epoch 4301, loss 0.3130, train acc 84.31%, f1 0.8430, precision 0.8432, recall 0.8429, auc 0.8431
epoch 4401, loss 0.3608, train acc 84.31%, f1 0.8431, precision 0.8431, recall 0.8431, auc 0.8431
epoch 4501, loss 0.3032, train acc 84.35%, f1 0.8436, precision 0.8434, recall 0.8437, auc 0.8435
epoch 4601, loss 0.3646, train acc 84.36%, f1 0.8436, precision 0.8435, recall 0.8436, auc 0.8436
epoch 4701, loss 0.2363, train acc 84.36%, f1 0.8436, precision 0.8435, recall 0.8437, auc 0.8436
epoch 4801, loss 0.3011, train acc 84.40%, f1 0.8440, precision 0.8439, recall 0.8440, auc 0.8440
epoch 4901, loss 0.3146, train acc 84.42%, f1 0.8442, precision 0.8442, recall 0.8442, auc 0.8442
epoch 5001, loss 0.3894, train acc 84.44%, f1 0.8444, precision 0.8442, recall 0.8446, auc 0.8444
epoch 5101, loss 0.3586, train acc 84.47%, f1 0.8447, precision 0.8445, recall 0.8449, auc 0.8447
epoch 5201, loss 0.2971, train acc 84.49%, f1 0.8449, precision 0.8449, recall 0.8449, auc 0.8449
epoch 5301, loss 0.3104, train acc 84.51%, f1 0.8452, precision 0.8450, recall 0.8453, auc 0.8451
epoch 5401, loss 0.2866, train acc 84.55%, f1 0.8455, precision 0.8453, recall 0.8458, auc 0.8455
epoch 5501, loss 0.2604, train acc 84.53%, f1 0.8453, precision 0.8452, recall 0.8453, auc 0.8453
epoch 5601, loss 0.3646, train acc 84.58%, f1 0.8458, precision 0.8456, recall 0.8460, auc 0.8458
epoch 5701, loss 0.2462, train acc 84.60%, f1 0.8460, precision 0.8460, recall 0.8460, auc 0.8460
epoch 5801, loss 0.4262, train acc 84.67%, f1 0.8467, precision 0.8465, recall 0.8468, auc 0.8467
epoch 5901, loss 0.4279, train acc 84.66%, f1 0.8466, precision 0.8465, recall 0.8467, auc 0.8466
epoch 6001, loss 0.2889, train acc 84.68%, f1 0.8469, precision 0.8466, recall 0.8471, auc 0.8468
epoch 6101, loss 0.4850, train acc 84.72%, f1 0.8472, precision 0.8470, recall 0.8474, auc 0.8472
epoch 6201, loss 0.3890, train acc 84.79%, f1 0.8479, precision 0.8477, recall 0.8482, auc 0.8479
epoch 6301, loss 0.3596, train acc 84.83%, f1 0.8483, precision 0.8481, recall 0.8485, auc 0.8483
epoch 6401, loss 0.2710, train acc 84.88%, f1 0.8488, precision 0.8487, recall 0.8490, auc 0.8488
epoch 6501, loss 0.4112, train acc 84.85%, f1 0.8485, precision 0.8484, recall 0.8485, auc 0.8485
epoch 6601, loss 0.2198, train acc 84.92%, f1 0.8492, precision 0.8491, recall 0.8493, auc 0.8492
epoch 6701, loss 0.4640, train acc 84.91%, f1 0.8491, precision 0.8490, recall 0.8493, auc 0.8491
epoch 6801, loss 0.3870, train acc 84.91%, f1 0.8491, precision 0.8492, recall 0.8489, auc 0.8491
epoch 6901, loss 0.2549, train acc 84.91%, f1 0.8491, precision 0.8492, recall 0.8489, auc 0.8491
epoch 7001, loss 0.2466, train acc 84.93%, f1 0.8493, precision 0.8494, recall 0.8492, auc 0.8493
epoch 7101, loss 0.2702, train acc 84.98%, f1 0.8498, precision 0.8498, recall 0.8498, auc 0.8498
epoch 7201, loss 0.3383, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8505, auc 0.8505
epoch 7301, loss 0.2886, train acc 85.03%, f1 0.8503, precision 0.8503, recall 0.8504, auc 0.8503
epoch 7401, loss 0.2062, train acc 85.06%, f1 0.8506, precision 0.8506, recall 0.8507, auc 0.8506
epoch 7501, loss 0.3091, train acc 85.12%, f1 0.8512, precision 0.8513, recall 0.8510, auc 0.8512
epoch 7601, loss 0.3719, train acc 85.12%, f1 0.8512, precision 0.8511, recall 0.8512, auc 0.8512
epoch 7701, loss 0.3427, train acc 85.14%, f1 0.8515, precision 0.8513, recall 0.8516, auc 0.8514
epoch 7801, loss 0.3259, train acc 85.17%, f1 0.8517, precision 0.8516, recall 0.8517, auc 0.8517
epoch 7901, loss 0.3261, train acc 85.27%, f1 0.8527, precision 0.8526, recall 0.8528, auc 0.8527
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_5
./test_pima/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6666981132075471

the Fscore is 0.6097560975609756

the precision is 0.45045045045045046

the recall is 0.9433962264150944

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_5
----------------------



epoch 1, loss 0.6930, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5861, train acc 78.86%, f1 0.7902, precision 0.7842, recall 0.7964, auc 0.7886
epoch 201, loss 0.4577, train acc 81.03%, f1 0.8105, precision 0.8096, recall 0.8115, auc 0.8103
epoch 301, loss 0.5196, train acc 82.44%, f1 0.8243, precision 0.8247, recall 0.8239, auc 0.8244
epoch 401, loss 0.4188, train acc 83.54%, f1 0.8354, precision 0.8356, recall 0.8353, auc 0.8354
epoch 501, loss 0.4451, train acc 83.94%, f1 0.8392, precision 0.8399, recall 0.8386, auc 0.8394
epoch 601, loss 0.2530, train acc 84.05%, f1 0.8404, precision 0.8409, recall 0.8400, auc 0.8405
epoch 701, loss 0.3597, train acc 84.19%, f1 0.8418, precision 0.8422, recall 0.8415, auc 0.8419
epoch 801, loss 0.2600, train acc 84.19%, f1 0.8418, precision 0.8422, recall 0.8414, auc 0.8419
epoch 901, loss 0.3592, train acc 84.25%, f1 0.8425, precision 0.8425, recall 0.8424, auc 0.8425
epoch 1001, loss 0.3480, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8422, auc 0.8423
epoch 1101, loss 0.3950, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8422, auc 0.8423
epoch 1201, loss 0.3088, train acc 84.23%, f1 0.8423, precision 0.8424, recall 0.8421, auc 0.8423
epoch 1301, loss 0.3415, train acc 84.23%, f1 0.8422, precision 0.8424, recall 0.8421, auc 0.8423
epoch 1401, loss 0.4335, train acc 84.25%, f1 0.8425, precision 0.8425, recall 0.8424, auc 0.8425
epoch 1501, loss 0.3566, train acc 84.23%, f1 0.8423, precision 0.8423, recall 0.8422, auc 0.8423
epoch 1601, loss 0.3107, train acc 84.24%, f1 0.8424, precision 0.8424, recall 0.8425, auc 0.8424
epoch 1701, loss 0.3555, train acc 84.25%, f1 0.8425, precision 0.8425, recall 0.8426, auc 0.8425
epoch 1801, loss 0.3797, train acc 84.21%, f1 0.8420, precision 0.8421, recall 0.8420, auc 0.8421
epoch 1901, loss 0.3936, train acc 84.20%, f1 0.8419, precision 0.8420, recall 0.8419, auc 0.8420
epoch 2001, loss 0.4248, train acc 84.20%, f1 0.8420, precision 0.8420, recall 0.8420, auc 0.8420
epoch 2101, loss 0.5941, train acc 84.17%, f1 0.8417, precision 0.8417, recall 0.8417, auc 0.8417
epoch 2201, loss 0.2088, train acc 84.21%, f1 0.8421, precision 0.8421, recall 0.8422, auc 0.8421
epoch 2301, loss 0.3956, train acc 84.25%, f1 0.8425, precision 0.8424, recall 0.8425, auc 0.8425
epoch 2401, loss 0.4515, train acc 84.24%, f1 0.8424, precision 0.8423, recall 0.8425, auc 0.8424
epoch 2501, loss 0.3088, train acc 84.26%, f1 0.8426, precision 0.8426, recall 0.8426, auc 0.8426
epoch 2601, loss 0.2649, train acc 84.24%, f1 0.8424, precision 0.8423, recall 0.8425, auc 0.8424
epoch 2701, loss 0.3066, train acc 84.25%, f1 0.8426, precision 0.8425, recall 0.8426, auc 0.8425
epoch 2801, loss 0.4407, train acc 84.24%, f1 0.8424, precision 0.8423, recall 0.8424, auc 0.8424
epoch 2901, loss 0.2734, train acc 84.25%, f1 0.8425, precision 0.8423, recall 0.8426, auc 0.8425
epoch 3001, loss 0.4523, train acc 84.23%, f1 0.8423, precision 0.8422, recall 0.8424, auc 0.8423
epoch 3101, loss 0.5646, train acc 84.29%, f1 0.8429, precision 0.8429, recall 0.8430, auc 0.8429
epoch 3201, loss 0.4118, train acc 84.32%, f1 0.8432, precision 0.8430, recall 0.8434, auc 0.8432
epoch 3301, loss 0.3314, train acc 84.27%, f1 0.8427, precision 0.8426, recall 0.8429, auc 0.8427
epoch 3401, loss 0.3422, train acc 84.23%, f1 0.8423, precision 0.8420, recall 0.8426, auc 0.8423
epoch 3501, loss 0.3473, train acc 84.24%, f1 0.8424, precision 0.8424, recall 0.8424, auc 0.8424
epoch 3601, loss 0.2599, train acc 84.29%, f1 0.8430, precision 0.8428, recall 0.8432, auc 0.8429
epoch 3701, loss 0.3120, train acc 84.29%, f1 0.8429, precision 0.8427, recall 0.8432, auc 0.8429
epoch 3801, loss 0.4101, train acc 84.26%, f1 0.8426, precision 0.8425, recall 0.8427, auc 0.8426
epoch 3901, loss 0.3133, train acc 84.28%, f1 0.8428, precision 0.8425, recall 0.8431, auc 0.8427
epoch 4001, loss 0.4029, train acc 84.29%, f1 0.8430, precision 0.8427, recall 0.8432, auc 0.8429
epoch 4101, loss 0.3786, train acc 84.32%, f1 0.8432, precision 0.8430, recall 0.8435, auc 0.8432
epoch 4201, loss 0.3171, train acc 84.31%, f1 0.8431, precision 0.8429, recall 0.8434, auc 0.8431
epoch 4301, loss 0.3393, train acc 84.31%, f1 0.8431, precision 0.8430, recall 0.8433, auc 0.8431
epoch 4401, loss 0.3887, train acc 84.26%, f1 0.8426, precision 0.8424, recall 0.8428, auc 0.8426
epoch 4501, loss 0.3916, train acc 84.32%, f1 0.8432, precision 0.8430, recall 0.8435, auc 0.8432
epoch 4601, loss 0.2260, train acc 84.33%, f1 0.8433, precision 0.8431, recall 0.8435, auc 0.8433
epoch 4701, loss 0.3146, train acc 84.39%, f1 0.8440, precision 0.8438, recall 0.8442, auc 0.8439
epoch 4801, loss 0.3497, train acc 84.41%, f1 0.8441, precision 0.8440, recall 0.8442, auc 0.8441
epoch 4901, loss 0.4926, train acc 84.39%, f1 0.8439, precision 0.8439, recall 0.8439, auc 0.8439
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_5
./test_pima/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6161320754716981

the Fscore is 0.576271186440678

the precision is 0.4112903225806452

the recall is 0.9622641509433962

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_5
----------------------



epoch 1, loss 0.6935, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.6239, train acc 78.06%, f1 0.7936, precision 0.7493, recall 0.8436, auc 0.7806
epoch 201, loss 0.4615, train acc 80.99%, f1 0.8114, precision 0.8053, recall 0.8175, auc 0.8099
epoch 301, loss 0.4676, train acc 82.68%, f1 0.8265, precision 0.8280, recall 0.8250, auc 0.8268
epoch 401, loss 0.3731, train acc 83.46%, f1 0.8341, precision 0.8366, recall 0.8317, auc 0.8346
epoch 501, loss 0.3315, train acc 83.93%, f1 0.8389, precision 0.8409, recall 0.8368, auc 0.8393
epoch 601, loss 0.4160, train acc 84.04%, f1 0.8397, precision 0.8435, recall 0.8360, auc 0.8404
epoch 701, loss 0.2603, train acc 84.17%, f1 0.8410, precision 0.8447, recall 0.8373, auc 0.8417
epoch 801, loss 0.5158, train acc 84.22%, f1 0.8413, precision 0.8458, recall 0.8369, auc 0.8422
epoch 901, loss 0.4627, train acc 84.19%, f1 0.8413, precision 0.8448, recall 0.8378, auc 0.8419
epoch 1001, loss 0.3530, train acc 84.21%, f1 0.8414, precision 0.8454, recall 0.8375, auc 0.8421
epoch 1101, loss 0.3210, train acc 84.19%, f1 0.8414, precision 0.8440, recall 0.8388, auc 0.8419
epoch 1201, loss 0.3064, train acc 84.22%, f1 0.8419, precision 0.8435, recall 0.8402, auc 0.8422
epoch 1301, loss 0.4137, train acc 84.22%, f1 0.8418, precision 0.8440, recall 0.8396, auc 0.8422
epoch 1401, loss 0.4944, train acc 84.19%, f1 0.8416, precision 0.8430, recall 0.8403, auc 0.8419
epoch 1501, loss 0.5098, train acc 84.17%, f1 0.8414, precision 0.8432, recall 0.8396, auc 0.8417
epoch 1601, loss 0.4267, train acc 84.23%, f1 0.8420, precision 0.8434, recall 0.8406, auc 0.8423
epoch 1701, loss 0.4772, train acc 84.25%, f1 0.8423, precision 0.8431, recall 0.8415, auc 0.8425
epoch 1801, loss 0.3392, train acc 84.23%, f1 0.8421, precision 0.8428, recall 0.8415, auc 0.8423
epoch 1901, loss 0.3897, train acc 84.23%, f1 0.8422, precision 0.8427, recall 0.8416, auc 0.8423
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_5
./test_pima/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.5900000000000001

the Fscore is 0.5638297872340425

the precision is 0.3925925925925926

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_5
----------------------



epoch 1, loss 0.6935, train acc 49.87%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6328, train acc 75.41%, f1 0.7101, precision 0.8681, recall 0.6007, auc 0.7545
epoch 201, loss 0.5157, train acc 80.56%, f1 0.7983, precision 0.8316, recall 0.7677, auc 0.8057
epoch 301, loss 0.5852, train acc 82.27%, f1 0.8210, precision 0.8307, recall 0.8116, auc 0.8227
epoch 401, loss 0.3862, train acc 83.29%, f1 0.8342, precision 0.8298, recall 0.8386, auc 0.8328
epoch 501, loss 0.3406, train acc 83.78%, f1 0.8394, precision 0.8331, recall 0.8459, auc 0.8378
epoch 601, loss 0.3704, train acc 84.06%, f1 0.8422, precision 0.8361, recall 0.8484, auc 0.8406
epoch 701, loss 0.4162, train acc 84.11%, f1 0.8428, precision 0.8360, recall 0.8496, auc 0.8411
epoch 801, loss 0.3591, train acc 84.21%, f1 0.8443, precision 0.8347, recall 0.8541, auc 0.8421
epoch 901, loss 0.3604, train acc 84.24%, f1 0.8440, precision 0.8379, recall 0.8501, auc 0.8424
epoch 1001, loss 0.2776, train acc 84.22%, f1 0.8440, precision 0.8364, recall 0.8517, auc 0.8421
epoch 1101, loss 0.3851, train acc 84.26%, f1 0.8439, precision 0.8388, recall 0.8492, auc 0.8426
epoch 1201, loss 0.4074, train acc 84.19%, f1 0.8436, precision 0.8368, recall 0.8506, auc 0.8419
epoch 1301, loss 0.4678, train acc 84.25%, f1 0.8440, precision 0.8381, recall 0.8499, auc 0.8425
epoch 1401, loss 0.3156, train acc 84.26%, f1 0.8436, precision 0.8404, recall 0.8467, auc 0.8426
epoch 1501, loss 0.4175, train acc 84.21%, f1 0.8439, precision 0.8367, recall 0.8512, auc 0.8421
epoch 1601, loss 0.3293, train acc 84.26%, f1 0.8442, precision 0.8378, recall 0.8507, auc 0.8426
epoch 1701, loss 0.2803, train acc 84.23%, f1 0.8439, precision 0.8375, recall 0.8505, auc 0.8423
epoch 1801, loss 0.2402, train acc 84.21%, f1 0.8433, precision 0.8388, recall 0.8478, auc 0.8421
epoch 1901, loss 0.3140, train acc 84.21%, f1 0.8425, precision 0.8422, recall 0.8428, auc 0.8421
epoch 2001, loss 0.4001, train acc 84.28%, f1 0.8439, precision 0.8400, recall 0.8478, auc 0.8427
epoch 2101, loss 0.3870, train acc 84.24%, f1 0.8432, precision 0.8408, recall 0.8457, auc 0.8424
epoch 2201, loss 0.3520, train acc 84.24%, f1 0.8434, precision 0.8402, recall 0.8467, auc 0.8424
epoch 2301, loss 0.4368, train acc 84.26%, f1 0.8434, precision 0.8410, recall 0.8458, auc 0.8425
epoch 2401, loss 0.3226, train acc 84.26%, f1 0.8433, precision 0.8415, recall 0.8451, auc 0.8426
epoch 2501, loss 0.4073, train acc 84.28%, f1 0.8435, precision 0.8420, recall 0.8451, auc 0.8428
epoch 2601, loss 0.4853, train acc 84.21%, f1 0.8426, precision 0.8423, recall 0.8428, auc 0.8421
epoch 2701, loss 0.3492, train acc 84.25%, f1 0.8430, precision 0.8423, recall 0.8437, auc 0.8425
epoch 2801, loss 0.3413, train acc 84.25%, f1 0.8431, precision 0.8420, recall 0.8442, auc 0.8425
epoch 2901, loss 0.2552, train acc 84.25%, f1 0.8428, precision 0.8429, recall 0.8428, auc 0.8425
epoch 3001, loss 0.4391, train acc 84.17%, f1 0.8424, precision 0.8407, recall 0.8441, auc 0.8417
epoch 3101, loss 0.4573, train acc 84.26%, f1 0.8432, precision 0.8421, recall 0.8443, auc 0.8426
epoch 3201, loss 0.4218, train acc 84.25%, f1 0.8432, precision 0.8417, recall 0.8447, auc 0.8425
epoch 3301, loss 0.3146, train acc 84.28%, f1 0.8437, precision 0.8409, recall 0.8466, auc 0.8428
epoch 3401, loss 0.4108, train acc 84.26%, f1 0.8432, precision 0.8421, recall 0.8444, auc 0.8426
epoch 3501, loss 0.3860, train acc 84.29%, f1 0.8436, precision 0.8422, recall 0.8449, auc 0.8429
epoch 3601, loss 0.3875, train acc 84.25%, f1 0.8431, precision 0.8417, recall 0.8445, auc 0.8424
epoch 3701, loss 0.3790, train acc 84.24%, f1 0.8432, precision 0.8411, recall 0.8452, auc 0.8424
epoch 3801, loss 0.2782, train acc 84.27%, f1 0.8436, precision 0.8409, recall 0.8463, auc 0.8427
epoch 3901, loss 0.3130, train acc 84.33%, f1 0.8438, precision 0.8432, recall 0.8444, auc 0.8433
epoch 4001, loss 0.4294, train acc 84.34%, f1 0.8440, precision 0.8428, recall 0.8453, auc 0.8434
epoch 4101, loss 0.3358, train acc 84.34%, f1 0.8442, precision 0.8422, recall 0.8461, auc 0.8434
epoch 4201, loss 0.4133, train acc 84.38%, f1 0.8445, precision 0.8424, recall 0.8467, auc 0.8437
epoch 4301, loss 0.4655, train acc 84.36%, f1 0.8442, precision 0.8434, recall 0.8449, auc 0.8436
epoch 4401, loss 0.2831, train acc 84.37%, f1 0.8440, precision 0.8447, recall 0.8433, auc 0.8437
epoch 4501, loss 0.3247, train acc 84.45%, f1 0.8445, precision 0.8463, recall 0.8428, auc 0.8445
epoch 4601, loss 0.3632, train acc 84.46%, f1 0.8452, precision 0.8444, recall 0.8460, auc 0.8446
epoch 4701, loss 0.2856, train acc 84.46%, f1 0.8449, precision 0.8454, recall 0.8443, auc 0.8446
epoch 4801, loss 0.3803, train acc 84.42%, f1 0.8444, precision 0.8452, recall 0.8436, auc 0.8442
epoch 4901, loss 0.3235, train acc 84.47%, f1 0.8451, precision 0.8455, recall 0.8446, auc 0.8447
epoch 5001, loss 0.3592, train acc 84.51%, f1 0.8453, precision 0.8461, recall 0.8446, auc 0.8451
epoch 5101, loss 0.3245, train acc 84.49%, f1 0.8453, precision 0.8457, recall 0.8448, auc 0.8449
epoch 5201, loss 0.3205, train acc 84.54%, f1 0.8460, precision 0.8448, recall 0.8472, auc 0.8454
epoch 5301, loss 0.2498, train acc 84.59%, f1 0.8460, precision 0.8474, recall 0.8446, auc 0.8459
epoch 5401, loss 0.3796, train acc 84.59%, f1 0.8463, precision 0.8462, recall 0.8463, auc 0.8459
epoch 5501, loss 0.3192, train acc 84.57%, f1 0.8457, precision 0.8477, recall 0.8438, auc 0.8457
epoch 5601, loss 0.4609, train acc 84.66%, f1 0.8467, precision 0.8484, recall 0.8450, auc 0.8466
epoch 5701, loss 0.2924, train acc 84.74%, f1 0.8475, precision 0.8492, recall 0.8458, auc 0.8474
epoch 5801, loss 0.2708, train acc 84.71%, f1 0.8475, precision 0.8475, recall 0.8475, auc 0.8471
epoch 5901, loss 0.4069, train acc 84.74%, f1 0.8479, precision 0.8473, recall 0.8485, auc 0.8474
epoch 6001, loss 0.4704, train acc 84.74%, f1 0.8477, precision 0.8482, recall 0.8473, auc 0.8474
epoch 6101, loss 0.2651, train acc 84.74%, f1 0.8478, precision 0.8478, recall 0.8478, auc 0.8474
epoch 6201, loss 0.4019, train acc 84.74%, f1 0.8477, precision 0.8483, recall 0.8471, auc 0.8474
epoch 6301, loss 0.2985, train acc 84.80%, f1 0.8483, precision 0.8491, recall 0.8475, auc 0.8480
epoch 6401, loss 0.2638, train acc 84.83%, f1 0.8480, precision 0.8519, recall 0.8441, auc 0.8483
epoch 6501, loss 0.3315, train acc 84.83%, f1 0.8490, precision 0.8473, recall 0.8508, auc 0.8483
epoch 6601, loss 0.3069, train acc 84.85%, f1 0.8489, precision 0.8485, recall 0.8493, auc 0.8485
epoch 6701, loss 0.2253, train acc 84.89%, f1 0.8495, precision 0.8485, recall 0.8505, auc 0.8489
epoch 6801, loss 0.3347, train acc 84.90%, f1 0.8495, precision 0.8488, recall 0.8502, auc 0.8490
epoch 6901, loss 0.4500, train acc 84.92%, f1 0.8497, precision 0.8491, recall 0.8504, auc 0.8492
epoch 7001, loss 0.3795, train acc 84.92%, f1 0.8495, precision 0.8500, recall 0.8489, auc 0.8492
epoch 7101, loss 0.3756, train acc 84.99%, f1 0.8505, precision 0.8494, recall 0.8516, auc 0.8499
epoch 7201, loss 0.3158, train acc 84.96%, f1 0.8499, precision 0.8508, recall 0.8489, auc 0.8496
epoch 7301, loss 0.4348, train acc 84.99%, f1 0.8505, precision 0.8493, recall 0.8518, auc 0.8499
epoch 7401, loss 0.3019, train acc 85.07%, f1 0.8510, precision 0.8515, recall 0.8505, auc 0.8507
epoch 7501, loss 0.3205, train acc 85.08%, f1 0.8513, precision 0.8509, recall 0.8516, auc 0.8508
epoch 7601, loss 0.3234, train acc 85.10%, f1 0.8514, precision 0.8510, recall 0.8518, auc 0.8510
epoch 7701, loss 0.2226, train acc 85.12%, f1 0.8517, precision 0.8509, recall 0.8525, auc 0.8511
epoch 7801, loss 0.3338, train acc 85.08%, f1 0.8511, precision 0.8515, recall 0.8507, auc 0.8508
epoch 7901, loss 0.3804, train acc 85.13%, f1 0.8516, precision 0.8524, recall 0.8508, auc 0.8514
epoch 8001, loss 0.2473, train acc 85.13%, f1 0.8516, precision 0.8519, recall 0.8514, auc 0.8513
epoch 8101, loss 0.2571, train acc 85.15%, f1 0.8518, precision 0.8524, recall 0.8513, auc 0.8515
epoch 8201, loss 0.4098, train acc 85.18%, f1 0.8522, precision 0.8521, recall 0.8523, auc 0.8518
epoch 8301, loss 0.4563, train acc 85.19%, f1 0.8524, precision 0.8517, recall 0.8531, auc 0.8519
epoch 8401, loss 0.3388, train acc 85.16%, f1 0.8521, precision 0.8517, recall 0.8524, auc 0.8516
epoch 8501, loss 0.3648, train acc 85.24%, f1 0.8525, precision 0.8544, recall 0.8506, auc 0.8524
epoch 8601, loss 0.2913, train acc 85.25%, f1 0.8527, precision 0.8540, recall 0.8514, auc 0.8525
epoch 8701, loss 0.3656, train acc 85.22%, f1 0.8526, precision 0.8524, recall 0.8528, auc 0.8522
epoch 8801, loss 0.4038, train acc 85.26%, f1 0.8533, precision 0.8514, recall 0.8551, auc 0.8526
epoch 8901, loss 0.4404, train acc 85.22%, f1 0.8524, precision 0.8536, recall 0.8511, auc 0.8522
epoch 9001, loss 0.2751, train acc 85.27%, f1 0.8530, precision 0.8535, recall 0.8524, auc 0.8527
epoch 9101, loss 0.3403, train acc 85.28%, f1 0.8532, precision 0.8534, recall 0.8530, auc 0.8528
epoch 9201, loss 0.3903, train acc 85.32%, f1 0.8533, precision 0.8545, recall 0.8522, auc 0.8532
epoch 9301, loss 0.3565, train acc 85.23%, f1 0.8525, precision 0.8536, recall 0.8514, auc 0.8524
epoch 9401, loss 0.2864, train acc 85.23%, f1 0.8530, precision 0.8515, recall 0.8544, auc 0.8523
epoch 9501, loss 0.3769, train acc 85.34%, f1 0.8539, precision 0.8531, recall 0.8547, auc 0.8534
epoch 9601, loss 0.3412, train acc 85.34%, f1 0.8541, precision 0.8526, recall 0.8555, auc 0.8534
epoch 9701, loss 0.3264, train acc 85.38%, f1 0.8539, precision 0.8552, recall 0.8526, auc 0.8538
epoch 9801, loss 0.4013, train acc 85.37%, f1 0.8540, precision 0.8545, recall 0.8535, auc 0.8537
epoch 9901, loss 0.2769, train acc 85.36%, f1 0.8540, precision 0.8542, recall 0.8537, auc 0.8536
epoch 10001, loss 0.3612, train acc 85.38%, f1 0.8546, precision 0.8518, recall 0.8575, auc 0.8537
epoch 10101, loss 0.3916, train acc 85.40%, f1 0.8536, precision 0.8578, recall 0.8495, auc 0.8540
epoch 10201, loss 0.2922, train acc 85.34%, f1 0.8537, precision 0.8539, recall 0.8535, auc 0.8534
epoch 10301, loss 0.3367, train acc 85.47%, f1 0.8549, precision 0.8559, recall 0.8540, auc 0.8547
epoch 10401, loss 0.3403, train acc 85.45%, f1 0.8548, precision 0.8552, recall 0.8544, auc 0.8545
epoch 10501, loss 0.3536, train acc 85.48%, f1 0.8551, precision 0.8555, recall 0.8547, auc 0.8548
epoch 10601, loss 0.3159, train acc 85.46%, f1 0.8551, precision 0.8542, recall 0.8560, auc 0.8546
epoch 10701, loss 0.2708, train acc 85.47%, f1 0.8551, precision 0.8544, recall 0.8559, auc 0.8546
epoch 10801, loss 0.2947, train acc 85.42%, f1 0.8545, precision 0.8546, recall 0.8544, auc 0.8542
epoch 10901, loss 0.3687, train acc 85.55%, f1 0.8557, precision 0.8569, recall 0.8544, auc 0.8555
epoch 11001, loss 0.3465, train acc 85.50%, f1 0.8556, precision 0.8544, recall 0.8568, auc 0.8550
epoch 11101, loss 0.3586, train acc 85.50%, f1 0.8554, precision 0.8550, recall 0.8558, auc 0.8550
epoch 11201, loss 0.4729, train acc 85.57%, f1 0.8559, precision 0.8571, recall 0.8546, auc 0.8557
epoch 11301, loss 0.4041, train acc 85.59%, f1 0.8560, precision 0.8572, recall 0.8548, auc 0.8559
epoch 11401, loss 0.3243, train acc 85.57%, f1 0.8558, precision 0.8573, recall 0.8543, auc 0.8557
epoch 11501, loss 0.3503, train acc 85.62%, f1 0.8565, precision 0.8567, recall 0.8563, auc 0.8562
epoch 11601, loss 0.3773, train acc 85.61%, f1 0.8569, precision 0.8547, recall 0.8591, auc 0.8561
epoch 11701, loss 0.3454, train acc 85.58%, f1 0.8561, precision 0.8562, recall 0.8560, auc 0.8558
epoch 11801, loss 0.3364, train acc 85.55%, f1 0.8556, precision 0.8570, recall 0.8542, auc 0.8555
epoch 11901, loss 0.3328, train acc 85.62%, f1 0.8568, precision 0.8551, recall 0.8586, auc 0.8562
epoch 12001, loss 0.3767, train acc 85.63%, f1 0.8566, precision 0.8566, recall 0.8567, auc 0.8563
epoch 12101, loss 0.4051, train acc 85.61%, f1 0.8565, precision 0.8567, recall 0.8562, auc 0.8561
epoch 12201, loss 0.4002, train acc 85.63%, f1 0.8568, precision 0.8565, recall 0.8570, auc 0.8563
epoch 12301, loss 0.3685, train acc 85.58%, f1 0.8562, precision 0.8559, recall 0.8565, auc 0.8558
epoch 12401, loss 0.2990, train acc 85.62%, f1 0.8564, precision 0.8572, recall 0.8555, auc 0.8562
epoch 12501, loss 0.2177, train acc 85.60%, f1 0.8558, precision 0.8593, recall 0.8524, auc 0.8560
epoch 12601, loss 0.4253, train acc 85.64%, f1 0.8568, precision 0.8562, recall 0.8575, auc 0.8564
epoch 12701, loss 0.3345, train acc 85.64%, f1 0.8567, precision 0.8571, recall 0.8563, auc 0.8564
epoch 12801, loss 0.3706, train acc 85.64%, f1 0.8568, precision 0.8568, recall 0.8568, auc 0.8564
epoch 12901, loss 0.3529, train acc 85.64%, f1 0.8567, precision 0.8570, recall 0.8565, auc 0.8564
epoch 13001, loss 0.4885, train acc 85.66%, f1 0.8569, precision 0.8571, recall 0.8566, auc 0.8566
epoch 13101, loss 0.3385, train acc 85.62%, f1 0.8561, precision 0.8588, recall 0.8535, auc 0.8562
epoch 13201, loss 0.3340, train acc 85.67%, f1 0.8570, precision 0.8573, recall 0.8567, auc 0.8567
epoch 13301, loss 0.3003, train acc 85.71%, f1 0.8574, precision 0.8578, recall 0.8569, auc 0.8571
epoch 13401, loss 0.3512, train acc 85.70%, f1 0.8576, precision 0.8563, recall 0.8588, auc 0.8570
epoch 13501, loss 0.3149, train acc 85.68%, f1 0.8571, precision 0.8576, recall 0.8565, auc 0.8568
epoch 13601, loss 0.3550, train acc 85.76%, f1 0.8580, precision 0.8581, recall 0.8579, auc 0.8576
epoch 13701, loss 0.3653, train acc 85.74%, f1 0.8575, precision 0.8589, recall 0.8561, auc 0.8574
epoch 13801, loss 0.2802, train acc 85.72%, f1 0.8574, precision 0.8583, recall 0.8564, auc 0.8572
epoch 13901, loss 0.2500, train acc 85.72%, f1 0.8575, precision 0.8581, recall 0.8569, auc 0.8572
epoch 14001, loss 0.3263, train acc 85.72%, f1 0.8574, precision 0.8580, recall 0.8569, auc 0.8572
epoch 14101, loss 0.2035, train acc 85.74%, f1 0.8576, precision 0.8587, recall 0.8564, auc 0.8574
epoch 14201, loss 0.4634, train acc 85.76%, f1 0.8574, precision 0.8608, recall 0.8540, auc 0.8576
epoch 14301, loss 0.3171, train acc 85.72%, f1 0.8576, precision 0.8578, recall 0.8574, auc 0.8572
epoch 14401, loss 0.2629, train acc 85.70%, f1 0.8572, precision 0.8586, recall 0.8558, auc 0.8570
epoch 14501, loss 0.2624, train acc 85.77%, f1 0.8577, precision 0.8596, recall 0.8558, auc 0.8577
epoch 14601, loss 0.2269, train acc 85.78%, f1 0.8581, precision 0.8586, recall 0.8576, auc 0.8578
epoch 14701, loss 0.3141, train acc 85.75%, f1 0.8575, precision 0.8595, recall 0.8556, auc 0.8575
epoch 14801, loss 0.2664, train acc 85.74%, f1 0.8578, precision 0.8575, recall 0.8581, auc 0.8574
epoch 14901, loss 0.2940, train acc 85.74%, f1 0.8575, precision 0.8591, recall 0.8559, auc 0.8574
epoch 15001, loss 0.4335, train acc 85.77%, f1 0.8577, precision 0.8602, recall 0.8552, auc 0.8577
epoch 15101, loss 0.3361, train acc 85.79%, f1 0.8583, precision 0.8577, recall 0.8590, auc 0.8579
epoch 15201, loss 0.4157, train acc 85.74%, f1 0.8574, precision 0.8595, recall 0.8553, auc 0.8574
epoch 15301, loss 0.3274, train acc 85.79%, f1 0.8577, precision 0.8611, recall 0.8543, auc 0.8579
epoch 15401, loss 0.2425, train acc 85.84%, f1 0.8586, precision 0.8596, recall 0.8576, auc 0.8584
epoch 15501, loss 0.2057, train acc 85.81%, f1 0.8583, precision 0.8596, recall 0.8569, auc 0.8581
epoch 15601, loss 0.2860, train acc 85.84%, f1 0.8591, precision 0.8572, recall 0.8610, auc 0.8584
epoch 15701, loss 0.3455, train acc 85.78%, f1 0.8585, precision 0.8568, recall 0.8601, auc 0.8578
epoch 15801, loss 0.3354, train acc 85.86%, f1 0.8587, precision 0.8604, recall 0.8570, auc 0.8586
epoch 15901, loss 0.4284, train acc 85.82%, f1 0.8581, precision 0.8612, recall 0.8550, auc 0.8583
epoch 16001, loss 0.3564, train acc 85.84%, f1 0.8587, precision 0.8592, recall 0.8581, auc 0.8584
epoch 16101, loss 0.2896, train acc 85.82%, f1 0.8587, precision 0.8575, recall 0.8600, auc 0.8582
epoch 16201, loss 0.3713, train acc 85.90%, f1 0.8589, precision 0.8618, recall 0.8560, auc 0.8590
epoch 16301, loss 0.4103, train acc 85.80%, f1 0.8583, precision 0.8588, recall 0.8578, auc 0.8580
epoch 16401, loss 0.3488, train acc 85.88%, f1 0.8592, precision 0.8589, recall 0.8595, auc 0.8588
epoch 16501, loss 0.2837, train acc 85.81%, f1 0.8583, precision 0.8592, recall 0.8574, auc 0.8581/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.2739, train acc 85.76%, f1 0.8579, precision 0.8584, recall 0.8574, auc 0.8576
epoch 16701, loss 0.2985, train acc 85.81%, f1 0.8584, precision 0.8588, recall 0.8580, auc 0.8581
epoch 16801, loss 0.3040, train acc 85.90%, f1 0.8588, precision 0.8621, recall 0.8555, auc 0.8590
epoch 16901, loss 0.3561, train acc 85.89%, f1 0.8594, precision 0.8587, recall 0.8601, auc 0.8589
epoch 17001, loss 0.4384, train acc 85.88%, f1 0.8589, precision 0.8608, recall 0.8570, auc 0.8588
epoch 17101, loss 0.3326, train acc 85.89%, f1 0.8594, precision 0.8590, recall 0.8597, auc 0.8589
epoch 17201, loss 0.3081, train acc 85.89%, f1 0.8590, precision 0.8605, recall 0.8575, auc 0.8589
epoch 17301, loss 0.3071, train acc 85.85%, f1 0.8588, precision 0.8589, recall 0.8587, auc 0.8585
epoch 17401, loss 0.3698, train acc 85.86%, f1 0.8586, precision 0.8605, recall 0.8568, auc 0.8586
epoch 17501, loss 0.3162, train acc 85.80%, f1 0.8580, precision 0.8606, recall 0.8554, auc 0.8581
epoch 17601, loss 0.2990, train acc 85.89%, f1 0.8592, precision 0.8598, recall 0.8585, auc 0.8589
epoch 17701, loss 0.2773, train acc 85.85%, f1 0.8588, precision 0.8593, recall 0.8583, auc 0.8585
epoch 17801, loss 0.2818, train acc 85.89%, f1 0.8593, precision 0.8589, recall 0.8598, auc 0.8589
epoch 17901, loss 0.3277, train acc 85.91%, f1 0.8595, precision 0.8590, recall 0.8601, auc 0.8591
epoch 18001, loss 0.3851, train acc 85.90%, f1 0.8599, precision 0.8565, recall 0.8632, auc 0.8589
epoch 18101, loss 0.4554, train acc 85.91%, f1 0.8596, precision 0.8587, recall 0.8605, auc 0.8591
epoch 18201, loss 0.2357, train acc 85.91%, f1 0.8593, precision 0.8602, recall 0.8584, auc 0.8591
epoch 18301, loss 0.3566, train acc 85.89%, f1 0.8596, precision 0.8575, recall 0.8617, auc 0.8589
epoch 18401, loss 0.3087, train acc 85.93%, f1 0.8594, precision 0.8608, recall 0.8580, auc 0.8593
epoch 18501, loss 0.2665, train acc 85.91%, f1 0.8594, precision 0.8598, recall 0.8591, auc 0.8591
epoch 18601, loss 0.2471, train acc 85.89%, f1 0.8595, precision 0.8579, recall 0.8610, auc 0.8589
epoch 18701, loss 0.2937, train acc 85.93%, f1 0.8598, precision 0.8592, recall 0.8604, auc 0.8593
epoch 18801, loss 0.3293, train acc 85.97%, f1 0.8596, precision 0.8624, recall 0.8568, auc 0.8597
epoch 18901, loss 0.3348, train acc 85.94%, f1 0.8597, precision 0.8597, recall 0.8597, auc 0.8594
epoch 19001, loss 0.3190, train acc 85.94%, f1 0.8598, precision 0.8594, recall 0.8601, auc 0.8594
epoch 19101, loss 0.3142, train acc 85.95%, f1 0.8598, precision 0.8605, recall 0.8591, auc 0.8595
epoch 19201, loss 0.2375, train acc 85.93%, f1 0.8595, precision 0.8605, recall 0.8585, auc 0.8593
epoch 19301, loss 0.3861, train acc 85.95%, f1 0.8596, precision 0.8612, recall 0.8580, auc 0.8595
epoch 19401, loss 0.3818, train acc 86.03%, f1 0.8608, precision 0.8597, recall 0.8619, auc 0.8603
epoch 19501, loss 0.3477, train acc 85.99%, f1 0.8601, precision 0.8610, recall 0.8592, auc 0.8599
epoch 19601, loss 0.2911, train acc 85.97%, f1 0.8602, precision 0.8592, recall 0.8611, auc 0.8596
epoch 19701, loss 0.2989, train acc 85.98%, f1 0.8600, precision 0.8610, recall 0.8591, auc 0.8599
epoch 19801, loss 0.3229, train acc 86.00%, f1 0.8603, precision 0.8606, recall 0.8600, auc 0.8600
epoch 19901, loss 0.3291, train acc 86.00%, f1 0.8602, precision 0.8611, recall 0.8593, auc 0.8600
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_5
./test_pima/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6605660377358491

the Fscore is 0.608187134502924

the precision is 0.4406779661016949

the recall is 0.9811320754716981

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_5
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6128, train acc 77.82%, f1 0.7681, precision 0.8048, recall 0.7346, auc 0.7782
epoch 201, loss 0.5285, train acc 80.43%, f1 0.8024, precision 0.8102, recall 0.7947, auc 0.8043
epoch 301, loss 0.3824, train acc 82.33%, f1 0.8229, precision 0.8247, recall 0.8212, auc 0.8233
epoch 401, loss 0.4191, train acc 83.37%, f1 0.8342, precision 0.8317, recall 0.8367, auc 0.8337
epoch 501, loss 0.3249, train acc 83.86%, f1 0.8392, precision 0.8361, recall 0.8424, auc 0.8386
epoch 601, loss 0.3140, train acc 84.02%, f1 0.8409, precision 0.8372, recall 0.8446, auc 0.8402
epoch 701, loss 0.2522, train acc 84.15%, f1 0.8421, precision 0.8392, recall 0.8449, auc 0.8415
epoch 801, loss 0.2965, train acc 84.23%, f1 0.8427, precision 0.8409, recall 0.8445, auc 0.8423
epoch 901, loss 0.1868, train acc 84.22%, f1 0.8433, precision 0.8378, recall 0.8488, auc 0.8422
epoch 1001, loss 0.3864, train acc 84.21%, f1 0.8427, precision 0.8393, recall 0.8462, auc 0.8421
epoch 1101, loss 0.2094, train acc 84.21%, f1 0.8428, precision 0.8390, recall 0.8466, auc 0.8421
epoch 1201, loss 0.3505, train acc 84.19%, f1 0.8425, precision 0.8396, recall 0.8453, auc 0.8419
epoch 1301, loss 0.3331, train acc 84.21%, f1 0.8427, precision 0.8395, recall 0.8459, auc 0.8421
epoch 1401, loss 0.4154, train acc 84.23%, f1 0.8426, precision 0.8411, recall 0.8440, auc 0.8423
epoch 1501, loss 0.3246, train acc 84.24%, f1 0.8427, precision 0.8410, recall 0.8444, auc 0.8424
epoch 1601, loss 0.4267, train acc 84.24%, f1 0.8428, precision 0.8410, recall 0.8445, auc 0.8424
epoch 1701, loss 0.2293, train acc 84.27%, f1 0.8429, precision 0.8420, recall 0.8438, auc 0.8427
epoch 1801, loss 0.3008, train acc 84.21%, f1 0.8428, precision 0.8390, recall 0.8467, auc 0.8421
epoch 1901, loss 0.3006, train acc 84.26%, f1 0.8430, precision 0.8410, recall 0.8450, auc 0.8426
epoch 2001, loss 0.4499, train acc 84.25%, f1 0.8429, precision 0.8406, recall 0.8453, auc 0.8425
epoch 2101, loss 0.4510, train acc 84.23%, f1 0.8426, precision 0.8411, recall 0.8442, auc 0.8423
epoch 2201, loss 0.3599, train acc 84.29%, f1 0.8434, precision 0.8410, recall 0.8457, auc 0.8429
epoch 2301, loss 0.2555, train acc 84.23%, f1 0.8428, precision 0.8401, recall 0.8455, auc 0.8423
epoch 2401, loss 0.3885, train acc 84.24%, f1 0.8427, precision 0.8413, recall 0.8441, auc 0.8424
epoch 2501, loss 0.4507, train acc 84.23%, f1 0.8424, precision 0.8423, recall 0.8424, auc 0.8423
epoch 2601, loss 0.4625, train acc 84.26%, f1 0.8430, precision 0.8410, recall 0.8450, auc 0.8426
epoch 2701, loss 0.4476, train acc 84.27%, f1 0.8430, precision 0.8412, recall 0.8448, auc 0.8427
epoch 2801, loss 0.2490, train acc 84.29%, f1 0.8433, precision 0.8409, recall 0.8457, auc 0.8429
epoch 2901, loss 0.3231, train acc 84.24%, f1 0.8425, precision 0.8423, recall 0.8426, auc 0.8424
epoch 3001, loss 0.3475, train acc 84.28%, f1 0.8433, precision 0.8403, recall 0.8464, auc 0.8428
epoch 3101, loss 0.4546, train acc 84.26%, f1 0.8428, precision 0.8418, recall 0.8439, auc 0.8426
epoch 3201, loss 0.4582, train acc 84.27%, f1 0.8429, precision 0.8416, recall 0.8441, auc 0.8427
epoch 3301, loss 0.4747, train acc 84.25%, f1 0.8427, precision 0.8417, recall 0.8438, auc 0.8425
epoch 3401, loss 0.2714, train acc 84.26%, f1 0.8428, precision 0.8417, recall 0.8438, auc 0.8426
epoch 3501, loss 0.4305, train acc 84.24%, f1 0.8425, precision 0.8419, recall 0.8430, auc 0.8424
epoch 3601, loss 0.2968, train acc 84.19%, f1 0.8421, precision 0.8407, recall 0.8436, auc 0.8419
epoch 3701, loss 0.4847, train acc 84.24%, f1 0.8429, precision 0.8404, recall 0.8455, auc 0.8424
epoch 3801, loss 0.3876, train acc 84.24%, f1 0.8426, precision 0.8417, recall 0.8434, auc 0.8424
epoch 3901, loss 0.4114, train acc 84.26%, f1 0.8427, precision 0.8418, recall 0.8437, auc 0.8426
epoch 4001, loss 0.3850, train acc 84.30%, f1 0.8429, precision 0.8437, recall 0.8420, auc 0.8430
epoch 4101, loss 0.3491, train acc 84.32%, f1 0.8431, precision 0.8437, recall 0.8424, auc 0.8432
epoch 4201, loss 0.4265, train acc 84.30%, f1 0.8435, precision 0.8412, recall 0.8458, auc 0.8430
epoch 4301, loss 0.3236, train acc 84.29%, f1 0.8429, precision 0.8428, recall 0.8429, auc 0.8429
epoch 4401, loss 0.3638, train acc 84.28%, f1 0.8428, precision 0.8428, recall 0.8428, auc 0.8428
epoch 4501, loss 0.4315, train acc 84.37%, f1 0.8438, precision 0.8432, recall 0.8444, auc 0.8437
epoch 4601, loss 0.3367, train acc 84.30%, f1 0.8431, precision 0.8427, recall 0.8435, auc 0.8430
epoch 4701, loss 0.2910, train acc 84.33%, f1 0.8430, precision 0.8448, recall 0.8412, auc 0.8433
epoch 4801, loss 0.3407, train acc 84.36%, f1 0.8438, precision 0.8429, recall 0.8446, auc 0.8436
epoch 4901, loss 0.3821, train acc 84.36%, f1 0.8439, precision 0.8422, recall 0.8456, auc 0.8436
epoch 5001, loss 0.2673, train acc 84.39%, f1 0.8439, precision 0.8437, recall 0.8442, auc 0.8439
epoch 5101, loss 0.3943, train acc 84.43%, f1 0.8445, precision 0.8431, recall 0.8459, auc 0.8443
epoch 5201, loss 0.3923, train acc 84.42%, f1 0.8447, precision 0.8422, recall 0.8473, auc 0.8442
epoch 5301, loss 0.3238, train acc 84.42%, f1 0.8445, precision 0.8432, recall 0.8458, auc 0.8442
epoch 5401, loss 0.4125, train acc 84.46%, f1 0.8449, precision 0.8431, recall 0.8467, auc 0.8446
epoch 5501, loss 0.3166, train acc 84.44%, f1 0.8446, precision 0.8434, recall 0.8458, auc 0.8444
epoch 5601, loss 0.3348, train acc 84.51%, f1 0.8452, precision 0.8446, recall 0.8459, auc 0.8451
epoch 5701, loss 0.3190, train acc 84.53%, f1 0.8457, precision 0.8436, recall 0.8478, auc 0.8453
epoch 5801, loss 0.3384, train acc 84.51%, f1 0.8455, precision 0.8433, recall 0.8477, auc 0.8451
epoch 5901, loss 0.2939, train acc 84.57%, f1 0.8458, precision 0.8451, recall 0.8465, auc 0.8457
epoch 6001, loss 0.3260, train acc 84.55%, f1 0.8454, precision 0.8458, recall 0.8450, auc 0.8455
epoch 6101, loss 0.3877, train acc 84.67%, f1 0.8468, precision 0.8460, recall 0.8476, auc 0.8467
epoch 6201, loss 0.2784, train acc 84.65%, f1 0.8467, precision 0.8457, recall 0.8477, auc 0.8465
epoch 6301, loss 0.3749, train acc 84.67%, f1 0.8464, precision 0.8482, recall 0.8446, auc 0.8467
epoch 6401, loss 0.3610, train acc 84.66%, f1 0.8469, precision 0.8451, recall 0.8487, auc 0.8466
epoch 6501, loss 0.3600, train acc 84.68%, f1 0.8467, precision 0.8469, recall 0.8466, auc 0.8468
epoch 6601, loss 0.3607, train acc 84.70%, f1 0.8476, precision 0.8444, recall 0.8508, auc 0.8470
epoch 6701, loss 0.3682, train acc 84.77%, f1 0.8479, precision 0.8467, recall 0.8491, auc 0.8477
epoch 6801, loss 0.3042, train acc 84.71%, f1 0.8474, precision 0.8458, recall 0.8490, auc 0.8471
epoch 6901, loss 0.3209, train acc 84.75%, f1 0.8471, precision 0.8494, recall 0.8448, auc 0.8475
epoch 7001, loss 0.2660, train acc 84.79%, f1 0.8478, precision 0.8479, recall 0.8478, auc 0.8479
epoch 7101, loss 0.3171, train acc 84.81%, f1 0.8486, precision 0.8458, recall 0.8514, auc 0.8481
epoch 7201, loss 0.3201, train acc 84.79%, f1 0.8479, precision 0.8479, recall 0.8480, auc 0.8479
epoch 7301, loss 0.3367, train acc 84.87%, f1 0.8488, precision 0.8482, recall 0.8493, auc 0.8487
epoch 7401, loss 0.2824, train acc 84.95%, f1 0.8495, precision 0.8498, recall 0.8491, auc 0.8495
epoch 7501, loss 0.2784, train acc 84.97%, f1 0.8502, precision 0.8472, recall 0.8532, auc 0.8497
epoch 7601, loss 0.3085, train acc 84.97%, f1 0.8497, precision 0.8492, recall 0.8502, auc 0.8497
epoch 7701, loss 0.3679, train acc 85.04%, f1 0.8509, precision 0.8483, recall 0.8535, auc 0.8504
epoch 7801, loss 0.3350, train acc 85.04%, f1 0.8508, precision 0.8488, recall 0.8527, auc 0.8504
epoch 7901, loss 0.4188, train acc 85.07%, f1 0.8511, precision 0.8489, recall 0.8533, auc 0.8507
epoch 8001, loss 0.3132, train acc 85.10%, f1 0.8513, precision 0.8497, recall 0.8530, auc 0.8510
epoch 8101, loss 0.3078, train acc 85.12%, f1 0.8514, precision 0.8500, recall 0.8529, auc 0.8512
epoch 8201, loss 0.3492, train acc 85.05%, f1 0.8508, precision 0.8492, recall 0.8524, auc 0.8505/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2914, train acc 85.13%, f1 0.8514, precision 0.8511, recall 0.8517, auc 0.8513
epoch 8401, loss 0.2951, train acc 85.10%, f1 0.8512, precision 0.8501, recall 0.8523, auc 0.8510
epoch 8501, loss 0.3153, train acc 85.05%, f1 0.8505, precision 0.8506, recall 0.8504, auc 0.8505
epoch 8601, loss 0.2862, train acc 85.13%, f1 0.8518, precision 0.8493, recall 0.8543, auc 0.8513
epoch 8701, loss 0.3266, train acc 85.18%, f1 0.8519, precision 0.8513, recall 0.8525, auc 0.8518
epoch 8801, loss 0.2974, train acc 85.15%, f1 0.8517, precision 0.8507, recall 0.8528, auc 0.8515
epoch 8901, loss 0.4261, train acc 85.23%, f1 0.8523, precision 0.8523, recall 0.8523, auc 0.8523
epoch 9001, loss 0.4368, train acc 85.19%, f1 0.8522, precision 0.8506, recall 0.8538, auc 0.8519
epoch 9101, loss 0.3542, train acc 85.25%, f1 0.8532, precision 0.8496, recall 0.8568, auc 0.8525
epoch 9201, loss 0.2757, train acc 85.25%, f1 0.8523, precision 0.8532, recall 0.8515, auc 0.8525
epoch 9301, loss 0.3876, train acc 85.26%, f1 0.8530, precision 0.8505, recall 0.8554, auc 0.8526
epoch 9401, loss 0.3171, train acc 85.28%, f1 0.8526, precision 0.8535, recall 0.8517, auc 0.8528
epoch 9501, loss 0.3520, train acc 85.28%, f1 0.8530, precision 0.8516, recall 0.8545, auc 0.8528
epoch 9601, loss 0.3898, train acc 85.30%, f1 0.8528, precision 0.8544, recall 0.8512, auc 0.8530
epoch 9701, loss 0.3397, train acc 85.33%, f1 0.8535, precision 0.8520, recall 0.8550, auc 0.8533
epoch 9801, loss 0.3954, train acc 85.31%, f1 0.8531, precision 0.8529, recall 0.8533, auc 0.8531
epoch 9901, loss 0.2911, train acc 85.34%, f1 0.8537, precision 0.8517, recall 0.8558, auc 0.8534
epoch 10001, loss 0.3140, train acc 85.38%, f1 0.8542, precision 0.8517, recall 0.8567, auc 0.8538
epoch 10101, loss 0.4051, train acc 85.37%, f1 0.8535, precision 0.8541, recall 0.8530, auc 0.8537
epoch 10201, loss 0.1876, train acc 85.35%, f1 0.8540, precision 0.8514, recall 0.8566, auc 0.8535
epoch 10301, loss 0.3482, train acc 85.38%, f1 0.8542, precision 0.8514, recall 0.8571, auc 0.8538
epoch 10401, loss 0.2918, train acc 85.37%, f1 0.8538, precision 0.8530, recall 0.8546, auc 0.8537
epoch 10501, loss 0.4018, train acc 85.36%, f1 0.8537, precision 0.8531, recall 0.8542, auc 0.8536
epoch 10601, loss 0.3035, train acc 85.38%, f1 0.8538, precision 0.8534, recall 0.8543, auc 0.8538
epoch 10701, loss 0.2253, train acc 85.41%, f1 0.8543, precision 0.8528, recall 0.8558, auc 0.8541
epoch 10801, loss 0.2615, train acc 85.43%, f1 0.8546, precision 0.8527, recall 0.8565, auc 0.8543
epoch 10901, loss 0.3925, train acc 85.37%, f1 0.8537, precision 0.8537, recall 0.8537, auc 0.8537
epoch 11001, loss 0.3214, train acc 85.45%, f1 0.8549, precision 0.8526, recall 0.8572, auc 0.8545
epoch 11101, loss 0.3594, train acc 85.41%, f1 0.8543, precision 0.8533, recall 0.8553, auc 0.8541
epoch 11201, loss 0.3591, train acc 85.42%, f1 0.8544, precision 0.8536, recall 0.8552, auc 0.8542
epoch 11301, loss 0.4700, train acc 85.45%, f1 0.8548, precision 0.8527, recall 0.8569, auc 0.8545
epoch 11401, loss 0.3518, train acc 85.48%, f1 0.8551, precision 0.8533, recall 0.8569, auc 0.8548
epoch 11501, loss 0.1975, train acc 85.49%, f1 0.8552, precision 0.8532, recall 0.8573, auc 0.8549
epoch 11601, loss 0.3237, train acc 85.44%, f1 0.8545, precision 0.8535, recall 0.8556, auc 0.8544
epoch 11701, loss 0.2742, train acc 85.46%, f1 0.8550, precision 0.8528, recall 0.8571, auc 0.8546
epoch 11801, loss 0.2914, train acc 85.47%, f1 0.8553, precision 0.8519, recall 0.8588, auc 0.8547
epoch 11901, loss 0.2622, train acc 85.54%, f1 0.8557, precision 0.8540, recall 0.8573, auc 0.8554
epoch 12001, loss 0.2949, train acc 85.55%, f1 0.8560, precision 0.8531, recall 0.8589, auc 0.8555
epoch 12101, loss 0.2788, train acc 85.53%, f1 0.8554, precision 0.8547, recall 0.8561, auc 0.8553
epoch 12201, loss 0.2785, train acc 85.52%, f1 0.8557, precision 0.8528, recall 0.8586, auc 0.8552
epoch 12301, loss 0.3769, train acc 85.55%, f1 0.8560, precision 0.8534, recall 0.8586, auc 0.8555
epoch 12401, loss 0.4730, train acc 85.56%, f1 0.8557, precision 0.8548, recall 0.8567, auc 0.8556
epoch 12501, loss 0.3896, train acc 85.56%, f1 0.8559, precision 0.8543, recall 0.8576, auc 0.8556
epoch 12601, loss 0.3150, train acc 85.58%, f1 0.8560, precision 0.8549, recall 0.8570, auc 0.8558
epoch 12701, loss 0.3510, train acc 85.57%, f1 0.8559, precision 0.8547, recall 0.8571, auc 0.8557
epoch 12801, loss 0.3236, train acc 85.60%, f1 0.8565, precision 0.8535, recall 0.8595, auc 0.8560
epoch 12901, loss 0.2816, train acc 85.60%, f1 0.8562, precision 0.8552, recall 0.8571, auc 0.8560
epoch 13001, loss 0.5098, train acc 85.61%, f1 0.8562, precision 0.8554, recall 0.8571, auc 0.8561
epoch 13101, loss 0.3370, train acc 85.60%, f1 0.8561, precision 0.8556, recall 0.8567, auc 0.8560
epoch 13201, loss 0.3179, train acc 85.61%, f1 0.8562, precision 0.8555, recall 0.8569, auc 0.8561
epoch 13301, loss 0.2695, train acc 85.60%, f1 0.8567, precision 0.8522, recall 0.8613, auc 0.8560
epoch 13401, loss 0.3223, train acc 85.63%, f1 0.8564, precision 0.8556, recall 0.8572, auc 0.8563
epoch 13501, loss 0.3060, train acc 85.64%, f1 0.8568, precision 0.8549, recall 0.8586, auc 0.8564
epoch 13601, loss 0.3769, train acc 85.69%, f1 0.8571, precision 0.8562, recall 0.8580, auc 0.8569
epoch 13701, loss 0.3763, train acc 85.70%, f1 0.8571, precision 0.8561, recall 0.8582, auc 0.8570
epoch 13801, loss 0.2413, train acc 85.71%, f1 0.8571, precision 0.8569, recall 0.8573, auc 0.8571
epoch 13901, loss 0.3189, train acc 85.67%, f1 0.8574, precision 0.8530, recall 0.8619, auc 0.8567
epoch 14001, loss 0.3129, train acc 85.69%, f1 0.8571, precision 0.8556, recall 0.8586, auc 0.8569
epoch 14101, loss 0.3242, train acc 85.70%, f1 0.8572, precision 0.8564, recall 0.8580, auc 0.8570
epoch 14201, loss 0.3025, train acc 85.72%, f1 0.8575, precision 0.8555, recall 0.8594, auc 0.8572
epoch 14301, loss 0.2746, train acc 85.69%, f1 0.8573, precision 0.8546, recall 0.8602, auc 0.8569
epoch 14401, loss 0.3302, train acc 85.68%, f1 0.8570, precision 0.8556, recall 0.8585, auc 0.8568
epoch 14501, loss 0.2566, train acc 85.69%, f1 0.8574, precision 0.8544, recall 0.8604, auc 0.8569
epoch 14601, loss 0.2965, train acc 85.68%, f1 0.8570, precision 0.8556, recall 0.8584, auc 0.8568
epoch 14701, loss 0.2812, train acc 85.71%, f1 0.8571, precision 0.8568, recall 0.8575, auc 0.8571
epoch 14801, loss 0.3131, train acc 85.70%, f1 0.8573, precision 0.8551, recall 0.8596, auc 0.8570
epoch 14901, loss 0.3791, train acc 85.77%, f1 0.8581, precision 0.8556, recall 0.8606, auc 0.8577
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_5
./test_pima/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6261320754716981

the Fscore is 0.5828571428571427

the precision is 0.4180327868852459

the recall is 0.9622641509433962

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_5
----------------------



epoch 1, loss 0.6929, train acc 50.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5984, train acc 77.95%, f1 0.7759, precision 0.7828, recall 0.7691, auc 0.7795
epoch 201, loss 0.5429, train acc 80.76%, f1 0.8064, precision 0.8054, recall 0.8073, auc 0.8076
epoch 301, loss 0.3772, train acc 82.38%, f1 0.8229, precision 0.8210, recall 0.8249, auc 0.8239
epoch 401, loss 0.3550, train acc 83.39%, f1 0.8333, precision 0.8301, recall 0.8365, auc 0.8339
epoch 501, loss 0.4365, train acc 83.86%, f1 0.8381, precision 0.8341, recall 0.8422, auc 0.8386
epoch 601, loss 0.3712, train acc 84.01%, f1 0.8398, precision 0.8353, recall 0.8444, auc 0.8401
epoch 701, loss 0.3302, train acc 84.14%, f1 0.8407, precision 0.8383, recall 0.8431, auc 0.8415
epoch 801, loss 0.3724, train acc 84.17%, f1 0.8407, precision 0.8396, recall 0.8418, auc 0.8417
epoch 901, loss 0.3450, train acc 84.20%, f1 0.8415, precision 0.8379, recall 0.8451, auc 0.8420
epoch 1001, loss 0.4053, train acc 84.22%, f1 0.8418, precision 0.8374, recall 0.8463, auc 0.8422
epoch 1101, loss 0.3474, train acc 84.29%, f1 0.8426, precision 0.8380, recall 0.8472, auc 0.8429
epoch 1201, loss 0.3223, train acc 84.22%, f1 0.8414, precision 0.8395, recall 0.8433, auc 0.8422
epoch 1301, loss 0.3749, train acc 84.21%, f1 0.8417, precision 0.8377, recall 0.8457, auc 0.8421
epoch 1401, loss 0.4324, train acc 84.19%, f1 0.8413, precision 0.8384, recall 0.8441, auc 0.8419
epoch 1501, loss 0.2692, train acc 84.26%, f1 0.8423, precision 0.8377, recall 0.8471, auc 0.8427
epoch 1601, loss 0.2952, train acc 84.22%, f1 0.8415, precision 0.8389, recall 0.8440, auc 0.8422
epoch 1701, loss 0.4382, train acc 84.25%, f1 0.8419, precision 0.8390, recall 0.8448, auc 0.8425
epoch 1801, loss 0.3843, train acc 84.26%, f1 0.8422, precision 0.8379, recall 0.8465, auc 0.8426
epoch 1901, loss 0.4115, train acc 84.23%, f1 0.8417, precision 0.8384, recall 0.8451, auc 0.8423
epoch 2001, loss 0.3182, train acc 84.24%, f1 0.8417, precision 0.8392, recall 0.8442, auc 0.8424
epoch 2101, loss 0.3277, train acc 84.23%, f1 0.8413, precision 0.8401, recall 0.8425, auc 0.8423
epoch 2201, loss 0.3218, train acc 84.14%, f1 0.8405, precision 0.8391, recall 0.8420, auc 0.8414
epoch 2301, loss 0.3492, train acc 84.20%, f1 0.8411, precision 0.8396, recall 0.8427, auc 0.8420
epoch 2401, loss 0.2956, train acc 84.25%, f1 0.8419, precision 0.8389, recall 0.8451, auc 0.8426
epoch 2501, loss 0.3178, train acc 84.20%, f1 0.8415, precision 0.8379, recall 0.8451, auc 0.8420
epoch 2601, loss 0.4591, train acc 84.27%, f1 0.8422, precision 0.8384, recall 0.8460, auc 0.8427
epoch 2701, loss 0.3397, train acc 84.26%, f1 0.8418, precision 0.8397, recall 0.8439, auc 0.8426
epoch 2801, loss 0.4823, train acc 84.27%, f1 0.8421, precision 0.8388, recall 0.8455, auc 0.8427
epoch 2901, loss 0.1930, train acc 84.23%, f1 0.8419, precision 0.8377, recall 0.8462, auc 0.8423
epoch 3001, loss 0.3145, train acc 84.21%, f1 0.8412, precision 0.8397, recall 0.8426, auc 0.8421
epoch 3101, loss 0.3844, train acc 84.26%, f1 0.8421, precision 0.8387, recall 0.8455, auc 0.8426
epoch 3201, loss 0.2997, train acc 84.27%, f1 0.8417, precision 0.8410, recall 0.8424, auc 0.8427
epoch 3301, loss 0.2759, train acc 84.31%, f1 0.8426, precision 0.8389, recall 0.8462, auc 0.8431
epoch 3401, loss 0.3986, train acc 84.34%, f1 0.8427, precision 0.8400, recall 0.8455, auc 0.8434
epoch 3501, loss 0.4191, train acc 84.33%, f1 0.8424, precision 0.8407, recall 0.8441, auc 0.8433
epoch 3601, loss 0.5591, train acc 84.25%, f1 0.8417, precision 0.8401, recall 0.8432, auc 0.8426
epoch 3701, loss 0.3644, train acc 84.28%, f1 0.8424, precision 0.8385, recall 0.8463, auc 0.8429
epoch 3801, loss 0.2808, train acc 84.32%, f1 0.8430, precision 0.8378, recall 0.8483, auc 0.8433
epoch 3901, loss 0.3806, train acc 84.32%, f1 0.8427, precision 0.8392, recall 0.8463, auc 0.8433
epoch 4001, loss 0.4076, train acc 84.26%, f1 0.8418, precision 0.8397, recall 0.8440, auc 0.8426
epoch 4101, loss 0.3711, train acc 84.29%, f1 0.8429, precision 0.8367, recall 0.8492, auc 0.8429
epoch 4201, loss 0.3723, train acc 84.25%, f1 0.8418, precision 0.8390, recall 0.8447, auc 0.8425
epoch 4301, loss 0.3903, train acc 84.22%, f1 0.8417, precision 0.8379, recall 0.8456, auc 0.8422
epoch 4401, loss 0.3764, train acc 84.27%, f1 0.8421, precision 0.8390, recall 0.8451, auc 0.8427
epoch 4501, loss 0.3984, train acc 84.32%, f1 0.8427, precision 0.8391, recall 0.8463, auc 0.8432
epoch 4601, loss 0.3638, train acc 84.38%, f1 0.8432, precision 0.8399, recall 0.8465, auc 0.8438
epoch 4701, loss 0.3214, train acc 84.41%, f1 0.8430, precision 0.8426, recall 0.8434, auc 0.8441
epoch 4801, loss 0.4243, train acc 84.41%, f1 0.8433, precision 0.8412, recall 0.8454, auc 0.8441
epoch 4901, loss 0.3867, train acc 84.39%, f1 0.8430, precision 0.8415, recall 0.8445, auc 0.8439
epoch 5001, loss 0.3638, train acc 84.43%, f1 0.8434, precision 0.8422, recall 0.8446, auc 0.8443
epoch 5101, loss 0.3990, train acc 84.50%, f1 0.8446, precision 0.8404, recall 0.8488, auc 0.8450
epoch 5201, loss 0.3746, train acc 84.43%, f1 0.8441, precision 0.8390, recall 0.8493, auc 0.8444
epoch 5301, loss 0.4306, train acc 84.48%, f1 0.8445, precision 0.8398, recall 0.8493, auc 0.8448
epoch 5401, loss 0.3817, train acc 84.44%, f1 0.8437, precision 0.8411, recall 0.8463, auc 0.8444
epoch 5501, loss 0.3214, train acc 84.42%, f1 0.8439, precision 0.8394, recall 0.8483, auc 0.8442
epoch 5601, loss 0.4033, train acc 84.52%, f1 0.8443, precision 0.8432, recall 0.8453, auc 0.8452
epoch 5701, loss 0.3698, train acc 84.55%, f1 0.8450, precision 0.8417, recall 0.8483, auc 0.8455
epoch 5801, loss 0.3470, train acc 84.60%, f1 0.8452, precision 0.8433, recall 0.8471, auc 0.8460
epoch 5901, loss 0.4154, train acc 84.62%, f1 0.8459, precision 0.8414, recall 0.8505, auc 0.8463
epoch 6001, loss 0.3214, train acc 84.59%, f1 0.8450, precision 0.8438, recall 0.8462, auc 0.8459
epoch 6101, loss 0.2618, train acc 84.65%, f1 0.8456, precision 0.8440, recall 0.8473, auc 0.8465
epoch 6201, loss 0.4371, train acc 84.68%, f1 0.8462, precision 0.8433, recall 0.8491, auc 0.8468
epoch 6301, loss 0.2926, train acc 84.70%, f1 0.8464, precision 0.8433, recall 0.8496, auc 0.8470
epoch 6401, loss 0.3421, train acc 84.73%, f1 0.8465, precision 0.8444, recall 0.8486, auc 0.8473
epoch 6501, loss 0.2484, train acc 84.75%, f1 0.8470, precision 0.8434, recall 0.8506, auc 0.8475
epoch 6601, loss 0.2935, train acc 84.72%, f1 0.8460, precision 0.8463, recall 0.8457, auc 0.8472
epoch 6701, loss 0.3171, train acc 84.75%, f1 0.8468, precision 0.8444, recall 0.8491, auc 0.8475
epoch 6801, loss 0.3581, train acc 84.80%, f1 0.8473, precision 0.8449, recall 0.8498, auc 0.8481
epoch 6901, loss 0.4117, train acc 84.77%, f1 0.8473, precision 0.8435, recall 0.8511, auc 0.8478
epoch 7001, loss 0.3365, train acc 84.77%, f1 0.8466, precision 0.8462, recall 0.8470, auc 0.8477
epoch 7101, loss 0.3717, train acc 84.78%, f1 0.8471, precision 0.8448, recall 0.8494, auc 0.8478
epoch 7201, loss 0.3591, train acc 84.89%, f1 0.8483, precision 0.8453, recall 0.8513, auc 0.8489
epoch 7301, loss 0.3629, train acc 84.90%, f1 0.8484, precision 0.8456, recall 0.8511, auc 0.8490
epoch 7401, loss 0.4066, train acc 84.94%, f1 0.8487, precision 0.8461, recall 0.8513, auc 0.8494
epoch 7501, loss 0.3434, train acc 84.94%, f1 0.8485, precision 0.8475, recall 0.8495, auc 0.8494
epoch 7601, loss 0.4406, train acc 84.93%, f1 0.8488, precision 0.8454, recall 0.8523, auc 0.8493
epoch 7701, loss 0.3504, train acc 85.03%, f1 0.8496, precision 0.8475, recall 0.8516, auc 0.8503
epoch 7801, loss 0.2975, train acc 84.99%, f1 0.8491, precision 0.8471, recall 0.8512, auc 0.8499
epoch 7901, loss 0.3146, train acc 85.01%, f1 0.8492, precision 0.8475, recall 0.8510, auc 0.8501
epoch 8001, loss 0.3174, train acc 85.03%, f1 0.8498, precision 0.8467, recall 0.8529, auc 0.8504
epoch 8101, loss 0.2206, train acc 85.03%, f1 0.8494, precision 0.8483, recall 0.8504, auc 0.8503
epoch 8201, loss 0.5316, train acc 85.08%, f1 0.8496, precision 0.8500, recall 0.8493, auc 0.8508/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.3905, train acc 85.04%, f1 0.8497, precision 0.8474, recall 0.8520, auc 0.8504
epoch 8401, loss 0.3065, train acc 85.05%, f1 0.8501, precision 0.8462, recall 0.8541, auc 0.8505
epoch 8501, loss 0.3465, train acc 85.08%, f1 0.8501, precision 0.8476, recall 0.8527, auc 0.8508
epoch 8601, loss 0.3385, train acc 85.15%, f1 0.8507, precision 0.8485, recall 0.8530, auc 0.8515
epoch 8701, loss 0.3161, train acc 85.16%, f1 0.8511, precision 0.8476, recall 0.8547, auc 0.8516
epoch 8801, loss 0.3329, train acc 85.16%, f1 0.8510, precision 0.8481, recall 0.8540, auc 0.8516
epoch 8901, loss 0.3474, train acc 85.18%, f1 0.8511, precision 0.8485, recall 0.8537, auc 0.8518
epoch 9001, loss 0.2659, train acc 85.20%, f1 0.8512, precision 0.8497, recall 0.8526, auc 0.8520
epoch 9101, loss 0.3644, train acc 85.22%, f1 0.8515, precision 0.8490, recall 0.8541, auc 0.8522
epoch 9201, loss 0.3685, train acc 85.23%, f1 0.8518, precision 0.8480, recall 0.8557, auc 0.8523
epoch 9301, loss 0.3100, train acc 85.25%, f1 0.8518, precision 0.8497, recall 0.8539, auc 0.8525
epoch 9401, loss 0.3196, train acc 85.30%, f1 0.8528, precision 0.8477, recall 0.8580, auc 0.8531
epoch 9501, loss 0.2704, train acc 85.26%, f1 0.8521, precision 0.8488, recall 0.8554, auc 0.8526
epoch 9601, loss 0.3512, train acc 85.26%, f1 0.8515, precision 0.8513, recall 0.8518, auc 0.8526
epoch 9701, loss 0.3282, train acc 85.22%, f1 0.8517, precision 0.8483, recall 0.8552, auc 0.8523
epoch 9801, loss 0.4020, train acc 85.29%, f1 0.8523, precision 0.8495, recall 0.8551, auc 0.8529
epoch 9901, loss 0.4615, train acc 85.30%, f1 0.8521, precision 0.8506, recall 0.8537, auc 0.8530
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_5
./test_pima/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6505660377358491

the Fscore is 0.6011560693641619

the precision is 0.43333333333333335

the recall is 0.9811320754716981

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_5
----------------------



epoch 1, loss 0.6934, train acc 49.99%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6268, train acc 77.70%, f1 0.7565, precision 0.8334, recall 0.6926, auc 0.7770
epoch 201, loss 0.4740, train acc 80.93%, f1 0.8062, precision 0.8196, recall 0.7933, auc 0.8093
epoch 301, loss 0.4628, train acc 82.42%, f1 0.8244, precision 0.8235, recall 0.8254, auc 0.8242
epoch 401, loss 0.3362, train acc 83.43%, f1 0.8348, precision 0.8324, recall 0.8372, auc 0.8343
epoch 501, loss 0.3632, train acc 83.77%, f1 0.8384, precision 0.8349, recall 0.8419, auc 0.8377
epoch 601, loss 0.3009, train acc 84.00%, f1 0.8415, precision 0.8338, recall 0.8494, auc 0.8400
epoch 701, loss 0.3411, train acc 84.15%, f1 0.8426, precision 0.8367, recall 0.8486, auc 0.8415
epoch 801, loss 0.3176, train acc 84.17%, f1 0.8425, precision 0.8384, recall 0.8466, auc 0.8417
epoch 901, loss 0.5318, train acc 84.15%, f1 0.8425, precision 0.8377, recall 0.8473, auc 0.8415
epoch 1001, loss 0.3865, train acc 84.18%, f1 0.8429, precision 0.8372, recall 0.8487, auc 0.8418
epoch 1101, loss 0.3252, train acc 84.21%, f1 0.8427, precision 0.8399, recall 0.8454, auc 0.8421
epoch 1201, loss 0.4006, train acc 84.26%, f1 0.8431, precision 0.8405, recall 0.8458, auc 0.8426
epoch 1301, loss 0.4201, train acc 84.23%, f1 0.8434, precision 0.8377, recall 0.8492, auc 0.8423
epoch 1401, loss 0.3775, train acc 84.24%, f1 0.8435, precision 0.8377, recall 0.8493, auc 0.8424
epoch 1501, loss 0.3455, train acc 84.25%, f1 0.8432, precision 0.8397, recall 0.8467, auc 0.8425
epoch 1601, loss 0.4832, train acc 84.22%, f1 0.8428, precision 0.8398, recall 0.8458, auc 0.8422
epoch 1701, loss 0.2251, train acc 84.24%, f1 0.8432, precision 0.8394, recall 0.8470, auc 0.8424
epoch 1801, loss 0.4962, train acc 84.26%, f1 0.8428, precision 0.8420, recall 0.8435, auc 0.8426
epoch 1901, loss 0.4499, train acc 84.23%, f1 0.8433, precision 0.8384, recall 0.8482, auc 0.8423
epoch 2001, loss 0.3483, train acc 84.27%, f1 0.8433, precision 0.8400, recall 0.8467, auc 0.8427
epoch 2101, loss 0.3487, train acc 84.23%, f1 0.8430, precision 0.8392, recall 0.8469, auc 0.8423
epoch 2201, loss 0.3604, train acc 84.20%, f1 0.8429, precision 0.8382, recall 0.8475, auc 0.8420
epoch 2301, loss 0.5964, train acc 84.25%, f1 0.8429, precision 0.8410, recall 0.8448, auc 0.8425
epoch 2401, loss 0.3452, train acc 84.28%, f1 0.8432, precision 0.8412, recall 0.8452, auc 0.8428
epoch 2501, loss 0.2636, train acc 84.24%, f1 0.8425, precision 0.8419, recall 0.8431, auc 0.8424
epoch 2601, loss 0.3777, train acc 84.24%, f1 0.8430, precision 0.8401, recall 0.8459, auc 0.8424
epoch 2701, loss 0.3541, train acc 84.25%, f1 0.8430, precision 0.8408, recall 0.8451, auc 0.8425
epoch 2801, loss 0.3782, train acc 84.23%, f1 0.8426, precision 0.8415, recall 0.8436, auc 0.8423
epoch 2901, loss 0.2822, train acc 84.24%, f1 0.8429, precision 0.8405, recall 0.8453, auc 0.8424
epoch 3001, loss 0.3162, train acc 84.27%, f1 0.8436, precision 0.8392, recall 0.8481, auc 0.8427
epoch 3101, loss 0.3006, train acc 84.26%, f1 0.8431, precision 0.8407, recall 0.8456, auc 0.8426
epoch 3201, loss 0.3519, train acc 84.31%, f1 0.8433, precision 0.8424, recall 0.8442, auc 0.8431
epoch 3301, loss 0.4177, train acc 84.28%, f1 0.8429, precision 0.8424, recall 0.8434, auc 0.8428
epoch 3401, loss 0.3428, train acc 84.27%, f1 0.8429, precision 0.8418, recall 0.8440, auc 0.8427
epoch 3501, loss 0.2819, train acc 84.30%, f1 0.8433, precision 0.8419, recall 0.8447, auc 0.8430
epoch 3601, loss 0.4536, train acc 84.32%, f1 0.8438, precision 0.8407, recall 0.8469, auc 0.8432
epoch 3701, loss 0.2552, train acc 84.33%, f1 0.8435, precision 0.8425, recall 0.8444, auc 0.8433
epoch 3801, loss 0.3320, train acc 84.28%, f1 0.8434, precision 0.8403, recall 0.8466, auc 0.8428
epoch 3901, loss 0.2880, train acc 84.30%, f1 0.8433, precision 0.8420, recall 0.8446, auc 0.8430
epoch 4001, loss 0.3043, train acc 84.31%, f1 0.8430, precision 0.8437, recall 0.8422, auc 0.8431
epoch 4101, loss 0.3952, train acc 84.25%, f1 0.8428, precision 0.8412, recall 0.8444, auc 0.8425
epoch 4201, loss 0.3161, train acc 84.28%, f1 0.8435, precision 0.8402, recall 0.8468, auc 0.8428
epoch 4301, loss 0.3610, train acc 84.31%, f1 0.8429, precision 0.8443, recall 0.8415, auc 0.8431
epoch 4401, loss 0.3225, train acc 84.37%, f1 0.8436, precision 0.8444, recall 0.8428, auc 0.8437
epoch 4501, loss 0.4463, train acc 84.42%, f1 0.8441, precision 0.8446, recall 0.8436, auc 0.8442
epoch 4601, loss 0.3526, train acc 84.43%, f1 0.8449, precision 0.8423, recall 0.8474, auc 0.8443
epoch 4701, loss 0.3123, train acc 84.47%, f1 0.8449, precision 0.8441, recall 0.8456, auc 0.8447
epoch 4801, loss 0.4280, train acc 84.46%, f1 0.8447, precision 0.8446, recall 0.8449, auc 0.8446
epoch 4901, loss 0.2440, train acc 84.39%, f1 0.8442, precision 0.8430, recall 0.8453, auc 0.8439
epoch 5001, loss 0.3243, train acc 84.41%, f1 0.8448, precision 0.8413, recall 0.8484, auc 0.8441
epoch 5101, loss 0.4829, train acc 84.32%, f1 0.8437, precision 0.8414, recall 0.8460, auc 0.8432
epoch 5201, loss 0.3379, train acc 84.40%, f1 0.8438, precision 0.8448, recall 0.8428, auc 0.8440
epoch 5301, loss 0.3483, train acc 84.46%, f1 0.8445, precision 0.8452, recall 0.8437, auc 0.8446
epoch 5401, loss 0.4138, train acc 84.50%, f1 0.8450, precision 0.8453, recall 0.8448, auc 0.8450
epoch 5501, loss 0.2464, train acc 84.52%, f1 0.8456, precision 0.8438, recall 0.8474, auc 0.8452
epoch 5601, loss 0.3163, train acc 84.49%, f1 0.8451, precision 0.8439, recall 0.8464, auc 0.8449
epoch 5701, loss 0.3790, train acc 84.50%, f1 0.8449, precision 0.8453, recall 0.8445, auc 0.8450
epoch 5801, loss 0.2861, train acc 84.53%, f1 0.8454, precision 0.8452, recall 0.8457, auc 0.8453
epoch 5901, loss 0.3297, train acc 84.56%, f1 0.8458, precision 0.8447, recall 0.8470, auc 0.8456
epoch 6001, loss 0.3492, train acc 84.54%, f1 0.8457, precision 0.8444, recall 0.8471, auc 0.8454
epoch 6101, loss 0.1969, train acc 84.52%, f1 0.8455, precision 0.8439, recall 0.8472, auc 0.8452
epoch 6201, loss 0.3028, train acc 84.57%, f1 0.8454, precision 0.8469, recall 0.8439, auc 0.8457
epoch 6301, loss 0.3776, train acc 84.62%, f1 0.8465, precision 0.8449, recall 0.8481, auc 0.8462
epoch 6401, loss 0.2853, train acc 84.64%, f1 0.8469, precision 0.8442, recall 0.8496, auc 0.8464
epoch 6501, loss 0.4161, train acc 84.61%, f1 0.8467, precision 0.8440, recall 0.8494, auc 0.8461
epoch 6601, loss 0.3861, train acc 84.65%, f1 0.8469, precision 0.8449, recall 0.8489, auc 0.8465
epoch 6701, loss 0.3687, train acc 84.69%, f1 0.8471, precision 0.8464, recall 0.8478, auc 0.8469
epoch 6801, loss 0.3032, train acc 84.72%, f1 0.8471, precision 0.8480, recall 0.8462, auc 0.8472
epoch 6901, loss 0.3325, train acc 84.79%, f1 0.8478, precision 0.8487, recall 0.8468, auc 0.8479
epoch 7001, loss 0.3927, train acc 84.77%, f1 0.8475, precision 0.8488, recall 0.8462, auc 0.8477
epoch 7101, loss 0.2994, train acc 84.80%, f1 0.8482, precision 0.8473, recall 0.8491, auc 0.8480
epoch 7201, loss 0.2830, train acc 84.82%, f1 0.8488, precision 0.8455, recall 0.8522, auc 0.8482
epoch 7301, loss 0.3391, train acc 84.78%, f1 0.8481, precision 0.8464, recall 0.8498, auc 0.8478
epoch 7401, loss 0.3521, train acc 84.78%, f1 0.8479, precision 0.8474, recall 0.8485, auc 0.8478
epoch 7501, loss 0.3100, train acc 84.86%, f1 0.8484, precision 0.8493, recall 0.8475, auc 0.8486
epoch 7601, loss 0.2805, train acc 84.89%, f1 0.8491, precision 0.8479, recall 0.8503, auc 0.8489
epoch 7701, loss 0.3160, train acc 84.95%, f1 0.8500, precision 0.8471, recall 0.8530, auc 0.8495
epoch 7801, loss 0.4196, train acc 84.94%, f1 0.8497, precision 0.8481, recall 0.8514, auc 0.8494
epoch 7901, loss 0.2570, train acc 84.91%, f1 0.8492, precision 0.8486, recall 0.8498, auc 0.8491
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_5
./test_pima/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.641132075471698

the Fscore is 0.5930232558139534

the precision is 0.42857142857142855

the recall is 0.9622641509433962

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_5
----------------------



epoch 1, loss 0.6933, train acc 50.04%, f1 0.6670, precision 0.5004, recall 1.0000, auc 0.5000
epoch 101, loss 0.5948, train acc 76.21%, f1 0.7854, precision 0.7159, recall 0.8698, auc 0.7621
epoch 201, loss 0.5345, train acc 80.57%, f1 0.8082, precision 0.7987, recall 0.8178, auc 0.8057
epoch 301, loss 0.4535, train acc 82.19%, f1 0.8222, precision 0.8215, recall 0.8229, auc 0.8219
epoch 401, loss 0.3437, train acc 83.39%, f1 0.8336, precision 0.8357, recall 0.8316, auc 0.8339
epoch 501, loss 0.3237, train acc 83.75%, f1 0.8367, precision 0.8415, recall 0.8321, auc 0.8375
epoch 601, loss 0.4017, train acc 84.01%, f1 0.8392, precision 0.8443, recall 0.8343, auc 0.8401
epoch 701, loss 0.3153, train acc 84.14%, f1 0.8407, precision 0.8450, recall 0.8365, auc 0.8414
epoch 801, loss 0.3371, train acc 84.13%, f1 0.8403, precision 0.8462, recall 0.8346, auc 0.8413
epoch 901, loss 0.4037, train acc 84.18%, f1 0.8411, precision 0.8453, recall 0.8369, auc 0.8418
epoch 1001, loss 0.3091, train acc 84.20%, f1 0.8409, precision 0.8472, recall 0.8348, auc 0.8420
epoch 1101, loss 0.2885, train acc 84.24%, f1 0.8417, precision 0.8458, recall 0.8377, auc 0.8424
epoch 1201, loss 0.3741, train acc 84.26%, f1 0.8416, precision 0.8473, recall 0.8360, auc 0.8426
epoch 1301, loss 0.4784, train acc 84.27%, f1 0.8426, precision 0.8440, recall 0.8411, auc 0.8427
epoch 1401, loss 0.5113, train acc 84.24%, f1 0.8417, precision 0.8464, recall 0.8370, auc 0.8424
epoch 1501, loss 0.3030, train acc 84.24%, f1 0.8421, precision 0.8444, recall 0.8398, auc 0.8424
epoch 1601, loss 0.3527, train acc 84.26%, f1 0.8428, precision 0.8424, recall 0.8432, auc 0.8426
epoch 1701, loss 0.3995, train acc 84.19%, f1 0.8417, precision 0.8434, recall 0.8400, auc 0.8419
epoch 1801, loss 0.3673, train acc 84.25%, f1 0.8425, precision 0.8428, recall 0.8423, auc 0.8425
epoch 1901, loss 0.2154, train acc 84.23%, f1 0.8417, precision 0.8456, recall 0.8378, auc 0.8423
epoch 2001, loss 0.3218, train acc 84.21%, f1 0.8416, precision 0.8448, recall 0.8385, auc 0.8421
epoch 2101, loss 0.3317, train acc 84.24%, f1 0.8422, precision 0.8439, recall 0.8404, auc 0.8424
epoch 2201, loss 0.3944, train acc 84.22%, f1 0.8419, precision 0.8440, recall 0.8399, auc 0.8422
epoch 2301, loss 0.3067, train acc 84.26%, f1 0.8426, precision 0.8433, recall 0.8418, auc 0.8426
epoch 2401, loss 0.2937, train acc 84.26%, f1 0.8422, precision 0.8450, recall 0.8394, auc 0.8426
epoch 2501, loss 0.2680, train acc 84.26%, f1 0.8421, precision 0.8453, recall 0.8390, auc 0.8426
epoch 2601, loss 0.2768, train acc 84.20%, f1 0.8417, precision 0.8442, recall 0.8391, auc 0.8420
epoch 2701, loss 0.4323, train acc 84.20%, f1 0.8419, precision 0.8432, recall 0.8406, auc 0.8420
epoch 2801, loss 0.3558, train acc 84.28%, f1 0.8424, precision 0.8448, recall 0.8400, auc 0.8428
epoch 2901, loss 0.2705, train acc 84.25%, f1 0.8423, precision 0.8441, recall 0.8405, auc 0.8425
epoch 3001, loss 0.3096, train acc 84.25%, f1 0.8421, precision 0.8448, recall 0.8394, auc 0.8425
epoch 3101, loss 0.2876, train acc 84.27%, f1 0.8426, precision 0.8439, recall 0.8412, auc 0.8427
epoch 3201, loss 0.3802, train acc 84.21%, f1 0.8419, precision 0.8436, recall 0.8402, auc 0.8421
epoch 3301, loss 0.4178, train acc 84.23%, f1 0.8428, precision 0.8409, recall 0.8447, auc 0.8423
epoch 3401, loss 0.2190, train acc 84.24%, f1 0.8423, precision 0.8433, recall 0.8413, auc 0.8424
epoch 3501, loss 0.3709, train acc 84.27%, f1 0.8431, precision 0.8414, recall 0.8449, auc 0.8427
epoch 3601, loss 0.2916, train acc 84.27%, f1 0.8429, precision 0.8425, recall 0.8434, auc 0.8427
epoch 3701, loss 0.4333, train acc 84.27%, f1 0.8429, precision 0.8428, recall 0.8430, auc 0.8427
epoch 3801, loss 0.3171, train acc 84.32%, f1 0.8432, precision 0.8441, recall 0.8422, auc 0.8432
epoch 3901, loss 0.3047, train acc 84.33%, f1 0.8430, precision 0.8453, recall 0.8408, auc 0.8434
epoch 4001, loss 0.3929, train acc 84.33%, f1 0.8437, precision 0.8425, recall 0.8448, auc 0.8433
epoch 4101, loss 0.2939, train acc 84.33%, f1 0.8432, precision 0.8441, recall 0.8424, auc 0.8433
epoch 4201, loss 0.4467, train acc 84.36%, f1 0.8435, precision 0.8449, recall 0.8421, auc 0.8436
epoch 4301, loss 0.3761, train acc 84.38%, f1 0.8437, precision 0.8451, recall 0.8423, auc 0.8439
epoch 4401, loss 0.2764, train acc 84.39%, f1 0.8441, precision 0.8438, recall 0.8444, auc 0.8439
epoch 4501, loss 0.3323, train acc 84.34%, f1 0.8439, precision 0.8417, recall 0.8461, auc 0.8434
epoch 4601, loss 0.4090, train acc 84.41%, f1 0.8445, precision 0.8431, recall 0.8459, auc 0.8441
epoch 4701, loss 0.3862, train acc 84.43%, f1 0.8442, precision 0.8454, recall 0.8431, auc 0.8443
epoch 4801, loss 0.4868, train acc 84.43%, f1 0.8448, precision 0.8425, recall 0.8471, auc 0.8443
epoch 4901, loss 0.3119, train acc 84.49%, f1 0.8448, precision 0.8460, recall 0.8436, auc 0.8449
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_5
./test_pima/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.6305660377358491

the Fscore is 0.5875706214689266

the precision is 0.41935483870967744

the recall is 0.9811320754716981

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_5
----------------------



epoch 1, loss 0.6929, train acc 50.01%, f1 0.6668, precision 0.5001, recall 1.0000, auc 0.5000
epoch 101, loss 0.6083, train acc 78.13%, f1 0.7835, precision 0.7760, recall 0.7911, auc 0.7813
epoch 201, loss 0.4857, train acc 81.08%, f1 0.8110, precision 0.8105, recall 0.8114, auc 0.8108
epoch 301, loss 0.4167, train acc 82.42%, f1 0.8242, precision 0.8242, recall 0.8242, auc 0.8242
epoch 401, loss 0.4193, train acc 83.29%, f1 0.8324, precision 0.8349, recall 0.8299, auc 0.8329
epoch 501, loss 0.3426, train acc 83.73%, f1 0.8369, precision 0.8395, recall 0.8343, auc 0.8373
epoch 601, loss 0.2595, train acc 84.06%, f1 0.8401, precision 0.8430, recall 0.8373, auc 0.8406
epoch 701, loss 0.4765, train acc 84.10%, f1 0.8407, precision 0.8426, recall 0.8388, auc 0.8410
epoch 801, loss 0.3382, train acc 84.20%, f1 0.8417, precision 0.8432, recall 0.8403, auc 0.8420
epoch 901, loss 0.4837, train acc 84.16%, f1 0.8415, precision 0.8422, recall 0.8409, auc 0.8416
epoch 1001, loss 0.3461, train acc 84.22%, f1 0.8422, precision 0.8422, recall 0.8422, auc 0.8422
epoch 1101, loss 0.4245, train acc 84.31%, f1 0.8433, precision 0.8427, recall 0.8438, auc 0.8431
epoch 1201, loss 0.4398, train acc 84.29%, f1 0.8427, precision 0.8439, recall 0.8415, auc 0.8429
epoch 1301, loss 0.3371, train acc 84.22%, f1 0.8424, precision 0.8415, recall 0.8432, auc 0.8422
epoch 1401, loss 0.4435, train acc 84.21%, f1 0.8420, precision 0.8429, recall 0.8410, auc 0.8421
epoch 1501, loss 0.3705, train acc 84.27%, f1 0.8428, precision 0.8426, recall 0.8430, auc 0.8427
epoch 1601, loss 0.3635, train acc 84.27%, f1 0.8425, precision 0.8438, recall 0.8412, auc 0.8427
epoch 1701, loss 0.3559, train acc 84.26%, f1 0.8424, precision 0.8439, recall 0.8408, auc 0.8426
epoch 1801, loss 0.4081, train acc 84.25%, f1 0.8424, precision 0.8427, recall 0.8422, auc 0.8425
epoch 1901, loss 0.5094, train acc 84.27%, f1 0.8425, precision 0.8438, recall 0.8413, auc 0.8427
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_pima/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_5
./test_pima/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.595

the Fscore is 0.5668449197860963

the precision is 0.39552238805970147

the recall is 1.0

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_5
----------------------



epoch 1, loss 0.7000, train acc 34.96%, f1 0.5181, precision 0.3496, recall 1.0000, auc 0.5000
epoch 101, loss 0.6267, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5983, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5729, train acc 64.88%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5498, train acc 67.15%, f1 0.1368, precision 0.8421, recall 0.0744, auc 0.5335
epoch 501, loss 0.5309, train acc 70.73%, f1 0.3333, precision 0.8182, recall 0.2093, auc 0.5922
epoch 601, loss 0.5161, train acc 73.98%, f1 0.4737, precision 0.8090, recall 0.3349, auc 0.6462
epoch 701, loss 0.5047, train acc 76.10%, f1 0.5532, precision 0.7982, recall 0.4233, auc 0.6829
epoch 801, loss 0.4961, train acc 76.42%, f1 0.5961, precision 0.7431, recall 0.4977, auc 0.7026
epoch 901, loss 0.4897, train acc 76.42%, f1 0.6154, precision 0.7160, recall 0.5395, auc 0.7123
epoch 1001, loss 0.4849, train acc 77.24%, f1 0.6354, precision 0.7219, recall 0.5674, auc 0.7250
epoch 1101, loss 0.4810, train acc 77.56%, f1 0.6443, precision 0.7225, recall 0.5814, auc 0.7307
epoch 1201, loss 0.4776, train acc 78.54%, f1 0.6667, precision 0.7293, recall 0.6140, auc 0.7457
epoch 1301, loss 0.4737, train acc 77.89%, f1 0.6548, precision 0.7207, recall 0.6000, auc 0.7375
epoch 1401, loss 0.4691, train acc 79.02%, f1 0.6701, precision 0.7443, recall 0.6093, auc 0.7484
epoch 1501, loss 0.4641, train acc 79.35%, f1 0.6735, precision 0.7529, recall 0.6093, auc 0.7509
epoch 1601, loss 0.4592, train acc 79.19%, f1 0.6718, precision 0.7486, recall 0.6093, auc 0.7497
epoch 1701, loss 0.4545, train acc 79.51%, f1 0.6736, precision 0.7602, recall 0.6047, auc 0.7511
epoch 1801, loss 0.4502, train acc 79.67%, f1 0.6787, precision 0.7586, recall 0.6140, auc 0.7545
epoch 1901, loss 0.4462, train acc 79.67%, f1 0.6803, precision 0.7557, recall 0.6186, auc 0.7556
epoch 2001, loss 0.4427, train acc 79.67%, f1 0.6819, precision 0.7528, recall 0.6233, auc 0.7566
epoch 2101, loss 0.4397, train acc 79.84%, f1 0.6837, precision 0.7571, recall 0.6233, auc 0.7579
epoch 2201, loss 0.4371, train acc 79.67%, f1 0.6835, precision 0.7500, recall 0.6279, auc 0.7577
epoch 2301, loss 0.4348, train acc 79.84%, f1 0.6884, precision 0.7486, recall 0.6372, auc 0.7611
epoch 2401, loss 0.4328, train acc 79.84%, f1 0.6884, precision 0.7486, recall 0.6372, auc 0.7611
epoch 2501, loss 0.4310, train acc 80.00%, f1 0.6933, precision 0.7473, recall 0.6465, auc 0.7645
epoch 2601, loss 0.4294, train acc 80.00%, f1 0.6933, precision 0.7473, recall 0.6465, auc 0.7645
epoch 2701, loss 0.4279, train acc 79.84%, f1 0.6915, precision 0.7433, recall 0.6465, auc 0.7633
epoch 2801, loss 0.4264, train acc 80.16%, f1 0.6980, precision 0.7460, recall 0.6558, auc 0.7679
epoch 2901, loss 0.4249, train acc 80.00%, f1 0.6963, precision 0.7421, recall 0.6558, auc 0.7667
epoch 3001, loss 0.4234, train acc 80.16%, f1 0.6995, precision 0.7435, recall 0.6605, auc 0.7690
epoch 3101, loss 0.4219, train acc 80.33%, f1 0.7027, precision 0.7448, recall 0.6651, auc 0.7713
epoch 3201, loss 0.4205, train acc 80.16%, f1 0.6995, precision 0.7435, recall 0.6605, auc 0.7690
epoch 3301, loss 0.4192, train acc 80.33%, f1 0.7012, precision 0.7474, recall 0.6605, auc 0.7702
epoch 3401, loss 0.4178, train acc 80.65%, f1 0.7076, precision 0.7500, recall 0.6698, auc 0.7749
epoch 3501, loss 0.4165, train acc 80.65%, f1 0.7076, precision 0.7500, recall 0.6698, auc 0.7749
epoch 3601, loss 0.4152, train acc 80.98%, f1 0.7139, precision 0.7526, recall 0.6791, auc 0.7795
epoch 3701, loss 0.4139, train acc 81.30%, f1 0.7188, precision 0.7577, recall 0.6837, auc 0.7831
epoch 3801, loss 0.4124, train acc 81.46%, f1 0.7220, precision 0.7590, recall 0.6884, auc 0.7854
epoch 3901, loss 0.4111, train acc 81.63%, f1 0.7251, precision 0.7602, recall 0.6930, auc 0.7878
epoch 4001, loss 0.4098, train acc 82.11%, f1 0.7317, precision 0.7692, recall 0.6977, auc 0.7926
epoch 4101, loss 0.4083, train acc 82.11%, f1 0.7317, precision 0.7692, recall 0.6977, auc 0.7926
epoch 4201, loss 0.4067, train acc 81.95%, f1 0.7299, precision 0.7653, recall 0.6977, auc 0.7913
epoch 4301, loss 0.4048, train acc 81.63%, f1 0.7264, precision 0.7576, recall 0.6977, auc 0.7888
epoch 4401, loss 0.4028, train acc 81.46%, f1 0.7246, precision 0.7538, recall 0.6977, auc 0.7876
epoch 4501, loss 0.4006, train acc 81.30%, f1 0.7188, precision 0.7577, recall 0.6837, auc 0.7831
epoch 4601, loss 0.3985, train acc 81.79%, f1 0.7268, precision 0.7641, recall 0.6930, auc 0.7890
epoch 4701, loss 0.3966, train acc 81.79%, f1 0.7268, precision 0.7641, recall 0.6930, auc 0.7890
epoch 4801, loss 0.3948, train acc 81.95%, f1 0.7299, precision 0.7653, recall 0.6977, auc 0.7913
epoch 4901, loss 0.3932, train acc 82.11%, f1 0.7317, precision 0.7692, recall 0.6977, auc 0.7926
epoch 5001, loss 0.3916, train acc 81.95%, f1 0.7299, precision 0.7653, recall 0.6977, auc 0.7913
epoch 5101, loss 0.3900, train acc 82.44%, f1 0.7353, precision 0.7772, recall 0.6977, auc 0.7951
epoch 5201, loss 0.3883, train acc 82.93%, f1 0.7420, precision 0.7865, recall 0.7023, auc 0.7999
epoch 5301, loss 0.3866, train acc 82.93%, f1 0.7420, precision 0.7865, recall 0.7023, auc 0.7999
epoch 5401, loss 0.3845, train acc 82.76%, f1 0.7389, precision 0.7853, recall 0.6977, auc 0.7976
epoch 5501, loss 0.3816, train acc 82.76%, f1 0.7389, precision 0.7853, recall 0.6977, auc 0.7976
epoch 5601, loss 0.3773, train acc 82.76%, f1 0.7389, precision 0.7853, recall 0.6977, auc 0.7976
epoch 5701, loss 0.3739, train acc 83.25%, f1 0.7482, precision 0.7887, recall 0.7116, auc 0.8046
epoch 5801, loss 0.3710, train acc 83.58%, f1 0.7531, precision 0.7938, recall 0.7163, auc 0.8081
epoch 5901, loss 0.3683, train acc 83.58%, f1 0.7531, precision 0.7938, recall 0.7163, auc 0.8081
epoch 6001, loss 0.3657, train acc 83.90%, f1 0.7579, precision 0.7990, recall 0.7209, auc 0.8117
epoch 6101, loss 0.3630, train acc 83.58%, f1 0.7531, precision 0.7938, recall 0.7163, auc 0.8081
epoch 6201, loss 0.3601, train acc 84.23%, f1 0.7651, precision 0.7980, recall 0.7349, auc 0.8174
epoch 6301, loss 0.3564, train acc 84.39%, f1 0.7681, precision 0.7990, recall 0.7395, auc 0.8198
epoch 6401, loss 0.3527, train acc 84.55%, f1 0.7689, precision 0.8061, recall 0.7349, auc 0.8199
epoch 6501, loss 0.3495, train acc 84.39%, f1 0.7659, precision 0.8051, recall 0.7302, auc 0.8176
epoch 6601, loss 0.3466, train acc 85.04%, f1 0.7756, precision 0.8154, recall 0.7395, auc 0.8248
epoch 6701, loss 0.3429, train acc 84.88%, f1 0.7737, precision 0.8112, recall 0.7395, auc 0.8235
epoch 6801, loss 0.3382, train acc 85.04%, f1 0.7767, precision 0.8122, recall 0.7442, auc 0.8258
epoch 6901, loss 0.3340, train acc 86.02%, f1 0.7923, precision 0.8241, recall 0.7628, auc 0.8376
epoch 7001, loss 0.3304, train acc 86.02%, f1 0.7933, precision 0.8209, recall 0.7674, auc 0.8387
epoch 7101, loss 0.3270, train acc 86.02%, f1 0.7943, precision 0.8177, recall 0.7721, auc 0.8398
epoch 7201, loss 0.3233, train acc 86.34%, f1 0.7990, precision 0.8227, recall 0.7767, auc 0.8434
epoch 7301, loss 0.3185, train acc 86.18%, f1 0.7952, precision 0.8250, recall 0.7674, auc 0.8400
epoch 7401, loss 0.3144, train acc 86.02%, f1 0.7923, precision 0.8241, recall 0.7628, auc 0.8376
epoch 7501, loss 0.3109, train acc 86.18%, f1 0.7962, precision 0.8218, recall 0.7721, auc 0.8410
epoch 7601, loss 0.3074, train acc 86.18%, f1 0.7942, precision 0.8283, recall 0.7628, auc 0.8389
epoch 7701, loss 0.3044, train acc 86.34%, f1 0.7961, precision 0.8325, recall 0.7628, auc 0.8401
epoch 7801, loss 0.3017, train acc 86.34%, f1 0.7961, precision 0.8325, recall 0.7628, auc 0.8401
epoch 7901, loss 0.2992, train acc 86.83%, f1 0.8029, precision 0.8418, recall 0.7674, auc 0.8450
epoch 8001, loss 0.2967, train acc 87.32%, f1 0.8116, precision 0.8442, recall 0.7814, auc 0.8519
epoch 8101, loss 0.2943, train acc 87.64%, f1 0.8164, precision 0.8492, recall 0.7860, auc 0.8555
epoch 8201, loss 0.2915, train acc 88.46%, f1 0.8289, precision 0.8600, recall 0.8000, auc 0.8650
epoch 8301, loss 0.2890, train acc 88.13%, f1 0.8241, precision 0.8550, recall 0.7953, auc 0.8614
epoch 8401, loss 0.2865, train acc 87.97%, f1 0.8221, precision 0.8507, recall 0.7953, auc 0.8602
epoch 8501, loss 0.2843, train acc 87.97%, f1 0.8230, precision 0.8473, recall 0.8000, auc 0.8613
epoch 8601, loss 0.2822, train acc 88.29%, f1 0.8286, precision 0.8488, recall 0.8093, auc 0.8659
epoch 8701, loss 0.2803, train acc 88.94%, f1 0.8396, precision 0.8517, recall 0.8279, auc 0.8752
epoch 8801, loss 0.2785, train acc 88.78%, f1 0.8376, precision 0.8476, recall 0.8279, auc 0.8740
epoch 8901, loss 0.2766, train acc 88.78%, f1 0.8384, precision 0.8443, recall 0.8326, auc 0.8750
epoch 9001, loss 0.2746, train acc 88.94%, f1 0.8411, precision 0.8451, recall 0.8372, auc 0.8774
epoch 9101, loss 0.2724, train acc 89.27%, f1 0.8451, precision 0.8531, recall 0.8372, auc 0.8799
epoch 9201, loss 0.2703, train acc 89.11%, f1 0.8431, precision 0.8491, recall 0.8372, auc 0.8786
epoch 9301, loss 0.2684, train acc 88.78%, f1 0.8384, precision 0.8443, recall 0.8326, auc 0.8750
epoch 9401, loss 0.2666, train acc 88.94%, f1 0.8404, precision 0.8483, recall 0.8326, auc 0.8763
epoch 9501, loss 0.2648, train acc 89.11%, f1 0.8424, precision 0.8524, recall 0.8326, auc 0.8775
epoch 9601, loss 0.2631, train acc 88.94%, f1 0.8404, precision 0.8483, recall 0.8326, auc 0.8763
epoch 9701, loss 0.2615, train acc 88.94%, f1 0.8411, precision 0.8451, recall 0.8372, auc 0.8774
epoch 9801, loss 0.2600, train acc 89.11%, f1 0.8431, precision 0.8491, recall 0.8372, auc 0.8786
epoch 9901, loss 0.2586, train acc 89.11%, f1 0.8431, precision 0.8491, recall 0.8372, auc 0.8786
epoch 10001, loss 0.2573, train acc 89.11%, f1 0.8438, precision 0.8458, recall 0.8419, auc 0.8797
epoch 10101, loss 0.2559, train acc 89.27%, f1 0.8458, precision 0.8498, recall 0.8419, auc 0.8809
epoch 10201, loss 0.2539, train acc 89.43%, f1 0.8485, precision 0.8505, recall 0.8465, auc 0.8833
epoch 10301, loss 0.2523, train acc 89.27%, f1 0.8458, precision 0.8498, recall 0.8419, auc 0.8809
epoch 10401, loss 0.2509, train acc 89.27%, f1 0.8458, precision 0.8498, recall 0.8419, auc 0.8809
epoch 10501, loss 0.2497, train acc 89.27%, f1 0.8458, precision 0.8498, recall 0.8419, auc 0.8809
epoch 10601, loss 0.2485, train acc 89.27%, f1 0.8458, precision 0.8498, recall 0.8419, auc 0.8809
epoch 10701, loss 0.2474, train acc 89.43%, f1 0.8478, precision 0.8538, recall 0.8419, auc 0.8822
epoch 10801, loss 0.2463, train acc 89.59%, f1 0.8505, precision 0.8545, recall 0.8465, auc 0.8845
epoch 10901, loss 0.2453, train acc 89.76%, f1 0.8531, precision 0.8551, recall 0.8512, auc 0.8868
epoch 11001, loss 0.2442, train acc 89.59%, f1 0.8512, precision 0.8512, recall 0.8512, auc 0.8856
epoch 11101, loss 0.2424, train acc 89.59%, f1 0.8505, precision 0.8545, recall 0.8465, auc 0.8845
epoch 11201, loss 0.2404, train acc 89.76%, f1 0.8518, precision 0.8619, recall 0.8419, auc 0.8847
epoch 11301, loss 0.2386, train acc 89.92%, f1 0.8545, precision 0.8626, recall 0.8465, auc 0.8870
epoch 11401, loss 0.2369, train acc 89.76%, f1 0.8525, precision 0.8585, recall 0.8465, auc 0.8858
epoch 11501, loss 0.2346, train acc 90.57%, f1 0.8645, precision 0.8685, recall 0.8605, auc 0.8952
epoch 11601, loss 0.2326, train acc 90.89%, f1 0.8698, precision 0.8698, recall 0.8698, auc 0.8999
epoch 11701, loss 0.2308, train acc 91.06%, f1 0.8712, precision 0.8774, recall 0.8651, auc 0.9001
epoch 11801, loss 0.2292, train acc 91.06%, f1 0.8712, precision 0.8774, recall 0.8651, auc 0.9001
epoch 11901, loss 0.2277, train acc 90.73%, f1 0.8665, precision 0.8726, recall 0.8605, auc 0.8965
epoch 12001, loss 0.2261, train acc 91.06%, f1 0.8706, precision 0.8810, recall 0.8605, auc 0.8990
epoch 12101, loss 0.2246, train acc 91.06%, f1 0.8706, precision 0.8810, recall 0.8605, auc 0.8990
epoch 12201, loss 0.2233, train acc 91.06%, f1 0.8706, precision 0.8810, recall 0.8605, auc 0.8990
epoch 12301, loss 0.2220, train acc 91.38%, f1 0.8753, precision 0.8857, recall 0.8651, auc 0.9026
epoch 12401, loss 0.2207, train acc 91.54%, f1 0.8774, precision 0.8900, recall 0.8651, auc 0.9038
epoch 12501, loss 0.2195, train acc 91.54%, f1 0.8779, precision 0.8863, recall 0.8698, auc 0.9049
epoch 12601, loss 0.2184, train acc 91.54%, f1 0.8779, precision 0.8863, recall 0.8698, auc 0.9049
epoch 12701, loss 0.2172, train acc 91.71%, f1 0.8800, precision 0.8905, recall 0.8698, auc 0.9061
epoch 12801, loss 0.2161, train acc 91.87%, f1 0.8832, precision 0.8873, recall 0.8791, auc 0.9095
epoch 12901, loss 0.2150, train acc 91.87%, f1 0.8832, precision 0.8873, recall 0.8791, auc 0.9095
epoch 13001, loss 0.2140, train acc 92.03%, f1 0.8852, precision 0.8915, recall 0.8791, auc 0.9108
epoch 13101, loss 0.2130, train acc 92.03%, f1 0.8852, precision 0.8915, recall 0.8791, auc 0.9108
epoch 13201, loss 0.2120, train acc 92.03%, f1 0.8852, precision 0.8915, recall 0.8791, auc 0.9108
epoch 13301, loss 0.2110, train acc 91.87%, f1 0.8826, precision 0.8910, recall 0.8744, auc 0.9085
epoch 13401, loss 0.2100, train acc 91.87%, f1 0.8826, precision 0.8910, recall 0.8744, auc 0.9085
epoch 13501, loss 0.2091, train acc 91.87%, f1 0.8826, precision 0.8910, recall 0.8744, auc 0.9085
epoch 13601, loss 0.2081, train acc 91.87%, f1 0.8826, precision 0.8910, recall 0.8744, auc 0.9085
epoch 13701, loss 0.2070, train acc 92.03%, f1 0.8847, precision 0.8952, recall 0.8744, auc 0.9097
epoch 13801, loss 0.2059, train acc 92.20%, f1 0.8868, precision 0.8995, recall 0.8744, auc 0.9110
epoch 13901, loss 0.2050, train acc 92.20%, f1 0.8868, precision 0.8995, recall 0.8744, auc 0.9110
epoch 14001, loss 0.2042, train acc 92.20%, f1 0.8868, precision 0.8995, recall 0.8744, auc 0.9110
epoch 14101, loss 0.2034, train acc 92.20%, f1 0.8868, precision 0.8995, recall 0.8744, auc 0.9110
epoch 14201, loss 0.2026, train acc 92.03%, f1 0.8842, precision 0.8990, recall 0.8698, auc 0.9086
epoch 14301, loss 0.2019, train acc 92.03%, f1 0.8842, precision 0.8990, recall 0.8698, auc 0.9086
epoch 14401, loss 0.2011, train acc 92.03%, f1 0.8842, precision 0.8990, recall 0.8698, auc 0.9086
epoch 14501, loss 0.2004, train acc 92.20%, f1 0.8863, precision 0.9034, recall 0.8698, auc 0.9099
epoch 14601, loss 0.1996, train acc 92.20%, f1 0.8863, precision 0.9034, recall 0.8698, auc 0.9099
epoch 14701, loss 0.1989, train acc 92.36%, f1 0.8889, precision 0.9038, recall 0.8744, auc 0.9122
epoch 14801, loss 0.1982, train acc 92.52%, f1 0.8915, precision 0.9043, recall 0.8791, auc 0.9145
epoch 14901, loss 0.1975, train acc 92.52%, f1 0.8915, precision 0.9043, recall 0.8791, auc 0.9145
epoch 15001, loss 0.1969, train acc 92.52%, f1 0.8915, precision 0.9043, recall 0.8791, auc 0.9145
epoch 15101, loss 0.1962, train acc 92.36%, f1 0.8894, precision 0.9000, recall 0.8791, auc 0.9133
epoch 15201, loss 0.1955, train acc 92.68%, f1 0.8946, precision 0.9009, recall 0.8884, auc 0.9179
epoch 15301, loss 0.1949, train acc 92.68%, f1 0.8946, precision 0.9009, recall 0.8884, auc 0.9179
epoch 15401, loss 0.1942, train acc 92.85%, f1 0.8972, precision 0.9014, recall 0.8930, auc 0.9203
epoch 15501, loss 0.1936, train acc 92.68%, f1 0.8951, precision 0.8972, recall 0.8930, auc 0.9190
epoch 15601, loss 0.1929, train acc 92.85%, f1 0.8972, precision 0.9014, recall 0.8930, auc 0.9203
epoch 15701, loss 0.1923, train acc 92.85%, f1 0.8972, precision 0.9014, recall 0.8930, auc 0.9203
epoch 15801, loss 0.1917, train acc 92.85%, f1 0.8972, precision 0.9014, recall 0.8930, auc 0.9203
epoch 15901, loss 0.1911, train acc 92.85%, f1 0.8972, precision 0.9014, recall 0.8930, auc 0.9203
epoch 16001, loss 0.1905, train acc 92.85%, f1 0.8972, precision 0.9014, recall 0.8930, auc 0.9203
epoch 16101, loss 0.1899, train acc 93.01%, f1 0.8998, precision 0.9019, recall 0.8977, auc 0.9226
epoch 16201, loss 0.1894, train acc 93.01%, f1 0.8998, precision 0.9019, recall 0.8977, auc 0.9226
epoch 16301, loss 0.1888, train acc 93.01%, f1 0.8998, precision 0.9019, recall 0.8977, auc 0.9226
epoch 16401, loss 0.1882, train acc 93.01%, f1 0.8998, precision 0.9019, recall 0.8977, auc 0.9226
epoch 16501, loss 0.1877, train acc 93.01%, f1 0.8998, precision 0.9019, recall 0.8977, auc 0.9226/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16601, loss 0.1872, train acc 93.01%, f1 0.8998, precision 0.9019, recall 0.8977, auc 0.9226
epoch 16701, loss 0.1866, train acc 93.01%, f1 0.8998, precision 0.9019, recall 0.8977, auc 0.9226
epoch 16801, loss 0.1861, train acc 93.01%, f1 0.8998, precision 0.9019, recall 0.8977, auc 0.9226
epoch 16901, loss 0.1856, train acc 93.17%, f1 0.9019, precision 0.9061, recall 0.8977, auc 0.9238
epoch 17001, loss 0.1851, train acc 93.17%, f1 0.9019, precision 0.9061, recall 0.8977, auc 0.9238
epoch 17101, loss 0.1846, train acc 93.17%, f1 0.9019, precision 0.9061, recall 0.8977, auc 0.9238
epoch 17201, loss 0.1841, train acc 93.50%, f1 0.9065, precision 0.9108, recall 0.9023, auc 0.9274
epoch 17301, loss 0.1836, train acc 93.66%, f1 0.9091, precision 0.9112, recall 0.9070, auc 0.9297
epoch 17401, loss 0.1831, train acc 93.82%, f1 0.9116, precision 0.9116, recall 0.9116, auc 0.9321
epoch 17501, loss 0.1826, train acc 93.82%, f1 0.9116, precision 0.9116, recall 0.9116, auc 0.9321
epoch 17601, loss 0.1821, train acc 93.82%, f1 0.9116, precision 0.9116, recall 0.9116, auc 0.9321
epoch 17701, loss 0.1817, train acc 93.82%, f1 0.9116, precision 0.9116, recall 0.9116, auc 0.9321
epoch 17801, loss 0.1812, train acc 93.82%, f1 0.9116, precision 0.9116, recall 0.9116, auc 0.9321
epoch 17901, loss 0.1808, train acc 93.82%, f1 0.9116, precision 0.9116, recall 0.9116, auc 0.9321
epoch 18001, loss 0.1803, train acc 93.82%, f1 0.9116, precision 0.9116, recall 0.9116, auc 0.9321
epoch 18101, loss 0.1799, train acc 93.98%, f1 0.9138, precision 0.9159, recall 0.9116, auc 0.9333
epoch 18201, loss 0.1794, train acc 93.98%, f1 0.9138, precision 0.9159, recall 0.9116, auc 0.9333
epoch 18301, loss 0.1790, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 18401, loss 0.1786, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 18501, loss 0.1782, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 18601, loss 0.1778, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 18701, loss 0.1774, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 18801, loss 0.1769, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 18901, loss 0.1766, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19001, loss 0.1762, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19101, loss 0.1758, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19201, loss 0.1754, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19301, loss 0.1750, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19401, loss 0.1746, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19501, loss 0.1743, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19601, loss 0.1739, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19701, loss 0.1735, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19801, loss 0.1732, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
epoch 19901, loss 0.1728, train acc 94.15%, f1 0.9159, precision 0.9202, recall 0.9116, auc 0.9346
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_normal_20000
normal
./test_pima/model_MLP_normal_20000/record_1/MLP_normal_20000_5
./test_pima/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.6452830188679245

the Fscore is 0.5252525252525252

the precision is 0.5652173913043478

the recall is 0.49056603773584906

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_5
----------------------



epoch 1, loss 0.6931, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6239, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5962, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5704, train acc 64.88%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5473, train acc 67.80%, f1 0.1750, precision 0.8400, recall 0.0977, auc 0.5438
epoch 501, loss 0.5286, train acc 71.38%, f1 0.3577, precision 0.8305, recall 0.2279, auc 0.6015
epoch 601, loss 0.5141, train acc 74.47%, f1 0.4919, precision 0.8085, recall 0.3535, auc 0.6542
epoch 701, loss 0.5030, train acc 75.12%, f1 0.5487, precision 0.7500, recall 0.4326, auc 0.6775
epoch 801, loss 0.4947, train acc 76.10%, f1 0.5950, precision 0.7297, recall 0.5023, auc 0.7012
epoch 901, loss 0.4885, train acc 76.75%, f1 0.6227, precision 0.7195, recall 0.5488, auc 0.7169
epoch 1001, loss 0.4838, train acc 77.24%, f1 0.6354, precision 0.7219, recall 0.5674, auc 0.7250
epoch 1101, loss 0.4801, train acc 77.89%, f1 0.6513, precision 0.7257, recall 0.5907, auc 0.7353
epoch 1201, loss 0.4765, train acc 78.54%, f1 0.6667, precision 0.7293, recall 0.6140, auc 0.7457
epoch 1301, loss 0.4722, train acc 78.37%, f1 0.6598, precision 0.7330, recall 0.6000, auc 0.7413
epoch 1401, loss 0.4672, train acc 78.86%, f1 0.6667, precision 0.7429, recall 0.6047, auc 0.7461
epoch 1501, loss 0.4621, train acc 79.02%, f1 0.6667, precision 0.7500, recall 0.6000, auc 0.7462
epoch 1601, loss 0.4573, train acc 79.35%, f1 0.6735, precision 0.7529, recall 0.6093, auc 0.7509
epoch 1701, loss 0.4529, train acc 79.67%, f1 0.6787, precision 0.7586, recall 0.6140, auc 0.7545
epoch 1801, loss 0.4489, train acc 80.00%, f1 0.6870, precision 0.7584, recall 0.6279, auc 0.7602
epoch 1901, loss 0.4453, train acc 80.16%, f1 0.6919, precision 0.7569, recall 0.6372, auc 0.7636
epoch 2001, loss 0.4422, train acc 80.00%, f1 0.6886, precision 0.7556, recall 0.6326, auc 0.7613
epoch 2101, loss 0.4395, train acc 79.35%, f1 0.6801, precision 0.7418, recall 0.6279, auc 0.7552
epoch 2201, loss 0.4372, train acc 80.00%, f1 0.6933, precision 0.7473, recall 0.6465, auc 0.7645
epoch 2301, loss 0.4352, train acc 80.00%, f1 0.6933, precision 0.7473, recall 0.6465, auc 0.7645
epoch 2401, loss 0.4334, train acc 80.00%, f1 0.6933, precision 0.7473, recall 0.6465, auc 0.7645
epoch 2501, loss 0.4317, train acc 80.16%, f1 0.6965, precision 0.7487, recall 0.6512, auc 0.7668
epoch 2601, loss 0.4301, train acc 80.33%, f1 0.6998, precision 0.7500, recall 0.6558, auc 0.7692
epoch 2701, loss 0.4286, train acc 80.16%, f1 0.6980, precision 0.7460, recall 0.6558, auc 0.7679
epoch 2801, loss 0.4271, train acc 80.33%, f1 0.6998, precision 0.7500, recall 0.6558, auc 0.7692
epoch 2901, loss 0.4255, train acc 80.00%, f1 0.6933, precision 0.7473, recall 0.6465, auc 0.7645
epoch 3001, loss 0.4239, train acc 80.00%, f1 0.6933, precision 0.7473, recall 0.6465, auc 0.7645
epoch 3101, loss 0.4223, train acc 80.00%, f1 0.6948, precision 0.7447, recall 0.6512, auc 0.7656
epoch 3201, loss 0.4206, train acc 80.00%, f1 0.6948, precision 0.7447, recall 0.6512, auc 0.7656
epoch 3301, loss 0.4189, train acc 80.33%, f1 0.7012, precision 0.7474, recall 0.6605, auc 0.7702
epoch 3401, loss 0.4173, train acc 80.98%, f1 0.7125, precision 0.7552, recall 0.6744, auc 0.7785
epoch 3501, loss 0.4157, train acc 80.98%, f1 0.7125, precision 0.7552, recall 0.6744, auc 0.7785
epoch 3601, loss 0.4142, train acc 81.14%, f1 0.7157, precision 0.7565, recall 0.6791, auc 0.7808
epoch 3701, loss 0.4125, train acc 80.98%, f1 0.7139, precision 0.7526, recall 0.6791, auc 0.7795
epoch 3801, loss 0.4107, train acc 81.30%, f1 0.7174, precision 0.7604, recall 0.6791, auc 0.7820
epoch 3901, loss 0.4088, train acc 81.30%, f1 0.7188, precision 0.7577, recall 0.6837, auc 0.7831
epoch 4001, loss 0.4069, train acc 81.14%, f1 0.7171, precision 0.7538, recall 0.6837, auc 0.7819
epoch 4101, loss 0.4050, train acc 80.98%, f1 0.7167, precision 0.7475, recall 0.6884, auc 0.7817
epoch 4201, loss 0.4030, train acc 80.65%, f1 0.7119, precision 0.7424, recall 0.6837, auc 0.7781
epoch 4301, loss 0.4009, train acc 80.81%, f1 0.7122, precision 0.7487, recall 0.6791, auc 0.7783
epoch 4401, loss 0.3988, train acc 80.98%, f1 0.7125, precision 0.7552, recall 0.6744, auc 0.7785
epoch 4501, loss 0.3966, train acc 81.14%, f1 0.7143, precision 0.7592, recall 0.6744, auc 0.7797
epoch 4601, loss 0.3942, train acc 81.79%, f1 0.7268, precision 0.7641, recall 0.6930, auc 0.7890
epoch 4701, loss 0.3913, train acc 81.79%, f1 0.7282, precision 0.7614, recall 0.6977, auc 0.7901
epoch 4801, loss 0.3885, train acc 82.28%, f1 0.7361, precision 0.7677, recall 0.7070, auc 0.7960
epoch 4901, loss 0.3856, train acc 82.28%, f1 0.7373, precision 0.7650, recall 0.7116, auc 0.7971
epoch 5001, loss 0.3825, train acc 82.11%, f1 0.7343, precision 0.7638, recall 0.7070, auc 0.7947
epoch 5101, loss 0.3793, train acc 82.44%, f1 0.7404, precision 0.7662, recall 0.7163, auc 0.7994
epoch 5201, loss 0.3763, train acc 82.93%, f1 0.7482, precision 0.7723, recall 0.7256, auc 0.8053
epoch 5301, loss 0.3734, train acc 83.41%, f1 0.7548, precision 0.7811, recall 0.7302, auc 0.8101
epoch 5401, loss 0.3708, train acc 83.58%, f1 0.7566, precision 0.7850, recall 0.7302, auc 0.8114
epoch 5501, loss 0.3681, train acc 83.74%, f1 0.7596, precision 0.7861, recall 0.7349, auc 0.8137
epoch 5601, loss 0.3655, train acc 83.41%, f1 0.7536, precision 0.7839, recall 0.7256, auc 0.8090
epoch 5701, loss 0.3631, train acc 83.58%, f1 0.7566, precision 0.7850, recall 0.7302, auc 0.8114
epoch 5801, loss 0.3606, train acc 83.58%, f1 0.7578, precision 0.7822, recall 0.7349, auc 0.8124
epoch 5901, loss 0.3581, train acc 83.25%, f1 0.7518, precision 0.7800, recall 0.7256, auc 0.8078
epoch 6001, loss 0.3555, train acc 83.41%, f1 0.7524, precision 0.7868, recall 0.7209, auc 0.8080
epoch 6101, loss 0.3529, train acc 83.25%, f1 0.7506, precision 0.7828, recall 0.7209, auc 0.8067
epoch 6201, loss 0.3503, train acc 84.07%, f1 0.7633, precision 0.7940, recall 0.7349, auc 0.8162
epoch 6301, loss 0.3478, train acc 83.90%, f1 0.7603, precision 0.7929, recall 0.7302, auc 0.8139
epoch 6401, loss 0.3451, train acc 84.39%, f1 0.7681, precision 0.7990, recall 0.7395, auc 0.8198
epoch 6501, loss 0.3419, train acc 84.55%, f1 0.7711, precision 0.8000, recall 0.7442, auc 0.8221
epoch 6601, loss 0.3390, train acc 84.88%, f1 0.7759, precision 0.8050, recall 0.7488, auc 0.8257
epoch 6701, loss 0.3363, train acc 84.88%, f1 0.7759, precision 0.8050, recall 0.7488, auc 0.8257
epoch 6801, loss 0.3336, train acc 84.88%, f1 0.7759, precision 0.8050, recall 0.7488, auc 0.8257
epoch 6901, loss 0.3302, train acc 85.20%, f1 0.7807, precision 0.8100, recall 0.7535, auc 0.8292
epoch 7001, loss 0.3263, train acc 85.69%, f1 0.7885, precision 0.8159, recall 0.7628, auc 0.8351
epoch 7101, loss 0.3227, train acc 86.02%, f1 0.7933, precision 0.8209, recall 0.7674, auc 0.8387
epoch 7201, loss 0.3196, train acc 86.02%, f1 0.7933, precision 0.8209, recall 0.7674, auc 0.8387
epoch 7301, loss 0.3165, train acc 86.18%, f1 0.7962, precision 0.8218, recall 0.7721, auc 0.8410
epoch 7401, loss 0.3132, train acc 86.02%, f1 0.7933, precision 0.8209, recall 0.7674, auc 0.8387
epoch 7501, loss 0.3100, train acc 86.67%, f1 0.8029, precision 0.8308, recall 0.7767, auc 0.8459
epoch 7601, loss 0.3071, train acc 86.83%, f1 0.8058, precision 0.8317, recall 0.7814, auc 0.8482
epoch 7701, loss 0.3044, train acc 87.15%, f1 0.8106, precision 0.8366, recall 0.7860, auc 0.8518
epoch 7801, loss 0.3017, train acc 87.32%, f1 0.8134, precision 0.8374, recall 0.7907, auc 0.8541
epoch 7901, loss 0.2989, train acc 87.15%, f1 0.8106, precision 0.8366, recall 0.7860, auc 0.8518
epoch 8001, loss 0.2963, train acc 87.48%, f1 0.8162, precision 0.8382, recall 0.7953, auc 0.8564
epoch 8101, loss 0.2941, train acc 87.48%, f1 0.8162, precision 0.8382, recall 0.7953, auc 0.8564
epoch 8201, loss 0.2921, train acc 87.64%, f1 0.8182, precision 0.8424, recall 0.7953, auc 0.8577/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2898, train acc 87.80%, f1 0.8210, precision 0.8431, recall 0.8000, auc 0.8600
epoch 8401, loss 0.2866, train acc 88.46%, f1 0.8322, precision 0.8462, recall 0.8186, auc 0.8693
epoch 8501, loss 0.2841, train acc 88.78%, f1 0.8376, precision 0.8476, recall 0.8279, auc 0.8740
epoch 8601, loss 0.2820, train acc 88.94%, f1 0.8396, precision 0.8517, recall 0.8279, auc 0.8752
epoch 8701, loss 0.2800, train acc 89.11%, f1 0.8416, precision 0.8558, recall 0.8279, auc 0.8765
epoch 8801, loss 0.2779, train acc 89.27%, f1 0.8443, precision 0.8565, recall 0.8326, auc 0.8788
epoch 8901, loss 0.2747, train acc 89.11%, f1 0.8416, precision 0.8558, recall 0.8279, auc 0.8765
epoch 9001, loss 0.2705, train acc 88.94%, f1 0.8396, precision 0.8517, recall 0.8279, auc 0.8752
epoch 9101, loss 0.2664, train acc 88.62%, f1 0.8349, precision 0.8469, recall 0.8233, auc 0.8716
epoch 9201, loss 0.2622, train acc 88.29%, f1 0.8302, precision 0.8421, recall 0.8186, auc 0.8681
epoch 9301, loss 0.2568, train acc 88.62%, f1 0.8349, precision 0.8469, recall 0.8233, auc 0.8716
epoch 9401, loss 0.2517, train acc 88.78%, f1 0.8376, precision 0.8476, recall 0.8279, auc 0.8740
epoch 9501, loss 0.2476, train acc 88.94%, f1 0.8404, precision 0.8483, recall 0.8326, auc 0.8763
epoch 9601, loss 0.2443, train acc 89.27%, f1 0.8465, precision 0.8465, recall 0.8465, auc 0.8820
epoch 9701, loss 0.2414, train acc 89.27%, f1 0.8472, precision 0.8433, recall 0.8512, auc 0.8831
epoch 9801, loss 0.2385, train acc 89.76%, f1 0.8545, precision 0.8486, recall 0.8605, auc 0.8890
epoch 9901, loss 0.2361, train acc 89.92%, f1 0.8565, precision 0.8525, recall 0.8605, auc 0.8902
epoch 10001, loss 0.2339, train acc 89.92%, f1 0.8565, precision 0.8525, recall 0.8605, auc 0.8902
epoch 10101, loss 0.2318, train acc 90.08%, f1 0.8585, precision 0.8565, recall 0.8605, auc 0.8915
epoch 10201, loss 0.2298, train acc 90.41%, f1 0.8625, precision 0.8645, recall 0.8605, auc 0.8940
epoch 10301, loss 0.2278, train acc 90.57%, f1 0.8645, precision 0.8685, recall 0.8605, auc 0.8952
epoch 10401, loss 0.2258, train acc 90.57%, f1 0.8638, precision 0.8720, recall 0.8558, auc 0.8942
epoch 10501, loss 0.2235, train acc 91.22%, f1 0.8726, precision 0.8852, recall 0.8605, auc 0.9002
epoch 10601, loss 0.2212, train acc 91.06%, f1 0.8706, precision 0.8810, recall 0.8605, auc 0.8990
epoch 10701, loss 0.2193, train acc 91.06%, f1 0.8712, precision 0.8774, recall 0.8651, auc 0.9001
epoch 10801, loss 0.2177, train acc 90.89%, f1 0.8685, precision 0.8768, recall 0.8605, auc 0.8977
epoch 10901, loss 0.2162, train acc 90.89%, f1 0.8685, precision 0.8768, recall 0.8605, auc 0.8977
epoch 11001, loss 0.2147, train acc 90.89%, f1 0.8685, precision 0.8768, recall 0.8605, auc 0.8977
epoch 11101, loss 0.2133, train acc 91.06%, f1 0.8706, precision 0.8810, recall 0.8605, auc 0.8990
epoch 11201, loss 0.2119, train acc 91.06%, f1 0.8706, precision 0.8810, recall 0.8605, auc 0.8990
epoch 11301, loss 0.2106, train acc 91.22%, f1 0.8726, precision 0.8852, recall 0.8605, auc 0.9002
epoch 11401, loss 0.2093, train acc 91.22%, f1 0.8726, precision 0.8852, recall 0.8605, auc 0.9002
epoch 11501, loss 0.2081, train acc 91.54%, f1 0.8774, precision 0.8900, recall 0.8651, auc 0.9038
epoch 11601, loss 0.2067, train acc 91.38%, f1 0.8747, precision 0.8894, recall 0.8605, auc 0.9015
epoch 11701, loss 0.2048, train acc 91.38%, f1 0.8747, precision 0.8894, recall 0.8605, auc 0.9015
epoch 11801, loss 0.2018, train acc 91.22%, f1 0.8720, precision 0.8889, recall 0.8558, auc 0.8992
epoch 11901, loss 0.1990, train acc 91.71%, f1 0.8794, precision 0.8942, recall 0.8651, auc 0.9051
epoch 12001, loss 0.1961, train acc 91.22%, f1 0.8720, precision 0.8889, recall 0.8558, auc 0.8992
epoch 12101, loss 0.1935, train acc 91.71%, f1 0.8794, precision 0.8942, recall 0.8651, auc 0.9051
epoch 12201, loss 0.1909, train acc 92.52%, f1 0.8915, precision 0.9043, recall 0.8791, auc 0.9145
epoch 12301, loss 0.1886, train acc 92.68%, f1 0.8936, precision 0.9087, recall 0.8791, auc 0.9158
epoch 12401, loss 0.1863, train acc 92.52%, f1 0.8915, precision 0.9043, recall 0.8791, auc 0.9145
epoch 12501, loss 0.1842, train acc 92.52%, f1 0.8915, precision 0.9043, recall 0.8791, auc 0.9145
epoch 12601, loss 0.1824, train acc 92.68%, f1 0.8936, precision 0.9087, recall 0.8791, auc 0.9158
epoch 12701, loss 0.1807, train acc 92.52%, f1 0.8910, precision 0.9082, recall 0.8744, auc 0.9135
epoch 12801, loss 0.1792, train acc 92.68%, f1 0.8931, precision 0.9126, recall 0.8744, auc 0.9147
epoch 12901, loss 0.1777, train acc 92.68%, f1 0.8931, precision 0.9126, recall 0.8744, auc 0.9147
epoch 13001, loss 0.1762, train acc 92.68%, f1 0.8931, precision 0.9126, recall 0.8744, auc 0.9147
epoch 13101, loss 0.1748, train acc 92.85%, f1 0.8957, precision 0.9130, recall 0.8791, auc 0.9170
epoch 13201, loss 0.1734, train acc 93.01%, f1 0.8979, precision 0.9175, recall 0.8791, auc 0.9183
epoch 13301, loss 0.1721, train acc 93.01%, f1 0.8979, precision 0.9175, recall 0.8791, auc 0.9183
epoch 13401, loss 0.1708, train acc 93.01%, f1 0.8979, precision 0.9175, recall 0.8791, auc 0.9183
epoch 13501, loss 0.1695, train acc 93.17%, f1 0.9005, precision 0.9179, recall 0.8837, auc 0.9206
epoch 13601, loss 0.1682, train acc 93.33%, f1 0.9031, precision 0.9183, recall 0.8884, auc 0.9229
epoch 13701, loss 0.1669, train acc 93.33%, f1 0.9031, precision 0.9183, recall 0.8884, auc 0.9229
epoch 13801, loss 0.1657, train acc 93.17%, f1 0.9005, precision 0.9179, recall 0.8837, auc 0.9206
epoch 13901, loss 0.1645, train acc 93.17%, f1 0.9005, precision 0.9179, recall 0.8837, auc 0.9206
epoch 14001, loss 0.1634, train acc 93.66%, f1 0.9069, precision 0.9314, recall 0.8837, auc 0.9244
epoch 14101, loss 0.1622, train acc 93.66%, f1 0.9069, precision 0.9314, recall 0.8837, auc 0.9244
epoch 14201, loss 0.1611, train acc 93.66%, f1 0.9074, precision 0.9272, recall 0.8884, auc 0.9254
epoch 14301, loss 0.1600, train acc 93.82%, f1 0.9100, precision 0.9275, recall 0.8930, auc 0.9278
epoch 14401, loss 0.1589, train acc 93.82%, f1 0.9100, precision 0.9275, recall 0.8930, auc 0.9278
epoch 14501, loss 0.1578, train acc 93.98%, f1 0.9121, precision 0.9320, recall 0.8930, auc 0.9290
epoch 14601, loss 0.1567, train acc 93.98%, f1 0.9121, precision 0.9320, recall 0.8930, auc 0.9290
epoch 14701, loss 0.1557, train acc 93.98%, f1 0.9121, precision 0.9320, recall 0.8930, auc 0.9290
epoch 14801, loss 0.1547, train acc 93.98%, f1 0.9121, precision 0.9320, recall 0.8930, auc 0.9290
epoch 14901, loss 0.1537, train acc 93.98%, f1 0.9121, precision 0.9320, recall 0.8930, auc 0.9290
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_normal_15000
normal
./test_pima/model_MLP_normal_15000/record_1/MLP_normal_15000_5
./test_pima/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.6464150943396226

the Fscore is 0.5161290322580645

the precision is 0.6

the recall is 0.4528301886792453

Done
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_5
----------------------



epoch 1, loss 0.6910, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6226, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5951, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5693, train acc 64.88%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5463, train acc 68.13%, f1 0.1901, precision 0.8519, recall 0.1070, auc 0.5485
epoch 501, loss 0.5278, train acc 71.38%, f1 0.3577, precision 0.8305, recall 0.2279, auc 0.6015
epoch 601, loss 0.5134, train acc 74.96%, f1 0.5064, precision 0.8144, recall 0.3674, auc 0.6612
epoch 701, loss 0.5024, train acc 75.12%, f1 0.5487, precision 0.7500, recall 0.4326, auc 0.6775
epoch 801, loss 0.4942, train acc 76.26%, f1 0.5989, precision 0.7315, recall 0.5070, auc 0.7035
epoch 901, loss 0.4881, train acc 76.75%, f1 0.6227, precision 0.7195, recall 0.5488, auc 0.7169
epoch 1001, loss 0.4835, train acc 77.24%, f1 0.6354, precision 0.7219, recall 0.5674, auc 0.7250
epoch 1101, loss 0.4799, train acc 77.89%, f1 0.6531, precision 0.7232, recall 0.5953, auc 0.7364
epoch 1201, loss 0.4767, train acc 78.54%, f1 0.6667, precision 0.7293, recall 0.6140, auc 0.7457
epoch 1301, loss 0.4729, train acc 77.89%, f1 0.6583, precision 0.7158, recall 0.6093, auc 0.7397
epoch 1401, loss 0.4684, train acc 79.02%, f1 0.6718, precision 0.7416, recall 0.6140, auc 0.7495
epoch 1501, loss 0.4638, train acc 79.35%, f1 0.6735, precision 0.7529, recall 0.6093, auc 0.7509
epoch 1601, loss 0.4591, train acc 79.19%, f1 0.6718, precision 0.7486, recall 0.6093, auc 0.7497
epoch 1701, loss 0.4546, train acc 79.51%, f1 0.6736, precision 0.7602, recall 0.6047, auc 0.7511
epoch 1801, loss 0.4504, train acc 79.67%, f1 0.6787, precision 0.7586, recall 0.6140, auc 0.7545
epoch 1901, loss 0.4465, train acc 79.84%, f1 0.6837, precision 0.7571, recall 0.6233, auc 0.7579
epoch 2001, loss 0.4430, train acc 79.84%, f1 0.6869, precision 0.7514, recall 0.6326, auc 0.7600
epoch 2101, loss 0.4399, train acc 79.84%, f1 0.6853, precision 0.7542, recall 0.6279, auc 0.7590
epoch 2201, loss 0.4372, train acc 79.84%, f1 0.6869, precision 0.7514, recall 0.6326, auc 0.7600
epoch 2301, loss 0.4349, train acc 80.00%, f1 0.6917, precision 0.7500, recall 0.6419, auc 0.7634
epoch 2401, loss 0.4328, train acc 80.33%, f1 0.6983, precision 0.7527, recall 0.6512, auc 0.7681
epoch 2501, loss 0.4310, train acc 79.84%, f1 0.6915, precision 0.7433, recall 0.6465, auc 0.7633
epoch 2601, loss 0.4293, train acc 80.00%, f1 0.6948, precision 0.7447, recall 0.6512, auc 0.7656
epoch 2701, loss 0.4277, train acc 80.33%, f1 0.6983, precision 0.7527, recall 0.6512, auc 0.7681
epoch 2801, loss 0.4261, train acc 80.49%, f1 0.7015, precision 0.7540, recall 0.6558, auc 0.7704
epoch 2901, loss 0.4245, train acc 80.49%, f1 0.7015, precision 0.7540, recall 0.6558, auc 0.7704
epoch 3001, loss 0.4229, train acc 80.81%, f1 0.7094, precision 0.7539, recall 0.6698, auc 0.7761
epoch 3101, loss 0.4213, train acc 80.81%, f1 0.7094, precision 0.7539, recall 0.6698, auc 0.7761
epoch 3201, loss 0.4196, train acc 80.65%, f1 0.7076, precision 0.7500, recall 0.6698, auc 0.7749
epoch 3301, loss 0.4179, train acc 80.49%, f1 0.7044, precision 0.7487, recall 0.6651, auc 0.7726
epoch 3401, loss 0.4158, train acc 80.81%, f1 0.7079, precision 0.7566, recall 0.6651, auc 0.7751
epoch 3501, loss 0.4133, train acc 80.81%, f1 0.7094, precision 0.7539, recall 0.6698, auc 0.7761
epoch 3601, loss 0.4104, train acc 81.14%, f1 0.7157, precision 0.7565, recall 0.6791, auc 0.7808
epoch 3701, loss 0.4072, train acc 80.98%, f1 0.7125, precision 0.7552, recall 0.6744, auc 0.7785
epoch 3801, loss 0.4039, train acc 81.63%, f1 0.7224, precision 0.7656, recall 0.6837, auc 0.7856
epoch 3901, loss 0.4007, train acc 81.95%, f1 0.7273, precision 0.7708, recall 0.6884, auc 0.7892
epoch 4001, loss 0.3977, train acc 82.11%, f1 0.7291, precision 0.7749, recall 0.6884, auc 0.7904
epoch 4101, loss 0.3944, train acc 82.60%, f1 0.7345, precision 0.7872, recall 0.6884, auc 0.7942
epoch 4201, loss 0.3911, train acc 83.25%, f1 0.7457, precision 0.7947, recall 0.7023, auc 0.8024
epoch 4301, loss 0.3879, train acc 83.25%, f1 0.7457, precision 0.7947, recall 0.7023, auc 0.8024
epoch 4401, loss 0.3850, train acc 82.60%, f1 0.7332, precision 0.7903, recall 0.6837, auc 0.7931
epoch 4501, loss 0.3820, train acc 82.76%, f1 0.7376, precision 0.7884, recall 0.6930, auc 0.7965
epoch 4601, loss 0.3790, train acc 82.76%, f1 0.7376, precision 0.7884, recall 0.6930, auc 0.7965
epoch 4701, loss 0.3759, train acc 82.93%, f1 0.7395, precision 0.7926, recall 0.6930, auc 0.7978
epoch 4801, loss 0.3730, train acc 83.09%, f1 0.7426, precision 0.7937, recall 0.6977, auc 0.8001
epoch 4901, loss 0.3702, train acc 83.25%, f1 0.7457, precision 0.7947, recall 0.7023, auc 0.8024
epoch 5001, loss 0.3675, train acc 83.58%, f1 0.7506, precision 0.8000, recall 0.7070, auc 0.8060
epoch 5101, loss 0.3647, train acc 83.58%, f1 0.7506, precision 0.8000, recall 0.7070, auc 0.8060
epoch 5201, loss 0.3617, train acc 83.74%, f1 0.7537, precision 0.8010, recall 0.7116, auc 0.8083
epoch 5301, loss 0.3584, train acc 84.07%, f1 0.7598, precision 0.8031, recall 0.7209, auc 0.8130
epoch 5401, loss 0.3548, train acc 84.23%, f1 0.7640, precision 0.8010, recall 0.7302, auc 0.8164
epoch 5501, loss 0.3509, train acc 84.07%, f1 0.7621, precision 0.7970, recall 0.7302, auc 0.8151
epoch 5601, loss 0.3466, train acc 84.55%, f1 0.7700, precision 0.8030, recall 0.7395, auc 0.8210
epoch 5701, loss 0.3417, train acc 84.07%, f1 0.7633, precision 0.7940, recall 0.7349, auc 0.8162
epoch 5801, loss 0.3358, train acc 84.23%, f1 0.7663, precision 0.7950, recall 0.7395, auc 0.8185
epoch 5901, loss 0.3309, train acc 85.04%, f1 0.7788, precision 0.8060, recall 0.7535, auc 0.8280
epoch 6001, loss 0.3266, train acc 85.53%, f1 0.7876, precision 0.8088, recall 0.7674, auc 0.8350
epoch 6101, loss 0.3226, train acc 86.18%, f1 0.7981, precision 0.8155, recall 0.7814, auc 0.8432
epoch 6201, loss 0.3188, train acc 86.83%, f1 0.8067, precision 0.8284, recall 0.7860, auc 0.8493
epoch 6301, loss 0.3151, train acc 87.32%, f1 0.8125, precision 0.8408, recall 0.7860, auc 0.8530
epoch 6401, loss 0.3115, train acc 87.15%, f1 0.8106, precision 0.8366, recall 0.7860, auc 0.8518
epoch 6501, loss 0.3082, train acc 87.64%, f1 0.8182, precision 0.8424, recall 0.7953, auc 0.8577
epoch 6601, loss 0.3050, train acc 87.48%, f1 0.8162, precision 0.8382, recall 0.7953, auc 0.8564
epoch 6701, loss 0.3020, train acc 87.48%, f1 0.8171, precision 0.8350, recall 0.8000, auc 0.8575
epoch 6801, loss 0.2992, train acc 87.64%, f1 0.8199, precision 0.8357, recall 0.8047, auc 0.8598
epoch 6901, loss 0.2965, train acc 88.29%, f1 0.8310, precision 0.8389, recall 0.8233, auc 0.8691
epoch 7001, loss 0.2938, train acc 88.29%, f1 0.8310, precision 0.8389, recall 0.8233, auc 0.8691
epoch 7101, loss 0.2913, train acc 88.78%, f1 0.8384, precision 0.8443, recall 0.8326, auc 0.8750
epoch 7201, loss 0.2889, train acc 88.94%, f1 0.8404, precision 0.8483, recall 0.8326, auc 0.8763
epoch 7301, loss 0.2865, train acc 88.94%, f1 0.8411, precision 0.8451, recall 0.8372, auc 0.8774
epoch 7401, loss 0.2842, train acc 89.43%, f1 0.8485, precision 0.8505, recall 0.8465, auc 0.8833
epoch 7501, loss 0.2819, train acc 89.59%, f1 0.8512, precision 0.8512, recall 0.8512, auc 0.8856
epoch 7601, loss 0.2797, train acc 89.76%, f1 0.8538, precision 0.8519, recall 0.8558, auc 0.8879
epoch 7701, loss 0.2776, train acc 89.76%, f1 0.8538, precision 0.8519, recall 0.8558, auc 0.8879
epoch 7801, loss 0.2755, train acc 89.76%, f1 0.8538, precision 0.8519, recall 0.8558, auc 0.8879
epoch 7901, loss 0.2735, train acc 89.76%, f1 0.8538, precision 0.8519, recall 0.8558, auc 0.8879
epoch 8001, loss 0.2715, train acc 89.59%, f1 0.8519, precision 0.8479, recall 0.8558, auc 0.8867
epoch 8101, loss 0.2697, train acc 89.43%, f1 0.8492, precision 0.8472, recall 0.8512, auc 0.8843
epoch 8201, loss 0.2679, train acc 89.43%, f1 0.8492, precision 0.8472, recall 0.8512, auc 0.8843/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.2661, train acc 89.43%, f1 0.8492, precision 0.8472, recall 0.8512, auc 0.8843
epoch 8401, loss 0.2644, train acc 89.43%, f1 0.8492, precision 0.8472, recall 0.8512, auc 0.8843
epoch 8501, loss 0.2628, train acc 89.43%, f1 0.8492, precision 0.8472, recall 0.8512, auc 0.8843
epoch 8601, loss 0.2612, train acc 89.43%, f1 0.8492, precision 0.8472, recall 0.8512, auc 0.8843
epoch 8701, loss 0.2596, train acc 89.59%, f1 0.8519, precision 0.8479, recall 0.8558, auc 0.8867
epoch 8801, loss 0.2580, train acc 89.76%, f1 0.8545, precision 0.8486, recall 0.8605, auc 0.8890
epoch 8901, loss 0.2563, train acc 89.59%, f1 0.8519, precision 0.8479, recall 0.8558, auc 0.8867
epoch 9001, loss 0.2544, train acc 89.43%, f1 0.8492, precision 0.8472, recall 0.8512, auc 0.8843
epoch 9101, loss 0.2527, train acc 89.43%, f1 0.8492, precision 0.8472, recall 0.8512, auc 0.8843
epoch 9201, loss 0.2511, train acc 89.59%, f1 0.8519, precision 0.8479, recall 0.8558, auc 0.8867
epoch 9301, loss 0.2497, train acc 89.59%, f1 0.8519, precision 0.8479, recall 0.8558, auc 0.8867
epoch 9401, loss 0.2483, train acc 89.92%, f1 0.8565, precision 0.8525, recall 0.8605, auc 0.8902
epoch 9501, loss 0.2469, train acc 89.92%, f1 0.8565, precision 0.8525, recall 0.8605, auc 0.8902
epoch 9601, loss 0.2456, train acc 89.76%, f1 0.8545, precision 0.8486, recall 0.8605, auc 0.8890
epoch 9701, loss 0.2444, train acc 89.92%, f1 0.8571, precision 0.8493, recall 0.8651, auc 0.8913
epoch 9801, loss 0.2431, train acc 89.92%, f1 0.8571, precision 0.8493, recall 0.8651, auc 0.8913
epoch 9901, loss 0.2419, train acc 90.08%, f1 0.8591, precision 0.8532, recall 0.8651, auc 0.8926
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_normal_10000
normal
./test_pima/model_MLP_normal_10000/record_1/MLP_normal_10000_5
./test_pima/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.6780188679245284

the Fscore is 0.576923076923077

the precision is 0.5882352941176471

the recall is 0.5660377358490566

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_5
----------------------



epoch 1, loss 0.6980, train acc 34.96%, f1 0.5181, precision 0.3496, recall 1.0000, auc 0.5000
epoch 101, loss 0.6250, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5970, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5717, train acc 64.88%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5488, train acc 67.48%, f1 0.1525, precision 0.8571, recall 0.0837, auc 0.5381
epoch 501, loss 0.5301, train acc 71.06%, f1 0.3456, precision 0.8246, recall 0.2186, auc 0.5968
epoch 601, loss 0.5154, train acc 74.31%, f1 0.4837, precision 0.8132, recall 0.3442, auc 0.6508
epoch 701, loss 0.5042, train acc 76.10%, f1 0.5586, precision 0.7881, recall 0.4326, auc 0.6850
epoch 801, loss 0.4957, train acc 76.59%, f1 0.6000, precision 0.7448, recall 0.5023, auc 0.7049
epoch 901, loss 0.4893, train acc 76.59%, f1 0.6190, precision 0.7178, recall 0.5442, auc 0.7146
epoch 1001, loss 0.4845, train acc 77.24%, f1 0.6354, precision 0.7219, recall 0.5674, auc 0.7250
epoch 1101, loss 0.4807, train acc 77.72%, f1 0.6478, precision 0.7241, recall 0.5860, auc 0.7330
epoch 1201, loss 0.4771, train acc 78.54%, f1 0.6667, precision 0.7293, recall 0.6140, auc 0.7457
epoch 1301, loss 0.4729, train acc 78.21%, f1 0.6599, precision 0.7263, recall 0.6047, auc 0.7411
epoch 1401, loss 0.4681, train acc 79.35%, f1 0.6752, precision 0.7500, recall 0.6140, auc 0.7520
epoch 1501, loss 0.4630, train acc 79.35%, f1 0.6735, precision 0.7529, recall 0.6093, auc 0.7509
epoch 1601, loss 0.4580, train acc 79.35%, f1 0.6735, precision 0.7529, recall 0.6093, auc 0.7509
epoch 1701, loss 0.4533, train acc 79.67%, f1 0.6787, precision 0.7586, recall 0.6140, auc 0.7545
epoch 1801, loss 0.4491, train acc 79.84%, f1 0.6837, precision 0.7571, recall 0.6233, auc 0.7579
epoch 1901, loss 0.4454, train acc 80.33%, f1 0.6921, precision 0.7640, recall 0.6326, auc 0.7638
epoch 2001, loss 0.4421, train acc 80.33%, f1 0.6921, precision 0.7640, recall 0.6326, auc 0.7638
epoch 2101, loss 0.4394, train acc 79.51%, f1 0.6834, precision 0.7432, recall 0.6326, auc 0.7575
epoch 2201, loss 0.4370, train acc 79.67%, f1 0.6867, precision 0.7446, recall 0.6372, auc 0.7599
epoch 2301, loss 0.4349, train acc 79.84%, f1 0.6915, precision 0.7433, recall 0.6465, auc 0.7633
epoch 2401, loss 0.4330, train acc 79.84%, f1 0.6915, precision 0.7433, recall 0.6465, auc 0.7633
epoch 2501, loss 0.4313, train acc 80.00%, f1 0.6948, precision 0.7447, recall 0.6512, auc 0.7656
epoch 2601, loss 0.4297, train acc 80.00%, f1 0.6948, precision 0.7447, recall 0.6512, auc 0.7656
epoch 2701, loss 0.4282, train acc 80.16%, f1 0.6980, precision 0.7460, recall 0.6558, auc 0.7679
epoch 2801, loss 0.4267, train acc 80.16%, f1 0.6980, precision 0.7460, recall 0.6558, auc 0.7679
epoch 2901, loss 0.4254, train acc 80.33%, f1 0.6998, precision 0.7500, recall 0.6558, auc 0.7692
epoch 3001, loss 0.4240, train acc 80.81%, f1 0.7094, precision 0.7539, recall 0.6698, auc 0.7761
epoch 3101, loss 0.4223, train acc 80.81%, f1 0.7094, precision 0.7539, recall 0.6698, auc 0.7761
epoch 3201, loss 0.4207, train acc 80.81%, f1 0.7094, precision 0.7539, recall 0.6698, auc 0.7761
epoch 3301, loss 0.4191, train acc 81.14%, f1 0.7143, precision 0.7592, recall 0.6744, auc 0.7797
epoch 3401, loss 0.4176, train acc 80.98%, f1 0.7111, precision 0.7579, recall 0.6698, auc 0.7774
epoch 3501, loss 0.4161, train acc 80.81%, f1 0.7079, precision 0.7566, recall 0.6651, auc 0.7751
epoch 3601, loss 0.4147, train acc 80.65%, f1 0.7047, precision 0.7553, recall 0.6605, auc 0.7727
epoch 3701, loss 0.4132, train acc 80.81%, f1 0.7079, precision 0.7566, recall 0.6651, auc 0.7751
epoch 3801, loss 0.4118, train acc 80.65%, f1 0.7062, precision 0.7526, recall 0.6651, auc 0.7738
epoch 3901, loss 0.4104, train acc 80.65%, f1 0.7062, precision 0.7526, recall 0.6651, auc 0.7738
epoch 4001, loss 0.4089, train acc 80.65%, f1 0.7076, precision 0.7500, recall 0.6698, auc 0.7749
epoch 4101, loss 0.4072, train acc 80.65%, f1 0.7076, precision 0.7500, recall 0.6698, auc 0.7749
epoch 4201, loss 0.4054, train acc 80.98%, f1 0.7111, precision 0.7579, recall 0.6698, auc 0.7774
epoch 4301, loss 0.4037, train acc 80.81%, f1 0.7094, precision 0.7539, recall 0.6698, auc 0.7761
epoch 4401, loss 0.4020, train acc 81.30%, f1 0.7174, precision 0.7604, recall 0.6791, auc 0.7820
epoch 4501, loss 0.4003, train acc 81.14%, f1 0.7157, precision 0.7565, recall 0.6791, auc 0.7808
epoch 4601, loss 0.3987, train acc 81.30%, f1 0.7188, precision 0.7577, recall 0.6837, auc 0.7831
epoch 4701, loss 0.3972, train acc 81.46%, f1 0.7220, precision 0.7590, recall 0.6884, auc 0.7854
epoch 4801, loss 0.3955, train acc 81.95%, f1 0.7299, precision 0.7653, recall 0.6977, auc 0.7913
epoch 4901, loss 0.3937, train acc 82.44%, f1 0.7391, precision 0.7688, recall 0.7116, auc 0.7983
epoch 5001, loss 0.3916, train acc 82.44%, f1 0.7391, precision 0.7688, recall 0.7116, auc 0.7983
epoch 5101, loss 0.3889, train acc 82.76%, f1 0.7440, precision 0.7739, recall 0.7163, auc 0.8019
epoch 5201, loss 0.3856, train acc 83.74%, f1 0.7608, precision 0.7833, recall 0.7395, auc 0.8148
epoch 5301, loss 0.3816, train acc 83.74%, f1 0.7596, precision 0.7861, recall 0.7349, auc 0.8137
epoch 5401, loss 0.3777, train acc 84.07%, f1 0.7644, precision 0.7910, recall 0.7395, auc 0.8173
epoch 5501, loss 0.3744, train acc 84.23%, f1 0.7663, precision 0.7950, recall 0.7395, auc 0.8185
epoch 5601, loss 0.3712, train acc 84.39%, f1 0.7692, precision 0.7960, recall 0.7442, auc 0.8208
epoch 5701, loss 0.3680, train acc 84.55%, f1 0.7722, precision 0.7970, recall 0.7488, auc 0.8232
epoch 5801, loss 0.3641, train acc 84.55%, f1 0.7722, precision 0.7970, recall 0.7488, auc 0.8232
epoch 5901, loss 0.3598, train acc 83.90%, f1 0.7626, precision 0.7871, recall 0.7395, auc 0.8160
epoch 6001, loss 0.3559, train acc 84.23%, f1 0.7685, precision 0.7892, recall 0.7488, auc 0.8207
epoch 6101, loss 0.3524, train acc 84.23%, f1 0.7685, precision 0.7892, recall 0.7488, auc 0.8207
epoch 6201, loss 0.3491, train acc 84.88%, f1 0.7791, precision 0.7961, recall 0.7628, auc 0.8289
epoch 6301, loss 0.3457, train acc 84.72%, f1 0.7751, precision 0.7980, recall 0.7535, auc 0.8255
epoch 6401, loss 0.3417, train acc 84.88%, f1 0.7759, precision 0.8050, recall 0.7488, auc 0.8257
epoch 6501, loss 0.3380, train acc 84.88%, f1 0.7759, precision 0.8050, recall 0.7488, auc 0.8257
epoch 6601, loss 0.3349, train acc 85.04%, f1 0.7799, precision 0.8030, recall 0.7581, auc 0.8291
epoch 6701, loss 0.3322, train acc 85.04%, f1 0.7799, precision 0.8030, recall 0.7581, auc 0.8291
epoch 6801, loss 0.3298, train acc 85.20%, f1 0.7828, precision 0.8039, recall 0.7628, auc 0.8314
epoch 6901, loss 0.3275, train acc 85.37%, f1 0.7857, precision 0.8049, recall 0.7674, auc 0.8337
epoch 7001, loss 0.3251, train acc 85.85%, f1 0.7924, precision 0.8137, recall 0.7721, auc 0.8385
epoch 7101, loss 0.3228, train acc 86.02%, f1 0.7952, precision 0.8146, recall 0.7767, auc 0.8409
epoch 7201, loss 0.3204, train acc 85.85%, f1 0.7914, precision 0.8168, recall 0.7674, auc 0.8375
epoch 7301, loss 0.3182, train acc 86.50%, f1 0.8019, precision 0.8235, recall 0.7814, auc 0.8457
epoch 7401, loss 0.3160, train acc 86.02%, f1 0.7962, precision 0.8116, recall 0.7814, auc 0.8419
epoch 7501, loss 0.3140, train acc 86.02%, f1 0.7962, precision 0.8116, recall 0.7814, auc 0.8419
epoch 7601, loss 0.3121, train acc 86.50%, f1 0.8029, precision 0.8204, recall 0.7860, auc 0.8468
epoch 7701, loss 0.3102, train acc 86.99%, f1 0.8113, precision 0.8230, recall 0.8000, auc 0.8538
epoch 7801, loss 0.3084, train acc 87.32%, f1 0.8160, precision 0.8278, recall 0.8047, auc 0.8573
epoch 7901, loss 0.3068, train acc 87.32%, f1 0.8169, precision 0.8246, recall 0.8093, auc 0.8584
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_normal_8000
normal
./test_pima/model_MLP_normal_8000/record_1/MLP_normal_8000_5
./test_pima/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.710754716981132

the Fscore is 0.6238532110091742

the precision is 0.6071428571428571

the recall is 0.6415094339622641

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_5
----------------------



epoch 1, loss 0.6925, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6228, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5952, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5696, train acc 64.88%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5467, train acc 67.97%, f1 0.1826, precision 0.8462, recall 0.1023, auc 0.5462
epoch 501, loss 0.5282, train acc 71.38%, f1 0.3577, precision 0.8305, recall 0.2279, auc 0.6015
epoch 601, loss 0.5137, train acc 74.80%, f1 0.5016, precision 0.8125, recall 0.3628, auc 0.6589
epoch 701, loss 0.5027, train acc 75.12%, f1 0.5487, precision 0.7500, recall 0.4326, auc 0.6775
epoch 801, loss 0.4945, train acc 76.10%, f1 0.5950, precision 0.7297, recall 0.5023, auc 0.7012
epoch 901, loss 0.4883, train acc 76.75%, f1 0.6227, precision 0.7195, recall 0.5488, auc 0.7169
epoch 1001, loss 0.4837, train acc 77.24%, f1 0.6354, precision 0.7219, recall 0.5674, auc 0.7250
epoch 1101, loss 0.4799, train acc 77.72%, f1 0.6478, precision 0.7241, recall 0.5860, auc 0.7330
epoch 1201, loss 0.4763, train acc 78.54%, f1 0.6667, precision 0.7293, recall 0.6140, auc 0.7457
epoch 1301, loss 0.4722, train acc 78.21%, f1 0.6599, precision 0.7263, recall 0.6047, auc 0.7411
epoch 1401, loss 0.4675, train acc 79.19%, f1 0.6735, precision 0.7458, recall 0.6140, auc 0.7507
epoch 1501, loss 0.4627, train acc 79.35%, f1 0.6735, precision 0.7529, recall 0.6093, auc 0.7509
epoch 1601, loss 0.4580, train acc 79.35%, f1 0.6735, precision 0.7529, recall 0.6093, auc 0.7509
epoch 1701, loss 0.4535, train acc 79.67%, f1 0.6787, precision 0.7586, recall 0.6140, auc 0.7545
epoch 1801, loss 0.4493, train acc 79.84%, f1 0.6837, precision 0.7571, recall 0.6233, auc 0.7579
epoch 1901, loss 0.4456, train acc 80.16%, f1 0.6904, precision 0.7598, recall 0.6326, auc 0.7625
epoch 2001, loss 0.4424, train acc 80.00%, f1 0.6886, precision 0.7556, recall 0.6326, auc 0.7613
epoch 2101, loss 0.4397, train acc 79.51%, f1 0.6818, precision 0.7459, recall 0.6279, auc 0.7565
epoch 2201, loss 0.4374, train acc 79.51%, f1 0.6850, precision 0.7405, recall 0.6372, auc 0.7586
epoch 2301, loss 0.4354, train acc 79.84%, f1 0.6915, precision 0.7433, recall 0.6465, auc 0.7633
epoch 2401, loss 0.4336, train acc 80.00%, f1 0.6933, precision 0.7473, recall 0.6465, auc 0.7645
epoch 2501, loss 0.4319, train acc 80.16%, f1 0.6965, precision 0.7487, recall 0.6512, auc 0.7668
epoch 2601, loss 0.4304, train acc 80.16%, f1 0.6965, precision 0.7487, recall 0.6512, auc 0.7668
epoch 2701, loss 0.4288, train acc 80.00%, f1 0.6948, precision 0.7447, recall 0.6512, auc 0.7656
epoch 2801, loss 0.4273, train acc 80.00%, f1 0.6933, precision 0.7473, recall 0.6465, auc 0.7645
epoch 2901, loss 0.4257, train acc 80.16%, f1 0.6965, precision 0.7487, recall 0.6512, auc 0.7668
epoch 3001, loss 0.4243, train acc 80.16%, f1 0.6965, precision 0.7487, recall 0.6512, auc 0.7668
epoch 3101, loss 0.4229, train acc 80.00%, f1 0.6948, precision 0.7447, recall 0.6512, auc 0.7656
epoch 3201, loss 0.4215, train acc 79.84%, f1 0.6931, precision 0.7407, recall 0.6512, auc 0.7643
epoch 3301, loss 0.4200, train acc 79.84%, f1 0.6931, precision 0.7407, recall 0.6512, auc 0.7643
epoch 3401, loss 0.4182, train acc 80.49%, f1 0.7015, precision 0.7540, recall 0.6558, auc 0.7704
epoch 3501, loss 0.4166, train acc 80.33%, f1 0.6998, precision 0.7500, recall 0.6558, auc 0.7692
epoch 3601, loss 0.4150, train acc 80.49%, f1 0.7030, precision 0.7513, recall 0.6605, auc 0.7715
epoch 3701, loss 0.4134, train acc 80.65%, f1 0.7062, precision 0.7526, recall 0.6651, auc 0.7738
epoch 3801, loss 0.4119, train acc 80.81%, f1 0.7094, precision 0.7539, recall 0.6698, auc 0.7761
epoch 3901, loss 0.4103, train acc 80.98%, f1 0.7111, precision 0.7579, recall 0.6698, auc 0.7774
epoch 4001, loss 0.4087, train acc 80.81%, f1 0.7094, precision 0.7539, recall 0.6698, auc 0.7761
epoch 4101, loss 0.4071, train acc 81.46%, f1 0.7192, precision 0.7644, recall 0.6791, auc 0.7833
epoch 4201, loss 0.4055, train acc 81.46%, f1 0.7192, precision 0.7644, recall 0.6791, auc 0.7833
epoch 4301, loss 0.4038, train acc 81.46%, f1 0.7192, precision 0.7644, recall 0.6791, auc 0.7833
epoch 4401, loss 0.4020, train acc 81.14%, f1 0.7129, precision 0.7619, recall 0.6698, auc 0.7786
epoch 4501, loss 0.3999, train acc 81.30%, f1 0.7174, precision 0.7604, recall 0.6791, auc 0.7820
epoch 4601, loss 0.3977, train acc 80.98%, f1 0.7125, precision 0.7552, recall 0.6744, auc 0.7785
epoch 4701, loss 0.3950, train acc 81.30%, f1 0.7160, precision 0.7632, recall 0.6744, auc 0.7810
epoch 4801, loss 0.3917, train acc 81.30%, f1 0.7188, precision 0.7577, recall 0.6837, auc 0.7831
epoch 4901, loss 0.3881, train acc 81.14%, f1 0.7157, precision 0.7565, recall 0.6791, auc 0.7808
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_normal_5000
normal
./test_pima/model_MLP_normal_5000/record_1/MLP_normal_5000_5
./test_pima/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.6968867924528301

the Fscore is 0.6037735849056604

the precision is 0.6037735849056604

the recall is 0.6037735849056604

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_5
----------------------



epoch 1, loss 0.6914, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.6227, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.5951, train acc 65.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.5694, train acc 64.88%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4988
epoch 401, loss 0.5464, train acc 67.97%, f1 0.1893, precision 0.8214, recall 0.1070, auc 0.5472
epoch 501, loss 0.5279, train acc 71.38%, f1 0.3577, precision 0.8305, recall 0.2279, auc 0.6015
epoch 601, loss 0.5135, train acc 74.96%, f1 0.5064, precision 0.8144, recall 0.3674, auc 0.6612
epoch 701, loss 0.5025, train acc 74.96%, f1 0.5471, precision 0.7440, recall 0.4326, auc 0.6763
epoch 801, loss 0.4943, train acc 76.10%, f1 0.5973, precision 0.7267, recall 0.5070, auc 0.7022
epoch 901, loss 0.4882, train acc 76.75%, f1 0.6227, precision 0.7195, recall 0.5488, auc 0.7169
epoch 1001, loss 0.4836, train acc 77.24%, f1 0.6354, precision 0.7219, recall 0.5674, auc 0.7250
epoch 1101, loss 0.4799, train acc 78.05%, f1 0.6547, precision 0.7273, recall 0.5953, auc 0.7377
epoch 1201, loss 0.4765, train acc 78.54%, f1 0.6667, precision 0.7293, recall 0.6140, auc 0.7457
epoch 1301, loss 0.4724, train acc 77.89%, f1 0.6548, precision 0.7207, recall 0.6000, auc 0.7375
epoch 1401, loss 0.4676, train acc 78.86%, f1 0.6667, precision 0.7429, recall 0.6047, auc 0.7461
epoch 1501, loss 0.4625, train acc 79.35%, f1 0.6735, precision 0.7529, recall 0.6093, auc 0.7509
epoch 1601, loss 0.4578, train acc 79.35%, f1 0.6735, precision 0.7529, recall 0.6093, auc 0.7509
epoch 1701, loss 0.4534, train acc 79.51%, f1 0.6753, precision 0.7572, recall 0.6093, auc 0.7522
epoch 1801, loss 0.4495, train acc 80.16%, f1 0.6904, precision 0.7598, recall 0.6326, auc 0.7625
epoch 1901, loss 0.4462, train acc 80.16%, f1 0.6935, precision 0.7541, recall 0.6419, auc 0.7647
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_normal_2000
normal
./test_pima/model_MLP_normal_2000/record_1/MLP_normal_2000_5
./test_pima/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.7113207547169812

the Fscore is 0.6226415094339622

the precision is 0.6226415094339622

the recall is 0.6226415094339622

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_5
----------------------



epoch 1, loss 0.6932, train acc 69.29%, f1 0.7042, precision 0.6792, recall 0.7311, auc 0.6929
epoch 101, loss 0.3461, train acc 87.06%, f1 0.8709, precision 0.8690, recall 0.8729, auc 0.8706
epoch 201, loss 0.1251, train acc 97.24%, f1 0.9724, precision 0.9721, recall 0.9727, auc 0.9724
epoch 301, loss 0.1096, train acc 98.50%, f1 0.9850, precision 0.9850, recall 0.9850, auc 0.9850
epoch 401, loss 0.0517, train acc 98.80%, f1 0.9880, precision 0.9881, recall 0.9880, auc 0.9880
epoch 501, loss 0.0296, train acc 98.98%, f1 0.9898, precision 0.9898, recall 0.9897, auc 0.9898
epoch 601, loss 0.0270, train acc 99.16%, f1 0.9916, precision 0.9917, recall 0.9915, auc 0.9916
epoch 701, loss 0.0227, train acc 99.24%, f1 0.9924, precision 0.9924, recall 0.9924, auc 0.9924
epoch 801, loss 0.0254, train acc 99.33%, f1 0.9933, precision 0.9934, recall 0.9933, auc 0.9933
epoch 901, loss 0.0171, train acc 99.41%, f1 0.9941, precision 0.9942, recall 0.9940, auc 0.9941
epoch 1001, loss 0.0286, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1101, loss 0.0151, train acc 99.53%, f1 0.9953, precision 0.9954, recall 0.9952, auc 0.9953
epoch 1201, loss 0.0202, train acc 99.58%, f1 0.9958, precision 0.9959, recall 0.9957, auc 0.9958
epoch 1301, loss 0.0159, train acc 99.61%, f1 0.9961, precision 0.9962, recall 0.9960, auc 0.9961
epoch 1401, loss 0.0111, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1501, loss 0.0085, train acc 99.66%, f1 0.9966, precision 0.9967, recall 0.9966, auc 0.9966
epoch 1601, loss 0.0117, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 1701, loss 0.0094, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 1801, loss 0.0069, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 1901, loss 0.0059, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 2001, loss 0.0021, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
epoch 2101, loss 0.0107, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2201, loss 0.0022, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2301, loss 0.0054, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2401, loss 0.0094, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2501, loss 0.0067, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2601, loss 0.0075, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2701, loss 0.0042, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 2801, loss 0.0054, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 2901, loss 0.0050, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 3001, loss 0.0045, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 3101, loss 0.0055, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 3201, loss 0.0069, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 3301, loss 0.0033, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3401, loss 0.0037, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 3501, loss 0.0035, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3601, loss 0.0062, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 3701, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 3801, loss 0.0040, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 3901, loss 0.0050, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4001, loss 0.0020, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4101, loss 0.0013, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 4201, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 4301, loss 0.0058, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4401, loss 0.0039, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4501, loss 0.0043, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 4601, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 4701, loss 0.0003, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 4801, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4901, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5001, loss 0.0032, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 5101, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 5201, loss 0.0016, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 5301, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 5401, loss 0.0005, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 5501, loss 0.0029, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 5601, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 5701, loss 0.0034, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 5801, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 5901, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 6001, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 6101, loss 0.0039, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6201, loss 0.0035, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6301, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 6401, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 6501, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 6601, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 6701, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6801, loss 0.0003, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 6901, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 7001, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7101, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7201, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7301, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 7401, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7501, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7601, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7701, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7801, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 7901, loss 0.0022, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 8001, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 8101, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 8201, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 8301, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 8401, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8501, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8601, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8701, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 8801, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8901, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9001, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 9101, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9201, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 9301, loss 0.0014, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 9401, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9501, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 9601, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9701, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9801, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9901, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10001, loss 0.0006, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10101, loss 0.0009, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10201, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10301, loss 0.0001, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10401, loss 0.0007, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10501, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 10601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 10801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_5
./test_vehicle0/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.95

the Fscore is 0.9230769230769231

the precision is 0.9230769230769231

the recall is 0.9230769230769231

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_5
----------------------



epoch 1, loss 0.6935, train acc 51.53%, f1 0.6736, precision 0.5078, recall 1.0000, auc 0.5153
epoch 101, loss 0.2891, train acc 87.27%, f1 0.8728, precision 0.8726, recall 0.8730, auc 0.8727
epoch 201, loss 0.1497, train acc 97.16%, f1 0.9716, precision 0.9717, recall 0.9716, auc 0.9716
epoch 301, loss 0.0852, train acc 98.48%, f1 0.9848, precision 0.9848, recall 0.9848, auc 0.9848
epoch 401, loss 0.0645, train acc 98.79%, f1 0.9879, precision 0.9879, recall 0.9879, auc 0.9879
epoch 501, loss 0.0496, train acc 99.01%, f1 0.9901, precision 0.9901, recall 0.9901, auc 0.9901
epoch 601, loss 0.0254, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9916, auc 0.9916
epoch 701, loss 0.0332, train acc 99.27%, f1 0.9927, precision 0.9927, recall 0.9926, auc 0.9927
epoch 801, loss 0.0398, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9934, auc 0.9934
epoch 901, loss 0.0122, train acc 99.44%, f1 0.9944, precision 0.9944, recall 0.9944, auc 0.9944
epoch 1001, loss 0.0101, train acc 99.50%, f1 0.9950, precision 0.9950, recall 0.9950, auc 0.9950
epoch 1101, loss 0.0157, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1201, loss 0.0129, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9958, auc 0.9959
epoch 1301, loss 0.0178, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1401, loss 0.0094, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 1501, loss 0.0142, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 1601, loss 0.0098, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 1701, loss 0.0139, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 1801, loss 0.0088, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 1901, loss 0.0047, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 2001, loss 0.0058, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2101, loss 0.0061, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 2201, loss 0.0070, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2301, loss 0.0093, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2401, loss 0.0112, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2501, loss 0.0088, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2601, loss 0.0047, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2701, loss 0.0039, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2801, loss 0.0016, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 2901, loss 0.0050, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 3001, loss 0.0081, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 3101, loss 0.0095, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 3201, loss 0.0036, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 3301, loss 0.0063, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3401, loss 0.0046, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 3501, loss 0.0028, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3601, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3701, loss 0.0070, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3801, loss 0.0103, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 3901, loss 0.0079, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4001, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 4101, loss 0.0026, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4201, loss 0.0011, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4301, loss 0.0039, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4401, loss 0.0055, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 4501, loss 0.0031, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4601, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 4701, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4801, loss 0.0050, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 4901, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5001, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5101, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 5201, loss 0.0016, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5301, loss 0.0019, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 5401, loss 0.0043, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 5501, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5601, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 5701, loss 0.0036, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5801, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 5901, loss 0.0050, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6001, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6101, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6201, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6301, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6401, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6501, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6601, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6701, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6801, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6901, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7001, loss 0.0022, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7101, loss 0.0033, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7201, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7301, loss 0.0030, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7401, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7501, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7601, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7701, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7801, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7901, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8001, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8101, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 8201, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8401, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 8501, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8601, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 8701, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8801, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8901, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9001, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9101, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9201, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9301, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9401, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9501, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9601, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9701, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9801, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9901, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10001, loss 0.0001, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10101, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10201, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10301, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10401, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10501, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 10601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10801, loss 0.0000, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 10901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_5
./test_vehicle0/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9769230769230769

the Fscore is 0.9285714285714286

the precision is 0.8666666666666667

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_5
----------------------



epoch 1, loss 0.6933, train acc 61.17%, f1 0.7187, precision 0.5634, recall 0.9923, auc 0.6117
epoch 101, loss 0.3438, train acc 87.97%, f1 0.8794, precision 0.8815, recall 0.8773, auc 0.8797
epoch 201, loss 0.1388, train acc 97.43%, f1 0.9743, precision 0.9746, recall 0.9740, auc 0.9743
epoch 301, loss 0.0967, train acc 98.48%, f1 0.9848, precision 0.9848, recall 0.9848, auc 0.9848
epoch 401, loss 0.0429, train acc 98.84%, f1 0.9884, precision 0.9883, recall 0.9885, auc 0.9884
epoch 501, loss 0.0432, train acc 99.00%, f1 0.9900, precision 0.9899, recall 0.9900, auc 0.9900
epoch 601, loss 0.0340, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9917, auc 0.9916
epoch 701, loss 0.0309, train acc 99.24%, f1 0.9924, precision 0.9923, recall 0.9924, auc 0.9924
epoch 801, loss 0.0149, train acc 99.33%, f1 0.9933, precision 0.9932, recall 0.9934, auc 0.9933
epoch 901, loss 0.0172, train acc 99.42%, f1 0.9942, precision 0.9941, recall 0.9943, auc 0.9942
epoch 1001, loss 0.0175, train acc 99.48%, f1 0.9948, precision 0.9947, recall 0.9948, auc 0.9948
epoch 1101, loss 0.0153, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1201, loss 0.0113, train acc 99.57%, f1 0.9957, precision 0.9956, recall 0.9957, auc 0.9957
epoch 1301, loss 0.0242, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9962, auc 0.9961
epoch 1401, loss 0.0169, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 1501, loss 0.0101, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1601, loss 0.0088, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9969, auc 0.9969
epoch 1701, loss 0.0111, train acc 99.71%, f1 0.9971, precision 0.9972, recall 0.9971, auc 0.9971
epoch 1801, loss 0.0149, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 1901, loss 0.0138, train acc 99.74%, f1 0.9974, precision 0.9975, recall 0.9974, auc 0.9974
epoch 2001, loss 0.0110, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9976, auc 0.9976
epoch 2101, loss 0.0146, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2201, loss 0.0087, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9980, auc 0.9980
epoch 2301, loss 0.0022, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2401, loss 0.0063, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2501, loss 0.0026, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 2601, loss 0.0052, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2701, loss 0.0019, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 2801, loss 0.0101, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 2901, loss 0.0062, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3001, loss 0.0056, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3101, loss 0.0049, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 3201, loss 0.0084, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 3301, loss 0.0017, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3401, loss 0.0050, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3501, loss 0.0058, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 3601, loss 0.0009, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 3701, loss 0.0082, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 3801, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3901, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4001, loss 0.0040, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 4101, loss 0.0028, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4201, loss 0.0011, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4301, loss 0.0022, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4401, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 4501, loss 0.0011, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4601, loss 0.0022, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4701, loss 0.0046, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4801, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4901, loss 0.0023, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 5001, loss 0.0042, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5101, loss 0.0070, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9989, auc 0.9990
epoch 5201, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5301, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 5401, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5501, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 5601, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 5701, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 5801, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5901, loss 0.0011, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6001, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 6101, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6201, loss 0.0021, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6301, loss 0.0028, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 6401, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6501, loss 0.0034, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6601, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 6701, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 6801, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 6901, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7001, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7101, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7201, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7301, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 7401, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7501, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7601, loss 0.0021, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7701, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 7801, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 7901, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8001, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8101, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8201, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8401, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 8501, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8601, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 8701, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8801, loss 0.0002, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 8901, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 9001, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9101, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 9201, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 9301, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9401, loss 0.0007, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9501, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9601, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9701, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9801, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 9901, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_5
./test_vehicle0/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9769230769230769

the Fscore is 0.9285714285714286

the precision is 0.8666666666666667

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_5
----------------------



epoch 1, loss 0.6931, train acc 50.02%, f1 0.6668, precision 0.5001, recall 1.0000, auc 0.5002
epoch 101, loss 0.3098, train acc 87.34%, f1 0.8735, precision 0.8732, recall 0.8737, auc 0.8734
epoch 201, loss 0.1445, train acc 97.12%, f1 0.9712, precision 0.9712, recall 0.9712, auc 0.9712
epoch 301, loss 0.0676, train acc 98.45%, f1 0.9845, precision 0.9844, recall 0.9845, auc 0.9845
epoch 401, loss 0.0728, train acc 98.83%, f1 0.9883, precision 0.9883, recall 0.9883, auc 0.9883
epoch 501, loss 0.0458, train acc 98.99%, f1 0.9899, precision 0.9899, recall 0.9900, auc 0.9899
epoch 601, loss 0.0204, train acc 99.13%, f1 0.9913, precision 0.9913, recall 0.9913, auc 0.9913
epoch 701, loss 0.0189, train acc 99.25%, f1 0.9925, precision 0.9925, recall 0.9926, auc 0.9925
epoch 801, loss 0.0253, train acc 99.32%, f1 0.9932, precision 0.9932, recall 0.9932, auc 0.9932
epoch 901, loss 0.0146, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1001, loss 0.0271, train acc 99.49%, f1 0.9949, precision 0.9948, recall 0.9949, auc 0.9949
epoch 1101, loss 0.0141, train acc 99.55%, f1 0.9955, precision 0.9954, recall 0.9955, auc 0.9955
epoch 1201, loss 0.0086, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1301, loss 0.0176, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1401, loss 0.0153, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1501, loss 0.0170, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9967, auc 0.9966
epoch 1601, loss 0.0172, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9970, auc 0.9969
epoch 1701, loss 0.0122, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 1801, loss 0.0090, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 1901, loss 0.0128, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9974
epoch 2001, loss 0.0046, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9974, auc 0.9975
epoch 2101, loss 0.0120, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2201, loss 0.0116, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2301, loss 0.0070, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 2401, loss 0.0056, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9981, auc 0.9980
epoch 2501, loss 0.0092, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2601, loss 0.0101, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2701, loss 0.0052, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2801, loss 0.0070, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2901, loss 0.0027, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 3001, loss 0.0058, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3101, loss 0.0058, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3201, loss 0.0062, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3301, loss 0.0047, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 3401, loss 0.0090, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3501, loss 0.0070, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3601, loss 0.0051, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3701, loss 0.0045, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 3801, loss 0.0051, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3901, loss 0.0052, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 4001, loss 0.0070, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 4101, loss 0.0018, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 4201, loss 0.0030, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 4301, loss 0.0030, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 4401, loss 0.0039, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4501, loss 0.0045, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4601, loss 0.0063, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4701, loss 0.0072, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4801, loss 0.0050, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4901, loss 0.0046, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5001, loss 0.0039, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 5101, loss 0.0055, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 5201, loss 0.0020, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 5301, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 5401, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 5501, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 5601, loss 0.0039, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 5701, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5801, loss 0.0030, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9992, auc 0.9992
epoch 5901, loss 0.0040, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 6001, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 6101, loss 0.0052, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6201, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6301, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6401, loss 0.0031, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6501, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6601, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 6701, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6801, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6901, loss 0.0002, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7001, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7101, loss 0.0003, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7201, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7301, loss 0.0023, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7401, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7501, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7601, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7701, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7801, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7901, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_Mirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_8000/record_1/MLP_concat_Mirror_8000_5
./test_vehicle0/result_MLP_concat_Mirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9807692307692308

the Fscore is 0.9397590361445783

the precision is 0.8863636363636364

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_5
----------------------



epoch 1, loss 0.6936, train acc 51.04%, f1 0.6710, precision 0.5053, recall 0.9987, auc 0.5104
epoch 101, loss 0.3339, train acc 87.05%, f1 0.8705, precision 0.8705, recall 0.8705, auc 0.8705
epoch 201, loss 0.1372, train acc 97.14%, f1 0.9714, precision 0.9714, recall 0.9715, auc 0.9714
epoch 301, loss 0.0653, train acc 98.47%, f1 0.9847, precision 0.9847, recall 0.9847, auc 0.9847
epoch 401, loss 0.0498, train acc 98.74%, f1 0.9874, precision 0.9875, recall 0.9873, auc 0.9874
epoch 501, loss 0.0375, train acc 98.95%, f1 0.9895, precision 0.9896, recall 0.9895, auc 0.9895
epoch 601, loss 0.0193, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9916, auc 0.9916
epoch 701, loss 0.0349, train acc 99.24%, f1 0.9924, precision 0.9925, recall 0.9924, auc 0.9924
epoch 801, loss 0.0274, train acc 99.33%, f1 0.9933, precision 0.9933, recall 0.9932, auc 0.9933
epoch 901, loss 0.0138, train acc 99.40%, f1 0.9940, precision 0.9940, recall 0.9939, auc 0.9940
epoch 1001, loss 0.0170, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1101, loss 0.0289, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1201, loss 0.0081, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1301, loss 0.0132, train acc 99.62%, f1 0.9962, precision 0.9961, recall 0.9962, auc 0.9962
epoch 1401, loss 0.0171, train acc 99.64%, f1 0.9964, precision 0.9963, recall 0.9964, auc 0.9964
epoch 1501, loss 0.0136, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 1601, loss 0.0090, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 1701, loss 0.0230, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 1801, loss 0.0074, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 1901, loss 0.0122, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2001, loss 0.0107, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2101, loss 0.0145, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2201, loss 0.0091, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2301, loss 0.0064, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 2401, loss 0.0076, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 2501, loss 0.0034, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2601, loss 0.0088, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2701, loss 0.0012, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 2801, loss 0.0113, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 2901, loss 0.0088, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 3001, loss 0.0041, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9984, auc 0.9984
epoch 3101, loss 0.0067, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3201, loss 0.0026, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 3301, loss 0.0040, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3401, loss 0.0031, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 3501, loss 0.0066, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 3601, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3701, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 3801, loss 0.0076, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 3901, loss 0.0053, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4001, loss 0.0057, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4101, loss 0.0035, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 4201, loss 0.0039, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 4301, loss 0.0052, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 4401, loss 0.0024, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 4501, loss 0.0038, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 4601, loss 0.0054, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4701, loss 0.0024, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9989, auc 0.9990
epoch 4801, loss 0.0052, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4901, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_5
./test_vehicle0/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9730769230769231

the Fscore is 0.9176470588235294

the precision is 0.8478260869565217

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_5
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3457, train acc 87.98%, f1 0.8798, precision 0.8799, recall 0.8797, auc 0.8798
epoch 201, loss 0.1540, train acc 97.36%, f1 0.9736, precision 0.9736, recall 0.9736, auc 0.9736
epoch 301, loss 0.1024, train acc 98.49%, f1 0.9849, precision 0.9850, recall 0.9849, auc 0.9849
epoch 401, loss 0.0460, train acc 98.83%, f1 0.9883, precision 0.9883, recall 0.9883, auc 0.9883
epoch 501, loss 0.0375, train acc 99.02%, f1 0.9902, precision 0.9902, recall 0.9902, auc 0.9902
epoch 601, loss 0.0206, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9916, auc 0.9916
epoch 701, loss 0.0316, train acc 99.27%, f1 0.9927, precision 0.9927, recall 0.9926, auc 0.9927
epoch 801, loss 0.0193, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9934, auc 0.9934
epoch 901, loss 0.0280, train acc 99.41%, f1 0.9941, precision 0.9941, recall 0.9941, auc 0.9941
epoch 1001, loss 0.0105, train acc 99.50%, f1 0.9950, precision 0.9950, recall 0.9950, auc 0.9950
epoch 1101, loss 0.0169, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9954, auc 0.9954
epoch 1201, loss 0.0083, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 1301, loss 0.0072, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1401, loss 0.0154, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 1501, loss 0.0118, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 1601, loss 0.0135, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 1701, loss 0.0111, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 1801, loss 0.0156, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 1901, loss 0.0104, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9976, auc 0.9976
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_5
./test_vehicle0/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9653846153846154

the Fscore is 0.896551724137931

the precision is 0.8125

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_5
----------------------



epoch 1, loss 0.6933, train acc 49.96%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3032, train acc 86.82%, f1 0.8688, precision 0.8658, recall 0.8718, auc 0.8682
epoch 201, loss 0.1390, train acc 97.10%, f1 0.9710, precision 0.9714, recall 0.9706, auc 0.9710
epoch 301, loss 0.0720, train acc 98.54%, f1 0.9854, precision 0.9854, recall 0.9855, auc 0.9854
epoch 401, loss 0.0800, train acc 98.75%, f1 0.9876, precision 0.9872, recall 0.9879, auc 0.9875
epoch 501, loss 0.0285, train acc 98.99%, f1 0.9900, precision 0.9898, recall 0.9901, auc 0.9899
epoch 601, loss 0.0454, train acc 99.14%, f1 0.9914, precision 0.9911, recall 0.9917, auc 0.9914
epoch 701, loss 0.0269, train acc 99.25%, f1 0.9925, precision 0.9925, recall 0.9924, auc 0.9925
epoch 801, loss 0.0219, train acc 99.31%, f1 0.9931, precision 0.9930, recall 0.9933, auc 0.9931
epoch 901, loss 0.0222, train acc 99.39%, f1 0.9939, precision 0.9937, recall 0.9940, auc 0.9939
epoch 1001, loss 0.0212, train acc 99.46%, f1 0.9946, precision 0.9945, recall 0.9948, auc 0.9946
epoch 1101, loss 0.0093, train acc 99.52%, f1 0.9952, precision 0.9950, recall 0.9954, auc 0.9952
epoch 1201, loss 0.0226, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 1301, loss 0.0129, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9959, auc 0.9960
epoch 1401, loss 0.0177, train acc 99.62%, f1 0.9962, precision 0.9965, recall 0.9959, auc 0.9962
epoch 1501, loss 0.0084, train acc 99.66%, f1 0.9966, precision 0.9970, recall 0.9963, auc 0.9966
epoch 1601, loss 0.0108, train acc 99.69%, f1 0.9969, precision 0.9971, recall 0.9966, auc 0.9969
epoch 1701, loss 0.0095, train acc 99.71%, f1 0.9971, precision 0.9973, recall 0.9968, auc 0.9971
epoch 1801, loss 0.0099, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9971, auc 0.9972
epoch 1901, loss 0.0082, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9971, auc 0.9973
epoch 2001, loss 0.0119, train acc 99.75%, f1 0.9975, precision 0.9977, recall 0.9973, auc 0.9975
epoch 2101, loss 0.0059, train acc 99.77%, f1 0.9977, precision 0.9979, recall 0.9975, auc 0.9977
epoch 2201, loss 0.0078, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9978, auc 0.9978
epoch 2301, loss 0.0110, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 2401, loss 0.0027, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9979, auc 0.9981
epoch 2501, loss 0.0092, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9979, auc 0.9981
epoch 2601, loss 0.0098, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9980, auc 0.9981
epoch 2701, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 2801, loss 0.0073, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9979, auc 0.9982
epoch 2901, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9981, auc 0.9983
epoch 3001, loss 0.0023, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 3101, loss 0.0034, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 3201, loss 0.0055, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3301, loss 0.0067, train acc 99.84%, f1 0.9984, precision 0.9981, recall 0.9987, auc 0.9984
epoch 3401, loss 0.0016, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 3501, loss 0.0045, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9984, auc 0.9986
epoch 3601, loss 0.0019, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3701, loss 0.0044, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 3801, loss 0.0078, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 3901, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 4001, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9986, auc 0.9987
epoch 4101, loss 0.0026, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 4201, loss 0.0050, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4301, loss 0.0034, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 4401, loss 0.0061, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 4501, loss 0.0040, train acc 99.88%, f1 0.9988, precision 0.9991, recall 0.9986, auc 0.9988
epoch 4601, loss 0.0084, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 4701, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 4801, loss 0.0022, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9986, auc 0.9989
epoch 4901, loss 0.0037, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 5001, loss 0.0045, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9988, auc 0.9989
epoch 5101, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 5201, loss 0.0017, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 5301, loss 0.0043, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 5401, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 5501, loss 0.0037, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 5601, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5701, loss 0.0024, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5801, loss 0.0027, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 5901, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9992
epoch 6001, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6101, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6201, loss 0.0034, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6301, loss 0.0032, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 6401, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6501, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 6601, loss 0.0025, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 6701, loss 0.0029, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6801, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 6901, loss 0.0003, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7001, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7101, loss 0.0023, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7201, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7301, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7401, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7501, loss 0.0024, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 7601, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9993, auc 0.9996
epoch 7701, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7801, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7901, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 8001, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9995, auc 0.9996
epoch 8101, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 8201, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 8301, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 8401, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 8501, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 8601, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 8701, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8801, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8901, loss 0.0020, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 9001, loss 0.0018, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 9101, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 9201, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 9301, loss 0.0005, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9997, auc 0.9998
epoch 9401, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9501, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 9601, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 9701, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 9801, loss 0.0020, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 9901, loss 0.0006, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 10001, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10101, loss 0.0005, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 10201, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 10301, loss 0.0006, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10401, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 10501, loss 0.0007, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 10601, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 10701, loss 0.0004, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 10801, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 10901, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11001, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0010, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 11201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 11401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_notMirror_20000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_20000/record_1/MLP_concat_notMirror_20000_5
./test_vehicle0/result_MLP_concat_notMirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9551282051282052

the Fscore is 0.9135802469135802

the precision is 0.8809523809523809

the recall is 0.9487179487179487

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_5
----------------------



epoch 1, loss 0.6962, train acc 50.04%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3319, train acc 87.35%, f1 0.8727, precision 0.8774, recall 0.8680, auc 0.8735
epoch 201, loss 0.1659, train acc 97.36%, f1 0.9736, precision 0.9737, recall 0.9735, auc 0.9736
epoch 301, loss 0.0753, train acc 98.55%, f1 0.9855, precision 0.9853, recall 0.9856, auc 0.9855
epoch 401, loss 0.0632, train acc 98.83%, f1 0.9883, precision 0.9884, recall 0.9882, auc 0.9883
epoch 501, loss 0.0345, train acc 99.00%, f1 0.9900, precision 0.9904, recall 0.9897, auc 0.9900
epoch 601, loss 0.0437, train acc 99.14%, f1 0.9914, precision 0.9917, recall 0.9911, auc 0.9914
epoch 701, loss 0.0143, train acc 99.25%, f1 0.9925, precision 0.9926, recall 0.9925, auc 0.9925
epoch 801, loss 0.0225, train acc 99.32%, f1 0.9932, precision 0.9932, recall 0.9932, auc 0.9932
epoch 901, loss 0.0199, train acc 99.42%, f1 0.9942, precision 0.9946, recall 0.9937, auc 0.9942
epoch 1001, loss 0.0236, train acc 99.50%, f1 0.9950, precision 0.9952, recall 0.9949, auc 0.9950
epoch 1101, loss 0.0160, train acc 99.55%, f1 0.9955, precision 0.9957, recall 0.9952, auc 0.9955
epoch 1201, loss 0.0126, train acc 99.58%, f1 0.9958, precision 0.9961, recall 0.9955, auc 0.9958
epoch 1301, loss 0.0163, train acc 99.62%, f1 0.9962, precision 0.9963, recall 0.9961, auc 0.9962
epoch 1401, loss 0.0160, train acc 99.65%, f1 0.9965, precision 0.9969, recall 0.9962, auc 0.9965
epoch 1501, loss 0.0124, train acc 99.68%, f1 0.9968, precision 0.9971, recall 0.9965, auc 0.9968
epoch 1601, loss 0.0169, train acc 99.70%, f1 0.9970, precision 0.9972, recall 0.9969, auc 0.9970
epoch 1701, loss 0.0163, train acc 99.72%, f1 0.9972, precision 0.9975, recall 0.9970, auc 0.9972
epoch 1801, loss 0.0186, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9973, auc 0.9974
epoch 1901, loss 0.0081, train acc 99.74%, f1 0.9974, precision 0.9977, recall 0.9971, auc 0.9974
epoch 2001, loss 0.0116, train acc 99.77%, f1 0.9977, precision 0.9979, recall 0.9975, auc 0.9977
epoch 2101, loss 0.0120, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9980, auc 0.9978
epoch 2201, loss 0.0024, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 2301, loss 0.0098, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 2401, loss 0.0047, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 2501, loss 0.0066, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2601, loss 0.0117, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 2701, loss 0.0035, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 2801, loss 0.0086, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 2901, loss 0.0067, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3001, loss 0.0048, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9985, auc 0.9984
epoch 3101, loss 0.0042, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3201, loss 0.0056, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 3301, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3401, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9989, recall 0.9983, auc 0.9986
epoch 3501, loss 0.0074, train acc 99.86%, f1 0.9986, precision 0.9989, recall 0.9983, auc 0.9986
epoch 3601, loss 0.0034, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9985, auc 0.9986
epoch 3701, loss 0.0058, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 3801, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 3901, loss 0.0068, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4001, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 4101, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 4201, loss 0.0024, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 4301, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9985, recall 0.9991, auc 0.9988
epoch 4401, loss 0.0023, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 4501, loss 0.0039, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 4601, loss 0.0012, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9987, auc 0.9989
epoch 4701, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9988, auc 0.9990
epoch 4801, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9991, auc 0.9990
epoch 4901, loss 0.0039, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9991, auc 0.9990
epoch 5001, loss 0.0029, train acc 99.90%, f1 0.9990, precision 0.9993, recall 0.9987, auc 0.9990
epoch 5101, loss 0.0015, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 5201, loss 0.0039, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5301, loss 0.0041, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 5401, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5501, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 5601, loss 0.0021, train acc 99.93%, f1 0.9993, precision 0.9991, recall 0.9994, auc 0.9993
epoch 5701, loss 0.0038, train acc 99.91%, f1 0.9991, precision 0.9994, recall 0.9989, auc 0.9991
epoch 5801, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5901, loss 0.0032, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6001, loss 0.0027, train acc 99.93%, f1 0.9993, precision 0.9995, recall 0.9991, auc 0.9993
epoch 6101, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 6201, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9995, recall 0.9990, auc 0.9993
epoch 6301, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6401, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 6501, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6601, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9992, auc 0.9994
epoch 6701, loss 0.0034, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 6801, loss 0.0043, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9996, auc 0.9994
epoch 6901, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 7001, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 7101, loss 0.0002, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 7201, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7301, loss 0.0027, train acc 99.96%, f1 0.9996, precision 0.9993, recall 0.9998, auc 0.9996
epoch 7401, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7501, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9998, auc 0.9996
epoch 7601, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7701, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 7801, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 7901, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 8001, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8101, loss 0.0035, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 8301, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9995, recall 0.9999, auc 0.9997
epoch 8401, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 8501, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 8601, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 8701, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 8801, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 8901, loss 0.0004, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 9001, loss 0.0009, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 9101, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 9201, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9301, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 9401, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9501, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9998, auc 0.9998
epoch 9601, loss 0.0007, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9701, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9801, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 9901, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 10001, loss 0.0018, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 10101, loss 0.0003, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 10201, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10301, loss 0.0010, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 10401, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 10501, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10601, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 10701, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10801, loss 0.0005, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 10901, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11001, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11101, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11201, loss 0.0006, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 11301, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11401, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11501, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11601, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11701, loss 0.0008, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 11801, loss 0.0001, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 11901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 0.9999, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0005, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 12101, loss 0.0004, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 12201, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 12301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0002, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 12901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_notMirror_15000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_15000/record_1/MLP_concat_notMirror_15000_5
./test_vehicle0/result_MLP_concat_notMirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.967948717948718

the Fscore is 0.9268292682926831

the precision is 0.8837209302325582

the recall is 0.9743589743589743

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_5
----------------------



epoch 1, loss 0.6928, train acc 49.76%, f1 0.6646, precision 0.4976, recall 1.0000, auc 0.5000
epoch 101, loss 0.2968, train acc 87.44%, f1 0.8736, precision 0.8745, recall 0.8728, auc 0.8744
epoch 201, loss 0.1566, train acc 97.31%, f1 0.9731, precision 0.9713, recall 0.9748, auc 0.9731
epoch 301, loss 0.0767, train acc 98.49%, f1 0.9848, precision 0.9847, recall 0.9850, auc 0.9849
epoch 401, loss 0.0628, train acc 98.83%, f1 0.9883, precision 0.9884, recall 0.9881, auc 0.9883
epoch 501, loss 0.0222, train acc 98.98%, f1 0.9898, precision 0.9896, recall 0.9900, auc 0.9898
epoch 601, loss 0.0293, train acc 99.16%, f1 0.9916, precision 0.9918, recall 0.9913, auc 0.9916
epoch 701, loss 0.0320, train acc 99.26%, f1 0.9926, precision 0.9924, recall 0.9927, auc 0.9926
epoch 801, loss 0.0225, train acc 99.34%, f1 0.9933, precision 0.9933, recall 0.9934, auc 0.9934
epoch 901, loss 0.0163, train acc 99.39%, f1 0.9939, precision 0.9940, recall 0.9938, auc 0.9939
epoch 1001, loss 0.0169, train acc 99.48%, f1 0.9948, precision 0.9949, recall 0.9947, auc 0.9948
epoch 1101, loss 0.0184, train acc 99.53%, f1 0.9953, precision 0.9950, recall 0.9955, auc 0.9953
epoch 1201, loss 0.0133, train acc 99.57%, f1 0.9957, precision 0.9955, recall 0.9958, auc 0.9957
epoch 1301, loss 0.0070, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9962, auc 0.9961
epoch 1401, loss 0.0092, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9963, auc 0.9964
epoch 1501, loss 0.0128, train acc 99.66%, f1 0.9966, precision 0.9963, recall 0.9968, auc 0.9966
epoch 1601, loss 0.0096, train acc 99.68%, f1 0.9968, precision 0.9970, recall 0.9966, auc 0.9968
epoch 1701, loss 0.0089, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9971, auc 0.9970
epoch 1801, loss 0.0036, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9973, auc 0.9972
epoch 1901, loss 0.0087, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9974, auc 0.9974
epoch 2001, loss 0.0027, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9975, auc 0.9976
epoch 2101, loss 0.0127, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2201, loss 0.0144, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 2301, loss 0.0040, train acc 99.80%, f1 0.9980, precision 0.9979, recall 0.9981, auc 0.9980
epoch 2401, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 2501, loss 0.0103, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 2601, loss 0.0093, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 2701, loss 0.0041, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 2801, loss 0.0083, train acc 99.82%, f1 0.9982, precision 0.9985, recall 0.9979, auc 0.9982
epoch 2901, loss 0.0062, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9984, auc 0.9983
epoch 3001, loss 0.0042, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9982, auc 0.9983
epoch 3101, loss 0.0046, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 3201, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9982, auc 0.9984
epoch 3301, loss 0.0049, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 3401, loss 0.0054, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3501, loss 0.0043, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9984, auc 0.9985
epoch 3601, loss 0.0037, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 3701, loss 0.0040, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 3801, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 3901, loss 0.0025, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 4001, loss 0.0030, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9983, auc 0.9987
epoch 4101, loss 0.0014, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 4201, loss 0.0029, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 4301, loss 0.0046, train acc 99.87%, f1 0.9987, precision 0.9990, recall 0.9985, auc 0.9987
epoch 4401, loss 0.0030, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4501, loss 0.0012, train acc 99.89%, f1 0.9988, precision 0.9987, recall 0.9990, auc 0.9989
epoch 4601, loss 0.0014, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9986, auc 0.9988
epoch 4701, loss 0.0037, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 4801, loss 0.0003, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9990, auc 0.9988
epoch 4901, loss 0.0030, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 5001, loss 0.0021, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9991, auc 0.9989
epoch 5101, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5201, loss 0.0048, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5301, loss 0.0020, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9989, auc 0.9990
epoch 5401, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9988, auc 0.9991
epoch 5501, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 5601, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 5701, loss 0.0033, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 5801, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 5901, loss 0.0013, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9993, auc 0.9991
epoch 6001, loss 0.0012, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9992
epoch 6101, loss 0.0005, train acc 99.93%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9993
epoch 6201, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 6301, loss 0.0057, train acc 99.93%, f1 0.9992, precision 0.9991, recall 0.9994, auc 0.9993
epoch 6401, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 6501, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 6601, loss 0.0025, train acc 99.93%, f1 0.9993, precision 0.9989, recall 0.9996, auc 0.9993
epoch 6701, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9992, auc 0.9994
epoch 6801, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 6901, loss 0.0032, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9992, auc 0.9994
epoch 7001, loss 0.0033, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9993, auc 0.9995
epoch 7101, loss 0.0030, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 7201, loss 0.0041, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 7301, loss 0.0035, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 7401, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7501, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7601, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 7701, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 7801, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 7901, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 8001, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 8101, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0021, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 8301, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 8401, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 8501, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 8601, loss 0.0030, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 8701, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 8801, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 8901, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 9001, loss 0.0040, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 9101, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 9201, loss 0.0022, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 9301, loss 0.0006, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 9401, loss 0.0003, train acc 99.98%, f1 0.9998, precision 0.9997, recall 1.0000, auc 0.9998
epoch 9501, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9997, auc 0.9998
epoch 9601, loss 0.0008, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 9701, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
epoch 9801, loss 0.0002, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 9901, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9998, recall 0.9999, auc 0.9999
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_notMirror_10000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_10000/record_1/MLP_concat_notMirror_10000_5
./test_vehicle0/result_MLP_concat_notMirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9807692307692308

the Fscore is 0.9397590361445783

the precision is 0.8863636363636364

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_5
----------------------



epoch 1, loss 0.6936, train acc 66.53%, f1 0.5485, precision 0.8452, recall 0.4059, auc 0.6657
epoch 101, loss 0.2930, train acc 87.57%, f1 0.8762, precision 0.8738, recall 0.8787, auc 0.8757
epoch 201, loss 0.1312, train acc 96.98%, f1 0.9698, precision 0.9695, recall 0.9702, auc 0.9698
epoch 301, loss 0.0939, train acc 98.47%, f1 0.9848, precision 0.9857, recall 0.9838, auc 0.9847
epoch 401, loss 0.0413, train acc 98.81%, f1 0.9881, precision 0.9891, recall 0.9871, auc 0.9881
epoch 501, loss 0.0555, train acc 98.99%, f1 0.9900, precision 0.9906, recall 0.9893, auc 0.9899
epoch 601, loss 0.0360, train acc 99.15%, f1 0.9915, precision 0.9921, recall 0.9909, auc 0.9915
epoch 701, loss 0.0280, train acc 99.23%, f1 0.9923, precision 0.9931, recall 0.9916, auc 0.9923
epoch 801, loss 0.0276, train acc 99.32%, f1 0.9932, precision 0.9939, recall 0.9925, auc 0.9932
epoch 901, loss 0.0202, train acc 99.40%, f1 0.9940, precision 0.9946, recall 0.9935, auc 0.9940
epoch 1001, loss 0.0138, train acc 99.49%, f1 0.9949, precision 0.9948, recall 0.9949, auc 0.9949
epoch 1101, loss 0.0220, train acc 99.52%, f1 0.9953, precision 0.9954, recall 0.9951, auc 0.9952
epoch 1201, loss 0.0170, train acc 99.55%, f1 0.9955, precision 0.9956, recall 0.9954, auc 0.9955
epoch 1301, loss 0.0123, train acc 99.59%, f1 0.9959, precision 0.9963, recall 0.9955, auc 0.9959
epoch 1401, loss 0.0209, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9963, auc 0.9964
epoch 1501, loss 0.0229, train acc 99.66%, f1 0.9966, precision 0.9968, recall 0.9964, auc 0.9966
epoch 1601, loss 0.0101, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9967, auc 0.9969
epoch 1701, loss 0.0191, train acc 99.72%, f1 0.9972, precision 0.9969, recall 0.9974, auc 0.9972
epoch 1801, loss 0.0078, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9974, auc 0.9973
epoch 1901, loss 0.0071, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9975, auc 0.9974
epoch 2001, loss 0.0113, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 2101, loss 0.0087, train acc 99.78%, f1 0.9978, precision 0.9975, recall 0.9980, auc 0.9978
epoch 2201, loss 0.0112, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9980, auc 0.9979
epoch 2301, loss 0.0075, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2401, loss 0.0044, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 2501, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9983, auc 0.9981
epoch 2601, loss 0.0014, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9985, auc 0.9983
epoch 2701, loss 0.0043, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9984, auc 0.9982
epoch 2801, loss 0.0107, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9986, auc 0.9983
epoch 2901, loss 0.0085, train acc 99.84%, f1 0.9984, precision 0.9981, recall 0.9986, auc 0.9984
epoch 3001, loss 0.0037, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 3101, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 3201, loss 0.0049, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 3301, loss 0.0068, train acc 99.85%, f1 0.9985, precision 0.9982, recall 0.9988, auc 0.9985
epoch 3401, loss 0.0037, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 3501, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 3601, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 3701, loss 0.0040, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 3801, loss 0.0030, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9988, auc 0.9987
epoch 3901, loss 0.0063, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 4001, loss 0.0036, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4101, loss 0.0034, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 4201, loss 0.0060, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 4301, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9987, auc 0.9989
epoch 4401, loss 0.0038, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 4501, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9987, auc 0.9989
epoch 4601, loss 0.0032, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 4701, loss 0.0031, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 4801, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 4901, loss 0.0043, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 5001, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9989, auc 0.9990
epoch 5101, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9989, auc 0.9991
epoch 5201, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9988, auc 0.9991
epoch 5301, loss 0.0035, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 5401, loss 0.0024, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 5501, loss 0.0009, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 5601, loss 0.0034, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9993, auc 0.9991
epoch 5701, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9992
epoch 5801, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 5901, loss 0.0043, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 6001, loss 0.0018, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 6101, loss 0.0024, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 6201, loss 0.0052, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 6301, loss 0.0039, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 6401, loss 0.0043, train acc 99.94%, f1 0.9994, precision 0.9996, recall 0.9992, auc 0.9994
epoch 6501, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 6601, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 6701, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 6801, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 6901, loss 0.0038, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 7001, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 7101, loss 0.0037, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 7201, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 7301, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7401, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7501, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 7601, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7701, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 7801, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 7901, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_notMirror_8000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_8000/record_1/MLP_concat_notMirror_8000_5
./test_vehicle0/result_MLP_concat_notMirror_8000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.967948717948718

the Fscore is 0.9268292682926831

the precision is 0.8837209302325582

the recall is 0.9743589743589743

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_5
----------------------



epoch 1, loss 0.6931, train acc 49.96%, f1 0.6664, precision 0.4996, recall 1.0000, auc 0.5000
epoch 101, loss 0.3155, train acc 87.59%, f1 0.8753, precision 0.8784, recall 0.8723, auc 0.8759
epoch 201, loss 0.1238, train acc 97.22%, f1 0.9722, precision 0.9722, recall 0.9722, auc 0.9722
epoch 301, loss 0.0684, train acc 98.38%, f1 0.9837, precision 0.9844, recall 0.9831, auc 0.9838
epoch 401, loss 0.0529, train acc 98.82%, f1 0.9882, precision 0.9885, recall 0.9878, auc 0.9882
epoch 501, loss 0.0476, train acc 98.96%, f1 0.9896, precision 0.9894, recall 0.9898, auc 0.9896
epoch 601, loss 0.0297, train acc 99.13%, f1 0.9913, precision 0.9915, recall 0.9911, auc 0.9913
epoch 701, loss 0.0293, train acc 99.21%, f1 0.9921, precision 0.9921, recall 0.9920, auc 0.9921
epoch 801, loss 0.0229, train acc 99.30%, f1 0.9930, precision 0.9933, recall 0.9928, auc 0.9930
epoch 901, loss 0.0198, train acc 99.40%, f1 0.9940, precision 0.9940, recall 0.9940, auc 0.9940
epoch 1001, loss 0.0181, train acc 99.45%, f1 0.9944, precision 0.9944, recall 0.9945, auc 0.9945
epoch 1101, loss 0.0218, train acc 99.51%, f1 0.9951, precision 0.9949, recall 0.9952, auc 0.9951
epoch 1201, loss 0.0132, train acc 99.56%, f1 0.9955, precision 0.9953, recall 0.9958, auc 0.9956
epoch 1301, loss 0.0116, train acc 99.61%, f1 0.9961, precision 0.9962, recall 0.9961, auc 0.9961
epoch 1401, loss 0.0199, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9965, auc 0.9963
epoch 1501, loss 0.0116, train acc 99.65%, f1 0.9965, precision 0.9963, recall 0.9968, auc 0.9965
epoch 1601, loss 0.0062, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9971, auc 0.9969
epoch 1701, loss 0.0093, train acc 99.71%, f1 0.9971, precision 0.9969, recall 0.9973, auc 0.9971
epoch 1801, loss 0.0077, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9974, auc 0.9973
epoch 1901, loss 0.0093, train acc 99.74%, f1 0.9974, precision 0.9976, recall 0.9973, auc 0.9974
epoch 2001, loss 0.0064, train acc 99.77%, f1 0.9977, precision 0.9975, recall 0.9978, auc 0.9977
epoch 2101, loss 0.0080, train acc 99.78%, f1 0.9978, precision 0.9976, recall 0.9980, auc 0.9978
epoch 2201, loss 0.0048, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 2301, loss 0.0090, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9981, auc 0.9979
epoch 2401, loss 0.0063, train acc 99.81%, f1 0.9981, precision 0.9978, recall 0.9983, auc 0.9981
epoch 2501, loss 0.0075, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 2601, loss 0.0055, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2701, loss 0.0080, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 2801, loss 0.0040, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9985, auc 0.9983
epoch 2901, loss 0.0069, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3001, loss 0.0046, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 3101, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 3201, loss 0.0070, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 3301, loss 0.0102, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 3401, loss 0.0085, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9988, auc 0.9986
epoch 3501, loss 0.0018, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9988, auc 0.9986
epoch 3601, loss 0.0089, train acc 99.86%, f1 0.9986, precision 0.9983, recall 0.9988, auc 0.9986
epoch 3701, loss 0.0066, train acc 99.87%, f1 0.9987, precision 0.9984, recall 0.9989, auc 0.9987
epoch 3801, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9988, auc 0.9987
epoch 3901, loss 0.0078, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9988, auc 0.9987
epoch 4001, loss 0.0063, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 4101, loss 0.0047, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9990, auc 0.9988
epoch 4201, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9984, recall 0.9992, auc 0.9988
epoch 4301, loss 0.0025, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 4401, loss 0.0037, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9991, auc 0.9988
epoch 4501, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 4601, loss 0.0051, train acc 99.89%, f1 0.9989, precision 0.9986, recall 0.9991, auc 0.9989
epoch 4701, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9987, recall 0.9993, auc 0.9990
epoch 4801, loss 0.0042, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 4901, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9992, auc 0.9990
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_notMirror_5000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_5000/record_1/MLP_concat_notMirror_5000_5
./test_vehicle0/result_MLP_concat_notMirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9807692307692308

the Fscore is 0.9397590361445783

the precision is 0.8863636363636364

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_5
----------------------



epoch 1, loss 0.6939, train acc 50.01%, f1 0.6668, precision 0.5001, recall 1.0000, auc 0.5000
epoch 101, loss 0.3058, train acc 87.35%, f1 0.8753, precision 0.8637, recall 0.8872, auc 0.8735
epoch 201, loss 0.1525, train acc 97.12%, f1 0.9712, precision 0.9703, recall 0.9722, auc 0.9712
epoch 301, loss 0.0817, train acc 98.51%, f1 0.9851, precision 0.9846, recall 0.9855, auc 0.9851
epoch 401, loss 0.0395, train acc 98.82%, f1 0.9882, precision 0.9883, recall 0.9881, auc 0.9882
epoch 501, loss 0.0350, train acc 99.04%, f1 0.9904, precision 0.9907, recall 0.9900, auc 0.9904
epoch 601, loss 0.0352, train acc 99.16%, f1 0.9916, precision 0.9915, recall 0.9917, auc 0.9916
epoch 701, loss 0.0225, train acc 99.24%, f1 0.9924, precision 0.9925, recall 0.9923, auc 0.9924
epoch 801, loss 0.0215, train acc 99.32%, f1 0.9932, precision 0.9932, recall 0.9933, auc 0.9932
epoch 901, loss 0.0240, train acc 99.39%, f1 0.9939, precision 0.9940, recall 0.9939, auc 0.9939
epoch 1001, loss 0.0270, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1101, loss 0.0182, train acc 99.54%, f1 0.9954, precision 0.9955, recall 0.9953, auc 0.9954
epoch 1201, loss 0.0120, train acc 99.58%, f1 0.9958, precision 0.9959, recall 0.9957, auc 0.9958
epoch 1301, loss 0.0136, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9963, auc 0.9962
epoch 1401, loss 0.0105, train acc 99.65%, f1 0.9965, precision 0.9960, recall 0.9970, auc 0.9965
epoch 1501, loss 0.0080, train acc 99.68%, f1 0.9968, precision 0.9967, recall 0.9970, auc 0.9968
epoch 1601, loss 0.0099, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9972, auc 0.9971
epoch 1701, loss 0.0055, train acc 99.72%, f1 0.9972, precision 0.9969, recall 0.9976, auc 0.9972
epoch 1801, loss 0.0097, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 1901, loss 0.0116, train acc 99.76%, f1 0.9976, precision 0.9971, recall 0.9981, auc 0.9976
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_concat_notMirror_2000
concat_pos_num_40_1
./test_vehicle0/model_MLP_concat_notMirror_2000/record_1/MLP_concat_notMirror_2000_5
./test_vehicle0/result_MLP_concat_notMirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.9769230769230769

the Fscore is 0.9285714285714286

the precision is 0.8666666666666667

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_5
----------------------



epoch 1, loss 0.6932, train acc 68.49%, f1 0.7560, precision 0.6169, recall 0.9761, auc 0.6849
epoch 101, loss 0.4069, train acc 83.74%, f1 0.8366, precision 0.8410, recall 0.8322, auc 0.8374
epoch 201, loss 0.2043, train acc 94.70%, f1 0.9470, precision 0.9478, recall 0.9462, auc 0.9470
epoch 301, loss 0.1243, train acc 97.92%, f1 0.9792, precision 0.9793, recall 0.9792, auc 0.9792
epoch 401, loss 0.0774, train acc 98.53%, f1 0.9853, precision 0.9851, recall 0.9854, auc 0.9853
epoch 501, loss 0.0570, train acc 98.77%, f1 0.9877, precision 0.9876, recall 0.9879, auc 0.9877
epoch 601, loss 0.0487, train acc 98.93%, f1 0.9893, precision 0.9892, recall 0.9895, auc 0.9893
epoch 701, loss 0.0422, train acc 99.05%, f1 0.9905, precision 0.9903, recall 0.9906, auc 0.9905
epoch 801, loss 0.0517, train acc 99.15%, f1 0.9915, precision 0.9914, recall 0.9917, auc 0.9915
epoch 901, loss 0.0388, train acc 99.24%, f1 0.9924, precision 0.9923, recall 0.9925, auc 0.9924
epoch 1001, loss 0.0242, train acc 99.32%, f1 0.9932, precision 0.9931, recall 0.9933, auc 0.9932
epoch 1101, loss 0.0307, train acc 99.36%, f1 0.9936, precision 0.9935, recall 0.9936, auc 0.9936
epoch 1201, loss 0.0124, train acc 99.43%, f1 0.9943, precision 0.9942, recall 0.9944, auc 0.9943
epoch 1301, loss 0.0289, train acc 99.47%, f1 0.9947, precision 0.9946, recall 0.9948, auc 0.9947
epoch 1401, loss 0.0158, train acc 99.52%, f1 0.9952, precision 0.9951, recall 0.9953, auc 0.9952
epoch 1501, loss 0.0140, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9956, auc 0.9956
epoch 1601, loss 0.0162, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9959, auc 0.9958
epoch 1701, loss 0.0099, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9961, auc 0.9961
epoch 1801, loss 0.0091, train acc 99.63%, f1 0.9963, precision 0.9962, recall 0.9964, auc 0.9963
epoch 1901, loss 0.0125, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9966, auc 0.9965
epoch 2001, loss 0.0103, train acc 99.68%, f1 0.9968, precision 0.9967, recall 0.9969, auc 0.9968
epoch 2101, loss 0.0150, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9971, auc 0.9970
epoch 2201, loss 0.0067, train acc 99.72%, f1 0.9972, precision 0.9971, recall 0.9972, auc 0.9972
epoch 2301, loss 0.0105, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2401, loss 0.0106, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9975, auc 0.9975
epoch 2501, loss 0.0128, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9975, auc 0.9975
epoch 2601, loss 0.0107, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2701, loss 0.0118, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9979, auc 0.9979
epoch 2801, loss 0.0162, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9980, auc 0.9979
epoch 2901, loss 0.0076, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3001, loss 0.0088, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3101, loss 0.0102, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9982, auc 0.9981
epoch 3201, loss 0.0058, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3301, loss 0.0066, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 3401, loss 0.0069, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3501, loss 0.0040, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3601, loss 0.0115, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 3701, loss 0.0039, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 3801, loss 0.0036, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3901, loss 0.0042, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9985, auc 0.9984
epoch 4001, loss 0.0094, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4101, loss 0.0084, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4201, loss 0.0087, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4301, loss 0.0046, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4401, loss 0.0078, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4501, loss 0.0037, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4601, loss 0.0040, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4701, loss 0.0037, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 4801, loss 0.0033, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4901, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5001, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5101, loss 0.0042, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5201, loss 0.0028, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5301, loss 0.0074, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 5401, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5501, loss 0.0021, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5601, loss 0.0071, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5701, loss 0.0062, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5801, loss 0.0019, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5901, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6001, loss 0.0016, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6101, loss 0.0024, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6201, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 6301, loss 0.0016, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6401, loss 0.0025, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6501, loss 0.0014, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6601, loss 0.0042, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 6701, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 6801, loss 0.0025, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 6901, loss 0.0065, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 7001, loss 0.0044, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7101, loss 0.0035, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7201, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7301, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7401, loss 0.0030, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 7501, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7601, loss 0.0030, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 7701, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7801, loss 0.0020, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7901, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8001, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8101, loss 0.0029, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8201, loss 0.0010, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8301, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8401, loss 0.0044, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8501, loss 0.0011, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8601, loss 0.0034, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 8701, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8801, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8901, loss 0.0005, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9001, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9101, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9201, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9301, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9401, loss 0.0039, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9501, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 9601, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9701, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9801, loss 0.0030, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9901, loss 0.0027, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10001, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10101, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10201, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10301, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 10401, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10501, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10601, loss 0.0017, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10701, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10801, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 10901, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 11001, loss 0.0035, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11101, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11201, loss 0.0003, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11301, loss 0.0024, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11401, loss 0.0028, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11501, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11601, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11701, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11801, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11901, loss 0.0002, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12001, loss 0.0038, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12101, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12201, loss 0.0002, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12301, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 12401, loss 0.0001, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12501, loss 0.0050, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12601, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 12701, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 12801, loss 0.0031, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 12901, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13001, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13101, loss 0.0020, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13201, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13301, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13401, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13501, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13601, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13701, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13801, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 13901, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14001, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14101, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14201, loss 0.0025, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14301, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14401, loss 0.0037, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14501, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14601, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14701, loss 0.0015, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14801, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14901, loss 0.0018, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15001, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15101, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15201, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15301, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15401, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15501, loss 0.0034, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15601, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15701, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 15801, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15901, loss 0.0000, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16001, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16101, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16201, loss 0.0013, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 16301, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 16401, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 16501, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 16701, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16801, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 16901, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17001, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17101, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17201, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17301, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17401, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17501, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17601, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17701, loss 0.0062, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 17801, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17901, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18001, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18101, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18201, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18301, loss 0.0000, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 18401, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18501, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 18601, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18701, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18801, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18901, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19001, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19101, loss 0.0025, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19201, loss 0.0011, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19301, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19401, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19501, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19601, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19701, loss 0.0027, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19801, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 19901, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_Mirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_20000/record_1/MLP_minus_Mirror_20000_5
./test_vehicle0/result_MLP_minus_Mirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9846153846153847

the Fscore is 0.951219512195122

the precision is 0.9069767441860465

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_5
----------------------



epoch 1, loss 0.6932, train acc 61.27%, f1 0.7178, precision 0.5646, recall 0.9853, auc 0.6127
epoch 101, loss 0.4172, train acc 83.99%, f1 0.8407, precision 0.8366, recall 0.8449, auc 0.8399
epoch 201, loss 0.1900, train acc 95.03%, f1 0.9503, precision 0.9496, recall 0.9511, auc 0.9503
epoch 301, loss 0.1358, train acc 97.87%, f1 0.9787, precision 0.9787, recall 0.9787, auc 0.9787
epoch 401, loss 0.0809, train acc 98.53%, f1 0.9852, precision 0.9854, recall 0.9851, auc 0.9853
epoch 501, loss 0.0753, train acc 98.76%, f1 0.9876, precision 0.9877, recall 0.9874, auc 0.9876
epoch 601, loss 0.0554, train acc 98.87%, f1 0.9887, precision 0.9889, recall 0.9886, auc 0.9887
epoch 701, loss 0.0439, train acc 99.06%, f1 0.9906, precision 0.9907, recall 0.9905, auc 0.9906
epoch 801, loss 0.0228, train acc 99.16%, f1 0.9916, precision 0.9917, recall 0.9914, auc 0.9916
epoch 901, loss 0.0301, train acc 99.25%, f1 0.9925, precision 0.9926, recall 0.9924, auc 0.9925
epoch 1001, loss 0.0224, train acc 99.28%, f1 0.9928, precision 0.9929, recall 0.9927, auc 0.9928
epoch 1101, loss 0.0289, train acc 99.34%, f1 0.9934, precision 0.9936, recall 0.9933, auc 0.9934
epoch 1201, loss 0.0217, train acc 99.40%, f1 0.9940, precision 0.9941, recall 0.9940, auc 0.9940
epoch 1301, loss 0.0256, train acc 99.48%, f1 0.9948, precision 0.9949, recall 0.9948, auc 0.9948
epoch 1401, loss 0.0188, train acc 99.52%, f1 0.9952, precision 0.9953, recall 0.9951, auc 0.9952
epoch 1501, loss 0.0179, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1601, loss 0.0131, train acc 99.58%, f1 0.9958, precision 0.9959, recall 0.9957, auc 0.9958
epoch 1701, loss 0.0127, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9960, auc 0.9961
epoch 1801, loss 0.0100, train acc 99.64%, f1 0.9964, precision 0.9965, recall 0.9964, auc 0.9964
epoch 1901, loss 0.0211, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9965, auc 0.9966
epoch 2001, loss 0.0084, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9967, auc 0.9967
epoch 2101, loss 0.0053, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9969, auc 0.9970
epoch 2201, loss 0.0118, train acc 99.71%, f1 0.9971, precision 0.9972, recall 0.9971, auc 0.9971
epoch 2301, loss 0.0086, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 2401, loss 0.0048, train acc 99.74%, f1 0.9974, precision 0.9975, recall 0.9974, auc 0.9974
epoch 2501, loss 0.0086, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2601, loss 0.0058, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9975, auc 0.9976
epoch 2701, loss 0.0074, train acc 99.77%, f1 0.9977, precision 0.9978, recall 0.9977, auc 0.9977
epoch 2801, loss 0.0072, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 2901, loss 0.0107, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 3001, loss 0.0091, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3101, loss 0.0071, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 3201, loss 0.0116, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3301, loss 0.0054, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 3401, loss 0.0104, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3501, loss 0.0081, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3601, loss 0.0052, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3701, loss 0.0045, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9982, auc 0.9983
epoch 3801, loss 0.0081, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 3901, loss 0.0037, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 4001, loss 0.0062, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4101, loss 0.0031, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4201, loss 0.0062, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 4301, loss 0.0082, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4401, loss 0.0032, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4501, loss 0.0093, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4601, loss 0.0065, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9985, auc 0.9985
epoch 4701, loss 0.0051, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4801, loss 0.0053, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 4901, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 5001, loss 0.0039, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 5101, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 5201, loss 0.0034, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 5301, loss 0.0084, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5401, loss 0.0043, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5501, loss 0.0020, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5601, loss 0.0037, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5701, loss 0.0013, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 5801, loss 0.0077, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 5901, loss 0.0056, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 6001, loss 0.0053, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6101, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6201, loss 0.0020, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 6301, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6401, loss 0.0008, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6501, loss 0.0038, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6601, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6701, loss 0.0061, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6801, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6901, loss 0.0029, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7001, loss 0.0019, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7101, loss 0.0025, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7201, loss 0.0034, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7301, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 7401, loss 0.0033, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 7501, loss 0.0022, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7601, loss 0.0045, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7701, loss 0.0053, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7801, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7901, loss 0.0030, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8001, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8101, loss 0.0038, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8201, loss 0.0048, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8301, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 8401, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8501, loss 0.0021, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8601, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8701, loss 0.0046, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8801, loss 0.0046, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8901, loss 0.0015, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9001, loss 0.0009, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9101, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9201, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9301, loss 0.0017, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9401, loss 0.0010, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9501, loss 0.0038, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 9601, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9701, loss 0.0009, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9801, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9901, loss 0.0029, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 10001, loss 0.0054, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 10101, loss 0.0035, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 10201, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10301, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10401, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10501, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10601, loss 0.0001, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 10701, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10801, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 10901, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11001, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11101, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11201, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11301, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11401, loss 0.0001, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 11501, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 11601, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 11701, loss 0.0006, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11801, loss 0.0003, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11901, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12001, loss 0.0003, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 12101, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12201, loss 0.0003, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12301, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12401, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12501, loss 0.0005, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 12601, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 12701, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12801, loss 0.0020, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 12901, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13001, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13101, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13201, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13301, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13401, loss 0.0009, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 13501, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13601, loss 0.0010, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13701, loss 0.0025, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 13801, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13901, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14001, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14101, loss 0.0021, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14201, loss 0.0002, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14301, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 14401, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14501, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14601, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 14701, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14801, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 14901, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_Mirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_15000/record_1/MLP_minus_Mirror_15000_5
./test_vehicle0/result_MLP_minus_Mirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9730769230769231

the Fscore is 0.9176470588235294

the precision is 0.8478260869565217

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_5
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4191, train acc 83.71%, f1 0.8372, precision 0.8363, recall 0.8382, auc 0.8371
epoch 201, loss 0.2273, train acc 95.14%, f1 0.9514, precision 0.9513, recall 0.9515, auc 0.9514
epoch 301, loss 0.1622, train acc 98.03%, f1 0.9803, precision 0.9803, recall 0.9803, auc 0.9803
epoch 401, loss 0.0800, train acc 98.45%, f1 0.9845, precision 0.9845, recall 0.9844, auc 0.9845
epoch 501, loss 0.0669, train acc 98.73%, f1 0.9873, precision 0.9873, recall 0.9873, auc 0.9873
epoch 601, loss 0.0562, train acc 98.93%, f1 0.9893, precision 0.9894, recall 0.9892, auc 0.9893
epoch 701, loss 0.0388, train acc 99.06%, f1 0.9906, precision 0.9906, recall 0.9905, auc 0.9906
epoch 801, loss 0.0362, train acc 99.17%, f1 0.9917, precision 0.9917, recall 0.9917, auc 0.9917
epoch 901, loss 0.0408, train acc 99.25%, f1 0.9925, precision 0.9925, recall 0.9925, auc 0.9925
epoch 1001, loss 0.0308, train acc 99.32%, f1 0.9932, precision 0.9932, recall 0.9931, auc 0.9932
epoch 1101, loss 0.0193, train acc 99.37%, f1 0.9937, precision 0.9937, recall 0.9937, auc 0.9937
epoch 1201, loss 0.0210, train acc 99.42%, f1 0.9942, precision 0.9942, recall 0.9942, auc 0.9942
epoch 1301, loss 0.0170, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9948, auc 0.9949
epoch 1401, loss 0.0204, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1501, loss 0.0172, train acc 99.55%, f1 0.9955, precision 0.9955, recall 0.9955, auc 0.9955
epoch 1601, loss 0.0155, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 1701, loss 0.0254, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1801, loss 0.0138, train acc 99.63%, f1 0.9963, precision 0.9964, recall 0.9963, auc 0.9963
epoch 1901, loss 0.0095, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 2001, loss 0.0124, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2101, loss 0.0053, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2201, loss 0.0181, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2301, loss 0.0056, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2401, loss 0.0099, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2501, loss 0.0090, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2601, loss 0.0030, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2701, loss 0.0100, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2801, loss 0.0069, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 2901, loss 0.0109, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3001, loss 0.0067, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3101, loss 0.0059, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3201, loss 0.0090, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3301, loss 0.0029, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3401, loss 0.0097, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3501, loss 0.0078, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3601, loss 0.0110, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3701, loss 0.0039, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3801, loss 0.0043, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3901, loss 0.0032, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4001, loss 0.0086, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4101, loss 0.0036, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4201, loss 0.0044, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4301, loss 0.0101, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4401, loss 0.0050, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4501, loss 0.0082, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4601, loss 0.0053, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4701, loss 0.0050, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9985, auc 0.9986
epoch 4801, loss 0.0028, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4901, loss 0.0076, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5001, loss 0.0073, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5101, loss 0.0088, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5201, loss 0.0034, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 5301, loss 0.0012, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5401, loss 0.0039, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5501, loss 0.0025, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5601, loss 0.0069, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9986, auc 0.9987
epoch 5701, loss 0.0032, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5801, loss 0.0054, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 5901, loss 0.0054, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6001, loss 0.0021, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 6101, loss 0.0042, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6201, loss 0.0056, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6301, loss 0.0021, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6401, loss 0.0059, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6501, loss 0.0029, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6601, loss 0.0038, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6701, loss 0.0060, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6801, loss 0.0009, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6901, loss 0.0059, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7001, loss 0.0081, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 7101, loss 0.0037, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7201, loss 0.0028, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7301, loss 0.0016, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 7401, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7501, loss 0.0043, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7601, loss 0.0046, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7701, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7801, loss 0.0063, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 7901, loss 0.0024, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 8001, loss 0.0039, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8101, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8201, loss 0.0022, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8301, loss 0.0004, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8401, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8501, loss 0.0006, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8601, loss 0.0026, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8701, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 8801, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 8901, loss 0.0036, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9001, loss 0.0024, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9101, loss 0.0027, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9201, loss 0.0023, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9301, loss 0.0041, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9401, loss 0.0031, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 9501, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9601, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9701, loss 0.0008, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9801, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9901, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_Mirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_10000/record_1/MLP_minus_Mirror_10000_5
./test_vehicle0/result_MLP_minus_Mirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9807692307692308

the Fscore is 0.9397590361445783

the precision is 0.8863636363636364

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_5
----------------------



epoch 1, loss 0.6930, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4133, train acc 83.28%, f1 0.8330, precision 0.8321, recall 0.8338, auc 0.8328
epoch 201, loss 0.2395, train acc 95.09%, f1 0.9509, precision 0.9507, recall 0.9512, auc 0.9509
epoch 301, loss 0.1051, train acc 97.95%, f1 0.9795, precision 0.9795, recall 0.9795, auc 0.9795
epoch 401, loss 0.0813, train acc 98.55%, f1 0.9855, precision 0.9855, recall 0.9855, auc 0.9855
epoch 501, loss 0.0690, train acc 98.76%, f1 0.9876, precision 0.9877, recall 0.9876, auc 0.9876
epoch 601, loss 0.0483, train acc 98.92%, f1 0.9892, precision 0.9893, recall 0.9892, auc 0.9892
epoch 701, loss 0.0596, train acc 99.07%, f1 0.9907, precision 0.9908, recall 0.9907, auc 0.9907
epoch 801, loss 0.0398, train acc 99.16%, f1 0.9916, precision 0.9916, recall 0.9915, auc 0.9916
epoch 901, loss 0.0383, train acc 99.24%, f1 0.9924, precision 0.9924, recall 0.9923, auc 0.9924
epoch 1001, loss 0.0228, train acc 99.31%, f1 0.9931, precision 0.9931, recall 0.9930, auc 0.9931
epoch 1101, loss 0.0209, train acc 99.35%, f1 0.9935, precision 0.9935, recall 0.9935, auc 0.9935
epoch 1201, loss 0.0257, train acc 99.44%, f1 0.9944, precision 0.9944, recall 0.9944, auc 0.9944
epoch 1301, loss 0.0145, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9947, auc 0.9947
epoch 1401, loss 0.0172, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9951, auc 0.9951
epoch 1501, loss 0.0176, train acc 99.54%, f1 0.9954, precision 0.9955, recall 0.9954, auc 0.9954
epoch 1601, loss 0.0173, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1701, loss 0.0185, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9961, auc 0.9961
epoch 1801, loss 0.0200, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1901, loss 0.0125, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9966, auc 0.9967
epoch 2001, loss 0.0129, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2101, loss 0.0109, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 2201, loss 0.0093, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2301, loss 0.0100, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2401, loss 0.0081, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2501, loss 0.0080, train acc 99.75%, f1 0.9975, precision 0.9975, recall 0.9975, auc 0.9975
epoch 2601, loss 0.0048, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2701, loss 0.0127, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 2801, loss 0.0094, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 2901, loss 0.0092, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 3001, loss 0.0076, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 3101, loss 0.0046, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3201, loss 0.0040, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3301, loss 0.0035, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3401, loss 0.0032, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3501, loss 0.0079, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3601, loss 0.0044, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3701, loss 0.0048, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 3801, loss 0.0043, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3901, loss 0.0039, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4001, loss 0.0050, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4101, loss 0.0008, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4201, loss 0.0038, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4301, loss 0.0058, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4401, loss 0.0047, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4501, loss 0.0029, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4601, loss 0.0036, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4701, loss 0.0047, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4801, loss 0.0013, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4901, loss 0.0022, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5001, loss 0.0085, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5101, loss 0.0055, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 5201, loss 0.0027, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 5301, loss 0.0053, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 5401, loss 0.0042, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 5501, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 5601, loss 0.0068, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 5701, loss 0.0042, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 5801, loss 0.0039, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5901, loss 0.0016, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6001, loss 0.0006, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6101, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6201, loss 0.0027, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6301, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6401, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 6501, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6601, loss 0.0031, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6701, loss 0.0018, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6801, loss 0.0038, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 6901, loss 0.0018, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7001, loss 0.0062, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7101, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7201, loss 0.0004, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7301, loss 0.0043, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7401, loss 0.0046, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7501, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7601, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7701, loss 0.0011, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7801, loss 0.0033, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 7901, loss 0.0023, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_Mirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_8000/record_1/MLP_minus_Mirror_8000_5
./test_vehicle0/result_MLP_minus_Mirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9807692307692308

the Fscore is 0.9397590361445783

the precision is 0.8863636363636364

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_5
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.3921, train acc 83.54%, f1 0.8358, precision 0.8341, recall 0.8375, auc 0.8354
epoch 201, loss 0.2527, train acc 94.93%, f1 0.9493, precision 0.9490, recall 0.9497, auc 0.9493
epoch 301, loss 0.1359, train acc 97.94%, f1 0.9794, precision 0.9794, recall 0.9794, auc 0.9794
epoch 401, loss 0.0767, train acc 98.62%, f1 0.9862, precision 0.9862, recall 0.9862, auc 0.9862
epoch 501, loss 0.0723, train acc 98.75%, f1 0.9875, precision 0.9875, recall 0.9874, auc 0.9875
epoch 601, loss 0.0435, train acc 98.92%, f1 0.9892, precision 0.9893, recall 0.9892, auc 0.9892
epoch 701, loss 0.0526, train acc 99.08%, f1 0.9908, precision 0.9909, recall 0.9907, auc 0.9908
epoch 801, loss 0.0323, train acc 99.17%, f1 0.9917, precision 0.9918, recall 0.9917, auc 0.9917
epoch 901, loss 0.0240, train acc 99.24%, f1 0.9924, precision 0.9924, recall 0.9924, auc 0.9924
epoch 1001, loss 0.0140, train acc 99.31%, f1 0.9931, precision 0.9931, recall 0.9931, auc 0.9931
epoch 1101, loss 0.0300, train acc 99.36%, f1 0.9936, precision 0.9937, recall 0.9936, auc 0.9936
epoch 1201, loss 0.0308, train acc 99.43%, f1 0.9943, precision 0.9943, recall 0.9942, auc 0.9943
epoch 1301, loss 0.0183, train acc 99.48%, f1 0.9948, precision 0.9948, recall 0.9948, auc 0.9948
epoch 1401, loss 0.0200, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9952, auc 0.9953
epoch 1501, loss 0.0240, train acc 99.57%, f1 0.9957, precision 0.9957, recall 0.9957, auc 0.9957
epoch 1601, loss 0.0166, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1701, loss 0.0095, train acc 99.61%, f1 0.9961, precision 0.9961, recall 0.9960, auc 0.9961
epoch 1801, loss 0.0174, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1901, loss 0.0090, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9966, auc 0.9966
epoch 2001, loss 0.0170, train acc 99.68%, f1 0.9968, precision 0.9968, recall 0.9968, auc 0.9968
epoch 2101, loss 0.0063, train acc 99.69%, f1 0.9969, precision 0.9970, recall 0.9969, auc 0.9969
epoch 2201, loss 0.0085, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2301, loss 0.0085, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9972, auc 0.9972
epoch 2401, loss 0.0080, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9973, auc 0.9973
epoch 2501, loss 0.0155, train acc 99.74%, f1 0.9974, precision 0.9975, recall 0.9974, auc 0.9974
epoch 2601, loss 0.0049, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 2701, loss 0.0037, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2801, loss 0.0146, train acc 99.78%, f1 0.9978, precision 0.9979, recall 0.9978, auc 0.9978
epoch 2901, loss 0.0069, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9979, auc 0.9979
epoch 3001, loss 0.0060, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9979, auc 0.9979
epoch 3101, loss 0.0062, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9980, auc 0.9980
epoch 3201, loss 0.0057, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3301, loss 0.0039, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3401, loss 0.0086, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3501, loss 0.0053, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3601, loss 0.0062, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3701, loss 0.0081, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3801, loss 0.0045, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3901, loss 0.0075, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4001, loss 0.0025, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 4101, loss 0.0098, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4201, loss 0.0040, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4301, loss 0.0018, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9984, auc 0.9984
epoch 4401, loss 0.0027, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4501, loss 0.0061, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4601, loss 0.0044, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4701, loss 0.0020, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4801, loss 0.0023, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
epoch 4901, loss 0.0048, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9985, auc 0.9985
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_Mirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_5000/record_1/MLP_minus_Mirror_5000_5
./test_vehicle0/result_MLP_minus_Mirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9730769230769231

the Fscore is 0.9176470588235294

the precision is 0.8478260869565217

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_5
----------------------



epoch 1, loss 0.6931, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4029, train acc 83.08%, f1 0.8337, precision 0.8199, recall 0.8480, auc 0.8308
epoch 201, loss 0.2343, train acc 94.56%, f1 0.9457, precision 0.9441, recall 0.9472, auc 0.9456
epoch 301, loss 0.1305, train acc 97.96%, f1 0.9796, precision 0.9795, recall 0.9797, auc 0.9796
epoch 401, loss 0.0818, train acc 98.46%, f1 0.9846, precision 0.9849, recall 0.9844, auc 0.9846
epoch 501, loss 0.0610, train acc 98.72%, f1 0.9872, precision 0.9876, recall 0.9868, auc 0.9872
epoch 601, loss 0.0440, train acc 98.93%, f1 0.9893, precision 0.9896, recall 0.9890, auc 0.9893
epoch 701, loss 0.0440, train acc 99.03%, f1 0.9903, precision 0.9907, recall 0.9900, auc 0.9903
epoch 801, loss 0.0422, train acc 99.13%, f1 0.9913, precision 0.9916, recall 0.9911, auc 0.9913
epoch 901, loss 0.0291, train acc 99.21%, f1 0.9921, precision 0.9924, recall 0.9919, auc 0.9921
epoch 1001, loss 0.0202, train acc 99.28%, f1 0.9928, precision 0.9930, recall 0.9926, auc 0.9928
epoch 1101, loss 0.0193, train acc 99.34%, f1 0.9933, precision 0.9936, recall 0.9931, auc 0.9934
epoch 1201, loss 0.0280, train acc 99.37%, f1 0.9937, precision 0.9939, recall 0.9935, auc 0.9937
epoch 1301, loss 0.0253, train acc 99.47%, f1 0.9947, precision 0.9949, recall 0.9945, auc 0.9947
epoch 1401, loss 0.0165, train acc 99.49%, f1 0.9949, precision 0.9950, recall 0.9947, auc 0.9949
epoch 1501, loss 0.0261, train acc 99.55%, f1 0.9955, precision 0.9956, recall 0.9953, auc 0.9955
epoch 1601, loss 0.0094, train acc 99.57%, f1 0.9957, precision 0.9958, recall 0.9956, auc 0.9957
epoch 1701, loss 0.0098, train acc 99.60%, f1 0.9960, precision 0.9961, recall 0.9959, auc 0.9960
epoch 1801, loss 0.0082, train acc 99.62%, f1 0.9962, precision 0.9963, recall 0.9961, auc 0.9962
epoch 1901, loss 0.0154, train acc 99.65%, f1 0.9965, precision 0.9966, recall 0.9964, auc 0.9965
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_Mirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_Mirror_2000/record_1/MLP_minus_Mirror_2000_5
./test_vehicle0/result_MLP_minus_Mirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9692307692307692

the Fscore is 0.9069767441860465

the precision is 0.8297872340425532

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_5
----------------------



epoch 1, loss 0.6939, train acc 49.87%, f1 0.6655, precision 0.4987, recall 1.0000, auc 0.5000
epoch 101, loss 0.4432, train acc 83.35%, f1 0.8341, precision 0.8289, recall 0.8393, auc 0.8335
epoch 201, loss 0.2628, train acc 94.73%, f1 0.9472, precision 0.9467, recall 0.9478, auc 0.9473
epoch 301, loss 0.1142, train acc 97.93%, f1 0.9792, precision 0.9799, recall 0.9785, auc 0.9793
epoch 401, loss 0.0920, train acc 98.44%, f1 0.9844, precision 0.9849, recall 0.9839, auc 0.9844
epoch 501, loss 0.0867, train acc 98.80%, f1 0.9880, precision 0.9883, recall 0.9876, auc 0.9880
epoch 601, loss 0.0552, train acc 98.90%, f1 0.9889, precision 0.9894, recall 0.9884, auc 0.9890
epoch 701, loss 0.0477, train acc 99.05%, f1 0.9905, precision 0.9910, recall 0.9900, auc 0.9905
epoch 801, loss 0.0408, train acc 99.13%, f1 0.9913, precision 0.9919, recall 0.9906, auc 0.9913
epoch 901, loss 0.0189, train acc 99.23%, f1 0.9923, precision 0.9929, recall 0.9917, auc 0.9923
epoch 1001, loss 0.0215, train acc 99.29%, f1 0.9928, precision 0.9935, recall 0.9922, auc 0.9929
epoch 1101, loss 0.0153, train acc 99.32%, f1 0.9932, precision 0.9935, recall 0.9928, auc 0.9932
epoch 1201, loss 0.0286, train acc 99.41%, f1 0.9940, precision 0.9945, recall 0.9936, auc 0.9941
epoch 1301, loss 0.0194, train acc 99.45%, f1 0.9945, precision 0.9947, recall 0.9943, auc 0.9945
epoch 1401, loss 0.0168, train acc 99.49%, f1 0.9949, precision 0.9951, recall 0.9947, auc 0.9949
epoch 1501, loss 0.0204, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9952, auc 0.9953
epoch 1601, loss 0.0250, train acc 99.58%, f1 0.9957, precision 0.9957, recall 0.9958, auc 0.9958
epoch 1701, loss 0.0168, train acc 99.60%, f1 0.9960, precision 0.9961, recall 0.9960, auc 0.9960
epoch 1801, loss 0.0189, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1901, loss 0.0195, train acc 99.65%, f1 0.9965, precision 0.9965, recall 0.9965, auc 0.9965
epoch 2001, loss 0.0070, train acc 99.67%, f1 0.9967, precision 0.9967, recall 0.9967, auc 0.9967
epoch 2101, loss 0.0095, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9970, auc 0.9970
epoch 2201, loss 0.0116, train acc 99.71%, f1 0.9971, precision 0.9970, recall 0.9971, auc 0.9971
epoch 2301, loss 0.0110, train acc 99.73%, f1 0.9973, precision 0.9972, recall 0.9973, auc 0.9973
epoch 2401, loss 0.0061, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9976, auc 0.9974
epoch 2501, loss 0.0136, train acc 99.76%, f1 0.9976, precision 0.9975, recall 0.9978, auc 0.9976
epoch 2601, loss 0.0076, train acc 99.77%, f1 0.9977, precision 0.9975, recall 0.9979, auc 0.9977
epoch 2701, loss 0.0076, train acc 99.79%, f1 0.9979, precision 0.9977, recall 0.9980, auc 0.9979
epoch 2801, loss 0.0107, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9983, auc 0.9980
epoch 2901, loss 0.0124, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 3001, loss 0.0071, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9983, auc 0.9981
epoch 3101, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9979, recall 0.9984, auc 0.9982
epoch 3201, loss 0.0060, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 3301, loss 0.0078, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9985, auc 0.9982
epoch 3401, loss 0.0102, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9985, auc 0.9983
epoch 3501, loss 0.0028, train acc 99.83%, f1 0.9983, precision 0.9980, recall 0.9986, auc 0.9983
epoch 3601, loss 0.0066, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 3701, loss 0.0060, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 3801, loss 0.0081, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9987, auc 0.9985
epoch 3901, loss 0.0033, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4001, loss 0.0079, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 4101, loss 0.0060, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9987, auc 0.9985
epoch 4201, loss 0.0067, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 4301, loss 0.0075, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 4401, loss 0.0018, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 4501, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4601, loss 0.0035, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4701, loss 0.0015, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4801, loss 0.0014, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 4901, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 5001, loss 0.0081, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
epoch 5101, loss 0.0041, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 5201, loss 0.0007, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 5301, loss 0.0048, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9990, auc 0.9989
epoch 5401, loss 0.0071, train acc 99.88%, f1 0.9988, precision 0.9986, recall 0.9989, auc 0.9988
epoch 5501, loss 0.0031, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 5601, loss 0.0023, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 5701, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9991, auc 0.9989
epoch 5801, loss 0.0062, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 5901, loss 0.0018, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 6001, loss 0.0034, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 6101, loss 0.0012, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9990
epoch 6201, loss 0.0040, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 6301, loss 0.0011, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 6401, loss 0.0042, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9992, auc 0.9990
epoch 6501, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9991, auc 0.9991
epoch 6601, loss 0.0033, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 6701, loss 0.0036, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6801, loss 0.0039, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9992, auc 0.9990
epoch 6901, loss 0.0020, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 7001, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 7101, loss 0.0016, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 7201, loss 0.0038, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 7301, loss 0.0024, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 7401, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 7501, loss 0.0012, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 7601, loss 0.0013, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 7701, loss 0.0056, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9992
epoch 7801, loss 0.0025, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9992
epoch 7901, loss 0.0060, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 8001, loss 0.0011, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 8101, loss 0.0011, train acc 99.93%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9993
epoch 8201, loss 0.0061, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 8301, loss 0.0016, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9992
epoch 8401, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9993, auc 0.9993
epoch 8501, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 8601, loss 0.0033, train acc 99.93%, f1 0.9992, precision 0.9992, recall 0.9993, auc 0.9993
epoch 8701, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8801, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 8901, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 9001, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9101, loss 0.0028, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 9201, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9301, loss 0.0023, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 9401, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 9501, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 9601, loss 0.0045, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 9701, loss 0.0002, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 9801, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 9901, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10001, loss 0.0026, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 10101, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9996, auc 0.9994
epoch 10201, loss 0.0014, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 10301, loss 0.0010, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 10401, loss 0.0015, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 10501, loss 0.0018, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 10601, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 10701, loss 0.0013, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9994, auc 0.9994
epoch 10801, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 10901, loss 0.0009, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9996, auc 0.9994
epoch 11001, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 11101, loss 0.0019, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11201, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 11301, loss 0.0007, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 11401, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 11501, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9996, auc 0.9994
epoch 11601, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 11701, loss 0.0014, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 11801, loss 0.0029, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 11901, loss 0.0024, train acc 99.95%, f1 0.9995, precision 0.9993, recall 0.9996, auc 0.9995
epoch 12001, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12101, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12201, loss 0.0013, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12301, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12401, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9997, auc 0.9995
epoch 12501, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 12601, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12701, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9997, auc 0.9996
epoch 12801, loss 0.0002, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12901, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 13001, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 13101, loss 0.0019, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9997, auc 0.9995
epoch 13201, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 13301, loss 0.0011, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 13401, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9997, auc 0.9996
epoch 13501, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 13601, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 13701, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9997, auc 0.9996
epoch 13801, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 13901, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 14001, loss 0.0031, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 14101, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 14201, loss 0.0027, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 14301, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 14401, loss 0.0000, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 14501, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 14601, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 14701, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9994, recall 0.9998, auc 0.9996
epoch 14801, loss 0.0006, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 14901, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 15001, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 15101, loss 0.0011, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 15201, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 15301, loss 0.0017, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 15401, loss 0.0012, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 15501, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 15601, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 15701, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 15801, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 15901, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 16001, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 16101, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 16201, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 16301, loss 0.0032, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 16401, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16501, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 16601, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 16701, loss 0.0005, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 16801, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 16901, loss 0.0008, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 17001, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 17101, loss 0.0023, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 17201, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 17301, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 17401, loss 0.0001, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 17501, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 17601, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 17701, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 17801, loss 0.0019, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9998, auc 0.9996
epoch 17901, loss 0.0003, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 18001, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 18101, loss 0.0012, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 18201, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 18301, loss 0.0037, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9997, auc 0.9996
epoch 18401, loss 0.0009, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 18501, loss 0.0007, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18601, loss 0.0010, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 18701, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 18801, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 18901, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 19001, loss 0.0022, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 19101, loss 0.0022, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 19201, loss 0.0017, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 19301, loss 0.0004, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19401, loss 0.0022, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9998, auc 0.9997
epoch 19501, loss 0.0002, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19601, loss 0.0006, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19701, loss 0.0016, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 19801, loss 0.0021, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 19901, loss 0.0018, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_notMirror_20000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_20000/record_1/MLP_minus_notMirror_20000_5
./test_vehicle0/result_MLP_minus_notMirror_20000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9730769230769231

the Fscore is 0.9176470588235294

the precision is 0.8478260869565217

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_5
----------------------



epoch 1, loss 0.6939, train acc 49.75%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4120, train acc 83.35%, f1 0.8343, precision 0.8346, recall 0.8339, auc 0.8335
epoch 201, loss 0.2190, train acc 94.58%, f1 0.9461, precision 0.9463, recall 0.9458, auc 0.9458
epoch 301, loss 0.1120, train acc 97.91%, f1 0.9792, precision 0.9800, recall 0.9785, auc 0.9791
epoch 401, loss 0.0634, train acc 98.45%, f1 0.9845, precision 0.9850, recall 0.9841, auc 0.9845
epoch 501, loss 0.0687, train acc 98.73%, f1 0.9873, precision 0.9875, recall 0.9871, auc 0.9873
epoch 601, loss 0.0817, train acc 98.86%, f1 0.9887, precision 0.9884, recall 0.9890, auc 0.9886
epoch 701, loss 0.0324, train acc 99.04%, f1 0.9904, precision 0.9902, recall 0.9906, auc 0.9904
epoch 801, loss 0.0367, train acc 99.14%, f1 0.9914, precision 0.9915, recall 0.9913, auc 0.9914
epoch 901, loss 0.0385, train acc 99.20%, f1 0.9921, precision 0.9922, recall 0.9919, auc 0.9920
epoch 1001, loss 0.0176, train acc 99.27%, f1 0.9927, precision 0.9929, recall 0.9925, auc 0.9927
epoch 1101, loss 0.0269, train acc 99.34%, f1 0.9934, precision 0.9936, recall 0.9932, auc 0.9934
epoch 1201, loss 0.0188, train acc 99.39%, f1 0.9939, precision 0.9942, recall 0.9937, auc 0.9939
epoch 1301, loss 0.0225, train acc 99.44%, f1 0.9945, precision 0.9947, recall 0.9942, auc 0.9944
epoch 1401, loss 0.0188, train acc 99.51%, f1 0.9951, precision 0.9951, recall 0.9951, auc 0.9951
epoch 1501, loss 0.0182, train acc 99.52%, f1 0.9952, precision 0.9952, recall 0.9952, auc 0.9952
epoch 1601, loss 0.0154, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9958, auc 0.9958
epoch 1701, loss 0.0100, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9963, auc 0.9961
epoch 1801, loss 0.0151, train acc 99.63%, f1 0.9963, precision 0.9963, recall 0.9963, auc 0.9963
epoch 1901, loss 0.0101, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9967, auc 0.9966
epoch 2001, loss 0.0181, train acc 99.69%, f1 0.9969, precision 0.9967, recall 0.9971, auc 0.9969
epoch 2101, loss 0.0121, train acc 99.71%, f1 0.9971, precision 0.9971, recall 0.9971, auc 0.9971
epoch 2201, loss 0.0103, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 2301, loss 0.0103, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9973
epoch 2401, loss 0.0111, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9976, auc 0.9975
epoch 2501, loss 0.0107, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2601, loss 0.0110, train acc 99.76%, f1 0.9976, precision 0.9977, recall 0.9976, auc 0.9976
epoch 2701, loss 0.0118, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9979, auc 0.9980
epoch 2801, loss 0.0105, train acc 99.80%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9980
epoch 2901, loss 0.0060, train acc 99.80%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9980
epoch 3001, loss 0.0037, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 3101, loss 0.0048, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 3201, loss 0.0037, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9981, auc 0.9982
epoch 3301, loss 0.0057, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 3401, loss 0.0035, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9981, auc 0.9983
epoch 3501, loss 0.0074, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9982, auc 0.9983
epoch 3601, loss 0.0047, train acc 99.84%, f1 0.9984, precision 0.9985, recall 0.9983, auc 0.9984
epoch 3701, loss 0.0036, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9983, auc 0.9984
epoch 3801, loss 0.0030, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9983, auc 0.9984
epoch 3901, loss 0.0031, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9983, auc 0.9985
epoch 4001, loss 0.0064, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9984, auc 0.9985
epoch 4101, loss 0.0044, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9983, auc 0.9985
epoch 4201, loss 0.0026, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9984, auc 0.9986
epoch 4301, loss 0.0037, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 4401, loss 0.0007, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9984, auc 0.9986
epoch 4501, loss 0.0060, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9985, auc 0.9987
epoch 4601, loss 0.0034, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9987, auc 0.9987
epoch 4701, loss 0.0034, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9985, auc 0.9987
epoch 4801, loss 0.0022, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 4901, loss 0.0035, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 5001, loss 0.0061, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 5101, loss 0.0036, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 5201, loss 0.0036, train acc 99.88%, f1 0.9988, precision 0.9990, recall 0.9987, auc 0.9988
epoch 5301, loss 0.0060, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9987, auc 0.9989
epoch 5401, loss 0.0051, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 5501, loss 0.0091, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9987, auc 0.9989
epoch 5601, loss 0.0019, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 5701, loss 0.0017, train acc 99.89%, f1 0.9989, precision 0.9991, recall 0.9987, auc 0.9989
epoch 5801, loss 0.0011, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 5901, loss 0.0019, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 6001, loss 0.0042, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 6101, loss 0.0021, train acc 99.89%, f1 0.9990, precision 0.9991, recall 0.9988, auc 0.9989
epoch 6201, loss 0.0054, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 6301, loss 0.0005, train acc 99.90%, f1 0.9990, precision 0.9992, recall 0.9989, auc 0.9990
epoch 6401, loss 0.0016, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 6501, loss 0.0026, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6601, loss 0.0056, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9989, auc 0.9991
epoch 6701, loss 0.0008, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 6801, loss 0.0064, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 6901, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 7001, loss 0.0035, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 7101, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 7201, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 7301, loss 0.0043, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 7401, loss 0.0041, train acc 99.91%, f1 0.9991, precision 0.9993, recall 0.9990, auc 0.9991
epoch 7501, loss 0.0001, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 7601, loss 0.0036, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 7701, loss 0.0022, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 7801, loss 0.0019, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 7901, loss 0.0037, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 8001, loss 0.0024, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 8101, loss 0.0018, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9992/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 8301, loss 0.0022, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9991, auc 0.9993
epoch 8401, loss 0.0014, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 8501, loss 0.0016, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 8601, loss 0.0015, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9991, auc 0.9993
epoch 8701, loss 0.0003, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9991, auc 0.9993
epoch 8801, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 8901, loss 0.0007, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 9001, loss 0.0021, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 9101, loss 0.0028, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9992, auc 0.9994
epoch 9201, loss 0.0007, train acc 99.93%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9993
epoch 9301, loss 0.0033, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 9401, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 9501, loss 0.0003, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9601, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 9701, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 9801, loss 0.0002, train acc 99.93%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9993
epoch 9901, loss 0.0002, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 10001, loss 0.0007, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10101, loss 0.0024, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10201, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 10301, loss 0.0043, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10401, loss 0.0005, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 10501, loss 0.0002, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10601, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 10701, loss 0.0016, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 10801, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 10901, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 11001, loss 0.0006, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11101, loss 0.0012, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 11201, loss 0.0015, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 11301, loss 0.0022, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11401, loss 0.0001, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11501, loss 0.0005, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 11601, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 11701, loss 0.0012, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11801, loss 0.0005, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9994, auc 0.9994
epoch 11901, loss 0.0021, train acc 99.95%, f1 0.9995, precision 0.9997, recall 0.9994, auc 0.9995
epoch 12001, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 12101, loss 0.0003, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 12201, loss 0.0003, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 12301, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 12401, loss 0.0008, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 12501, loss 0.0003, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 12601, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9994, auc 0.9996
epoch 12701, loss 0.0004, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 12801, loss 0.0007, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 12901, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9994, auc 0.9996
epoch 13001, loss 0.0032, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 13101, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 13201, loss 0.0001, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 13301, loss 0.0004, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 13401, loss 0.0013, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 13501, loss 0.0000, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9995, auc 0.9996
epoch 13601, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9995, auc 0.9996
epoch 13701, loss 0.0026, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 13801, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9994, auc 0.9996
epoch 13901, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 14001, loss 0.0010, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 14101, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 14201, loss 0.0008, train acc 99.96%, f1 0.9996, precision 0.9998, recall 0.9995, auc 0.9996
epoch 14301, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 14401, loss 0.0002, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 14501, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 14601, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 14701, loss 0.0009, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 14801, loss 0.0005, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 14901, loss 0.0014, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_notMirror_15000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_15000/record_1/MLP_minus_notMirror_15000_5
./test_vehicle0/result_MLP_minus_notMirror_15000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9641025641025641

the Fscore is 0.9156626506024097

the precision is 0.8636363636363636

the recall is 0.9743589743589743

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_5
----------------------



epoch 1, loss 0.6934, train acc 49.95%, f1 0.6662, precision 0.4995, recall 1.0000, auc 0.5000
epoch 101, loss 0.4093, train acc 83.69%, f1 0.8365, precision 0.8376, recall 0.8354, auc 0.8369
epoch 201, loss 0.2186, train acc 94.82%, f1 0.9481, precision 0.9488, recall 0.9474, auc 0.9482
epoch 301, loss 0.1155, train acc 97.83%, f1 0.9782, precision 0.9785, recall 0.9780, auc 0.9783
epoch 401, loss 0.0792, train acc 98.50%, f1 0.9850, precision 0.9845, recall 0.9856, auc 0.9850
epoch 501, loss 0.0557, train acc 98.77%, f1 0.9877, precision 0.9872, recall 0.9881, auc 0.9877
epoch 601, loss 0.0462, train acc 98.92%, f1 0.9892, precision 0.9887, recall 0.9897, auc 0.9892
epoch 701, loss 0.0353, train acc 99.03%, f1 0.9903, precision 0.9898, recall 0.9908, auc 0.9903
epoch 801, loss 0.0275, train acc 99.12%, f1 0.9912, precision 0.9906, recall 0.9918, auc 0.9912
epoch 901, loss 0.0330, train acc 99.20%, f1 0.9920, precision 0.9916, recall 0.9923, auc 0.9920
epoch 1001, loss 0.0317, train acc 99.28%, f1 0.9928, precision 0.9925, recall 0.9930, auc 0.9928
epoch 1101, loss 0.0273, train acc 99.35%, f1 0.9935, precision 0.9933, recall 0.9937, auc 0.9935
epoch 1201, loss 0.0170, train acc 99.39%, f1 0.9939, precision 0.9938, recall 0.9940, auc 0.9939
epoch 1301, loss 0.0213, train acc 99.48%, f1 0.9947, precision 0.9946, recall 0.9949, auc 0.9948
epoch 1401, loss 0.0094, train acc 99.49%, f1 0.9949, precision 0.9945, recall 0.9952, auc 0.9949
epoch 1501, loss 0.0285, train acc 99.54%, f1 0.9954, precision 0.9952, recall 0.9956, auc 0.9954
epoch 1601, loss 0.0094, train acc 99.57%, f1 0.9957, precision 0.9955, recall 0.9960, auc 0.9957
epoch 1701, loss 0.0062, train acc 99.62%, f1 0.9962, precision 0.9959, recall 0.9964, auc 0.9962
epoch 1801, loss 0.0139, train acc 99.64%, f1 0.9964, precision 0.9963, recall 0.9965, auc 0.9964
epoch 1901, loss 0.0093, train acc 99.66%, f1 0.9966, precision 0.9964, recall 0.9969, auc 0.9966
epoch 2001, loss 0.0153, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9969, auc 0.9969
epoch 2101, loss 0.0143, train acc 99.70%, f1 0.9970, precision 0.9969, recall 0.9971, auc 0.9970
epoch 2201, loss 0.0067, train acc 99.72%, f1 0.9972, precision 0.9972, recall 0.9973, auc 0.9972
epoch 2301, loss 0.0226, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9974, auc 0.9973
epoch 2401, loss 0.0113, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 2501, loss 0.0109, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2601, loss 0.0065, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9978, auc 0.9977
epoch 2701, loss 0.0092, train acc 99.78%, f1 0.9978, precision 0.9978, recall 0.9977, auc 0.9978
epoch 2801, loss 0.0105, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 2901, loss 0.0043, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9980, auc 0.9981
epoch 3001, loss 0.0074, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 3101, loss 0.0034, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 3201, loss 0.0036, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 3301, loss 0.0074, train acc 99.81%, f1 0.9981, precision 0.9981, recall 0.9981, auc 0.9981
epoch 3401, loss 0.0061, train acc 99.82%, f1 0.9982, precision 0.9983, recall 0.9982, auc 0.9982
epoch 3501, loss 0.0065, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9983, auc 0.9983
epoch 3601, loss 0.0045, train acc 99.83%, f1 0.9983, precision 0.9984, recall 0.9983, auc 0.9983
epoch 3701, loss 0.0014, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3801, loss 0.0076, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 3901, loss 0.0058, train acc 99.84%, f1 0.9984, precision 0.9984, recall 0.9984, auc 0.9984
epoch 4001, loss 0.0057, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4101, loss 0.0038, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4201, loss 0.0041, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 4301, loss 0.0040, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 4401, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 4501, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 4601, loss 0.0045, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 4701, loss 0.0047, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9988, auc 0.9986
epoch 4801, loss 0.0056, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 4901, loss 0.0013, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9987, auc 0.9987
epoch 5001, loss 0.0019, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 5101, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 5201, loss 0.0029, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 5301, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9987, auc 0.9988
epoch 5401, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 5501, loss 0.0028, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 5601, loss 0.0032, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9988, auc 0.9988
epoch 5701, loss 0.0050, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 5801, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 5901, loss 0.0036, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 6001, loss 0.0032, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 6101, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 6201, loss 0.0027, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 6301, loss 0.0014, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 6401, loss 0.0021, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 6501, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 6601, loss 0.0012, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 6701, loss 0.0013, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9989, auc 0.9990
epoch 6801, loss 0.0005, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6901, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7001, loss 0.0017, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7101, loss 0.0011, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7201, loss 0.0059, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9989, auc 0.9991
epoch 7301, loss 0.0014, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7401, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7501, loss 0.0010, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7601, loss 0.0027, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7701, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7801, loss 0.0041, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9990, auc 0.9992
epoch 7901, loss 0.0028, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 8001, loss 0.0008, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 8101, loss 0.0064, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0030, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 8301, loss 0.0043, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 8401, loss 0.0021, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 8501, loss 0.0017, train acc 99.93%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9993
epoch 8601, loss 0.0031, train acc 99.92%, f1 0.9992, precision 0.9994, recall 0.9991, auc 0.9992
epoch 8701, loss 0.0028, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 8801, loss 0.0057, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 8901, loss 0.0047, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 9001, loss 0.0029, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 9101, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 9201, loss 0.0013, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 9301, loss 0.0019, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 9401, loss 0.0008, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9992, auc 0.9994
epoch 9501, loss 0.0034, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9992, auc 0.9993
epoch 9601, loss 0.0048, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 9701, loss 0.0017, train acc 99.93%, f1 0.9993, precision 0.9995, recall 0.9992, auc 0.9993
epoch 9801, loss 0.0003, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 9901, loss 0.0004, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_notMirror_10000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_10000/record_1/MLP_minus_notMirror_10000_5
./test_vehicle0/result_MLP_minus_notMirror_10000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9730769230769231

the Fscore is 0.9176470588235294

the precision is 0.8478260869565217

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_5
----------------------



epoch 1, loss 0.6932, train acc 50.32%, f1 0.6688, precision 0.5024, recall 1.0000, auc 0.5017
epoch 101, loss 0.4220, train acc 83.71%, f1 0.8374, precision 0.8383, recall 0.8365, auc 0.8371
epoch 201, loss 0.1974, train acc 94.96%, f1 0.9497, precision 0.9510, recall 0.9484, auc 0.9496
epoch 301, loss 0.1378, train acc 97.98%, f1 0.9798, precision 0.9801, recall 0.9795, auc 0.9798
epoch 401, loss 0.0912, train acc 98.50%, f1 0.9851, precision 0.9852, recall 0.9849, auc 0.9850
epoch 501, loss 0.0560, train acc 98.78%, f1 0.9878, precision 0.9878, recall 0.9879, auc 0.9878
epoch 601, loss 0.0500, train acc 98.93%, f1 0.9894, precision 0.9898, recall 0.9890, auc 0.9894
epoch 701, loss 0.0413, train acc 99.09%, f1 0.9909, precision 0.9910, recall 0.9907, auc 0.9909
epoch 801, loss 0.0340, train acc 99.20%, f1 0.9920, precision 0.9924, recall 0.9917, auc 0.9920
epoch 901, loss 0.0347, train acc 99.25%, f1 0.9925, precision 0.9929, recall 0.9922, auc 0.9925
epoch 1001, loss 0.0181, train acc 99.32%, f1 0.9932, precision 0.9935, recall 0.9929, auc 0.9932
epoch 1101, loss 0.0290, train acc 99.36%, f1 0.9936, precision 0.9939, recall 0.9933, auc 0.9936
epoch 1201, loss 0.0248, train acc 99.42%, f1 0.9942, precision 0.9944, recall 0.9940, auc 0.9942
epoch 1301, loss 0.0208, train acc 99.47%, f1 0.9947, precision 0.9949, recall 0.9946, auc 0.9947
epoch 1401, loss 0.0162, train acc 99.51%, f1 0.9951, precision 0.9953, recall 0.9949, auc 0.9951
epoch 1501, loss 0.0178, train acc 99.54%, f1 0.9954, precision 0.9955, recall 0.9954, auc 0.9954
epoch 1601, loss 0.0103, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 1701, loss 0.0203, train acc 99.61%, f1 0.9961, precision 0.9962, recall 0.9960, auc 0.9961
epoch 1801, loss 0.0134, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9962, auc 0.9962
epoch 1901, loss 0.0113, train acc 99.65%, f1 0.9965, precision 0.9967, recall 0.9962, auc 0.9965
epoch 2001, loss 0.0122, train acc 99.67%, f1 0.9967, precision 0.9968, recall 0.9966, auc 0.9967
epoch 2101, loss 0.0167, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9969, auc 0.9970
epoch 2201, loss 0.0117, train acc 99.70%, f1 0.9970, precision 0.9973, recall 0.9968, auc 0.9970
epoch 2301, loss 0.0143, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9972, auc 0.9973
epoch 2401, loss 0.0103, train acc 99.73%, f1 0.9973, precision 0.9975, recall 0.9972, auc 0.9973
epoch 2501, loss 0.0034, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9973, auc 0.9975
epoch 2601, loss 0.0102, train acc 99.76%, f1 0.9976, precision 0.9976, recall 0.9976, auc 0.9976
epoch 2701, loss 0.0070, train acc 99.78%, f1 0.9978, precision 0.9980, recall 0.9977, auc 0.9978
epoch 2801, loss 0.0066, train acc 99.79%, f1 0.9979, precision 0.9980, recall 0.9978, auc 0.9979
epoch 2901, loss 0.0086, train acc 99.80%, f1 0.9980, precision 0.9982, recall 0.9977, auc 0.9980
epoch 3001, loss 0.0064, train acc 99.80%, f1 0.9980, precision 0.9982, recall 0.9978, auc 0.9980
epoch 3101, loss 0.0058, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 3201, loss 0.0063, train acc 99.81%, f1 0.9981, precision 0.9983, recall 0.9979, auc 0.9981
epoch 3301, loss 0.0097, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 3401, loss 0.0078, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9980, auc 0.9982
epoch 3501, loss 0.0054, train acc 99.82%, f1 0.9982, precision 0.9984, recall 0.9981, auc 0.9982
epoch 3601, loss 0.0048, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9981, auc 0.9983
epoch 3701, loss 0.0053, train acc 99.83%, f1 0.9983, precision 0.9985, recall 0.9981, auc 0.9983
epoch 3801, loss 0.0048, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9982, auc 0.9984
epoch 3901, loss 0.0084, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9982, auc 0.9984
epoch 4001, loss 0.0052, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9982, auc 0.9984
epoch 4101, loss 0.0032, train acc 99.84%, f1 0.9984, precision 0.9987, recall 0.9982, auc 0.9984
epoch 4201, loss 0.0047, train acc 99.84%, f1 0.9984, precision 0.9986, recall 0.9982, auc 0.9984
epoch 4301, loss 0.0076, train acc 99.85%, f1 0.9985, precision 0.9986, recall 0.9983, auc 0.9985
epoch 4401, loss 0.0048, train acc 99.85%, f1 0.9985, precision 0.9987, recall 0.9983, auc 0.9985
epoch 4501, loss 0.0116, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 4601, loss 0.0027, train acc 99.86%, f1 0.9986, precision 0.9988, recall 0.9984, auc 0.9986
epoch 4701, loss 0.0030, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9986, auc 0.9986
epoch 4801, loss 0.0058, train acc 99.86%, f1 0.9987, precision 0.9988, recall 0.9985, auc 0.9986
epoch 4901, loss 0.0061, train acc 99.87%, f1 0.9987, precision 0.9988, recall 0.9986, auc 0.9987
epoch 5001, loss 0.0074, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9986, auc 0.9987
epoch 5101, loss 0.0024, train acc 99.86%, f1 0.9986, precision 0.9987, recall 0.9985, auc 0.9986
epoch 5201, loss 0.0033, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 5301, loss 0.0009, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9986, auc 0.9987
epoch 5401, loss 0.0040, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9986, auc 0.9987
epoch 5501, loss 0.0020, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9986, auc 0.9987
epoch 5601, loss 0.0035, train acc 99.87%, f1 0.9987, precision 0.9989, recall 0.9986, auc 0.9987
epoch 5701, loss 0.0034, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 5801, loss 0.0030, train acc 99.88%, f1 0.9988, precision 0.9989, recall 0.9987, auc 0.9988
epoch 5901, loss 0.0049, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 6001, loss 0.0045, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6101, loss 0.0005, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 6201, loss 0.0004, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6301, loss 0.0022, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9988, auc 0.9989
epoch 6401, loss 0.0038, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 6501, loss 0.0043, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9988, auc 0.9989
epoch 6601, loss 0.0039, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6701, loss 0.0048, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6801, loss 0.0049, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 6901, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 7001, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 7101, loss 0.0009, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 7201, loss 0.0046, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 7301, loss 0.0042, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 7401, loss 0.0031, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 7501, loss 0.0029, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 7601, loss 0.0004, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 7701, loss 0.0024, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 7801, loss 0.0014, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 7901, loss 0.0030, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_notMirror_8000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_8000/record_1/MLP_minus_notMirror_8000_5
./test_vehicle0/result_MLP_minus_notMirror_8000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9730769230769231

the Fscore is 0.9176470588235294

the precision is 0.8478260869565217

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_5
----------------------



epoch 1, loss 0.6935, train acc 50.19%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4184, train acc 83.49%, f1 0.8347, precision 0.8325, recall 0.8368, auc 0.8349
epoch 201, loss 0.2302, train acc 94.65%, f1 0.9462, precision 0.9464, recall 0.9461, auc 0.9465
epoch 301, loss 0.1325, train acc 97.93%, f1 0.9792, precision 0.9790, recall 0.9794, auc 0.9793
epoch 401, loss 0.0907, train acc 98.47%, f1 0.9847, precision 0.9843, recall 0.9851, auc 0.9847
epoch 501, loss 0.0478, train acc 98.73%, f1 0.9872, precision 0.9866, recall 0.9878, auc 0.9873
epoch 601, loss 0.0525, train acc 98.90%, f1 0.9890, precision 0.9887, recall 0.9892, auc 0.9890
epoch 701, loss 0.0349, train acc 99.03%, f1 0.9903, precision 0.9902, recall 0.9904, auc 0.9903
epoch 801, loss 0.0519, train acc 99.14%, f1 0.9914, precision 0.9912, recall 0.9915, auc 0.9914
epoch 901, loss 0.0387, train acc 99.22%, f1 0.9921, precision 0.9920, recall 0.9922, auc 0.9922
epoch 1001, loss 0.0389, train acc 99.28%, f1 0.9928, precision 0.9925, recall 0.9930, auc 0.9928
epoch 1101, loss 0.0217, train acc 99.35%, f1 0.9935, precision 0.9934, recall 0.9935, auc 0.9935
epoch 1201, loss 0.0241, train acc 99.40%, f1 0.9940, precision 0.9941, recall 0.9939, auc 0.9940
epoch 1301, loss 0.0143, train acc 99.43%, f1 0.9943, precision 0.9941, recall 0.9945, auc 0.9943
epoch 1401, loss 0.0053, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 1501, loss 0.0122, train acc 99.54%, f1 0.9954, precision 0.9954, recall 0.9953, auc 0.9954
epoch 1601, loss 0.0134, train acc 99.58%, f1 0.9958, precision 0.9958, recall 0.9957, auc 0.9958
epoch 1701, loss 0.0241, train acc 99.60%, f1 0.9960, precision 0.9960, recall 0.9960, auc 0.9960
epoch 1801, loss 0.0133, train acc 99.63%, f1 0.9963, precision 0.9964, recall 0.9962, auc 0.9963
epoch 1901, loss 0.0129, train acc 99.66%, f1 0.9966, precision 0.9967, recall 0.9965, auc 0.9966
epoch 2001, loss 0.0076, train acc 99.68%, f1 0.9967, precision 0.9970, recall 0.9965, auc 0.9968
epoch 2101, loss 0.0085, train acc 99.69%, f1 0.9969, precision 0.9969, recall 0.9968, auc 0.9969
epoch 2201, loss 0.0147, train acc 99.71%, f1 0.9971, precision 0.9973, recall 0.9970, auc 0.9971
epoch 2301, loss 0.0184, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 2401, loss 0.0097, train acc 99.73%, f1 0.9973, precision 0.9974, recall 0.9972, auc 0.9973
epoch 2501, loss 0.0145, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9974, auc 0.9975
epoch 2601, loss 0.0134, train acc 99.77%, f1 0.9977, precision 0.9977, recall 0.9977, auc 0.9977
epoch 2701, loss 0.0203, train acc 99.79%, f1 0.9979, precision 0.9979, recall 0.9978, auc 0.9979
epoch 2801, loss 0.0057, train acc 99.80%, f1 0.9980, precision 0.9981, recall 0.9979, auc 0.9980
epoch 2901, loss 0.0052, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9980, auc 0.9981
epoch 3001, loss 0.0068, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 3101, loss 0.0073, train acc 99.81%, f1 0.9981, precision 0.9982, recall 0.9981, auc 0.9981
epoch 3201, loss 0.0026, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9982, auc 0.9982
epoch 3301, loss 0.0112, train acc 99.82%, f1 0.9982, precision 0.9982, recall 0.9983, auc 0.9982
epoch 3401, loss 0.0084, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9983, auc 0.9983
epoch 3501, loss 0.0081, train acc 99.83%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9983
epoch 3601, loss 0.0072, train acc 99.84%, f1 0.9983, precision 0.9983, recall 0.9984, auc 0.9984
epoch 3701, loss 0.0053, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 3801, loss 0.0070, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9984, auc 0.9985
epoch 3901, loss 0.0056, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4001, loss 0.0062, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4101, loss 0.0099, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 4201, loss 0.0054, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 4301, loss 0.0033, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4401, loss 0.0054, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4501, loss 0.0059, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4601, loss 0.0020, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9986, auc 0.9986
epoch 4701, loss 0.0030, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4801, loss 0.0024, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 4901, loss 0.0031, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9988, auc 0.9988
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_notMirror_5000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_5000/record_1/MLP_minus_notMirror_5000_5
./test_vehicle0/result_MLP_minus_notMirror_5000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9730769230769231

the Fscore is 0.9176470588235294

the precision is 0.8478260869565217

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_5
----------------------



epoch 1, loss 0.6934, train acc 50.21%, f1 0.6685, precision 0.5021, recall 1.0000, auc 0.5000
epoch 101, loss 0.4122, train acc 83.33%, f1 0.8332, precision 0.8373, recall 0.8291, auc 0.8333
epoch 201, loss 0.2558, train acc 94.80%, f1 0.9481, precision 0.9493, recall 0.9470, auc 0.9480
epoch 301, loss 0.1166, train acc 97.92%, f1 0.9793, precision 0.9801, recall 0.9784, auc 0.9792
epoch 401, loss 0.0742, train acc 98.52%, f1 0.9852, precision 0.9852, recall 0.9852, auc 0.9852
epoch 501, loss 0.0792, train acc 98.73%, f1 0.9874, precision 0.9873, recall 0.9875, auc 0.9873
epoch 601, loss 0.0544, train acc 98.90%, f1 0.9890, precision 0.9891, recall 0.9889, auc 0.9890
epoch 701, loss 0.0472, train acc 99.04%, f1 0.9905, precision 0.9906, recall 0.9903, auc 0.9904
epoch 801, loss 0.0358, train acc 99.13%, f1 0.9913, precision 0.9911, recall 0.9915, auc 0.9913
epoch 901, loss 0.0451, train acc 99.22%, f1 0.9922, precision 0.9922, recall 0.9922, auc 0.9922
epoch 1001, loss 0.0276, train acc 99.29%, f1 0.9929, precision 0.9929, recall 0.9928, auc 0.9929
epoch 1101, loss 0.0272, train acc 99.36%, f1 0.9936, precision 0.9936, recall 0.9937, auc 0.9936
epoch 1201, loss 0.0112, train acc 99.41%, f1 0.9941, precision 0.9940, recall 0.9941, auc 0.9941
epoch 1301, loss 0.0228, train acc 99.49%, f1 0.9950, precision 0.9950, recall 0.9949, auc 0.9949
epoch 1401, loss 0.0254, train acc 99.52%, f1 0.9952, precision 0.9950, recall 0.9953, auc 0.9952
epoch 1501, loss 0.0162, train acc 99.56%, f1 0.9956, precision 0.9954, recall 0.9958, auc 0.9956
epoch 1601, loss 0.0166, train acc 99.58%, f1 0.9959, precision 0.9957, recall 0.9960, auc 0.9958
epoch 1701, loss 0.0110, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9962, auc 0.9961
epoch 1801, loss 0.0144, train acc 99.64%, f1 0.9964, precision 0.9961, recall 0.9967, auc 0.9964
epoch 1901, loss 0.0131, train acc 99.67%, f1 0.9967, precision 0.9965, recall 0.9969, auc 0.9967
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_minus_notMirror_2000
minus_pos_num_40_1
./test_vehicle0/model_MLP_minus_notMirror_2000/record_1/MLP_minus_notMirror_2000_5
./test_vehicle0/result_MLP_minus_notMirror_2000_minus_pos_num_40_1/record_1/
----------------------



the AUC is 0.9692307692307692

the Fscore is 0.9069767441860465

the precision is 0.8297872340425532

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_5
----------------------



epoch 1, loss 0.6968, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4696, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4124, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3701, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3339, train acc 76.66%, f1 0.0366, precision 0.7500, recall 0.0187, auc 0.5084
epoch 501, loss 0.3027, train acc 77.40%, f1 0.0947, precision 0.8889, recall 0.0500, auc 0.5240
epoch 601, loss 0.2756, train acc 78.88%, f1 0.2099, precision 0.9048, recall 0.1187, auc 0.5574
epoch 701, loss 0.2518, train acc 82.42%, f1 0.4195, precision 0.9556, recall 0.2687, auc 0.6324
epoch 801, loss 0.2307, train acc 87.44%, f1 0.6473, precision 0.9630, recall 0.4875, auc 0.7408
epoch 901, loss 0.2120, train acc 90.69%, f1 0.7640, precision 0.9533, recall 0.6375, auc 0.8139
epoch 1001, loss 0.1956, train acc 93.80%, f1 0.8542, precision 0.9609, recall 0.7688, auc 0.8795
epoch 1101, loss 0.1811, train acc 95.42%, f1 0.8963, precision 0.9640, recall 0.8375, auc 0.9139
epoch 1201, loss 0.1682, train acc 97.05%, f1 0.9355, precision 0.9667, recall 0.9062, auc 0.9483
epoch 1301, loss 0.1567, train acc 97.49%, f1 0.9457, precision 0.9673, recall 0.9250, auc 0.9577
epoch 1401, loss 0.1463, train acc 97.93%, f1 0.9554, precision 0.9740, recall 0.9375, auc 0.9649
epoch 1501, loss 0.1368, train acc 97.93%, f1 0.9554, precision 0.9740, recall 0.9375, auc 0.9649
epoch 1601, loss 0.1282, train acc 98.08%, f1 0.9587, precision 0.9742, recall 0.9437, auc 0.9680
epoch 1701, loss 0.1202, train acc 98.52%, f1 0.9686, precision 0.9747, recall 0.9625, auc 0.9774
epoch 1801, loss 0.1128, train acc 98.67%, f1 0.9718, precision 0.9748, recall 0.9688, auc 0.9805
epoch 1901, loss 0.1061, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 2001, loss 0.0999, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 2101, loss 0.0941, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 2201, loss 0.0888, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 2301, loss 0.0838, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2401, loss 0.0792, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2501, loss 0.0748, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2601, loss 0.0708, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2701, loss 0.0670, train acc 99.26%, f1 0.9844, precision 0.9814, recall 0.9875, auc 0.9908
epoch 2801, loss 0.0634, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 2901, loss 0.0601, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 3001, loss 0.0569, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 3101, loss 0.0539, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 3201, loss 0.0511, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 3301, loss 0.0485, train acc 99.56%, f1 0.9907, precision 0.9876, recall 0.9938, auc 0.9949
epoch 3401, loss 0.0460, train acc 99.56%, f1 0.9907, precision 0.9876, recall 0.9938, auc 0.9949
epoch 3501, loss 0.0437, train acc 99.56%, f1 0.9907, precision 0.9876, recall 0.9938, auc 0.9949
epoch 3601, loss 0.0414, train acc 99.56%, f1 0.9907, precision 0.9876, recall 0.9938, auc 0.9949
epoch 3701, loss 0.0393, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3801, loss 0.0373, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3901, loss 0.0354, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 4001, loss 0.0336, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0319, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0303, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0287, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0273, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0259, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0246, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0233, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0221, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0210, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0199, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0189, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0178, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0168, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0159, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0151, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0143, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0136, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0129, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0122, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0116, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0110, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0104, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0099, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0094, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0089, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0085, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0080, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0076, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0072, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0069, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0065, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0062, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0059, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0056, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0053, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0051, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0048, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0044, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8201, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0035, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0034, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0030, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_normal_20000
normal
./test_vehicle0/model_MLP_normal_20000/record_1/MLP_normal_20000_5
./test_vehicle0/result_MLP_normal_20000_normal/record_1/
----------------------



the AUC is 0.9846153846153847

the Fscore is 0.951219512195122

the precision is 0.9069767441860465

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_5
----------------------



epoch 1, loss 0.6967, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4706, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4133, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3709, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3346, train acc 76.66%, f1 0.0366, precision 0.7500, recall 0.0187, auc 0.5084
epoch 501, loss 0.3032, train acc 77.40%, f1 0.0947, precision 0.8889, recall 0.0500, auc 0.5240
epoch 601, loss 0.2760, train acc 78.88%, f1 0.2099, precision 0.9048, recall 0.1187, auc 0.5574
epoch 701, loss 0.2521, train acc 82.27%, f1 0.4118, precision 0.9545, recall 0.2625, auc 0.6293
epoch 801, loss 0.2309, train acc 87.44%, f1 0.6473, precision 0.9630, recall 0.4875, auc 0.7408
epoch 901, loss 0.2121, train acc 90.40%, f1 0.7547, precision 0.9524, recall 0.6250, auc 0.8077
epoch 1001, loss 0.1956, train acc 93.80%, f1 0.8542, precision 0.9609, recall 0.7688, auc 0.8795
epoch 1101, loss 0.1811, train acc 95.57%, f1 0.9000, precision 0.9643, recall 0.8438, auc 0.9170
epoch 1201, loss 0.1682, train acc 97.05%, f1 0.9355, precision 0.9667, recall 0.9062, auc 0.9483
epoch 1301, loss 0.1567, train acc 97.49%, f1 0.9457, precision 0.9673, recall 0.9250, auc 0.9577
epoch 1401, loss 0.1463, train acc 97.93%, f1 0.9554, precision 0.9740, recall 0.9375, auc 0.9649
epoch 1501, loss 0.1368, train acc 98.08%, f1 0.9587, precision 0.9742, recall 0.9437, auc 0.9680
epoch 1601, loss 0.1281, train acc 98.08%, f1 0.9587, precision 0.9742, recall 0.9437, auc 0.9680
epoch 1701, loss 0.1202, train acc 98.38%, f1 0.9653, precision 0.9745, recall 0.9563, auc 0.9743
epoch 1801, loss 0.1128, train acc 98.67%, f1 0.9718, precision 0.9748, recall 0.9688, auc 0.9805
epoch 1901, loss 0.1061, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 2001, loss 0.0998, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 2101, loss 0.0941, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 2201, loss 0.0887, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2301, loss 0.0837, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2401, loss 0.0790, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2501, loss 0.0747, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2601, loss 0.0706, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2701, loss 0.0667, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 2801, loss 0.0631, train acc 99.56%, f1 0.9906, precision 0.9937, recall 0.9875, auc 0.9928
epoch 2901, loss 0.0598, train acc 99.56%, f1 0.9906, precision 0.9937, recall 0.9875, auc 0.9928
epoch 3001, loss 0.0566, train acc 99.56%, f1 0.9906, precision 0.9937, recall 0.9875, auc 0.9928
epoch 3101, loss 0.0536, train acc 99.56%, f1 0.9906, precision 0.9937, recall 0.9875, auc 0.9928
epoch 3201, loss 0.0508, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3301, loss 0.0481, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3401, loss 0.0456, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3501, loss 0.0432, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3601, loss 0.0410, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3701, loss 0.0389, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3801, loss 0.0369, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3901, loss 0.0350, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 4001, loss 0.0332, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0315, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0298, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0283, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0268, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0254, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0241, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0229, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0217, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0205, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0195, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0185, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0175, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0166, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0157, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0149, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0142, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0134, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0127, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0121, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0115, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0109, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0103, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0097, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0092, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0088, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0083, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0079, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0075, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0071, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0068, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0064, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0061, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0058, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0055, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0052, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0050, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0047, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0045, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0043, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0035, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0033, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0030, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0027, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10001, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10101, loss 0.0015, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10201, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10301, loss 0.0014, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10401, loss 0.0013, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10501, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10601, loss 0.0012, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10701, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10801, loss 0.0011, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 10901, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11001, loss 0.0010, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11101, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11201, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11301, loss 0.0009, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11401, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11501, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11601, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11701, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 11901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12001, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12301, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_normal_15000
normal
./test_vehicle0/model_MLP_normal_15000/record_1/MLP_normal_15000_5
./test_vehicle0/result_MLP_normal_15000_normal/record_1/
----------------------



the AUC is 0.9846153846153847

the Fscore is 0.951219512195122

the precision is 0.9069767441860465

the recall is 1.0

Done
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_5
----------------------



epoch 1, loss 0.6975, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4690, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4121, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3699, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3338, train acc 76.66%, f1 0.0366, precision 0.7500, recall 0.0187, auc 0.5084
epoch 501, loss 0.3027, train acc 77.40%, f1 0.0947, precision 0.8889, recall 0.0500, auc 0.5240
epoch 601, loss 0.2758, train acc 78.88%, f1 0.2099, precision 0.9048, recall 0.1187, auc 0.5574
epoch 701, loss 0.2523, train acc 82.42%, f1 0.4195, precision 0.9556, recall 0.2687, auc 0.6324
epoch 801, loss 0.2314, train acc 87.15%, f1 0.6360, precision 0.9620, recall 0.4750, auc 0.7346
epoch 901, loss 0.2127, train acc 90.55%, f1 0.7594, precision 0.9528, recall 0.6312, auc 0.8108
epoch 1001, loss 0.1961, train acc 93.80%, f1 0.8542, precision 0.9609, recall 0.7688, auc 0.8795
epoch 1101, loss 0.1816, train acc 95.27%, f1 0.8926, precision 0.9638, recall 0.8313, auc 0.9108
epoch 1201, loss 0.1687, train acc 97.19%, f1 0.9389, precision 0.9669, recall 0.9125, auc 0.9514
epoch 1301, loss 0.1573, train acc 97.49%, f1 0.9457, precision 0.9673, recall 0.9250, auc 0.9577
epoch 1401, loss 0.1469, train acc 97.78%, f1 0.9524, precision 0.9677, recall 0.9375, auc 0.9639
epoch 1501, loss 0.1374, train acc 98.08%, f1 0.9587, precision 0.9742, recall 0.9437, auc 0.9680
epoch 1601, loss 0.1287, train acc 98.08%, f1 0.9587, precision 0.9742, recall 0.9437, auc 0.9680
epoch 1701, loss 0.1206, train acc 98.23%, f1 0.9620, precision 0.9744, recall 0.9500, auc 0.9711
epoch 1801, loss 0.1132, train acc 98.67%, f1 0.9718, precision 0.9748, recall 0.9688, auc 0.9805
epoch 1901, loss 0.1063, train acc 98.97%, f1 0.9781, precision 0.9811, recall 0.9750, auc 0.9846
epoch 2001, loss 0.0999, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2101, loss 0.0941, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2201, loss 0.0886, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2301, loss 0.0836, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2401, loss 0.0789, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2501, loss 0.0745, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2601, loss 0.0704, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2701, loss 0.0665, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2801, loss 0.0629, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 2901, loss 0.0595, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 3001, loss 0.0563, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3101, loss 0.0533, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3201, loss 0.0505, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3301, loss 0.0478, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3401, loss 0.0453, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3501, loss 0.0429, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3601, loss 0.0407, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3701, loss 0.0386, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3801, loss 0.0366, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 3901, loss 0.0347, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0329, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0311, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4201, loss 0.0295, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0280, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0265, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0252, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0239, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0226, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0215, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0204, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0193, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0183, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0174, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0165, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0156, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0148, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0141, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0133, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0127, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0120, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0114, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0108, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0102, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0097, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0092, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0088, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0083, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0079, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0075, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0071, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0068, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0064, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0061, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0058, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0055, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0052, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0050, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0047, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0045, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0043, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8001, loss 0.0041, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8101, loss 0.0039, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0037, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8301, loss 0.0035, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8401, loss 0.0033, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8501, loss 0.0032, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8601, loss 0.0030, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8701, loss 0.0029, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8801, loss 0.0028, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 8901, loss 0.0026, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9001, loss 0.0025, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9101, loss 0.0024, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9201, loss 0.0023, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9301, loss 0.0022, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9401, loss 0.0021, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9501, loss 0.0020, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9601, loss 0.0019, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9701, loss 0.0018, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9801, loss 0.0017, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 9901, loss 0.0016, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_normal_10000
normal
./test_vehicle0/model_MLP_normal_10000/record_1/MLP_normal_10000_5
./test_vehicle0/result_MLP_normal_10000_normal/record_1/
----------------------



the AUC is 0.9846153846153847

the Fscore is 0.951219512195122

the precision is 0.9069767441860465

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_5
----------------------



epoch 1, loss 0.6895, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.4684, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4118, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3695, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3332, train acc 76.66%, f1 0.0366, precision 0.7500, recall 0.0187, auc 0.5084
epoch 501, loss 0.3018, train acc 77.40%, f1 0.0947, precision 0.8889, recall 0.0500, auc 0.5240
epoch 601, loss 0.2745, train acc 78.88%, f1 0.2099, precision 0.9048, recall 0.1187, auc 0.5574
epoch 701, loss 0.2507, train acc 83.16%, f1 0.4571, precision 0.9600, recall 0.3000, auc 0.6481
epoch 801, loss 0.2296, train acc 87.44%, f1 0.6473, precision 0.9630, recall 0.4875, auc 0.7408
epoch 901, loss 0.2110, train acc 90.99%, f1 0.7732, precision 0.9541, recall 0.6500, auc 0.8202
epoch 1001, loss 0.1945, train acc 93.94%, f1 0.8581, precision 0.9612, recall 0.7750, auc 0.8827
epoch 1101, loss 0.1800, train acc 95.27%, f1 0.8933, precision 0.9571, recall 0.8375, auc 0.9129
epoch 1201, loss 0.1671, train acc 97.19%, f1 0.9389, precision 0.9669, recall 0.9125, auc 0.9514
epoch 1301, loss 0.1556, train acc 97.49%, f1 0.9457, precision 0.9673, recall 0.9250, auc 0.9577
epoch 1401, loss 0.1452, train acc 97.93%, f1 0.9554, precision 0.9740, recall 0.9375, auc 0.9649
epoch 1501, loss 0.1357, train acc 97.93%, f1 0.9554, precision 0.9740, recall 0.9375, auc 0.9649
epoch 1601, loss 0.1270, train acc 98.08%, f1 0.9587, precision 0.9742, recall 0.9437, auc 0.9680
epoch 1701, loss 0.1191, train acc 98.38%, f1 0.9653, precision 0.9745, recall 0.9563, auc 0.9743
epoch 1801, loss 0.1117, train acc 98.67%, f1 0.9718, precision 0.9748, recall 0.9688, auc 0.9805
epoch 1901, loss 0.1049, train acc 98.82%, f1 0.9750, precision 0.9750, recall 0.9750, auc 0.9836
epoch 2001, loss 0.0987, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 2101, loss 0.0929, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2201, loss 0.0875, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2301, loss 0.0826, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2401, loss 0.0779, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2501, loss 0.0736, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2601, loss 0.0695, train acc 99.26%, f1 0.9843, precision 0.9874, recall 0.9812, auc 0.9887
epoch 2701, loss 0.0657, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 2801, loss 0.0622, train acc 99.56%, f1 0.9906, precision 0.9937, recall 0.9875, auc 0.9928
epoch 2901, loss 0.0588, train acc 99.56%, f1 0.9906, precision 0.9937, recall 0.9875, auc 0.9928
epoch 3001, loss 0.0557, train acc 99.56%, f1 0.9906, precision 0.9937, recall 0.9875, auc 0.9928
epoch 3101, loss 0.0527, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3201, loss 0.0499, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3301, loss 0.0473, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3401, loss 0.0448, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3501, loss 0.0424, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3601, loss 0.0402, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3701, loss 0.0381, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3801, loss 0.0361, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3901, loss 0.0343, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4001, loss 0.0325, train acc 99.85%, f1 0.9969, precision 0.9938, recall 1.0000, auc 0.9990
epoch 4101, loss 0.0308, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4201, loss 0.0292, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0277, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0262, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0249, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0236, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0224, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0212, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0201, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5001, loss 0.0191, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5101, loss 0.0181, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5201, loss 0.0172, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5301, loss 0.0163, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5401, loss 0.0154, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5501, loss 0.0146, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5601, loss 0.0139, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5701, loss 0.0132, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5801, loss 0.0125, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 5901, loss 0.0118, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6001, loss 0.0112, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6101, loss 0.0107, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6201, loss 0.0101, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6301, loss 0.0096, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6401, loss 0.0091, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6501, loss 0.0086, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6601, loss 0.0082, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6701, loss 0.0078, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6801, loss 0.0074, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 6901, loss 0.0070, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7001, loss 0.0067, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7101, loss 0.0063, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7201, loss 0.0060, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7301, loss 0.0057, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7401, loss 0.0054, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7501, loss 0.0052, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7601, loss 0.0049, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7701, loss 0.0046, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7801, loss 0.0044, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 7901, loss 0.0042, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_normal_8000
normal
./test_vehicle0/model_MLP_normal_8000/record_1/MLP_normal_8000_5
./test_vehicle0/result_MLP_normal_8000_normal/record_1/
----------------------



the AUC is 0.9846153846153847

the Fscore is 0.951219512195122

the precision is 0.9069767441860465

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_5
----------------------



epoch 1, loss 0.7090, train acc 23.63%, f1 0.3823, precision 0.2363, recall 1.0000, auc 0.5000
epoch 101, loss 0.4724, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4140, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3711, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3292, train acc 76.96%, f1 0.0602, precision 0.8333, recall 0.0312, auc 0.5147
epoch 501, loss 0.2866, train acc 79.76%, f1 0.2674, precision 0.9259, recall 0.1562, auc 0.5762
epoch 601, loss 0.2495, train acc 85.67%, f1 0.5837, precision 0.9315, recall 0.4250, auc 0.7077
epoch 701, loss 0.2180, train acc 91.88%, f1 0.8014, precision 0.9487, recall 0.6937, auc 0.8411
epoch 801, loss 0.1915, train acc 94.83%, f1 0.8822, precision 0.9562, recall 0.8187, auc 0.9036
epoch 901, loss 0.1694, train acc 97.19%, f1 0.9393, precision 0.9608, recall 0.9187, auc 0.9536
epoch 1001, loss 0.1510, train acc 97.49%, f1 0.9464, precision 0.9554, recall 0.9375, auc 0.9620
epoch 1101, loss 0.1354, train acc 97.49%, f1 0.9464, precision 0.9554, recall 0.9375, auc 0.9620
epoch 1201, loss 0.1222, train acc 97.64%, f1 0.9497, precision 0.9557, recall 0.9437, auc 0.9651
epoch 1301, loss 0.1108, train acc 98.08%, f1 0.9592, precision 0.9623, recall 0.9563, auc 0.9723
epoch 1401, loss 0.1010, train acc 98.82%, f1 0.9752, precision 0.9691, recall 0.9812, auc 0.9858
epoch 1501, loss 0.0925, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 1601, loss 0.0850, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 1701, loss 0.0784, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
epoch 1801, loss 0.0726, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 1901, loss 0.0673, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2001, loss 0.0626, train acc 99.11%, f1 0.9812, precision 0.9812, recall 0.9812, auc 0.9877
epoch 2101, loss 0.0583, train acc 99.26%, f1 0.9844, precision 0.9814, recall 0.9875, auc 0.9908
epoch 2201, loss 0.0544, train acc 99.26%, f1 0.9844, precision 0.9814, recall 0.9875, auc 0.9908
epoch 2301, loss 0.0509, train acc 99.26%, f1 0.9844, precision 0.9814, recall 0.9875, auc 0.9908
epoch 2401, loss 0.0476, train acc 99.41%, f1 0.9875, precision 0.9875, recall 0.9875, auc 0.9918
epoch 2501, loss 0.0446, train acc 99.56%, f1 0.9906, precision 0.9937, recall 0.9875, auc 0.9928
epoch 2601, loss 0.0418, train acc 99.56%, f1 0.9906, precision 0.9937, recall 0.9875, auc 0.9928
epoch 2701, loss 0.0392, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 2801, loss 0.0368, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 2901, loss 0.0345, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3001, loss 0.0324, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3101, loss 0.0304, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3201, loss 0.0286, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3301, loss 0.0269, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3401, loss 0.0252, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3501, loss 0.0237, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3601, loss 0.0222, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3701, loss 0.0209, train acc 99.70%, f1 0.9938, precision 0.9938, recall 0.9938, auc 0.9959
epoch 3801, loss 0.0196, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 3901, loss 0.0184, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4001, loss 0.0173, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4101, loss 0.0162, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4201, loss 0.0152, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4301, loss 0.0143, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4401, loss 0.0134, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4501, loss 0.0126, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4601, loss 0.0118, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4701, loss 0.0111, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4801, loss 0.0104, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 4901, loss 0.0098, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_normal_5000
normal
./test_vehicle0/model_MLP_normal_5000/record_1/MLP_normal_5000_5
./test_vehicle0/result_MLP_normal_5000_normal/record_1/
----------------------



the AUC is 0.967948717948718

the Fscore is 0.9268292682926831

the precision is 0.8837209302325582

the recall is 0.9743589743589743

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_5
----------------------



epoch 1, loss 0.7050, train acc 23.63%, f1 0.3823, precision 0.2363, recall 1.0000, auc 0.5000
epoch 101, loss 0.4701, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 201, loss 0.4125, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 301, loss 0.3702, train acc 76.37%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 401, loss 0.3342, train acc 76.66%, f1 0.0247, precision 1.0000, recall 0.0125, auc 0.5062
epoch 501, loss 0.3032, train acc 76.96%, f1 0.0602, precision 0.8333, recall 0.0312, auc 0.5147
epoch 601, loss 0.2764, train acc 79.03%, f1 0.2111, precision 0.9500, recall 0.1187, auc 0.5584
epoch 701, loss 0.2528, train acc 82.13%, f1 0.4039, precision 0.9535, recall 0.2562, auc 0.6262
epoch 801, loss 0.2318, train acc 86.85%, f1 0.6245, precision 0.9610, recall 0.4625, auc 0.7283
epoch 901, loss 0.2132, train acc 90.40%, f1 0.7547, precision 0.9524, recall 0.6250, auc 0.8077
epoch 1001, loss 0.1968, train acc 93.50%, f1 0.8462, precision 0.9603, recall 0.7562, auc 0.8733
epoch 1101, loss 0.1822, train acc 95.42%, f1 0.8963, precision 0.9640, recall 0.8375, auc 0.9139
epoch 1201, loss 0.1693, train acc 97.05%, f1 0.9355, precision 0.9667, recall 0.9062, auc 0.9483
epoch 1301, loss 0.1577, train acc 97.34%, f1 0.9423, precision 0.9671, recall 0.9187, auc 0.9545
epoch 1401, loss 0.1472, train acc 97.93%, f1 0.9554, precision 0.9740, recall 0.9375, auc 0.9649
epoch 1501, loss 0.1377, train acc 97.93%, f1 0.9554, precision 0.9740, recall 0.9375, auc 0.9649
epoch 1601, loss 0.1290, train acc 98.08%, f1 0.9587, precision 0.9742, recall 0.9437, auc 0.9680
epoch 1701, loss 0.1210, train acc 98.52%, f1 0.9686, precision 0.9747, recall 0.9625, auc 0.9774
epoch 1801, loss 0.1137, train acc 98.67%, f1 0.9718, precision 0.9748, recall 0.9688, auc 0.9805
epoch 1901, loss 0.1069, train acc 98.97%, f1 0.9782, precision 0.9752, recall 0.9812, auc 0.9868
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_vehicle0/standlization_data/vehicle0_std_train_5.csv
./test_vehicle0/standlization_data/vehicle0_std_test_5.csv
MLP_normal_2000
normal
./test_vehicle0/model_MLP_normal_2000/record_1/MLP_normal_2000_5
./test_vehicle0/result_MLP_normal_2000_normal/record_1/
----------------------



the AUC is 0.9717948717948719

the Fscore is 0.9382716049382716

the precision is 0.9047619047619048

the recall is 0.9743589743589743

Done
./test_yeast3/standlization_data/yeast3_std_train_5.csv
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_5
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.3806, train acc 97.90%, f1 0.9790, precision 0.9783, recall 0.9797, auc 0.9790
epoch 201, loss 0.1463, train acc 98.50%, f1 0.9850, precision 0.9850, recall 0.9850, auc 0.9850
epoch 301, loss 0.0829, train acc 98.72%, f1 0.9872, precision 0.9872, recall 0.9872, auc 0.9872
epoch 401, loss 0.0588, train acc 98.80%, f1 0.9880, precision 0.9880, recall 0.9880, auc 0.9880
epoch 501, loss 0.0476, train acc 98.83%, f1 0.9883, precision 0.9884, recall 0.9883, auc 0.9883
epoch 601, loss 0.0409, train acc 98.87%, f1 0.9887, precision 0.9887, recall 0.9887, auc 0.9887
epoch 701, loss 0.0373, train acc 98.89%, f1 0.9889, precision 0.9890, recall 0.9888, auc 0.9889
epoch 801, loss 0.0340, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9889, auc 0.9890
epoch 901, loss 0.0302, train acc 98.91%, f1 0.9891, precision 0.9891, recall 0.9890, auc 0.9891
epoch 1001, loss 0.0307, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9889, auc 0.9890
epoch 1101, loss 0.0291, train acc 98.89%, f1 0.9889, precision 0.9890, recall 0.9889, auc 0.9889
epoch 1201, loss 0.0298, train acc 98.89%, f1 0.9889, precision 0.9889, recall 0.9889, auc 0.9889
epoch 1301, loss 0.0282, train acc 98.89%, f1 0.9889, precision 0.9889, recall 0.9889, auc 0.9889
epoch 1401, loss 0.0287, train acc 98.89%, f1 0.9889, precision 0.9890, recall 0.9889, auc 0.9889
epoch 1501, loss 0.0280, train acc 98.89%, f1 0.9889, precision 0.9889, recall 0.9889, auc 0.9889
epoch 1601, loss 0.0281, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9889, auc 0.9890
epoch 1701, loss 0.0255, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9890, auc 0.9890
epoch 1801, loss 0.0275, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9890, auc 0.9890
epoch 1901, loss 0.0263, train acc 98.90%, f1 0.9890, precision 0.9890, recall 0.9890, auc 0.9890
epoch 2001, loss 0.0270, train acc 98.91%, f1 0.9891, precision 0.9891, recall 0.9890, auc 0.9891
epoch 2101, loss 0.0255, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9892, auc 0.9892
epoch 2201, loss 0.0262, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9892, auc 0.9892
epoch 2301, loss 0.0262, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9892, auc 0.9892
epoch 2401, loss 0.0247, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9892, auc 0.9892
epoch 2501, loss 0.0257, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9892, auc 0.9892
epoch 2601, loss 0.0258, train acc 98.91%, f1 0.9891, precision 0.9891, recall 0.9891, auc 0.9891
epoch 2701, loss 0.0241, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9891, auc 0.9892
epoch 2801, loss 0.0246, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9892, auc 0.9892
epoch 2901, loss 0.0237, train acc 98.92%, f1 0.9892, precision 0.9892, recall 0.9892, auc 0.9892
epoch 3001, loss 0.0246, train acc 98.93%, f1 0.9893, precision 0.9893, recall 0.9892, auc 0.9893
epoch 3101, loss 0.0246, train acc 98.94%, f1 0.9894, precision 0.9894, recall 0.9893, auc 0.9894
epoch 3201, loss 0.0238, train acc 98.95%, f1 0.9895, precision 0.9894, recall 0.9895, auc 0.9895
epoch 3301, loss 0.0241, train acc 98.96%, f1 0.9896, precision 0.9896, recall 0.9896, auc 0.9896
epoch 3401, loss 0.0233, train acc 98.96%, f1 0.9896, precision 0.9896, recall 0.9896, auc 0.9896
epoch 3501, loss 0.0223, train acc 98.97%, f1 0.9897, precision 0.9897, recall 0.9897, auc 0.9897
epoch 3601, loss 0.0228, train acc 98.98%, f1 0.9898, precision 0.9899, recall 0.9897, auc 0.9898
epoch 3701, loss 0.0225, train acc 99.00%, f1 0.9900, precision 0.9901, recall 0.9899, auc 0.9900
epoch 3801, loss 0.0224, train acc 99.02%, f1 0.9902, precision 0.9903, recall 0.9902, auc 0.9902
epoch 3901, loss 0.0221, train acc 99.03%, f1 0.9903, precision 0.9904, recall 0.9902, auc 0.9903
epoch 4001, loss 0.0207, train acc 99.04%, f1 0.9904, precision 0.9905, recall 0.9904, auc 0.9904
epoch 4101, loss 0.0211, train acc 99.06%, f1 0.9906, precision 0.9907, recall 0.9906, auc 0.9906
epoch 4201, loss 0.0209, train acc 99.08%, f1 0.9908, precision 0.9908, recall 0.9908, auc 0.9908
epoch 4301, loss 0.0199, train acc 99.10%, f1 0.9910, precision 0.9911, recall 0.9910, auc 0.9910
epoch 4401, loss 0.0193, train acc 99.13%, f1 0.9913, precision 0.9913, recall 0.9913, auc 0.9913
epoch 4501, loss 0.0195, train acc 99.14%, f1 0.9914, precision 0.9914, recall 0.9914, auc 0.9914
epoch 4601, loss 0.0192, train acc 99.16%, f1 0.9916, precision 0.9915, recall 0.9916, auc 0.9916
epoch 4701, loss 0.0187, train acc 99.18%, f1 0.9918, precision 0.9917, recall 0.9918, auc 0.9918
epoch 4801, loss 0.0182, train acc 99.19%, f1 0.9919, precision 0.9919, recall 0.9919, auc 0.9919
epoch 4901, loss 0.0175, train acc 99.21%, f1 0.9921, precision 0.9921, recall 0.9922, auc 0.9921
epoch 5001, loss 0.0173, train acc 99.25%, f1 0.9925, precision 0.9924, recall 0.9925, auc 0.9925
epoch 5101, loss 0.0151, train acc 99.27%, f1 0.9927, precision 0.9927, recall 0.9928, auc 0.9927
epoch 5201, loss 0.0154, train acc 99.31%, f1 0.9931, precision 0.9931, recall 0.9931, auc 0.9931
epoch 5301, loss 0.0155, train acc 99.34%, f1 0.9934, precision 0.9934, recall 0.9935, auc 0.9934
epoch 5401, loss 0.0134, train acc 99.36%, f1 0.9936, precision 0.9936, recall 0.9937, auc 0.9936
epoch 5501, loss 0.0146, train acc 99.39%, f1 0.9939, precision 0.9939, recall 0.9940, auc 0.9939
epoch 5601, loss 0.0136, train acc 99.42%, f1 0.9942, precision 0.9941, recall 0.9942, auc 0.9942
epoch 5701, loss 0.0138, train acc 99.44%, f1 0.9944, precision 0.9944, recall 0.9944, auc 0.9944
epoch 5801, loss 0.0127, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9947, auc 0.9947
epoch 5901, loss 0.0128, train acc 99.50%, f1 0.9950, precision 0.9950, recall 0.9949, auc 0.9950
epoch 6001, loss 0.0118, train acc 99.53%, f1 0.9953, precision 0.9952, recall 0.9955, auc 0.9953
epoch 6101, loss 0.0118, train acc 99.56%, f1 0.9956, precision 0.9956, recall 0.9956, auc 0.9956
epoch 6201, loss 0.0112, train acc 99.58%, f1 0.9958, precision 0.9957, recall 0.9958, auc 0.9958
epoch 6301, loss 0.0101, train acc 99.59%, f1 0.9959, precision 0.9959, recall 0.9959, auc 0.9959
epoch 6401, loss 0.0100, train acc 99.61%, f1 0.9961, precision 0.9960, recall 0.9961, auc 0.9961
epoch 6501, loss 0.0096, train acc 99.62%, f1 0.9962, precision 0.9962, recall 0.9963, auc 0.9962
epoch 6601, loss 0.0096, train acc 99.64%, f1 0.9964, precision 0.9964, recall 0.9964, auc 0.9964
epoch 6701, loss 0.0092, train acc 99.66%, f1 0.9966, precision 0.9966, recall 0.9967, auc 0.9966
epoch 6801, loss 0.0087, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9968, auc 0.9968
epoch 6901, loss 0.0079, train acc 99.70%, f1 0.9970, precision 0.9971, recall 0.9970, auc 0.9970
epoch 7001, loss 0.0077, train acc 99.72%, f1 0.9972, precision 0.9973, recall 0.9972, auc 0.9972
epoch 7101, loss 0.0075, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9974, auc 0.9974
epoch 7201, loss 0.0068, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 7301, loss 0.0072, train acc 99.77%, f1 0.9977, precision 0.9976, recall 0.9977, auc 0.9977
epoch 7401, loss 0.0058, train acc 99.79%, f1 0.9979, precision 0.9978, recall 0.9980, auc 0.9979
epoch 7501, loss 0.0061, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9982, auc 0.9982
epoch 7601, loss 0.0063, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 7701, loss 0.0061, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 7801, loss 0.0058, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 7901, loss 0.0054, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 8001, loss 0.0054, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 8101, loss 0.0051, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 8201, loss 0.0051, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 8301, loss 0.0049, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 8401, loss 0.0047, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 8501, loss 0.0039, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9987, auc 0.9987
epoch 8601, loss 0.0044, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8701, loss 0.0043, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9988, auc 0.9988
epoch 8801, loss 0.0035, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 8901, loss 0.0040, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9001, loss 0.0036, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9101, loss 0.0035, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9201, loss 0.0036, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9301, loss 0.0035, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9401, loss 0.0025, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9501, loss 0.0024, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9601, loss 0.0031, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9989, auc 0.9989
epoch 9701, loss 0.0031, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 9801, loss 0.0030, train acc 99.89%, f1 0.9989, precision 0.9990, recall 0.9989, auc 0.9989
epoch 9901, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 10001, loss 0.0027, train acc 99.90%, f1 0.9990, precision 0.9991, recall 0.9990, auc 0.9990
epoch 10101, loss 0.0026, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9990, auc 0.9991
epoch 10201, loss 0.0025, train acc 99.91%, f1 0.9991, precision 0.9992, recall 0.9991, auc 0.9991
epoch 10301, loss 0.0024, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 10401, loss 0.0023, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9992, auc 0.9991
epoch 10501, loss 0.0023, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9992, auc 0.9992
epoch 10601, loss 0.0021, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 10701, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9993, auc 0.9993
epoch 10801, loss 0.0020, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 10901, loss 0.0020, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 11001, loss 0.0019, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 11101, loss 0.0018, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 11201, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 11301, loss 0.0017, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 11401, loss 0.0016, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 11501, loss 0.0016, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 11601, loss 0.0015, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 11701, loss 0.0014, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 11801, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 11901, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12001, loss 0.0013, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12101, loss 0.0012, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 12201, loss 0.0011, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 12301, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 12401, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12501, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12601, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 12701, loss 0.0008, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12801, loss 0.0008, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 12901, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13101, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13201, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13601, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13901, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14301, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15101, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15201, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0000, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_yeast3/standlization_data/yeast3_std_train_5.csv
./test_yeast3/standlization_data/yeast3_std_test_5.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_yeast3/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_5
./test_yeast3/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.8228803716608594

the Fscore is 0.5714285714285715

the precision is 0.5

the recall is 0.6666666666666666

Done
train_mlp_7_2.sh: line 433: 21725 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=yeast3 dataset_index=5 record_index=1 device_id=7 train_method=MLP_concat_Mirror_15000
