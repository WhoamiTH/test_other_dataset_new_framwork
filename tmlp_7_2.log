nohup: ignoring input
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_5
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
Validation loss decreased (inf --> 0.692812).  Saving model ...
Validation loss decreased (0.692812 --> 0.692660).  Saving model ...
Validation loss decreased (0.692660 --> 0.692526).  Saving model ...
Validation loss decreased (0.692526 --> 0.692388).  Saving model ...
Validation loss decreased (0.692388 --> 0.692204).  Saving model ...
Validation loss decreased (0.692204 --> 0.691996).  Saving model ...
Validation loss decreased (0.691996 --> 0.691755).  Saving model ...
Validation loss decreased (0.691755 --> 0.691499).  Saving model ...
Validation loss decreased (0.691499 --> 0.691213).  Saving model ...
Validation loss decreased (0.691213 --> 0.690921).  Saving model ...
Validation loss decreased (0.690921 --> 0.690598).  Saving model ...
Validation loss decreased (0.690598 --> 0.690241).  Saving model ...
Validation loss decreased (0.690241 --> 0.689852).  Saving model ...
Validation loss decreased (0.689852 --> 0.689419).  Saving model ...
Validation loss decreased (0.689419 --> 0.688950).  Saving model ...
Validation loss decreased (0.688950 --> 0.688433).  Saving model ...
Validation loss decreased (0.688433 --> 0.687894).  Saving model ...
Validation loss decreased (0.687894 --> 0.687292).  Saving model ...
Validation loss decreased (0.687292 --> 0.686659).  Saving model ...
Validation loss decreased (0.686659 --> 0.686011).  Saving model ...
Validation loss decreased (0.686011 --> 0.685350).  Saving model ...
Validation loss decreased (0.685350 --> 0.684629).  Saving model ...
Validation loss decreased (0.684629 --> 0.683874).  Saving model ...
Validation loss decreased (0.683874 --> 0.683117).  Saving model ...
Validation loss decreased (0.683117 --> 0.682379).  Saving model ...
Validation loss decreased (0.682379 --> 0.681570).  Saving model ...
Validation loss decreased (0.681570 --> 0.680719).  Saving model ...
Validation loss decreased (0.680719 --> 0.679802).  Saving model ...
Validation loss decreased (0.679802 --> 0.678781).  Saving model ...
Validation loss decreased (0.678781 --> 0.677725).  Saving model ...
Validation loss decreased (0.677725 --> 0.676628).  Saving model ...
Validation loss decreased (0.676628 --> 0.675419).  Saving model ...
Validation loss decreased (0.675419 --> 0.674207).  Saving model ...
Validation loss decreased (0.674207 --> 0.672911).  Saving model ...
Validation loss decreased (0.672911 --> 0.671589).  Saving model ...
Validation loss decreased (0.671589 --> 0.670229).  Saving model ...
Validation loss decreased (0.670229 --> 0.668804).  Saving model ...
Validation loss decreased (0.668804 --> 0.667298).  Saving model ...
Validation loss decreased (0.667298 --> 0.665722).  Saving model ...
Validation loss decreased (0.665722 --> 0.664114).  Saving model ...
Validation loss decreased (0.664114 --> 0.662501).  Saving model ...
Validation loss decreased (0.662501 --> 0.660782).  Saving model ...
Validation loss decreased (0.660782 --> 0.659053).  Saving model ...
Validation loss decreased (0.659053 --> 0.657225).  Saving model ...
Validation loss decreased (0.657225 --> 0.655257).  Saving model ...
Validation loss decreased (0.655257 --> 0.653164).  Saving model ...
Validation loss decreased (0.653164 --> 0.651065).  Saving model ...
Validation loss decreased (0.651065 --> 0.648954).  Saving model ...
Validation loss decreased (0.648954 --> 0.646800).  Saving model ...
Validation loss decreased (0.646800 --> 0.644579).  Saving model ...
Validation loss decreased (0.644579 --> 0.642250).  Saving model ...
Validation loss decreased (0.642250 --> 0.639870).  Saving model ...
Validation loss decreased (0.639870 --> 0.637505).  Saving model ...
Validation loss decreased (0.637505 --> 0.635089).  Saving model ...
Validation loss decreased (0.635089 --> 0.632584).  Saving model ...
Validation loss decreased (0.632584 --> 0.630083).  Saving model ...
Validation loss decreased (0.630083 --> 0.627523).  Saving model ...
Validation loss decreased (0.627523 --> 0.624938).  Saving model ...
Validation loss decreased (0.624938 --> 0.622303).  Saving model ...
Validation loss decreased (0.622303 --> 0.619595).  Saving model ...
Validation loss decreased (0.619595 --> 0.616867).  Saving model ...
Validation loss decreased (0.616867 --> 0.614108).  Saving model ...
Validation loss decreased (0.614108 --> 0.611205).  Saving model ...
Validation loss decreased (0.611205 --> 0.608304).  Saving model ...
Validation loss decreased (0.608304 --> 0.605325).  Saving model ...
Validation loss decreased (0.605325 --> 0.602312).  Saving model ...
Validation loss decreased (0.602312 --> 0.599420).  Saving model ...
Validation loss decreased (0.599420 --> 0.596561).  Saving model ...
Validation loss decreased (0.596561 --> 0.593682).  Saving model ...
Validation loss decreased (0.593682 --> 0.590759).  Saving model ...
Validation loss decreased (0.590759 --> 0.587883).  Saving model ...
Validation loss decreased (0.587883 --> 0.584953).  Saving model ...
Validation loss decreased (0.584953 --> 0.582099).  Saving model ...
Validation loss decreased (0.582099 --> 0.579290).  Saving model ...
Validation loss decreased (0.579290 --> 0.576477).  Saving model ...
Validation loss decreased (0.576477 --> 0.573574).  Saving model ...
Validation loss decreased (0.573574 --> 0.570755).  Saving model ...
Validation loss decreased (0.570755 --> 0.567865).  Saving model ...
Validation loss decreased (0.567865 --> 0.564950).  Saving model ...
Validation loss decreased (0.564950 --> 0.562103).  Saving model ...
Validation loss decreased (0.562103 --> 0.559177).  Saving model ...
Validation loss decreased (0.559177 --> 0.556147).  Saving model ...
Validation loss decreased (0.556147 --> 0.553248).  Saving model ...
Validation loss decreased (0.553248 --> 0.550385).  Saving model ...
Validation loss decreased (0.550385 --> 0.547535).  Saving model ...
Validation loss decreased (0.547535 --> 0.544687).  Saving model ...
Validation loss decreased (0.544687 --> 0.541817).  Saving model ...
Validation loss decreased (0.541817 --> 0.539218).  Saving model ...
Validation loss decreased (0.539218 --> 0.536723).  Saving model ...
Validation loss decreased (0.536723 --> 0.534260).  Saving model ...
Validation loss decreased (0.534260 --> 0.531847).  Saving model ...
Validation loss decreased (0.531847 --> 0.529477).  Saving model ...
Validation loss decreased (0.529477 --> 0.527071).  Saving model ...
Validation loss decreased (0.527071 --> 0.524569).  Saving model ...
Validation loss decreased (0.524569 --> 0.522261).  Saving model ...
Validation loss decreased (0.522261 --> 0.519985).  Saving model ...
Validation loss decreased (0.519985 --> 0.517821).  Saving model ...
Validation loss decreased (0.517821 --> 0.515694).  Saving model ...
Validation loss decreased (0.515694 --> 0.513622).  Saving model ...
Validation loss decreased (0.513622 --> 0.511629).  Saving model ...
epoch 101, loss 0.5313, train acc 84.50%, f1 0.8442, precision 0.8485, recall 0.8400, auc 0.8450
Validation loss decreased (0.511629 --> 0.509576).  Saving model ...
Validation loss decreased (0.509576 --> 0.507634).  Saving model ...
Validation loss decreased (0.507634 --> 0.505591).  Saving model ...
Validation loss decreased (0.505591 --> 0.503720).  Saving model ...
Validation loss decreased (0.503720 --> 0.501878).  Saving model ...
Validation loss decreased (0.501878 --> 0.500069).  Saving model ...
Validation loss decreased (0.500069 --> 0.498062).  Saving model ...
Validation loss decreased (0.498062 --> 0.496011).  Saving model ...
Validation loss decreased (0.496011 --> 0.493946).  Saving model ...
Validation loss decreased (0.493946 --> 0.491861).  Saving model ...
Validation loss decreased (0.491861 --> 0.489827).  Saving model ...
Validation loss decreased (0.489827 --> 0.487815).  Saving model ...
Validation loss decreased (0.487815 --> 0.485956).  Saving model ...
Validation loss decreased (0.485956 --> 0.484212).  Saving model .../home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Validation loss decreased (0.484212 --> 0.482523).  Saving model ...
Validation loss decreased (0.482523 --> 0.480960).  Saving model ...
Validation loss decreased (0.480960 --> 0.479416).  Saving model ...
Validation loss decreased (0.479416 --> 0.477807).  Saving model ...
Validation loss decreased (0.477807 --> 0.476279).  Saving model ...
Validation loss decreased (0.476279 --> 0.474953).  Saving model ...
Validation loss decreased (0.474953 --> 0.473700).  Saving model ...
Validation loss decreased (0.473700 --> 0.472544).  Saving model ...
Validation loss decreased (0.472544 --> 0.471355).  Saving model ...
Validation loss decreased (0.471355 --> 0.470255).  Saving model ...
Validation loss decreased (0.470255 --> 0.469150).  Saving model ...
Validation loss decreased (0.469150 --> 0.468061).  Saving model ...
Validation loss decreased (0.468061 --> 0.467086).  Saving model ...
Validation loss decreased (0.467086 --> 0.466149).  Saving model ...
Validation loss decreased (0.466149 --> 0.465179).  Saving model ...
Validation loss decreased (0.465179 --> 0.464253).  Saving model ...
Validation loss decreased (0.464253 --> 0.463335).  Saving model ...
Validation loss decreased (0.463335 --> 0.462413).  Saving model ...
Validation loss decreased (0.462413 --> 0.461346).  Saving model ...
Validation loss decreased (0.461346 --> 0.460511).  Saving model ...
Validation loss decreased (0.460511 --> 0.459615).  Saving model ...
Validation loss decreased (0.459615 --> 0.458641).  Saving model ...
Validation loss decreased (0.458641 --> 0.457576).  Saving model ...
Validation loss decreased (0.457576 --> 0.456373).  Saving model ...
Validation loss decreased (0.456373 --> 0.455227).  Saving model ...
Validation loss decreased (0.455227 --> 0.454271).  Saving model ...
Validation loss decreased (0.454271 --> 0.453123).  Saving model ...
Validation loss decreased (0.453123 --> 0.452224).  Saving model ...
Validation loss decreased (0.452224 --> 0.451388).  Saving model ...
Validation loss decreased (0.451388 --> 0.450650).  Saving model ...
Validation loss decreased (0.450650 --> 0.449856).  Saving model ...
Validation loss decreased (0.449856 --> 0.448882).  Saving model ...
Validation loss decreased (0.448882 --> 0.448041).  Saving model ...
Validation loss decreased (0.448041 --> 0.447301).  Saving model ...
Validation loss decreased (0.447301 --> 0.446552).  Saving model ...
Validation loss decreased (0.446552 --> 0.445785).  Saving model ...
Validation loss decreased (0.445785 --> 0.445054).  Saving model ...
Validation loss decreased (0.445054 --> 0.444178).  Saving model ...
Validation loss decreased (0.444178 --> 0.443263).  Saving model ...
Validation loss decreased (0.443263 --> 0.442343).  Saving model ...
Validation loss decreased (0.442343 --> 0.441580).  Saving model ...
Validation loss decreased (0.441580 --> 0.441047).  Saving model ...
Validation loss decreased (0.441047 --> 0.440486).  Saving model ...
Validation loss decreased (0.440486 --> 0.439768).  Saving model ...
Validation loss decreased (0.439768 --> 0.439180).  Saving model ...
Validation loss decreased (0.439180 --> 0.438420).  Saving model ...
Validation loss decreased (0.438420 --> 0.437687).  Saving model ...
Validation loss decreased (0.437687 --> 0.436810).  Saving model ...
Validation loss decreased (0.436810 --> 0.436009).  Saving model ...
Validation loss decreased (0.436009 --> 0.435212).  Saving model ...
Validation loss decreased (0.435212 --> 0.434431).  Saving model ...
Validation loss decreased (0.434431 --> 0.433699).  Saving model ...
Validation loss decreased (0.433699 --> 0.433158).  Saving model ...
Validation loss decreased (0.433158 --> 0.432518).  Saving model ...
Validation loss decreased (0.432518 --> 0.432030).  Saving model ...
Validation loss decreased (0.432030 --> 0.431631).  Saving model ...
Validation loss decreased (0.431631 --> 0.431306).  Saving model ...
Validation loss decreased (0.431306 --> 0.430847).  Saving model ...
Validation loss decreased (0.430847 --> 0.430355).  Saving model ...
Validation loss decreased (0.430355 --> 0.429864).  Saving model ...
Validation loss decreased (0.429864 --> 0.429461).  Saving model ...
Validation loss decreased (0.429461 --> 0.429036).  Saving model ...
Validation loss decreased (0.429036 --> 0.428680).  Saving model ...
Validation loss decreased (0.428680 --> 0.428105).  Saving model ...
Validation loss decreased (0.428105 --> 0.427572).  Saving model ...
Validation loss decreased (0.427572 --> 0.427109).  Saving model ...
Validation loss decreased (0.427109 --> 0.426590).  Saving model ...
Validation loss decreased (0.426590 --> 0.426096).  Saving model ...
Validation loss decreased (0.426096 --> 0.425786).  Saving model ...
Validation loss decreased (0.425786 --> 0.425618).  Saving model ...
Validation loss decreased (0.425618 --> 0.425525).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
Validation loss decreased (0.425525 --> 0.425323).  Saving model ...
Validation loss decreased (0.425323 --> 0.425119).  Saving model ...
Validation loss decreased (0.425119 --> 0.424837).  Saving model ...
Validation loss decreased (0.424837 --> 0.424743).  Saving model ...
Validation loss decreased (0.424743 --> 0.424370).  Saving model ...
epoch 201, loss 0.4940, train acc 78.75%, f1 0.7870, precision 0.7889, recall 0.7850, auc 0.7875
Validation loss decreased (0.424370 --> 0.424312).  Saving model ...
Validation loss decreased (0.424312 --> 0.424098).  Saving model ...
Validation loss decreased (0.424098 --> 0.423950).  Saving model ...
Validation loss decreased (0.423950 --> 0.423791).  Saving model ...
Validation loss decreased (0.423791 --> 0.423599).  Saving model ...
Validation loss decreased (0.423599 --> 0.423469).  Saving model ...
Validation loss decreased (0.423469 --> 0.423386).  Saving model ...
Validation loss decreased (0.423386 --> 0.423311).  Saving model ...
EarlyStopping counter: 1 out of 20
EarlyStopping counter: 2 out of 20
EarlyStopping counter: 3 out of 20
EarlyStopping counter: 4 out of 20
EarlyStopping counter: 5 out of 20
EarlyStopping counter: 6 out of 20
EarlyStopping counter: 7 out of 20
EarlyStopping counter: 8 out of 20
EarlyStopping counter: 9 out of 20
EarlyStopping counter: 10 out of 20
EarlyStopping counter: 11 out of 20
EarlyStopping counter: 12 out of 20
EarlyStopping counter: 13 out of 20
EarlyStopping counter: 14 out of 20
EarlyStopping counter: 15 out of 20
EarlyStopping counter: 16 out of 20
EarlyStopping counter: 17 out of 20
EarlyStopping counter: 18 out of 20
EarlyStopping counter: 19 out of 20
EarlyStopping counter: 20 out of 20
Early stopping epoch 228, loss 0.3316, train acc 78.50%, f1 0.7850, precision 0.7850, recall 0.7850, auc 0.7850



/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_Mirror_True
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_True/record_1/MLP_concat_Mirror_True_5
./test_pima/result_MLP_concat_Mirror_True_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6455660377358491

the Fscore is 0.5977011494252873

the precision is 0.4297520661157025

the recall is 0.9811320754716981

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_5
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5533, train acc 78.96%, f1 0.7911, precision 0.7854, recall 0.7970, auc 0.7896
epoch 201, loss 0.5530, train acc 82.05%, f1 0.8204, precision 0.8209, recall 0.8200, auc 0.8205
epoch 301, loss 0.4417, train acc 83.66%, f1 0.8363, precision 0.8381, recall 0.8344, auc 0.8366
epoch 401, loss 0.3518, train acc 83.99%, f1 0.8396, precision 0.8413, recall 0.8378, auc 0.8399
epoch 501, loss 0.2577, train acc 84.15%, f1 0.8411, precision 0.8432, recall 0.8390, auc 0.8415
epoch 601, loss 0.2798, train acc 84.17%, f1 0.8412, precision 0.8437, recall 0.8388, auc 0.8417
epoch 701, loss 0.4728, train acc 84.26%, f1 0.8422, precision 0.8446, recall 0.8398, auc 0.8426
epoch 801, loss 0.4259, train acc 84.23%, f1 0.8420, precision 0.8435, recall 0.8405, auc 0.8423
epoch 901, loss 0.3826, train acc 84.18%, f1 0.8415, precision 0.8430, recall 0.8400, auc 0.8418
epoch 1001, loss 0.3377, train acc 84.17%, f1 0.8414, precision 0.8429, recall 0.8398, auc 0.8417
epoch 1101, loss 0.3210, train acc 84.21%, f1 0.8419, precision 0.8430, recall 0.8408, auc 0.8421
epoch 1201, loss 0.3267, train acc 84.19%, f1 0.8417, precision 0.8425, recall 0.8409, auc 0.8419
epoch 1301, loss 0.4427, train acc 84.21%, f1 0.8420, precision 0.8423, recall 0.8417, auc 0.8421
epoch 1401, loss 0.3931, train acc 84.15%, f1 0.8414, precision 0.8419, recall 0.8410, auc 0.8415
epoch 1501, loss 0.2900, train acc 84.18%, f1 0.8418, precision 0.8421, recall 0.8414, auc 0.8418
epoch 1601, loss 0.3694, train acc 84.22%, f1 0.8422, precision 0.8421, recall 0.8423, auc 0.8422
epoch 1701, loss 0.4292, train acc 84.25%, f1 0.8426, precision 0.8424, recall 0.8428, auc 0.8425
epoch 1801, loss 0.4294, train acc 84.28%, f1 0.8428, precision 0.8428, recall 0.8427, auc 0.8428
epoch 1901, loss 0.2735, train acc 84.22%, f1 0.8423, precision 0.8419, recall 0.8428, auc 0.8422
epoch 2001, loss 0.4574, train acc 84.21%, f1 0.8422, precision 0.8418, recall 0.8426, auc 0.8421
epoch 2101, loss 0.4418, train acc 84.22%, f1 0.8422, precision 0.8420, recall 0.8424, auc 0.8422
epoch 2201, loss 0.3469, train acc 84.21%, f1 0.8422, precision 0.8419, recall 0.8425, auc 0.8421
epoch 2301, loss 0.4135, train acc 84.25%, f1 0.8426, precision 0.8424, recall 0.8428, auc 0.8425
epoch 2401, loss 0.2741, train acc 84.25%, f1 0.8425, precision 0.8424, recall 0.8426, auc 0.8425
epoch 2501, loss 0.4012, train acc 84.36%, f1 0.8435, precision 0.8442, recall 0.8428, auc 0.8436
epoch 2601, loss 0.2416, train acc 84.41%, f1 0.8440, precision 0.8442, recall 0.8438, auc 0.8441
epoch 2701, loss 0.4289, train acc 84.45%, f1 0.8446, precision 0.8444, recall 0.8447, auc 0.8445
epoch 2801, loss 0.4010, train acc 84.52%, f1 0.8453, precision 0.8450, recall 0.8455, auc 0.8452
epoch 2901, loss 0.3364, train acc 84.64%, f1 0.8464, precision 0.8464, recall 0.8465, auc 0.8464
epoch 3001, loss 0.3702, train acc 84.72%, f1 0.8473, precision 0.8467, recall 0.8479, auc 0.8472
epoch 3101, loss 0.3532, train acc 84.84%, f1 0.8484, precision 0.8485, recall 0.8483, auc 0.8484
epoch 3201, loss 0.3290, train acc 84.95%, f1 0.8495, precision 0.8494, recall 0.8496, auc 0.8495
epoch 3301, loss 0.3052, train acc 84.96%, f1 0.8495, precision 0.8499, recall 0.8491, auc 0.8496
epoch 3401, loss 0.3721, train acc 85.08%, f1 0.8509, precision 0.8505, recall 0.8512, auc 0.8508
epoch 3501, loss 0.3143, train acc 85.18%, f1 0.8519, precision 0.8516, recall 0.8521, auc 0.8518
epoch 3601, loss 0.3269, train acc 85.38%, f1 0.8540, precision 0.8532, recall 0.8547, auc 0.8538
epoch 3701, loss 0.3068, train acc 85.51%, f1 0.8552, precision 0.8545, recall 0.8559, auc 0.8551
epoch 3801, loss 0.4160, train acc 85.58%, f1 0.8559, precision 0.8555, recall 0.8562, auc 0.8558
epoch 3901, loss 0.2655, train acc 85.72%, f1 0.8573, precision 0.8569, recall 0.8577, auc 0.8572
epoch 4001, loss 0.2731, train acc 85.80%, f1 0.8581, precision 0.8575, recall 0.8586, auc 0.8580
epoch 4101, loss 0.2934, train acc 85.95%, f1 0.8595, precision 0.8594, recall 0.8596, auc 0.8595
epoch 4201, loss 0.2556, train acc 86.03%, f1 0.8604, precision 0.8600, recall 0.8607, auc 0.8603
epoch 4301, loss 0.4574, train acc 86.11%, f1 0.8610, precision 0.8617, recall 0.8604, auc 0.8611
epoch 4401, loss 0.3213, train acc 86.23%, f1 0.8623, precision 0.8622, recall 0.8623, auc 0.8623
epoch 4501, loss 0.3224, train acc 86.31%, f1 0.8631, precision 0.8630, recall 0.8633, auc 0.8631
epoch 4601, loss 0.2167, train acc 86.41%, f1 0.8641, precision 0.8640, recall 0.8642, auc 0.8641
epoch 4701, loss 0.3851, train acc 86.46%, f1 0.8646, precision 0.8644, recall 0.8648, auc 0.8646
epoch 4801, loss 0.4488, train acc 86.61%, f1 0.8661, precision 0.8659, recall 0.8662, auc 0.8661
epoch 4901, loss 0.2301, train acc 86.62%, f1 0.8662, precision 0.8659, recall 0.8665, auc 0.8662
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_Mirror_5000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_5000/record_1/MLP_concat_Mirror_5000_5
./test_pima/result_MLP_concat_Mirror_5000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6622641509433962

the Fscore is 0.6049382716049383

the precision is 0.44954128440366975

the recall is 0.9245283018867925

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_5
----------------------



epoch 1, loss 0.6930, train acc 55.10%, f1 0.6876, precision 0.5272, recall 0.9882, auc 0.5510
epoch 101, loss 0.5441, train acc 79.66%, f1 0.7944, precision 0.8032, recall 0.7858, auc 0.7966
epoch 201, loss 0.3624, train acc 81.97%, f1 0.8199, precision 0.8191, recall 0.8207, auc 0.8197
epoch 301, loss 0.4854, train acc 83.42%, f1 0.8348, precision 0.8315, recall 0.8382, auc 0.8342
epoch 401, loss 0.3324, train acc 84.01%, f1 0.8405, precision 0.8386, recall 0.8424, auc 0.8401
epoch 501, loss 0.4253, train acc 84.12%, f1 0.8417, precision 0.8392, recall 0.8441, auc 0.8412
epoch 601, loss 0.2862, train acc 84.13%, f1 0.8417, precision 0.8399, recall 0.8435, auc 0.8413
epoch 701, loss 0.6275, train acc 84.22%, f1 0.8425, precision 0.8407, recall 0.8444, auc 0.8422
epoch 801, loss 0.3940, train acc 84.19%, f1 0.8423, precision 0.8403, recall 0.8444, auc 0.8419
epoch 901, loss 0.2892, train acc 84.25%, f1 0.8427, precision 0.8419, recall 0.8435, auc 0.8425
epoch 1001, loss 0.3571, train acc 84.25%, f1 0.8425, precision 0.8422, recall 0.8428, auc 0.8425
epoch 1101, loss 0.4953, train acc 84.18%, f1 0.8420, precision 0.8411, recall 0.8429, auc 0.8418
epoch 1201, loss 0.3708, train acc 84.20%, f1 0.8421, precision 0.8413, recall 0.8430, auc 0.8420
epoch 1301, loss 0.2645, train acc 84.27%, f1 0.8428, precision 0.8420, recall 0.8436, auc 0.8427
epoch 1401, loss 0.2613, train acc 84.18%, f1 0.8420, precision 0.8410, recall 0.8430, auc 0.8418
epoch 1501, loss 0.4642, train acc 84.27%, f1 0.8427, precision 0.8426, recall 0.8428, auc 0.8427
epoch 1601, loss 0.3482, train acc 84.24%, f1 0.8426, precision 0.8416, recall 0.8436, auc 0.8424
epoch 1701, loss 0.4483, train acc 84.24%, f1 0.8425, precision 0.8420, recall 0.8430, auc 0.8424
epoch 1801, loss 0.3319, train acc 84.22%, f1 0.8423, precision 0.8416, recall 0.8429, auc 0.8422
epoch 1901, loss 0.4659, train acc 84.26%, f1 0.8428, precision 0.8420, recall 0.8436, auc 0.8426
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/standlization_data/pima_std_test_5.csv
MLP_concat_Mirror_2000
concat_pos_num_40_1
./test_pima/model_MLP_concat_Mirror_2000/record_1/MLP_concat_Mirror_2000_5
./test_pima/result_MLP_concat_Mirror_2000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.6

the Fscore is 0.5698924731182795

the precision is 0.39849624060150374

the recall is 1.0

Done
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_pima/standlization_data/pima_std_train_5.csv
./test_pima/model_MLP_concat_notMirror_True/record_1/MLP_concat_notMirror_True_5
----------------------



Traceback (most recent call last):
  File "./classifier_MLP/train_MLP.py", line 312, in <module>
    transformed_valid_data, transformed_valid_label = transform_data_to_train_form(transform_method, mirror_type, valid_pos_data, valid_neg_data)
  File "./classifier_MLP/train_MLP.py", line 192, in transform_data_to_train_form
    trans_pos_data, trans_pos_label, trans_neg_data, trans_neg_label = handleData_extend_not_mirror(positive_repeat_data, negetive_tile_data)
  File "./classifier_MLP/train_MLP.py", line 154, in handleData_extend_not_mirror
    transfrom_positive_data = transfrom_positive_data[positive_index[0]]
UnboundLocalError: local variable 'transfrom_positive_data' referenced before assignment
