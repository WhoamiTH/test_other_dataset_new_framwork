nohup: ignoring input
./test_abalone19/standlization_data/abalone19_std_train_5.csv
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_5
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5271, train acc 74.79%, f1 0.7555, precision 0.7334, recall 0.7790, auc 0.7479
epoch 201, loss 0.4433, train acc 79.19%, f1 0.7909, precision 0.7948, recall 0.7870, auc 0.7919
epoch 301, loss 0.3548, train acc 86.25%, f1 0.8625, precision 0.8626, recall 0.8624, auc 0.8625
epoch 401, loss 0.2844, train acc 90.40%, f1 0.9039, precision 0.9045, recall 0.9033, auc 0.9040
epoch 501, loss 0.2477, train acc 91.68%, f1 0.9166, precision 0.9184, recall 0.9148, auc 0.9168
epoch 601, loss 0.2295, train acc 92.17%, f1 0.9214, precision 0.9241, recall 0.9188, auc 0.9217
epoch 701, loss 0.2194, train acc 92.45%, f1 0.9242, precision 0.9273, recall 0.9212, auc 0.9245
epoch 801, loss 0.2131, train acc 92.62%, f1 0.9260, precision 0.9290, recall 0.9229, auc 0.9262
epoch 901, loss 0.2077, train acc 92.77%, f1 0.9275, precision 0.9306, recall 0.9244, auc 0.9277
epoch 1001, loss 0.2015, train acc 92.91%, f1 0.9288, precision 0.9317, recall 0.9260, auc 0.9291
epoch 1101, loss 0.1951, train acc 93.07%, f1 0.9305, precision 0.9330, recall 0.9279, auc 0.9307
epoch 1201, loss 0.1885, train acc 93.26%, f1 0.9324, precision 0.9348, recall 0.9300, auc 0.9326
epoch 1301, loss 0.1823, train acc 93.43%, f1 0.9342, precision 0.9365, recall 0.9318, auc 0.9343
epoch 1401, loss 0.1764, train acc 93.60%, f1 0.9358, precision 0.9382, recall 0.9335, auc 0.9360
epoch 1501, loss 0.1712, train acc 93.71%, f1 0.9369, precision 0.9395, recall 0.9344, auc 0.9371
epoch 1601, loss 0.1665, train acc 93.81%, f1 0.9380, precision 0.9404, recall 0.9356, auc 0.9381
epoch 1701, loss 0.1622, train acc 93.92%, f1 0.9391, precision 0.9414, recall 0.9368, auc 0.9392
epoch 1801, loss 0.1583, train acc 94.01%, f1 0.9400, precision 0.9424, recall 0.9376, auc 0.9401
epoch 1901, loss 0.1548, train acc 94.07%, f1 0.9405, precision 0.9430, recall 0.9380, auc 0.9407
epoch 2001, loss 0.1515, train acc 94.10%, f1 0.9409, precision 0.9435, recall 0.9383, auc 0.9410
epoch 2101, loss 0.1483, train acc 94.18%, f1 0.9417, precision 0.9442, recall 0.9391, auc 0.9418
epoch 2201, loss 0.1451, train acc 94.24%, f1 0.9423, precision 0.9447, recall 0.9399, auc 0.9424
epoch 2301, loss 0.1423, train acc 94.31%, f1 0.9430, precision 0.9453, recall 0.9408, auc 0.9431
epoch 2401, loss 0.1396, train acc 94.37%, f1 0.9435, precision 0.9457, recall 0.9413, auc 0.9437
epoch 2501, loss 0.1369, train acc 94.42%, f1 0.9441, precision 0.9462, recall 0.9420, auc 0.9442
epoch 2601, loss 0.1343, train acc 94.47%, f1 0.9446, precision 0.9466, recall 0.9426, auc 0.9447
epoch 2701, loss 0.1318, train acc 94.54%, f1 0.9453, precision 0.9473, recall 0.9433, auc 0.9454
epoch 2801, loss 0.1294, train acc 94.61%, f1 0.9460, precision 0.9479, recall 0.9441, auc 0.9461
epoch 2901, loss 0.1270, train acc 94.69%, f1 0.9468, precision 0.9485, recall 0.9450, auc 0.9469
epoch 3001, loss 0.1246, train acc 94.76%, f1 0.9475, precision 0.9491, recall 0.9459, auc 0.9476
epoch 3101, loss 0.1223, train acc 94.82%, f1 0.9482, precision 0.9497, recall 0.9467, auc 0.9482
epoch 3201, loss 0.1199, train acc 94.88%, f1 0.9487, precision 0.9502, recall 0.9472, auc 0.9488
epoch 3301, loss 0.1175, train acc 94.94%, f1 0.9493, precision 0.9508, recall 0.9479, auc 0.9494
epoch 3401, loss 0.1154, train acc 95.01%, f1 0.9500, precision 0.9514, recall 0.9487, auc 0.9501
epoch 3501, loss 0.1132, train acc 95.10%, f1 0.9510, precision 0.9521, recall 0.9498, auc 0.9510
epoch 3601, loss 0.1109, train acc 95.19%, f1 0.9518, precision 0.9529, recall 0.9508, auc 0.9519
epoch 3701, loss 0.1086, train acc 95.31%, f1 0.9531, precision 0.9542, recall 0.9520, auc 0.9531
epoch 3801, loss 0.1063, train acc 95.42%, f1 0.9541, precision 0.9553, recall 0.9529, auc 0.9542
epoch 3901, loss 0.1040, train acc 95.48%, f1 0.9548, precision 0.9559, recall 0.9537, auc 0.9548
epoch 4001, loss 0.1020, train acc 95.56%, f1 0.9556, precision 0.9566, recall 0.9546, auc 0.9556
epoch 4101, loss 0.0998, train acc 95.64%, f1 0.9563, precision 0.9574, recall 0.9553, auc 0.9564
epoch 4201, loss 0.0977, train acc 95.72%, f1 0.9571, precision 0.9581, recall 0.9562, auc 0.9572
epoch 4301, loss 0.0956, train acc 95.82%, f1 0.9581, precision 0.9591, recall 0.9572, auc 0.9582
epoch 4401, loss 0.0936, train acc 95.91%, f1 0.9591, precision 0.9599, recall 0.9582, auc 0.9591
epoch 4501, loss 0.0916, train acc 95.99%, f1 0.9599, precision 0.9607, recall 0.9592, auc 0.9599
epoch 4601, loss 0.0894, train acc 96.09%, f1 0.9608, precision 0.9616, recall 0.9601, auc 0.9609
epoch 4701, loss 0.0875, train acc 96.18%, f1 0.9618, precision 0.9624, recall 0.9611, auc 0.9618
epoch 4801, loss 0.0854, train acc 96.26%, f1 0.9626, precision 0.9630, recall 0.9622, auc 0.9626
epoch 4901, loss 0.0834, train acc 96.36%, f1 0.9636, precision 0.9640, recall 0.9633, auc 0.9636
epoch 5001, loss 0.0814, train acc 96.47%, f1 0.9647, precision 0.9649, recall 0.9644, auc 0.9647
epoch 5101, loss 0.0793, train acc 96.57%, f1 0.9657, precision 0.9658, recall 0.9656, auc 0.9657
epoch 5201, loss 0.0773, train acc 96.65%, f1 0.9665, precision 0.9666, recall 0.9663, auc 0.9665
epoch 5301, loss 0.0752, train acc 96.74%, f1 0.9674, precision 0.9676, recall 0.9672, auc 0.9674
epoch 5401, loss 0.0732, train acc 96.82%, f1 0.9682, precision 0.9684, recall 0.9680, auc 0.9682
epoch 5501, loss 0.0711, train acc 96.89%, f1 0.9689, precision 0.9692, recall 0.9687, auc 0.9689
epoch 5601, loss 0.0692, train acc 96.99%, f1 0.9699, precision 0.9700, recall 0.9697, auc 0.9699
epoch 5701, loss 0.0673, train acc 97.07%, f1 0.9707, precision 0.9710, recall 0.9704, auc 0.9707
epoch 5801, loss 0.0653, train acc 97.17%, f1 0.9717, precision 0.9719, recall 0.9714, auc 0.9717
epoch 5901, loss 0.0634, train acc 97.29%, f1 0.9729, precision 0.9730, recall 0.9727, auc 0.9729
epoch 6001, loss 0.0615, train acc 97.37%, f1 0.9737, precision 0.9737, recall 0.9736, auc 0.9737
epoch 6101, loss 0.0597, train acc 97.48%, f1 0.9748, precision 0.9748, recall 0.9748, auc 0.9748
epoch 6201, loss 0.0579, train acc 97.57%, f1 0.9757, precision 0.9756, recall 0.9759, auc 0.9757
epoch 6301, loss 0.0557, train acc 97.67%, f1 0.9767, precision 0.9765, recall 0.9770, auc 0.9767
epoch 6401, loss 0.0543, train acc 97.77%, f1 0.9777, precision 0.9774, recall 0.9779, auc 0.9777
epoch 6501, loss 0.0525, train acc 97.86%, f1 0.9786, precision 0.9783, recall 0.9789, auc 0.9786
epoch 6601, loss 0.0508, train acc 97.95%, f1 0.9796, precision 0.9793, recall 0.9798, auc 0.9795
epoch 6701, loss 0.0491, train acc 98.03%, f1 0.9803, precision 0.9799, recall 0.9807, auc 0.9803
epoch 6801, loss 0.0474, train acc 98.13%, f1 0.9813, precision 0.9809, recall 0.9817, auc 0.9813
epoch 6901, loss 0.0458, train acc 98.21%, f1 0.9821, precision 0.9816, recall 0.9827, auc 0.9821
epoch 7001, loss 0.0442, train acc 98.31%, f1 0.9831, precision 0.9826, recall 0.9836, auc 0.9831
epoch 7101, loss 0.0426, train acc 98.38%, f1 0.9838, precision 0.9835, recall 0.9842, auc 0.9838
epoch 7201, loss 0.0411, train acc 98.46%, f1 0.9846, precision 0.9842, recall 0.9850, auc 0.9846
epoch 7301, loss 0.0397, train acc 98.53%, f1 0.9853, precision 0.9848, recall 0.9857, auc 0.9853
epoch 7401, loss 0.0383, train acc 98.58%, f1 0.9858, precision 0.9854, recall 0.9863, auc 0.9858
epoch 7501, loss 0.0369, train acc 98.65%, f1 0.9865, precision 0.9861, recall 0.9870, auc 0.9865
epoch 7601, loss 0.0358, train acc 98.72%, f1 0.9872, precision 0.9867, recall 0.9877, auc 0.9872
epoch 7701, loss 0.0345, train acc 98.78%, f1 0.9878, precision 0.9873, recall 0.9883, auc 0.9878
epoch 7801, loss 0.0333, train acc 98.82%, f1 0.9882, precision 0.9879, recall 0.9886, auc 0.9882
epoch 7901, loss 0.0322, train acc 98.89%, f1 0.9889, precision 0.9885, recall 0.9893, auc 0.9889
epoch 8001, loss 0.0311, train acc 98.96%, f1 0.9896, precision 0.9891, recall 0.9900, auc 0.9896
epoch 8101, loss 0.0300, train acc 99.01%, f1 0.9901, precision 0.9898, recall 0.9903, auc 0.9901
epoch 8201, loss 0.0290, train acc 99.05%, f1 0.9905, precision 0.9902, recall 0.9907, auc 0.9905
epoch 8301, loss 0.0279, train acc 99.08%, f1 0.9908, precision 0.9905, recall 0.9911, auc 0.9908
epoch 8401, loss 0.0270, train acc 99.12%, f1 0.9912, precision 0.9910, recall 0.9914, auc 0.9912
epoch 8501, loss 0.0260, train acc 99.15%, f1 0.9916, precision 0.9914, recall 0.9918, auc 0.9915
epoch 8601, loss 0.0251, train acc 99.20%, f1 0.9920, precision 0.9918, recall 0.9922, auc 0.9920
epoch 8701, loss 0.0243, train acc 99.22%, f1 0.9922, precision 0.9920, recall 0.9924, auc 0.9922
epoch 8801, loss 0.0234, train acc 99.25%, f1 0.9925, precision 0.9923, recall 0.9927, auc 0.9925
epoch 8901, loss 0.0226, train acc 99.27%, f1 0.9927, precision 0.9925, recall 0.9930, auc 0.9927
epoch 9001, loss 0.0218, train acc 99.31%, f1 0.9931, precision 0.9928, recall 0.9934, auc 0.9931
epoch 9101, loss 0.0210, train acc 99.34%, f1 0.9934, precision 0.9932, recall 0.9937, auc 0.9934
epoch 9201, loss 0.0203, train acc 99.37%, f1 0.9937, precision 0.9935, recall 0.9939, auc 0.9937
epoch 9301, loss 0.0193, train acc 99.40%, f1 0.9940, precision 0.9938, recall 0.9943, auc 0.9940
epoch 9401, loss 0.0188, train acc 99.43%, f1 0.9943, precision 0.9940, recall 0.9945, auc 0.9943
epoch 9501, loss 0.0181, train acc 99.46%, f1 0.9946, precision 0.9943, recall 0.9949, auc 0.9946
epoch 9601, loss 0.0175, train acc 99.49%, f1 0.9949, precision 0.9947, recall 0.9951, auc 0.9949
epoch 9701, loss 0.0168, train acc 99.51%, f1 0.9951, precision 0.9949, recall 0.9954, auc 0.9951
epoch 9801, loss 0.0162, train acc 99.53%, f1 0.9953, precision 0.9951, recall 0.9956, auc 0.9953
epoch 9901, loss 0.0156, train acc 99.55%, f1 0.9955, precision 0.9952, recall 0.9957, auc 0.9955
epoch 10001, loss 0.0151, train acc 99.57%, f1 0.9957, precision 0.9955, recall 0.9959, auc 0.9957
epoch 10101, loss 0.0146, train acc 99.59%, f1 0.9959, precision 0.9957, recall 0.9960, auc 0.9959
epoch 10201, loss 0.0141, train acc 99.60%, f1 0.9960, precision 0.9958, recall 0.9962, auc 0.9960
epoch 10301, loss 0.0136, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9963, auc 0.9962
epoch 10401, loss 0.0131, train acc 99.63%, f1 0.9963, precision 0.9961, recall 0.9965, auc 0.9963
epoch 10501, loss 0.0127, train acc 99.64%, f1 0.9964, precision 0.9962, recall 0.9966, auc 0.9964
epoch 10601, loss 0.0122, train acc 99.66%, f1 0.9967, precision 0.9964, recall 0.9969, auc 0.9966
epoch 10701, loss 0.0118, train acc 99.68%, f1 0.9968, precision 0.9966, recall 0.9970, auc 0.9968
epoch 10801, loss 0.0114, train acc 99.69%, f1 0.9969, precision 0.9967, recall 0.9971, auc 0.9969
epoch 10901, loss 0.0110, train acc 99.71%, f1 0.9971, precision 0.9968, recall 0.9973, auc 0.9971
epoch 11001, loss 0.0106, train acc 99.72%, f1 0.9972, precision 0.9970, recall 0.9974, auc 0.9972
epoch 11101, loss 0.0102, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9976, auc 0.9973
epoch 11201, loss 0.0098, train acc 99.75%, f1 0.9975, precision 0.9972, recall 0.9978, auc 0.9975
epoch 11301, loss 0.0095, train acc 99.77%, f1 0.9977, precision 0.9973, recall 0.9980, auc 0.9977
epoch 11401, loss 0.0091, train acc 99.78%, f1 0.9978, precision 0.9975, recall 0.9980, auc 0.9978
epoch 11501, loss 0.0088, train acc 99.79%, f1 0.9979, precision 0.9976, recall 0.9982, auc 0.9979
epoch 11601, loss 0.0085, train acc 99.80%, f1 0.9980, precision 0.9978, recall 0.9982, auc 0.9980
epoch 11701, loss 0.0082, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9982, auc 0.9981
epoch 11801, loss 0.0080, train acc 99.81%, f1 0.9981, precision 0.9979, recall 0.9984, auc 0.9981
epoch 11901, loss 0.0077, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 12001, loss 0.0074, train acc 99.82%, f1 0.9982, precision 0.9980, recall 0.9984, auc 0.9982
epoch 12101, loss 0.0072, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 12201, loss 0.0070, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9985, auc 0.9983
epoch 12301, loss 0.0068, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 12401, loss 0.0066, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9986, auc 0.9985
epoch 12501, loss 0.0064, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9986, auc 0.9986
epoch 12601, loss 0.0062, train acc 99.86%, f1 0.9986, precision 0.9985, recall 0.9987, auc 0.9986
epoch 12701, loss 0.0060, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9988, auc 0.9987
epoch 12801, loss 0.0058, train acc 99.87%, f1 0.9987, precision 0.9986, recall 0.9989, auc 0.9987
epoch 12901, loss 0.0056, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 13001, loss 0.0054, train acc 99.88%, f1 0.9988, precision 0.9987, recall 0.9989, auc 0.9988
epoch 13101, loss 0.0053, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 13201, loss 0.0051, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9990, auc 0.9989
epoch 13301, loss 0.0050, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 13401, loss 0.0048, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 13501, loss 0.0047, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9991, auc 0.9990
epoch 13601, loss 0.0045, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9992, auc 0.9991
epoch 13701, loss 0.0044, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9992, auc 0.9991
epoch 13801, loss 0.0043, train acc 99.91%, f1 0.9991, precision 0.9990, recall 0.9993, auc 0.9991
epoch 13901, loss 0.0042, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 14001, loss 0.0040, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 14101, loss 0.0039, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9993, auc 0.9992
epoch 14201, loss 0.0038, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 14301, loss 0.0037, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 14401, loss 0.0036, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 14501, loss 0.0035, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 14601, loss 0.0034, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 14701, loss 0.0033, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9995, auc 0.9993
epoch 14801, loss 0.0032, train acc 99.94%, f1 0.9994, precision 0.9992, recall 0.9995, auc 0.9994
epoch 14901, loss 0.0031, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 15001, loss 0.0030, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 15101, loss 0.0029, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9995, auc 0.9994
epoch 15201, loss 0.0028, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9995, auc 0.9995
epoch 15301, loss 0.0027, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15401, loss 0.0026, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9995, auc 0.9995
epoch 15501, loss 0.0026, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9996, auc 0.9995
epoch 15601, loss 0.0025, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15701, loss 0.0024, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15801, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 15901, loss 0.0023, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9996, auc 0.9996
epoch 16001, loss 0.0022, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9996, auc 0.9996
epoch 16101, loss 0.0021, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 16201, loss 0.0020, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 16301, loss 0.0020, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 16401, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16501, loss 0.0019, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9997, auc 0.9997
epoch 16601, loss 0.0018, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 16701, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 16801, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9997, auc 0.9998
epoch 16901, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 17001, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 17101, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 17201, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9998, auc 0.9998
epoch 17301, loss 0.0014, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9998, auc 0.9998
epoch 17401, loss 0.0014, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 17501, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9998, auc 0.9999
epoch 17601, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 17701, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 17801, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 17901, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 18001, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 18101, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 18201, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 18301, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 0.9999, auc 0.9999
epoch 18401, loss 0.0009, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 18501, loss 0.0009, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 18601, loss 0.0008, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 18701, loss 0.0008, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 18801, loss 0.0008, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 18901, loss 0.0007, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 19001, loss 0.0007, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 19101, loss 0.0007, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 19201, loss 0.0006, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 19301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 19401, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 19501, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_5.csv
./test_abalone19/standlization_data/abalone19_std_test_5.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_5
./test_abalone19/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.49819059107358266

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
./test_abalone19/standlization_data/abalone19_std_train_5.csv
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_5
----------------------



epoch 1, loss 0.6930, train acc 50.01%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5001
epoch 101, loss 0.5217, train acc 75.07%, f1 0.7508, precision 0.7506, recall 0.7510, auc 0.7507
epoch 201, loss 0.4395, train acc 79.38%, f1 0.7941, precision 0.7931, recall 0.7950, auc 0.7938
epoch 301, loss 0.3505, train acc 86.56%, f1 0.8656, precision 0.8654, recall 0.8657, auc 0.8656
epoch 401, loss 0.2812, train acc 90.49%, f1 0.9049, precision 0.9049, recall 0.9050, auc 0.9049
epoch 501, loss 0.2455, train acc 91.71%, f1 0.9171, precision 0.9170, recall 0.9173, auc 0.9171
epoch 601, loss 0.2280, train acc 92.17%, f1 0.9217, precision 0.9215, recall 0.9219, auc 0.9217
epoch 701, loss 0.2185, train acc 92.47%, f1 0.9248, precision 0.9246, recall 0.9249, auc 0.9247
epoch 801, loss 0.2126, train acc 92.63%, f1 0.9263, precision 0.9261, recall 0.9264, auc 0.9263
epoch 901, loss 0.2078, train acc 92.78%, f1 0.9278, precision 0.9277, recall 0.9280, auc 0.9278
epoch 1001, loss 0.2022, train acc 92.91%, f1 0.9291, precision 0.9290, recall 0.9292, auc 0.9291
epoch 1101, loss 0.1960, train acc 93.04%, f1 0.9304, precision 0.9303, recall 0.9304, auc 0.9304
epoch 1201, loss 0.1896, train acc 93.22%, f1 0.9322, precision 0.9321, recall 0.9323, auc 0.9322
epoch 1301, loss 0.1834, train acc 93.39%, f1 0.9339, precision 0.9341, recall 0.9338, auc 0.9339
epoch 1401, loss 0.1776, train acc 93.57%, f1 0.9357, precision 0.9356, recall 0.9358, auc 0.9357
epoch 1501, loss 0.1725, train acc 93.72%, f1 0.9372, precision 0.9371, recall 0.9372, auc 0.9372
epoch 1601, loss 0.1679, train acc 93.81%, f1 0.9381, precision 0.9380, recall 0.9382, auc 0.9381
epoch 1701, loss 0.1637, train acc 93.89%, f1 0.9390, precision 0.9388, recall 0.9392, auc 0.9389
epoch 1801, loss 0.1596, train acc 93.99%, f1 0.9400, precision 0.9398, recall 0.9402, auc 0.9399
epoch 1901, loss 0.1557, train acc 94.10%, f1 0.9410, precision 0.9408, recall 0.9412, auc 0.9410
epoch 2001, loss 0.1517, train acc 94.20%, f1 0.9420, precision 0.9418, recall 0.9423, auc 0.9420
epoch 2101, loss 0.1479, train acc 94.28%, f1 0.9428, precision 0.9427, recall 0.9429, auc 0.9428
epoch 2201, loss 0.1444, train acc 94.35%, f1 0.9435, precision 0.9434, recall 0.9436, auc 0.9435
epoch 2301, loss 0.1410, train acc 94.43%, f1 0.9443, precision 0.9442, recall 0.9443, auc 0.9443
epoch 2401, loss 0.1377, train acc 94.49%, f1 0.9449, precision 0.9448, recall 0.9450, auc 0.9449
epoch 2501, loss 0.1348, train acc 94.56%, f1 0.9456, precision 0.9455, recall 0.9456, auc 0.9456
epoch 2601, loss 0.1319, train acc 94.63%, f1 0.9463, precision 0.9463, recall 0.9463, auc 0.9463
epoch 2701, loss 0.1293, train acc 94.71%, f1 0.9471, precision 0.9471, recall 0.9470, auc 0.9471
epoch 2801, loss 0.1267, train acc 94.76%, f1 0.9476, precision 0.9476, recall 0.9477, auc 0.9476
epoch 2901, loss 0.1242, train acc 94.83%, f1 0.9483, precision 0.9483, recall 0.9483, auc 0.9483
epoch 3001, loss 0.1219, train acc 94.89%, f1 0.9489, precision 0.9489, recall 0.9490, auc 0.9489
epoch 3101, loss 0.1191, train acc 94.96%, f1 0.9496, precision 0.9496, recall 0.9497, auc 0.9496
epoch 3201, loss 0.1173, train acc 95.04%, f1 0.9504, precision 0.9503, recall 0.9504, auc 0.9504
epoch 3301, loss 0.1150, train acc 95.12%, f1 0.9512, precision 0.9513, recall 0.9512, auc 0.9512
epoch 3401, loss 0.1127, train acc 95.21%, f1 0.9521, precision 0.9521, recall 0.9521, auc 0.9521
epoch 3501, loss 0.1105, train acc 95.28%, f1 0.9528, precision 0.9528, recall 0.9528, auc 0.9528
epoch 3601, loss 0.1084, train acc 95.36%, f1 0.9536, precision 0.9535, recall 0.9537, auc 0.9536
epoch 3701, loss 0.1065, train acc 95.41%, f1 0.9542, precision 0.9540, recall 0.9543, auc 0.9541
epoch 3801, loss 0.1044, train acc 95.48%, f1 0.9548, precision 0.9547, recall 0.9549, auc 0.9548
epoch 3901, loss 0.1027, train acc 95.56%, f1 0.9556, precision 0.9557, recall 0.9555, auc 0.9556
epoch 4001, loss 0.1008, train acc 95.64%, f1 0.9564, precision 0.9566, recall 0.9563, auc 0.9564
epoch 4101, loss 0.0989, train acc 95.72%, f1 0.9572, precision 0.9573, recall 0.9571, auc 0.9572
epoch 4201, loss 0.0970, train acc 95.82%, f1 0.9582, precision 0.9583, recall 0.9581, auc 0.9582
epoch 4301, loss 0.0950, train acc 95.90%, f1 0.9590, precision 0.9591, recall 0.9589, auc 0.9590
epoch 4401, loss 0.0930, train acc 95.99%, f1 0.9599, precision 0.9600, recall 0.9599, auc 0.9599
epoch 4501, loss 0.0911, train acc 96.08%, f1 0.9608, precision 0.9608, recall 0.9608, auc 0.9608
epoch 4601, loss 0.0891, train acc 96.16%, f1 0.9616, precision 0.9614, recall 0.9618, auc 0.9616
epoch 4701, loss 0.0871, train acc 96.25%, f1 0.9626, precision 0.9622, recall 0.9629, auc 0.9625
epoch 4801, loss 0.0851, train acc 96.34%, f1 0.9634, precision 0.9632, recall 0.9635, auc 0.9634
epoch 4901, loss 0.0830, train acc 96.44%, f1 0.9644, precision 0.9640, recall 0.9647, auc 0.9644
epoch 5001, loss 0.0810, train acc 96.54%, f1 0.9654, precision 0.9651, recall 0.9656, auc 0.9654
epoch 5101, loss 0.0788, train acc 96.65%, f1 0.9666, precision 0.9663, recall 0.9668, auc 0.9665
epoch 5201, loss 0.0767, train acc 96.76%, f1 0.9676, precision 0.9673, recall 0.9680, auc 0.9676
epoch 5301, loss 0.0746, train acc 96.83%, f1 0.9683, precision 0.9680, recall 0.9687, auc 0.9683
epoch 5401, loss 0.0724, train acc 96.93%, f1 0.9694, precision 0.9689, recall 0.9698, auc 0.9693
epoch 5501, loss 0.0702, train acc 97.03%, f1 0.9703, precision 0.9698, recall 0.9708, auc 0.9703
epoch 5601, loss 0.0680, train acc 97.14%, f1 0.9714, precision 0.9707, recall 0.9721, auc 0.9714
epoch 5701, loss 0.0658, train acc 97.23%, f1 0.9723, precision 0.9718, recall 0.9728, auc 0.9723
epoch 5801, loss 0.0637, train acc 97.34%, f1 0.9734, precision 0.9730, recall 0.9738, auc 0.9734
epoch 5901, loss 0.0617, train acc 97.42%, f1 0.9743, precision 0.9739, recall 0.9746, auc 0.9742
epoch 6001, loss 0.0596, train acc 97.55%, f1 0.9755, precision 0.9750, recall 0.9759, auc 0.9755
epoch 6101, loss 0.0577, train acc 97.63%, f1 0.9763, precision 0.9757, recall 0.9769, auc 0.9763
epoch 6201, loss 0.0557, train acc 97.72%, f1 0.9772, precision 0.9766, recall 0.9779, auc 0.9772
epoch 6301, loss 0.0538, train acc 97.83%, f1 0.9783, precision 0.9778, recall 0.9789, auc 0.9783
epoch 6401, loss 0.0520, train acc 97.91%, f1 0.9791, precision 0.9785, recall 0.9796, auc 0.9791
epoch 6501, loss 0.0501, train acc 97.99%, f1 0.9799, precision 0.9794, recall 0.9805, auc 0.9799
epoch 6601, loss 0.0483, train acc 98.07%, f1 0.9807, precision 0.9801, recall 0.9813, auc 0.9807
epoch 6701, loss 0.0466, train acc 98.15%, f1 0.9815, precision 0.9810, recall 0.9821, auc 0.9815
epoch 6801, loss 0.0449, train acc 98.24%, f1 0.9824, precision 0.9819, recall 0.9829, auc 0.9824
epoch 6901, loss 0.0433, train acc 98.31%, f1 0.9832, precision 0.9827, recall 0.9836, auc 0.9831
epoch 7001, loss 0.0418, train acc 98.38%, f1 0.9838, precision 0.9833, recall 0.9843, auc 0.9838
epoch 7101, loss 0.0403, train acc 98.44%, f1 0.9844, precision 0.9839, recall 0.9849, auc 0.9844
epoch 7201, loss 0.0388, train acc 98.52%, f1 0.9852, precision 0.9849, recall 0.9856, auc 0.9852
epoch 7301, loss 0.0374, train acc 98.60%, f1 0.9860, precision 0.9856, recall 0.9863, auc 0.9860
epoch 7401, loss 0.0361, train acc 98.66%, f1 0.9866, precision 0.9862, recall 0.9870, auc 0.9866
epoch 7501, loss 0.0347, train acc 98.73%, f1 0.9873, precision 0.9868, recall 0.9878, auc 0.9873
epoch 7601, loss 0.0335, train acc 98.78%, f1 0.9878, precision 0.9873, recall 0.9883, auc 0.9878
epoch 7701, loss 0.0322, train acc 98.84%, f1 0.9884, precision 0.9878, recall 0.9890, auc 0.9884
epoch 7801, loss 0.0311, train acc 98.90%, f1 0.9890, precision 0.9885, recall 0.9896, auc 0.9890
epoch 7901, loss 0.0299, train acc 98.95%, f1 0.9895, precision 0.9890, recall 0.9900, auc 0.9895
epoch 8001, loss 0.0288, train acc 99.01%, f1 0.9901, precision 0.9897, recall 0.9904, auc 0.9901
epoch 8101, loss 0.0278, train acc 99.04%, f1 0.9904, precision 0.9900, recall 0.9909, auc 0.9904/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0268, train acc 99.08%, f1 0.9908, precision 0.9904, recall 0.9912, auc 0.9908
epoch 8301, loss 0.0258, train acc 99.12%, f1 0.9912, precision 0.9908, recall 0.9917, auc 0.9912
epoch 8401, loss 0.0248, train acc 99.17%, f1 0.9917, precision 0.9912, recall 0.9921, auc 0.9917
epoch 8501, loss 0.0238, train acc 99.22%, f1 0.9922, precision 0.9918, recall 0.9925, auc 0.9922
epoch 8601, loss 0.0230, train acc 99.25%, f1 0.9925, precision 0.9922, recall 0.9928, auc 0.9925
epoch 8701, loss 0.0221, train acc 99.28%, f1 0.9928, precision 0.9925, recall 0.9931, auc 0.9928
epoch 8801, loss 0.0213, train acc 99.31%, f1 0.9931, precision 0.9928, recall 0.9934, auc 0.9931
epoch 8901, loss 0.0205, train acc 99.35%, f1 0.9935, precision 0.9931, recall 0.9938, auc 0.9935
epoch 9001, loss 0.0198, train acc 99.38%, f1 0.9938, precision 0.9935, recall 0.9941, auc 0.9938
epoch 9101, loss 0.0191, train acc 99.41%, f1 0.9941, precision 0.9939, recall 0.9943, auc 0.9941
epoch 9201, loss 0.0184, train acc 99.44%, f1 0.9944, precision 0.9943, recall 0.9946, auc 0.9944
epoch 9301, loss 0.0177, train acc 99.47%, f1 0.9947, precision 0.9947, recall 0.9947, auc 0.9947
epoch 9401, loss 0.0171, train acc 99.49%, f1 0.9949, precision 0.9949, recall 0.9949, auc 0.9949
epoch 9501, loss 0.0165, train acc 99.51%, f1 0.9951, precision 0.9952, recall 0.9951, auc 0.9951
epoch 9601, loss 0.0159, train acc 99.54%, f1 0.9954, precision 0.9955, recall 0.9953, auc 0.9954
epoch 9701, loss 0.0154, train acc 99.56%, f1 0.9956, precision 0.9957, recall 0.9955, auc 0.9956
epoch 9801, loss 0.0148, train acc 99.58%, f1 0.9958, precision 0.9959, recall 0.9957, auc 0.9958
epoch 9901, loss 0.0143, train acc 99.60%, f1 0.9960, precision 0.9962, recall 0.9959, auc 0.9960
epoch 10001, loss 0.0138, train acc 99.62%, f1 0.9962, precision 0.9964, recall 0.9960, auc 0.9962
epoch 10101, loss 0.0134, train acc 99.63%, f1 0.9963, precision 0.9965, recall 0.9961, auc 0.9963
epoch 10201, loss 0.0129, train acc 99.64%, f1 0.9964, precision 0.9966, recall 0.9963, auc 0.9964
epoch 10301, loss 0.0123, train acc 99.66%, f1 0.9966, precision 0.9967, recall 0.9965, auc 0.9966
epoch 10401, loss 0.0120, train acc 99.68%, f1 0.9968, precision 0.9969, recall 0.9967, auc 0.9968
epoch 10501, loss 0.0116, train acc 99.70%, f1 0.9970, precision 0.9970, recall 0.9970, auc 0.9970
epoch 10601, loss 0.0111, train acc 99.71%, f1 0.9971, precision 0.9972, recall 0.9971, auc 0.9971
epoch 10701, loss 0.0107, train acc 99.73%, f1 0.9973, precision 0.9973, recall 0.9972, auc 0.9973
epoch 10801, loss 0.0103, train acc 99.74%, f1 0.9974, precision 0.9974, recall 0.9975, auc 0.9974
epoch 10901, loss 0.0099, train acc 99.75%, f1 0.9975, precision 0.9976, recall 0.9975, auc 0.9975
epoch 11001, loss 0.0095, train acc 99.78%, f1 0.9978, precision 0.9977, recall 0.9978, auc 0.9978
epoch 11101, loss 0.0092, train acc 99.80%, f1 0.9980, precision 0.9980, recall 0.9980, auc 0.9980
epoch 11201, loss 0.0088, train acc 99.81%, f1 0.9981, precision 0.9980, recall 0.9982, auc 0.9981
epoch 11301, loss 0.0085, train acc 99.82%, f1 0.9982, precision 0.9981, recall 0.9983, auc 0.9982
epoch 11401, loss 0.0081, train acc 99.83%, f1 0.9983, precision 0.9982, recall 0.9984, auc 0.9983
epoch 11501, loss 0.0078, train acc 99.84%, f1 0.9984, precision 0.9983, recall 0.9985, auc 0.9984
epoch 11601, loss 0.0076, train acc 99.85%, f1 0.9985, precision 0.9984, recall 0.9985, auc 0.9985
epoch 11701, loss 0.0073, train acc 99.85%, f1 0.9985, precision 0.9985, recall 0.9986, auc 0.9985
epoch 11801, loss 0.0070, train acc 99.86%, f1 0.9986, precision 0.9986, recall 0.9987, auc 0.9986
epoch 11901, loss 0.0068, train acc 99.87%, f1 0.9987, precision 0.9987, recall 0.9988, auc 0.9987
epoch 12001, loss 0.0066, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 12101, loss 0.0063, train acc 99.88%, f1 0.9988, precision 0.9988, recall 0.9989, auc 0.9988
epoch 12201, loss 0.0061, train acc 99.89%, f1 0.9989, precision 0.9988, recall 0.9989, auc 0.9989
epoch 12301, loss 0.0059, train acc 99.89%, f1 0.9989, precision 0.9989, recall 0.9990, auc 0.9989
epoch 12401, loss 0.0057, train acc 99.90%, f1 0.9990, precision 0.9989, recall 0.9990, auc 0.9990
epoch 12501, loss 0.0055, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12601, loss 0.0053, train acc 99.90%, f1 0.9990, precision 0.9990, recall 0.9990, auc 0.9990
epoch 12701, loss 0.0052, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9990, auc 0.9991
epoch 12801, loss 0.0050, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 12901, loss 0.0049, train acc 99.91%, f1 0.9991, precision 0.9991, recall 0.9991, auc 0.9991
epoch 13001, loss 0.0047, train acc 99.92%, f1 0.9992, precision 0.9992, recall 0.9991, auc 0.9992
epoch 13101, loss 0.0045, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9991, auc 0.9992
epoch 13201, loss 0.0044, train acc 99.92%, f1 0.9992, precision 0.9993, recall 0.9992, auc 0.9992
epoch 13301, loss 0.0043, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9992, auc 0.9993
epoch 13401, loss 0.0041, train acc 99.93%, f1 0.9993, precision 0.9994, recall 0.9993, auc 0.9993
epoch 13501, loss 0.0040, train acc 99.94%, f1 0.9994, precision 0.9994, recall 0.9993, auc 0.9994
epoch 13601, loss 0.0039, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9993, auc 0.9994
epoch 13701, loss 0.0037, train acc 99.94%, f1 0.9994, precision 0.9995, recall 0.9994, auc 0.9994
epoch 13801, loss 0.0036, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 13901, loss 0.0035, train acc 99.95%, f1 0.9995, precision 0.9995, recall 0.9994, auc 0.9995
epoch 14001, loss 0.0034, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 14101, loss 0.0033, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9994, auc 0.9995
epoch 14201, loss 0.0031, train acc 99.95%, f1 0.9995, precision 0.9996, recall 0.9995, auc 0.9995
epoch 14301, loss 0.0030, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 14401, loss 0.0029, train acc 99.96%, f1 0.9996, precision 0.9997, recall 0.9995, auc 0.9996
epoch 14501, loss 0.0028, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9996, auc 0.9997
epoch 14601, loss 0.0027, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 14701, loss 0.0026, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 14801, loss 0.0025, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
epoch 14901, loss 0.0024, train acc 99.97%, f1 0.9997, precision 0.9998, recall 0.9996, auc 0.9997
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_5.csv
./test_abalone19/standlization_data/abalone19_std_test_5.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_5
./test_abalone19/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.49758745476477684

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
./test_abalone19/standlization_data/abalone19_std_train_5.csv
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_5
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.5000
epoch 101, loss 0.5229, train acc 75.06%, f1 0.7505, precision 0.7506, recall 0.7504, auc 0.7506
epoch 201, loss 0.4400, train acc 79.36%, f1 0.7934, precision 0.7945, recall 0.7922, auc 0.7936
epoch 301, loss 0.3508, train acc 86.54%, f1 0.8654, precision 0.8656, recall 0.8651, auc 0.8654
epoch 401, loss 0.2813, train acc 90.48%, f1 0.9048, precision 0.9048, recall 0.9048, auc 0.9048
epoch 501, loss 0.2455, train acc 91.71%, f1 0.9171, precision 0.9173, recall 0.9170, auc 0.9171
epoch 601, loss 0.2280, train acc 92.17%, f1 0.9217, precision 0.9219, recall 0.9215, auc 0.9217
epoch 701, loss 0.2175, train acc 92.48%, f1 0.9248, precision 0.9250, recall 0.9245, auc 0.9248
epoch 801, loss 0.2127, train acc 92.63%, f1 0.9263, precision 0.9265, recall 0.9260, auc 0.9263
epoch 901, loss 0.2083, train acc 92.79%, f1 0.9279, precision 0.9282, recall 0.9276, auc 0.9279
epoch 1001, loss 0.2030, train acc 92.90%, f1 0.9290, precision 0.9294, recall 0.9287, auc 0.9290
epoch 1101, loss 0.1968, train acc 93.02%, f1 0.9302, precision 0.9306, recall 0.9298, auc 0.9302
epoch 1201, loss 0.1903, train acc 93.21%, f1 0.9321, precision 0.9326, recall 0.9315, auc 0.9321
epoch 1301, loss 0.1840, train acc 93.39%, f1 0.9339, precision 0.9344, recall 0.9333, auc 0.9339
epoch 1401, loss 0.1781, train acc 93.57%, f1 0.9356, precision 0.9361, recall 0.9352, auc 0.9357
epoch 1501, loss 0.1730, train acc 93.71%, f1 0.9370, precision 0.9376, recall 0.9365, auc 0.9371
epoch 1601, loss 0.1684, train acc 93.80%, f1 0.9379, precision 0.9385, recall 0.9374, auc 0.9380
epoch 1701, loss 0.1643, train acc 93.90%, f1 0.9390, precision 0.9394, recall 0.9386, auc 0.9390
epoch 1801, loss 0.1605, train acc 93.99%, f1 0.9399, precision 0.9405, recall 0.9393, auc 0.9399
epoch 1901, loss 0.1564, train acc 94.09%, f1 0.9409, precision 0.9415, recall 0.9402, auc 0.9409
epoch 2001, loss 0.1528, train acc 94.17%, f1 0.9416, precision 0.9424, recall 0.9408, auc 0.9417
epoch 2101, loss 0.1492, train acc 94.26%, f1 0.9425, precision 0.9436, recall 0.9415, auc 0.9426
epoch 2201, loss 0.1455, train acc 94.33%, f1 0.9432, precision 0.9443, recall 0.9422, auc 0.9433
epoch 2301, loss 0.1420, train acc 94.41%, f1 0.9440, precision 0.9450, recall 0.9431, auc 0.9441
epoch 2401, loss 0.1387, train acc 94.47%, f1 0.9447, precision 0.9456, recall 0.9438, auc 0.9447
epoch 2501, loss 0.1356, train acc 94.54%, f1 0.9454, precision 0.9461, recall 0.9447, auc 0.9454
epoch 2601, loss 0.1327, train acc 94.62%, f1 0.9462, precision 0.9469, recall 0.9455, auc 0.9462
epoch 2701, loss 0.1299, train acc 94.70%, f1 0.9469, precision 0.9476, recall 0.9463, auc 0.9470
epoch 2801, loss 0.1272, train acc 94.77%, f1 0.9477, precision 0.9483, recall 0.9470, auc 0.9477
epoch 2901, loss 0.1247, train acc 94.83%, f1 0.9483, precision 0.9487, recall 0.9478, auc 0.9483
epoch 3001, loss 0.1222, train acc 94.89%, f1 0.9489, precision 0.9492, recall 0.9485, auc 0.9489
epoch 3101, loss 0.1198, train acc 94.98%, f1 0.9497, precision 0.9500, recall 0.9495, auc 0.9498
epoch 3201, loss 0.1174, train acc 95.06%, f1 0.9506, precision 0.9508, recall 0.9503, auc 0.9506
epoch 3301, loss 0.1150, train acc 95.12%, f1 0.9512, precision 0.9515, recall 0.9510, auc 0.9512
epoch 3401, loss 0.1129, train acc 95.21%, f1 0.9521, precision 0.9522, recall 0.9519, auc 0.9521
epoch 3501, loss 0.1107, train acc 95.27%, f1 0.9527, precision 0.9529, recall 0.9526, auc 0.9527
epoch 3601, loss 0.1086, train acc 95.33%, f1 0.9533, precision 0.9535, recall 0.9531, auc 0.9533
epoch 3701, loss 0.1066, train acc 95.40%, f1 0.9540, precision 0.9542, recall 0.9539, auc 0.9540
epoch 3801, loss 0.1046, train acc 95.49%, f1 0.9548, precision 0.9550, recall 0.9547, auc 0.9549
epoch 3901, loss 0.1027, train acc 95.58%, f1 0.9557, precision 0.9560, recall 0.9555, auc 0.9558
epoch 4001, loss 0.1007, train acc 95.64%, f1 0.9564, precision 0.9565, recall 0.9564, auc 0.9564
epoch 4101, loss 0.0988, train acc 95.71%, f1 0.9571, precision 0.9570, recall 0.9572, auc 0.9571
epoch 4201, loss 0.0968, train acc 95.79%, f1 0.9580, precision 0.9577, recall 0.9582, auc 0.9579
epoch 4301, loss 0.0952, train acc 95.88%, f1 0.9588, precision 0.9586, recall 0.9589, auc 0.9588
epoch 4401, loss 0.0933, train acc 95.95%, f1 0.9595, precision 0.9594, recall 0.9597, auc 0.9595
epoch 4501, loss 0.0916, train acc 96.04%, f1 0.9604, precision 0.9603, recall 0.9604, auc 0.9604
epoch 4601, loss 0.0899, train acc 96.11%, f1 0.9611, precision 0.9610, recall 0.9613, auc 0.9611
epoch 4701, loss 0.0881, train acc 96.17%, f1 0.9618, precision 0.9615, recall 0.9620, auc 0.9617
epoch 4801, loss 0.0864, train acc 96.25%, f1 0.9625, precision 0.9623, recall 0.9627, auc 0.9625
epoch 4901, loss 0.0845, train acc 96.35%, f1 0.9635, precision 0.9633, recall 0.9637, auc 0.9635
epoch 5001, loss 0.0827, train acc 96.44%, f1 0.9644, precision 0.9644, recall 0.9643, auc 0.9644
epoch 5101, loss 0.0808, train acc 96.53%, f1 0.9653, precision 0.9654, recall 0.9652, auc 0.9653
epoch 5201, loss 0.0788, train acc 96.60%, f1 0.9660, precision 0.9662, recall 0.9659, auc 0.9660
epoch 5301, loss 0.0769, train acc 96.69%, f1 0.9669, precision 0.9672, recall 0.9667, auc 0.9669
epoch 5401, loss 0.0748, train acc 96.78%, f1 0.9678, precision 0.9680, recall 0.9676, auc 0.9678
epoch 5501, loss 0.0728, train acc 96.87%, f1 0.9687, precision 0.9689, recall 0.9684, auc 0.9687
epoch 5601, loss 0.0707, train acc 96.97%, f1 0.9697, precision 0.9699, recall 0.9694, auc 0.9697
epoch 5701, loss 0.0686, train acc 97.08%, f1 0.9708, precision 0.9712, recall 0.9704, auc 0.9708
epoch 5801, loss 0.0664, train acc 97.18%, f1 0.9718, precision 0.9721, recall 0.9716, auc 0.9718
epoch 5901, loss 0.0643, train acc 97.26%, f1 0.9726, precision 0.9728, recall 0.9724, auc 0.9726
epoch 6001, loss 0.0621, train acc 97.35%, f1 0.9735, precision 0.9739, recall 0.9731, auc 0.9735
epoch 6101, loss 0.0600, train acc 97.46%, f1 0.9746, precision 0.9751, recall 0.9741, auc 0.9746
epoch 6201, loss 0.0579, train acc 97.57%, f1 0.9757, precision 0.9762, recall 0.9753, auc 0.9757
epoch 6301, loss 0.0559, train acc 97.67%, f1 0.9767, precision 0.9773, recall 0.9761, auc 0.9767
epoch 6401, loss 0.0539, train acc 97.78%, f1 0.9778, precision 0.9784, recall 0.9772, auc 0.9778
epoch 6501, loss 0.0520, train acc 97.89%, f1 0.9789, precision 0.9794, recall 0.9784, auc 0.9789
epoch 6601, loss 0.0501, train acc 98.00%, f1 0.9800, precision 0.9806, recall 0.9794, auc 0.9800
epoch 6701, loss 0.0482, train acc 98.10%, f1 0.9810, precision 0.9816, recall 0.9804, auc 0.9810
epoch 6801, loss 0.0464, train acc 98.19%, f1 0.9819, precision 0.9825, recall 0.9813, auc 0.9819
epoch 6901, loss 0.0447, train acc 98.28%, f1 0.9828, precision 0.9835, recall 0.9821, auc 0.9828
epoch 7001, loss 0.0430, train acc 98.37%, f1 0.9837, precision 0.9843, recall 0.9831, auc 0.9837
epoch 7101, loss 0.0414, train acc 98.46%, f1 0.9846, precision 0.9852, recall 0.9841, auc 0.9846
epoch 7201, loss 0.0398, train acc 98.55%, f1 0.9855, precision 0.9861, recall 0.9849, auc 0.9855
epoch 7301, loss 0.0382, train acc 98.64%, f1 0.9864, precision 0.9870, recall 0.9859, auc 0.9864
epoch 7401, loss 0.0367, train acc 98.72%, f1 0.9871, precision 0.9878, recall 0.9864, auc 0.9872
epoch 7501, loss 0.0352, train acc 98.79%, f1 0.9879, precision 0.9886, recall 0.9872, auc 0.9879
epoch 7601, loss 0.0338, train acc 98.86%, f1 0.9886, precision 0.9892, recall 0.9880, auc 0.9886
epoch 7701, loss 0.0325, train acc 98.92%, f1 0.9891, precision 0.9897, recall 0.9886, auc 0.9892
epoch 7801, loss 0.0311, train acc 98.98%, f1 0.9898, precision 0.9904, recall 0.9892, auc 0.9898
epoch 7901, loss 0.0299, train acc 99.03%, f1 0.9903, precision 0.9909, recall 0.9897, auc 0.9903
epoch 8001, loss 0.0287, train acc 99.08%, f1 0.9908, precision 0.9914, recall 0.9903, auc 0.9908
epoch 8101, loss 0.0275, train acc 99.13%, f1 0.9913, precision 0.9918, recall 0.9908, auc 0.9913/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0264, train acc 99.18%, f1 0.9918, precision 0.9923, recall 0.9914, auc 0.9918
epoch 8301, loss 0.0253, train acc 99.23%, f1 0.9923, precision 0.9928, recall 0.9919, auc 0.9923
epoch 8401, loss 0.0243, train acc 99.29%, f1 0.9929, precision 0.9933, recall 0.9924, auc 0.9929
epoch 8501, loss 0.0233, train acc 99.33%, f1 0.9933, precision 0.9938, recall 0.9929, auc 0.9933
epoch 8601, loss 0.0224, train acc 99.38%, f1 0.9938, precision 0.9941, recall 0.9935, auc 0.9938
epoch 8701, loss 0.0215, train acc 99.42%, f1 0.9942, precision 0.9946, recall 0.9938, auc 0.9942
epoch 8801, loss 0.0206, train acc 99.46%, f1 0.9946, precision 0.9950, recall 0.9942, auc 0.9946
epoch 8901, loss 0.0197, train acc 99.49%, f1 0.9949, precision 0.9952, recall 0.9945, auc 0.9949
epoch 9001, loss 0.0189, train acc 99.52%, f1 0.9952, precision 0.9956, recall 0.9948, auc 0.9952
epoch 9101, loss 0.0180, train acc 99.56%, f1 0.9956, precision 0.9958, recall 0.9953, auc 0.9956
epoch 9201, loss 0.0172, train acc 99.59%, f1 0.9959, precision 0.9961, recall 0.9956, auc 0.9959
epoch 9301, loss 0.0166, train acc 99.61%, f1 0.9961, precision 0.9964, recall 0.9958, auc 0.9961
epoch 9401, loss 0.0159, train acc 99.64%, f1 0.9964, precision 0.9968, recall 0.9961, auc 0.9964
epoch 9501, loss 0.0152, train acc 99.66%, f1 0.9966, precision 0.9970, recall 0.9963, auc 0.9966
epoch 9601, loss 0.0146, train acc 99.69%, f1 0.9969, precision 0.9972, recall 0.9966, auc 0.9969
epoch 9701, loss 0.0140, train acc 99.71%, f1 0.9971, precision 0.9974, recall 0.9967, auc 0.9971
epoch 9801, loss 0.0134, train acc 99.72%, f1 0.9972, precision 0.9976, recall 0.9968, auc 0.9972
epoch 9901, loss 0.0129, train acc 99.74%, f1 0.9974, precision 0.9977, recall 0.9970, auc 0.9974
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_5.csv
./test_abalone19/standlization_data/abalone19_std_test_5.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_5
./test_abalone19/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.591556091676719

the Fscore is 0.1

the precision is 0.06666666666666667

the recall is 0.2

Done
train_mlp_7_1.sh: line 27: 20585 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=abalone19 dataset_index=5 record_index=1 device_id=7 train_method=MLP_concat_Mirror_8000
