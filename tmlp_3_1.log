nohup: ignoring input
./test_abalone19/standlization_data/abalone19_std_train_2.csv
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_2
----------------------



epoch 1, loss 0.6932, train acc 50.00%, f1 0.6666, precision 0.5000, recall 0.9998, auc 0.5000
epoch 101, loss 0.5073, train acc 76.33%, f1 0.7677, precision 0.7536, recall 0.7824, auc 0.7633
epoch 201, loss 0.4028, train acc 81.87%, f1 0.8181, precision 0.8205, recall 0.8158, auc 0.8187
epoch 301, loss 0.3035, train acc 89.20%, f1 0.8920, precision 0.8917, recall 0.8923, auc 0.8920
epoch 401, loss 0.2326, train acc 92.92%, f1 0.9292, precision 0.9294, recall 0.9290, auc 0.9292
epoch 501, loss 0.1933, train acc 94.16%, f1 0.9415, precision 0.9422, recall 0.9408, auc 0.9416
epoch 601, loss 0.1717, train acc 94.66%, f1 0.9465, precision 0.9476, recall 0.9455, auc 0.9466
epoch 701, loss 0.1588, train acc 94.97%, f1 0.9497, precision 0.9510, recall 0.9484, auc 0.9497
epoch 801, loss 0.1505, train acc 95.20%, f1 0.9520, precision 0.9535, recall 0.9504, auc 0.9520
epoch 901, loss 0.1450, train acc 95.35%, f1 0.9534, precision 0.9551, recall 0.9517, auc 0.9535
epoch 1001, loss 0.1411, train acc 95.45%, f1 0.9545, precision 0.9560, recall 0.9529, auc 0.9545
epoch 1101, loss 0.1381, train acc 95.50%, f1 0.9549, precision 0.9565, recall 0.9534, auc 0.9550
epoch 1201, loss 0.1351, train acc 95.53%, f1 0.9552, precision 0.9566, recall 0.9538, auc 0.9553
epoch 1301, loss 0.1318, train acc 95.55%, f1 0.9555, precision 0.9567, recall 0.9542, auc 0.9555
epoch 1401, loss 0.1286, train acc 95.58%, f1 0.9558, precision 0.9569, recall 0.9546, auc 0.9558
epoch 1501, loss 0.1257, train acc 95.63%, f1 0.9563, precision 0.9573, recall 0.9553, auc 0.9563
epoch 1601, loss 0.1229, train acc 95.68%, f1 0.9568, precision 0.9578, recall 0.9558, auc 0.9568
epoch 1701, loss 0.1201, train acc 95.75%, f1 0.9575, precision 0.9584, recall 0.9565, auc 0.9575
epoch 1801, loss 0.1173, train acc 95.82%, f1 0.9581, precision 0.9590, recall 0.9573, auc 0.9582
epoch 1901, loss 0.1146, train acc 95.87%, f1 0.9587, precision 0.9596, recall 0.9578, auc 0.9587
epoch 2001, loss 0.1119, train acc 95.92%, f1 0.9592, precision 0.9600, recall 0.9584, auc 0.9592
epoch 2101, loss 0.1092, train acc 95.99%, f1 0.9599, precision 0.9606, recall 0.9592, auc 0.9599
epoch 2201, loss 0.1066, train acc 96.05%, f1 0.9605, precision 0.9612, recall 0.9598, auc 0.9605
epoch 2301, loss 0.1040, train acc 96.09%, f1 0.9609, precision 0.9616, recall 0.9601, auc 0.9609
epoch 2401, loss 0.1016, train acc 96.16%, f1 0.9615, precision 0.9623, recall 0.9608, auc 0.9616
epoch 2501, loss 0.0992, train acc 96.19%, f1 0.9619, precision 0.9628, recall 0.9610, auc 0.9619
epoch 2601, loss 0.0969, train acc 96.24%, f1 0.9624, precision 0.9633, recall 0.9615, auc 0.9624
epoch 2701, loss 0.0947, train acc 96.29%, f1 0.9629, precision 0.9638, recall 0.9621, auc 0.9629
epoch 2801, loss 0.0926, train acc 96.35%, f1 0.9635, precision 0.9643, recall 0.9626, auc 0.9635
epoch 2901, loss 0.0905, train acc 96.39%, f1 0.9638, precision 0.9647, recall 0.9630, auc 0.9639
epoch 3001, loss 0.0884, train acc 96.45%, f1 0.9645, precision 0.9654, recall 0.9637, auc 0.9645
epoch 3101, loss 0.0864, train acc 96.52%, f1 0.9652, precision 0.9659, recall 0.9645, auc 0.9652
epoch 3201, loss 0.0845, train acc 96.60%, f1 0.9659, precision 0.9667, recall 0.9652, auc 0.9660
epoch 3301, loss 0.0826, train acc 96.64%, f1 0.9664, precision 0.9672, recall 0.9656, auc 0.9664
epoch 3401, loss 0.0808, train acc 96.71%, f1 0.9671, precision 0.9679, recall 0.9663, auc 0.9671
epoch 3501, loss 0.0791, train acc 96.78%, f1 0.9678, precision 0.9685, recall 0.9670, auc 0.9678
epoch 3601, loss 0.0774, train acc 96.84%, f1 0.9683, precision 0.9692, recall 0.9675, auc 0.9684
epoch 3701, loss 0.0758, train acc 96.89%, f1 0.9689, precision 0.9698, recall 0.9680, auc 0.9689
epoch 3801, loss 0.0742, train acc 96.96%, f1 0.9696, precision 0.9702, recall 0.9690, auc 0.9696
epoch 3901, loss 0.0727, train acc 97.02%, f1 0.9702, precision 0.9708, recall 0.9696, auc 0.9702
epoch 4001, loss 0.0712, train acc 97.07%, f1 0.9707, precision 0.9713, recall 0.9701, auc 0.9707
epoch 4101, loss 0.0698, train acc 97.11%, f1 0.9711, precision 0.9717, recall 0.9705, auc 0.9711
epoch 4201, loss 0.0684, train acc 97.15%, f1 0.9715, precision 0.9721, recall 0.9708, auc 0.9715
epoch 4301, loss 0.0671, train acc 97.21%, f1 0.9721, precision 0.9728, recall 0.9714, auc 0.9721
epoch 4401, loss 0.0658, train acc 97.26%, f1 0.9726, precision 0.9733, recall 0.9719, auc 0.9726
epoch 4501, loss 0.0645, train acc 97.30%, f1 0.9730, precision 0.9735, recall 0.9724, auc 0.9730
epoch 4601, loss 0.0633, train acc 97.35%, f1 0.9735, precision 0.9740, recall 0.9729, auc 0.9735
epoch 4701, loss 0.0620, train acc 97.40%, f1 0.9739, precision 0.9745, recall 0.9734, auc 0.9740
epoch 4801, loss 0.0608, train acc 97.46%, f1 0.9745, precision 0.9750, recall 0.9741, auc 0.9746
epoch 4901, loss 0.0596, train acc 97.51%, f1 0.9751, precision 0.9755, recall 0.9748, auc 0.9751
epoch 5001, loss 0.0584, train acc 97.56%, f1 0.9756, precision 0.9759, recall 0.9752, auc 0.9756
epoch 5101, loss 0.0573, train acc 97.59%, f1 0.9759, precision 0.9763, recall 0.9756, auc 0.9759
epoch 5201, loss 0.0561, train acc 97.65%, f1 0.9765, precision 0.9769, recall 0.9761, auc 0.9765
epoch 5301, loss 0.0549, train acc 97.71%, f1 0.9771, precision 0.9775, recall 0.9767, auc 0.9771
epoch 5401, loss 0.0537, train acc 97.75%, f1 0.9775, precision 0.9780, recall 0.9770, auc 0.9775
epoch 5501, loss 0.0525, train acc 97.81%, f1 0.9781, precision 0.9786, recall 0.9776, auc 0.9781
epoch 5601, loss 0.0514, train acc 97.87%, f1 0.9787, precision 0.9792, recall 0.9781, auc 0.9787
epoch 5701, loss 0.0502, train acc 97.93%, f1 0.9792, precision 0.9799, recall 0.9786, auc 0.9793
epoch 5801, loss 0.0489, train acc 97.98%, f1 0.9798, precision 0.9804, recall 0.9791, auc 0.9798
epoch 5901, loss 0.0477, train acc 98.03%, f1 0.9803, precision 0.9808, recall 0.9798, auc 0.9803
epoch 6001, loss 0.0465, train acc 98.09%, f1 0.9809, precision 0.9815, recall 0.9803, auc 0.9809
epoch 6101, loss 0.0452, train acc 98.15%, f1 0.9814, precision 0.9820, recall 0.9809, auc 0.9815
epoch 6201, loss 0.0440, train acc 98.22%, f1 0.9822, precision 0.9828, recall 0.9816, auc 0.9822
epoch 6301, loss 0.0427, train acc 98.28%, f1 0.9828, precision 0.9835, recall 0.9821, auc 0.9828
epoch 6401, loss 0.0414, train acc 98.35%, f1 0.9834, precision 0.9842, recall 0.9827, auc 0.9835
epoch 6501, loss 0.0402, train acc 98.42%, f1 0.9842, precision 0.9849, recall 0.9835, auc 0.9842
epoch 6601, loss 0.0389, train acc 98.49%, f1 0.9849, precision 0.9856, recall 0.9842, auc 0.9849
epoch 6701, loss 0.0376, train acc 98.56%, f1 0.9856, precision 0.9863, recall 0.9848, auc 0.9856
epoch 6801, loss 0.0364, train acc 98.62%, f1 0.9862, precision 0.9869, recall 0.9855, auc 0.9862
epoch 6901, loss 0.0351, train acc 98.68%, f1 0.9868, precision 0.9875, recall 0.9861, auc 0.9868
epoch 7001, loss 0.0339, train acc 98.74%, f1 0.9874, precision 0.9881, recall 0.9867, auc 0.9874
epoch 7101, loss 0.0327, train acc 98.80%, f1 0.9880, precision 0.9886, recall 0.9873, auc 0.9880
epoch 7201, loss 0.0315, train acc 98.86%, f1 0.9886, precision 0.9892, recall 0.9880, auc 0.9886
epoch 7301, loss 0.0303, train acc 98.92%, f1 0.9892, precision 0.9897, recall 0.9886, auc 0.9892
epoch 7401, loss 0.0292, train acc 98.97%, f1 0.9897, precision 0.9902, recall 0.9892, auc 0.9897
epoch 7501, loss 0.0281, train acc 99.01%, f1 0.9901, precision 0.9906, recall 0.9895, auc 0.9901
epoch 7601, loss 0.0271, train acc 99.05%, f1 0.9905, precision 0.9911, recall 0.9900, auc 0.9905
epoch 7701, loss 0.0261, train acc 99.10%, f1 0.9910, precision 0.9915, recall 0.9905, auc 0.9910
epoch 7801, loss 0.0251, train acc 99.15%, f1 0.9915, precision 0.9921, recall 0.9909, auc 0.9915
epoch 7901, loss 0.0242, train acc 99.19%, f1 0.9919, precision 0.9926, recall 0.9913, auc 0.9919
epoch 8001, loss 0.0233, train acc 99.24%, f1 0.9924, precision 0.9931, recall 0.9917, auc 0.9924
epoch 8101, loss 0.0224, train acc 99.28%, f1 0.9928, precision 0.9936, recall 0.9919, auc 0.9928
epoch 8201, loss 0.0216, train acc 99.33%, f1 0.9933, precision 0.9942, recall 0.9924, auc 0.9933
epoch 8301, loss 0.0207, train acc 99.36%, f1 0.9936, precision 0.9945, recall 0.9928, auc 0.9936
epoch 8401, loss 0.0199, train acc 99.40%, f1 0.9940, precision 0.9950, recall 0.9929, auc 0.9940
epoch 8501, loss 0.0191, train acc 99.43%, f1 0.9943, precision 0.9954, recall 0.9932, auc 0.9943
epoch 8601, loss 0.0183, train acc 99.46%, f1 0.9946, precision 0.9958, recall 0.9935, auc 0.9946
epoch 8701, loss 0.0175, train acc 99.49%, f1 0.9949, precision 0.9961, recall 0.9937, auc 0.9949
epoch 8801, loss 0.0168, train acc 99.52%, f1 0.9952, precision 0.9964, recall 0.9940, auc 0.9952
epoch 8901, loss 0.0161, train acc 99.55%, f1 0.9955, precision 0.9967, recall 0.9944, auc 0.9955
epoch 9001, loss 0.0154, train acc 99.57%, f1 0.9957, precision 0.9969, recall 0.9945, auc 0.9957
epoch 9101, loss 0.0147, train acc 99.60%, f1 0.9960, precision 0.9971, recall 0.9948, auc 0.9960
epoch 9201, loss 0.0141, train acc 99.63%, f1 0.9963, precision 0.9974, recall 0.9951, auc 0.9963
epoch 9301, loss 0.0135, train acc 99.65%, f1 0.9965, precision 0.9975, recall 0.9954, auc 0.9965
epoch 9401, loss 0.0129, train acc 99.67%, f1 0.9967, precision 0.9978, recall 0.9957, auc 0.9967
epoch 9501, loss 0.0123, train acc 99.70%, f1 0.9970, precision 0.9980, recall 0.9959, auc 0.9970
epoch 9601, loss 0.0117, train acc 99.72%, f1 0.9971, precision 0.9981, recall 0.9962, auc 0.9972
epoch 9701, loss 0.0112, train acc 99.74%, f1 0.9974, precision 0.9983, recall 0.9964, auc 0.9974
epoch 9801, loss 0.0106, train acc 99.76%, f1 0.9976, precision 0.9986, recall 0.9966, auc 0.9976
epoch 9901, loss 0.0101, train acc 99.78%, f1 0.9978, precision 0.9987, recall 0.9969, auc 0.9978
epoch 10001, loss 0.0096, train acc 99.80%, f1 0.9980, precision 0.9989, recall 0.9970, auc 0.9980
epoch 10101, loss 0.0091, train acc 99.81%, f1 0.9981, precision 0.9991, recall 0.9972, auc 0.9981
epoch 10201, loss 0.0086, train acc 99.83%, f1 0.9983, precision 0.9992, recall 0.9974, auc 0.9983
epoch 10301, loss 0.0082, train acc 99.84%, f1 0.9984, precision 0.9992, recall 0.9976, auc 0.9984
epoch 10401, loss 0.0078, train acc 99.85%, f1 0.9985, precision 0.9993, recall 0.9977, auc 0.9985
epoch 10501, loss 0.0074, train acc 99.86%, f1 0.9986, precision 0.9994, recall 0.9978, auc 0.9986
epoch 10601, loss 0.0070, train acc 99.87%, f1 0.9987, precision 0.9994, recall 0.9980, auc 0.9987
epoch 10701, loss 0.0067, train acc 99.88%, f1 0.9988, precision 0.9995, recall 0.9981, auc 0.9988
epoch 10801, loss 0.0064, train acc 99.89%, f1 0.9989, precision 0.9995, recall 0.9982, auc 0.9989
epoch 10901, loss 0.0061, train acc 99.89%, f1 0.9989, precision 0.9996, recall 0.9983, auc 0.9989
epoch 11001, loss 0.0058, train acc 99.90%, f1 0.9990, precision 0.9996, recall 0.9984, auc 0.9990
epoch 11101, loss 0.0055, train acc 99.91%, f1 0.9991, precision 0.9996, recall 0.9985, auc 0.9991
epoch 11201, loss 0.0053, train acc 99.92%, f1 0.9992, precision 0.9997, recall 0.9986, auc 0.9992
epoch 11301, loss 0.0050, train acc 99.92%, f1 0.9992, precision 0.9997, recall 0.9987, auc 0.9992
epoch 11401, loss 0.0048, train acc 99.92%, f1 0.9992, precision 0.9997, recall 0.9987, auc 0.9992
epoch 11501, loss 0.0046, train acc 99.93%, f1 0.9993, precision 0.9998, recall 0.9988, auc 0.9993
epoch 11601, loss 0.0044, train acc 99.93%, f1 0.9993, precision 0.9998, recall 0.9989, auc 0.9993
epoch 11701, loss 0.0042, train acc 99.94%, f1 0.9994, precision 0.9998, recall 0.9990, auc 0.9994
epoch 11801, loss 0.0040, train acc 99.94%, f1 0.9994, precision 0.9998, recall 0.9990, auc 0.9994
epoch 11901, loss 0.0039, train acc 99.94%, f1 0.9994, precision 0.9998, recall 0.9991, auc 0.9994
epoch 12001, loss 0.0037, train acc 99.95%, f1 0.9995, precision 0.9999, recall 0.9992, auc 0.9995
epoch 12101, loss 0.0036, train acc 99.95%, f1 0.9995, precision 0.9999, recall 0.9992, auc 0.9995
epoch 12201, loss 0.0034, train acc 99.96%, f1 0.9996, precision 0.9999, recall 0.9993, auc 0.9996
epoch 12301, loss 0.0033, train acc 99.96%, f1 0.9996, precision 0.9999, recall 0.9993, auc 0.9996
epoch 12401, loss 0.0031, train acc 99.96%, f1 0.9996, precision 0.9999, recall 0.9994, auc 0.9996
epoch 12501, loss 0.0030, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9994, auc 0.9997
epoch 12601, loss 0.0029, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9995, auc 0.9997
epoch 12701, loss 0.0028, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9995, auc 0.9997
epoch 12801, loss 0.0027, train acc 99.97%, f1 0.9997, precision 0.9999, recall 0.9995, auc 0.9997
epoch 12901, loss 0.0026, train acc 99.98%, f1 0.9998, precision 0.9999, recall 0.9996, auc 0.9998
epoch 13001, loss 0.0025, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9996, auc 0.9998
epoch 13101, loss 0.0024, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9996, auc 0.9998
epoch 13201, loss 0.0023, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9996, auc 0.9998
epoch 13301, loss 0.0022, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9997, auc 0.9998
epoch 13401, loss 0.0021, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9997, auc 0.9998
epoch 13501, loss 0.0020, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9997, auc 0.9998
epoch 13601, loss 0.0019, train acc 99.98%, f1 0.9998, precision 1.0000, recall 0.9997, auc 0.9998
epoch 13701, loss 0.0019, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9997, auc 0.9999
epoch 13801, loss 0.0018, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9997, auc 0.9999
epoch 13901, loss 0.0017, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9997, auc 0.9999
epoch 14001, loss 0.0016, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9997, auc 0.9999
epoch 14101, loss 0.0016, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 14201, loss 0.0015, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 14301, loss 0.0014, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 14401, loss 0.0014, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 14501, loss 0.0013, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 14601, loss 0.0013, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 14701, loss 0.0012, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 14801, loss 0.0012, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 14901, loss 0.0011, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9998, auc 0.9999
epoch 15001, loss 0.0011, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 15101, loss 0.0010, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 15201, loss 0.0010, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 15301, loss 0.0009, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 15401, loss 0.0009, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 15501, loss 0.0009, train acc 99.99%, f1 0.9999, precision 1.0000, recall 0.9999, auc 0.9999
epoch 15601, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 15701, loss 0.0008, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 15801, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 15901, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 16001, loss 0.0007, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 16101, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 16201, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 16301, loss 0.0006, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000
epoch 16401, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 0.9999, auc 1.0000/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 16501, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16601, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16701, loss 0.0005, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16801, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16901, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17101, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17201, loss 0.0004, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17301, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17401, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17601, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17701, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17801, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17901, loss 0.0003, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18101, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18201, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18301, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18401, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18601, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18701, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18801, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18901, loss 0.0002, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19101, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19201, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19301, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19401, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19601, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19701, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19801, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19901, loss 0.0001, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_abalone19/standlization_data/abalone19_std_train_2.csv
./test_abalone19/standlization_data/abalone19_std_test_2.csv
MLP_concat_Mirror_20000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_20000/record_1/MLP_concat_Mirror_20000_2
./test_abalone19/result_MLP_concat_Mirror_20000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
./test_abalone19/standlization_data/abalone19_std_train_2.csv
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_2
----------------------



epoch 1, loss 0.6933, train acc 50.00%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 101, loss 0.5075, train acc 76.46%, f1 0.7641, precision 0.7657, recall 0.7626, auc 0.7646
epoch 201, loss 0.4012, train acc 81.97%, f1 0.8202, precision 0.8181, recall 0.8223, auc 0.8197
epoch 301, loss 0.3012, train acc 89.35%, f1 0.8935, precision 0.8934, recall 0.8935, auc 0.8935
epoch 401, loss 0.2308, train acc 92.98%, f1 0.9298, precision 0.9297, recall 0.9299, auc 0.9298
epoch 501, loss 0.1921, train acc 94.19%, f1 0.9419, precision 0.9416, recall 0.9423, auc 0.9419
epoch 601, loss 0.1709, train acc 94.67%, f1 0.9467, precision 0.9462, recall 0.9472, auc 0.9467
epoch 701, loss 0.1581, train acc 94.99%, f1 0.9499, precision 0.9494, recall 0.9504, auc 0.9499
epoch 801, loss 0.1500, train acc 95.21%, f1 0.9521, precision 0.9514, recall 0.9528, auc 0.9521
epoch 901, loss 0.1446, train acc 95.36%, f1 0.9536, precision 0.9528, recall 0.9545, auc 0.9536
epoch 1001, loss 0.1408, train acc 95.46%, f1 0.9546, precision 0.9539, recall 0.9553, auc 0.9546
epoch 1101, loss 0.1378, train acc 95.49%, f1 0.9549, precision 0.9541, recall 0.9556, auc 0.9549
epoch 1201, loss 0.1346, train acc 95.53%, f1 0.9553, precision 0.9545, recall 0.9561, auc 0.9553
epoch 1301, loss 0.1312, train acc 95.57%, f1 0.9557, precision 0.9550, recall 0.9564, auc 0.9557
epoch 1401, loss 0.1279, train acc 95.61%, f1 0.9561, precision 0.9555, recall 0.9566, auc 0.9561
epoch 1501, loss 0.1250, train acc 95.65%, f1 0.9565, precision 0.9560, recall 0.9570, auc 0.9565
epoch 1601, loss 0.1221, train acc 95.72%, f1 0.9572, precision 0.9566, recall 0.9578, auc 0.9572
epoch 1701, loss 0.1194, train acc 95.79%, f1 0.9579, precision 0.9574, recall 0.9585, auc 0.9579
epoch 1801, loss 0.1168, train acc 95.85%, f1 0.9585, precision 0.9581, recall 0.9590, auc 0.9585
epoch 1901, loss 0.1143, train acc 95.90%, f1 0.9590, precision 0.9586, recall 0.9593, auc 0.9590
epoch 2001, loss 0.1117, train acc 95.95%, f1 0.9595, precision 0.9590, recall 0.9599, auc 0.9595
epoch 2101, loss 0.1091, train acc 96.01%, f1 0.9601, precision 0.9596, recall 0.9606, auc 0.9601
epoch 2201, loss 0.1064, train acc 96.05%, f1 0.9606, precision 0.9600, recall 0.9611, auc 0.9605
epoch 2301, loss 0.1038, train acc 96.12%, f1 0.9612, precision 0.9606, recall 0.9618, auc 0.9612
epoch 2401, loss 0.1013, train acc 96.16%, f1 0.9616, precision 0.9610, recall 0.9622, auc 0.9616
epoch 2501, loss 0.0989, train acc 96.21%, f1 0.9622, precision 0.9615, recall 0.9629, auc 0.9621
epoch 2601, loss 0.0966, train acc 96.26%, f1 0.9627, precision 0.9620, recall 0.9633, auc 0.9626
epoch 2701, loss 0.0944, train acc 96.32%, f1 0.9632, precision 0.9625, recall 0.9640, auc 0.9632
epoch 2801, loss 0.0923, train acc 96.38%, f1 0.9638, precision 0.9631, recall 0.9645, auc 0.9638
epoch 2901, loss 0.0902, train acc 96.42%, f1 0.9642, precision 0.9635, recall 0.9649, auc 0.9642
epoch 3001, loss 0.0881, train acc 96.48%, f1 0.9648, precision 0.9641, recall 0.9656, auc 0.9648
epoch 3101, loss 0.0862, train acc 96.55%, f1 0.9656, precision 0.9649, recall 0.9663, auc 0.9655
epoch 3201, loss 0.0842, train acc 96.61%, f1 0.9661, precision 0.9655, recall 0.9668, auc 0.9661
epoch 3301, loss 0.0824, train acc 96.66%, f1 0.9667, precision 0.9661, recall 0.9672, auc 0.9666
epoch 3401, loss 0.0805, train acc 96.74%, f1 0.9674, precision 0.9669, recall 0.9678, auc 0.9674
epoch 3501, loss 0.0788, train acc 96.79%, f1 0.9680, precision 0.9675, recall 0.9685, auc 0.9679
epoch 3601, loss 0.0771, train acc 96.85%, f1 0.9685, precision 0.9679, recall 0.9691, auc 0.9685
epoch 3701, loss 0.0754, train acc 96.91%, f1 0.9691, precision 0.9685, recall 0.9696, auc 0.9691
epoch 3801, loss 0.0738, train acc 96.97%, f1 0.9697, precision 0.9693, recall 0.9701, auc 0.9697
epoch 3901, loss 0.0722, train acc 97.02%, f1 0.9702, precision 0.9698, recall 0.9707, auc 0.9702
epoch 4001, loss 0.0707, train acc 97.08%, f1 0.9708, precision 0.9704, recall 0.9713, auc 0.9708
epoch 4101, loss 0.0692, train acc 97.14%, f1 0.9714, precision 0.9712, recall 0.9717, auc 0.9714
epoch 4201, loss 0.0677, train acc 97.21%, f1 0.9721, precision 0.9718, recall 0.9724, auc 0.9721
epoch 4301, loss 0.0663, train acc 97.26%, f1 0.9726, precision 0.9723, recall 0.9729, auc 0.9726
epoch 4401, loss 0.0648, train acc 97.31%, f1 0.9731, precision 0.9728, recall 0.9734, auc 0.9731
epoch 4501, loss 0.0634, train acc 97.37%, f1 0.9737, precision 0.9733, recall 0.9740, auc 0.9737
epoch 4601, loss 0.0621, train acc 97.42%, f1 0.9742, precision 0.9736, recall 0.9747, auc 0.9742
epoch 4701, loss 0.0607, train acc 97.47%, f1 0.9747, precision 0.9743, recall 0.9751, auc 0.9747
epoch 4801, loss 0.0594, train acc 97.54%, f1 0.9754, precision 0.9749, recall 0.9758, auc 0.9754
epoch 4901, loss 0.0582, train acc 97.58%, f1 0.9758, precision 0.9755, recall 0.9761, auc 0.9758
epoch 5001, loss 0.0569, train acc 97.62%, f1 0.9762, precision 0.9759, recall 0.9764, auc 0.9762
epoch 5101, loss 0.0557, train acc 97.67%, f1 0.9767, precision 0.9764, recall 0.9770, auc 0.9767
epoch 5201, loss 0.0545, train acc 97.73%, f1 0.9773, precision 0.9770, recall 0.9777, auc 0.9773
epoch 5301, loss 0.0534, train acc 97.77%, f1 0.9777, precision 0.9775, recall 0.9779, auc 0.9777
epoch 5401, loss 0.0522, train acc 97.82%, f1 0.9782, precision 0.9780, recall 0.9784, auc 0.9782
epoch 5501, loss 0.0511, train acc 97.87%, f1 0.9787, precision 0.9785, recall 0.9789, auc 0.9787
epoch 5601, loss 0.0499, train acc 97.91%, f1 0.9791, precision 0.9789, recall 0.9793, auc 0.9791
epoch 5701, loss 0.0488, train acc 97.96%, f1 0.9796, precision 0.9795, recall 0.9797, auc 0.9796
epoch 5801, loss 0.0477, train acc 98.01%, f1 0.9801, precision 0.9801, recall 0.9802, auc 0.9801
epoch 5901, loss 0.0465, train acc 98.07%, f1 0.9807, precision 0.9808, recall 0.9807, auc 0.9807
epoch 6001, loss 0.0453, train acc 98.13%, f1 0.9813, precision 0.9814, recall 0.9811, auc 0.9813
epoch 6101, loss 0.0441, train acc 98.19%, f1 0.9819, precision 0.9819, recall 0.9819, auc 0.9819
epoch 6201, loss 0.0429, train acc 98.23%, f1 0.9823, precision 0.9824, recall 0.9822, auc 0.9823
epoch 6301, loss 0.0417, train acc 98.27%, f1 0.9827, precision 0.9828, recall 0.9826, auc 0.9827
epoch 6401, loss 0.0405, train acc 98.34%, f1 0.9834, precision 0.9836, recall 0.9832, auc 0.9834
epoch 6501, loss 0.0393, train acc 98.40%, f1 0.9840, precision 0.9843, recall 0.9838, auc 0.9840
epoch 6601, loss 0.0381, train acc 98.48%, f1 0.9848, precision 0.9851, recall 0.9845, auc 0.9848
epoch 6701, loss 0.0369, train acc 98.54%, f1 0.9854, precision 0.9857, recall 0.9851, auc 0.9854
epoch 6801, loss 0.0357, train acc 98.60%, f1 0.9860, precision 0.9862, recall 0.9858, auc 0.9860
epoch 6901, loss 0.0345, train acc 98.66%, f1 0.9866, precision 0.9867, recall 0.9864, auc 0.9866
epoch 7001, loss 0.0334, train acc 98.70%, f1 0.9870, precision 0.9873, recall 0.9868, auc 0.9870
epoch 7101, loss 0.0323, train acc 98.75%, f1 0.9875, precision 0.9878, recall 0.9873, auc 0.9875
epoch 7201, loss 0.0312, train acc 98.80%, f1 0.9880, precision 0.9883, recall 0.9877, auc 0.9880
epoch 7301, loss 0.0301, train acc 98.85%, f1 0.9885, precision 0.9888, recall 0.9881, auc 0.9885
epoch 7401, loss 0.0291, train acc 98.91%, f1 0.9891, precision 0.9893, recall 0.9888, auc 0.9891
epoch 7501, loss 0.0282, train acc 98.95%, f1 0.9895, precision 0.9897, recall 0.9892, auc 0.9895
epoch 7601, loss 0.0272, train acc 99.00%, f1 0.9900, precision 0.9901, recall 0.9898, auc 0.9900
epoch 7701, loss 0.0263, train acc 99.04%, f1 0.9904, precision 0.9905, recall 0.9903, auc 0.9904
epoch 7801, loss 0.0253, train acc 99.09%, f1 0.9909, precision 0.9910, recall 0.9907, auc 0.9909
epoch 7901, loss 0.0245, train acc 99.13%, f1 0.9913, precision 0.9913, recall 0.9913, auc 0.9913
epoch 8001, loss 0.0236, train acc 99.17%, f1 0.9917, precision 0.9918, recall 0.9917, auc 0.9917
epoch 8101, loss 0.0227, train acc 99.21%, f1 0.9921, precision 0.9921, recall 0.9920, auc 0.9921/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

epoch 8201, loss 0.0219, train acc 99.25%, f1 0.9925, precision 0.9924, recall 0.9926, auc 0.9925
epoch 8301, loss 0.0211, train acc 99.28%, f1 0.9928, precision 0.9927, recall 0.9929, auc 0.9928
epoch 8401, loss 0.0203, train acc 99.31%, f1 0.9931, precision 0.9929, recall 0.9934, auc 0.9931
epoch 8501, loss 0.0195, train acc 99.35%, f1 0.9935, precision 0.9932, recall 0.9937, auc 0.9935
epoch 8601, loss 0.0188, train acc 99.38%, f1 0.9938, precision 0.9935, recall 0.9941, auc 0.9938
epoch 8701, loss 0.0180, train acc 99.42%, f1 0.9942, precision 0.9940, recall 0.9944, auc 0.9942
epoch 8801, loss 0.0173, train acc 99.46%, f1 0.9946, precision 0.9944, recall 0.9948, auc 0.9946
epoch 8901, loss 0.0166, train acc 99.49%, f1 0.9949, precision 0.9948, recall 0.9950, auc 0.9949
epoch 9001, loss 0.0159, train acc 99.51%, f1 0.9951, precision 0.9949, recall 0.9952, auc 0.9951
epoch 9101, loss 0.0153, train acc 99.53%, f1 0.9953, precision 0.9953, recall 0.9954, auc 0.9953
epoch 9201, loss 0.0146, train acc 99.56%, f1 0.9956, precision 0.9955, recall 0.9957, auc 0.9956
epoch 9301, loss 0.0140, train acc 99.59%, f1 0.9959, precision 0.9958, recall 0.9961, auc 0.9959
epoch 9401, loss 0.0134, train acc 99.62%, f1 0.9962, precision 0.9960, recall 0.9964, auc 0.9962
epoch 9501, loss 0.0128, train acc 99.64%, f1 0.9964, precision 0.9962, recall 0.9965, auc 0.9964
epoch 9601, loss 0.0122, train acc 99.66%, f1 0.9966, precision 0.9965, recall 0.9968, auc 0.9966
epoch 9701, loss 0.0117, train acc 99.69%, f1 0.9969, precision 0.9967, recall 0.9971, auc 0.9969
epoch 9801, loss 0.0111, train acc 99.72%, f1 0.9972, precision 0.9970, recall 0.9974, auc 0.9972
epoch 9901, loss 0.0105, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9975, auc 0.9973
epoch 10001, loss 0.0100, train acc 99.75%, f1 0.9975, precision 0.9972, recall 0.9977, auc 0.9975
epoch 10101, loss 0.0095, train acc 99.77%, f1 0.9977, precision 0.9975, recall 0.9980, auc 0.9977
epoch 10201, loss 0.0091, train acc 99.79%, f1 0.9979, precision 0.9976, recall 0.9982, auc 0.9979
epoch 10301, loss 0.0086, train acc 99.81%, f1 0.9981, precision 0.9978, recall 0.9983, auc 0.9981
epoch 10401, loss 0.0082, train acc 99.82%, f1 0.9982, precision 0.9979, recall 0.9985, auc 0.9982
epoch 10501, loss 0.0078, train acc 99.83%, f1 0.9983, precision 0.9981, recall 0.9985, auc 0.9983
epoch 10601, loss 0.0075, train acc 99.84%, f1 0.9984, precision 0.9982, recall 0.9986, auc 0.9984
epoch 10701, loss 0.0071, train acc 99.85%, f1 0.9985, precision 0.9983, recall 0.9988, auc 0.9985
epoch 10801, loss 0.0068, train acc 99.86%, f1 0.9986, precision 0.9984, recall 0.9988, auc 0.9986
epoch 10901, loss 0.0065, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9989, auc 0.9987
epoch 11001, loss 0.0062, train acc 99.87%, f1 0.9987, precision 0.9985, recall 0.9990, auc 0.9987
epoch 11101, loss 0.0059, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 11201, loss 0.0056, train acc 99.89%, f1 0.9989, precision 0.9987, recall 0.9991, auc 0.9989
epoch 11301, loss 0.0054, train acc 99.90%, f1 0.9990, precision 0.9988, recall 0.9992, auc 0.9990
epoch 11401, loss 0.0051, train acc 99.91%, f1 0.9991, precision 0.9989, recall 0.9993, auc 0.9991
epoch 11501, loss 0.0049, train acc 99.92%, f1 0.9992, precision 0.9990, recall 0.9993, auc 0.9992
epoch 11601, loss 0.0047, train acc 99.92%, f1 0.9992, precision 0.9991, recall 0.9994, auc 0.9992
epoch 11701, loss 0.0045, train acc 99.93%, f1 0.9993, precision 0.9992, recall 0.9994, auc 0.9993
epoch 11801, loss 0.0042, train acc 99.93%, f1 0.9993, precision 0.9993, recall 0.9994, auc 0.9993
epoch 11901, loss 0.0040, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 12001, loss 0.0038, train acc 99.94%, f1 0.9994, precision 0.9993, recall 0.9995, auc 0.9994
epoch 12101, loss 0.0037, train acc 99.95%, f1 0.9995, precision 0.9994, recall 0.9996, auc 0.9995
epoch 12201, loss 0.0035, train acc 99.96%, f1 0.9996, precision 0.9995, recall 0.9996, auc 0.9996
epoch 12301, loss 0.0033, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 12401, loss 0.0031, train acc 99.96%, f1 0.9996, precision 0.9996, recall 0.9997, auc 0.9996
epoch 12501, loss 0.0030, train acc 99.97%, f1 0.9997, precision 0.9996, recall 0.9997, auc 0.9997
epoch 12601, loss 0.0028, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9997, auc 0.9997
epoch 12701, loss 0.0027, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 12801, loss 0.0025, train acc 99.97%, f1 0.9997, precision 0.9997, recall 0.9998, auc 0.9997
epoch 12901, loss 0.0024, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 13001, loss 0.0023, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 13101, loss 0.0022, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 13201, loss 0.0021, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 13301, loss 0.0020, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 13401, loss 0.0019, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 13501, loss 0.0018, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 13601, loss 0.0017, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 13701, loss 0.0016, train acc 99.98%, f1 0.9998, precision 0.9997, recall 0.9999, auc 0.9998
epoch 13801, loss 0.0015, train acc 99.98%, f1 0.9998, precision 0.9998, recall 0.9999, auc 0.9998
epoch 13901, loss 0.0015, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 14001, loss 0.0014, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 14101, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 14201, loss 0.0013, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 14301, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9998, recall 1.0000, auc 0.9999
epoch 14401, loss 0.0012, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14501, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14601, loss 0.0011, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14701, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14801, loss 0.0010, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
epoch 14901, loss 0.0009, train acc 99.99%, f1 0.9999, precision 0.9999, recall 1.0000, auc 0.9999
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_2.csv
./test_abalone19/standlization_data/abalone19_std_test_2.csv
MLP_concat_Mirror_15000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_15000/record_1/MLP_concat_Mirror_15000_2
./test_abalone19/result_MLP_concat_Mirror_15000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.4987951807228916

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
./test_abalone19/standlization_data/abalone19_std_train_2.csv
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_2
----------------------



epoch 1, loss 0.6932, train acc 50.05%, f1 0.6668, precision 0.5002, recall 0.9997, auc 0.5005
epoch 101, loss 0.5051, train acc 76.51%, f1 0.7651, precision 0.7652, recall 0.7651, auc 0.7651
epoch 201, loss 0.4006, train acc 81.98%, f1 0.8199, precision 0.8194, recall 0.8204, auc 0.8198
epoch 301, loss 0.3012, train acc 89.31%, f1 0.8931, precision 0.8932, recall 0.8930, auc 0.8931
epoch 401, loss 0.2308, train acc 92.97%, f1 0.9297, precision 0.9297, recall 0.9297, auc 0.9297
epoch 501, loss 0.1920, train acc 94.20%, f1 0.9420, precision 0.9419, recall 0.9421, auc 0.9420
epoch 601, loss 0.1707, train acc 94.67%, f1 0.9467, precision 0.9466, recall 0.9468, auc 0.9467
epoch 701, loss 0.1580, train acc 94.99%, f1 0.9499, precision 0.9498, recall 0.9500, auc 0.9499
epoch 801, loss 0.1499, train acc 95.21%, f1 0.9521, precision 0.9520, recall 0.9521, auc 0.9521
epoch 901, loss 0.1445, train acc 95.36%, f1 0.9536, precision 0.9535, recall 0.9538, auc 0.9536
epoch 1001, loss 0.1409, train acc 95.45%, f1 0.9546, precision 0.9544, recall 0.9547, auc 0.9545
epoch 1101, loss 0.1380, train acc 95.49%, f1 0.9549, precision 0.9549, recall 0.9550, auc 0.9549
epoch 1201, loss 0.1349, train acc 95.51%, f1 0.9551, precision 0.9552, recall 0.9551, auc 0.9551
epoch 1301, loss 0.1316, train acc 95.56%, f1 0.9556, precision 0.9556, recall 0.9556, auc 0.9556
epoch 1401, loss 0.1285, train acc 95.60%, f1 0.9560, precision 0.9562, recall 0.9557, auc 0.9560
epoch 1501, loss 0.1255, train acc 95.65%, f1 0.9565, precision 0.9567, recall 0.9562, auc 0.9565
epoch 1601, loss 0.1225, train acc 95.71%, f1 0.9571, precision 0.9573, recall 0.9569, auc 0.9571
epoch 1701, loss 0.1198, train acc 95.78%, f1 0.9578, precision 0.9579, recall 0.9577, auc 0.9578
epoch 1801, loss 0.1171, train acc 95.84%, f1 0.9584, precision 0.9586, recall 0.9583, auc 0.9584
epoch 1901, loss 0.1146, train acc 95.89%, f1 0.9589, precision 0.9591, recall 0.9586, auc 0.9589
epoch 2001, loss 0.1121, train acc 95.94%, f1 0.9594, precision 0.9596, recall 0.9592, auc 0.9594
epoch 2101, loss 0.1097, train acc 96.01%, f1 0.9601, precision 0.9602, recall 0.9599, auc 0.9601
epoch 2201, loss 0.1071, train acc 96.05%, f1 0.9605, precision 0.9606, recall 0.9604, auc 0.9605
epoch 2301, loss 0.1044, train acc 96.12%, f1 0.9612, precision 0.9613, recall 0.9610, auc 0.9612
epoch 2401, loss 0.1015, train acc 96.17%, f1 0.9617, precision 0.9619, recall 0.9615, auc 0.9617
epoch 2501, loss 0.0988, train acc 96.24%, f1 0.9624, precision 0.9626, recall 0.9623, auc 0.9624
epoch 2601, loss 0.0962, train acc 96.30%, f1 0.9629, precision 0.9632, recall 0.9627, auc 0.9630
epoch 2701, loss 0.0939, train acc 96.36%, f1 0.9636, precision 0.9639, recall 0.9633, auc 0.9636
epoch 2801, loss 0.0917, train acc 96.42%, f1 0.9642, precision 0.9644, recall 0.9640, auc 0.9642
epoch 2901, loss 0.0896, train acc 96.46%, f1 0.9646, precision 0.9647, recall 0.9646, auc 0.9646
epoch 3001, loss 0.0876, train acc 96.52%, f1 0.9652, precision 0.9652, recall 0.9652, auc 0.9652
epoch 3101, loss 0.0857, train acc 96.59%, f1 0.9659, precision 0.9659, recall 0.9659, auc 0.9659
epoch 3201, loss 0.0839, train acc 96.65%, f1 0.9665, precision 0.9666, recall 0.9665, auc 0.9665
epoch 3301, loss 0.0821, train acc 96.71%, f1 0.9671, precision 0.9671, recall 0.9671, auc 0.9671
epoch 3401, loss 0.0803, train acc 96.77%, f1 0.9677, precision 0.9678, recall 0.9677, auc 0.9677
epoch 3501, loss 0.0785, train acc 96.83%, f1 0.9683, precision 0.9682, recall 0.9683, auc 0.9683
epoch 3601, loss 0.0768, train acc 96.89%, f1 0.9689, precision 0.9689, recall 0.9689, auc 0.9689
epoch 3701, loss 0.0751, train acc 96.95%, f1 0.9695, precision 0.9695, recall 0.9695, auc 0.9695
epoch 3801, loss 0.0735, train acc 97.01%, f1 0.9701, precision 0.9700, recall 0.9702, auc 0.9701
epoch 3901, loss 0.0719, train acc 97.08%, f1 0.9708, precision 0.9708, recall 0.9709, auc 0.9708
epoch 4001, loss 0.0704, train acc 97.14%, f1 0.9714, precision 0.9714, recall 0.9713, auc 0.9714
epoch 4101, loss 0.0689, train acc 97.20%, f1 0.9720, precision 0.9721, recall 0.9719, auc 0.9720
epoch 4201, loss 0.0674, train acc 97.25%, f1 0.9725, precision 0.9726, recall 0.9725, auc 0.9725
epoch 4301, loss 0.0659, train acc 97.31%, f1 0.9731, precision 0.9730, recall 0.9731, auc 0.9731
epoch 4401, loss 0.0645, train acc 97.36%, f1 0.9736, precision 0.9736, recall 0.9736, auc 0.9736
epoch 4501, loss 0.0631, train acc 97.41%, f1 0.9741, precision 0.9741, recall 0.9741, auc 0.9741
epoch 4601, loss 0.0617, train acc 97.46%, f1 0.9746, precision 0.9747, recall 0.9745, auc 0.9746
epoch 4701, loss 0.0603, train acc 97.51%, f1 0.9751, precision 0.9753, recall 0.9750, auc 0.9751
epoch 4801, loss 0.0589, train acc 97.57%, f1 0.9757, precision 0.9759, recall 0.9755, auc 0.9757
epoch 4901, loss 0.0575, train acc 97.63%, f1 0.9763, precision 0.9764, recall 0.9762, auc 0.9763
epoch 5001, loss 0.0562, train acc 97.69%, f1 0.9768, precision 0.9771, recall 0.9766, auc 0.9769
epoch 5101, loss 0.0548, train acc 97.75%, f1 0.9775, precision 0.9777, recall 0.9772, auc 0.9775
epoch 5201, loss 0.0535, train acc 97.80%, f1 0.9780, precision 0.9781, recall 0.9778, auc 0.9780
epoch 5301, loss 0.0522, train acc 97.86%, f1 0.9786, precision 0.9786, recall 0.9785, auc 0.9786
epoch 5401, loss 0.0510, train acc 97.91%, f1 0.9791, precision 0.9792, recall 0.9790, auc 0.9791
epoch 5501, loss 0.0497, train acc 97.97%, f1 0.9797, precision 0.9797, recall 0.9797, auc 0.9797
epoch 5601, loss 0.0485, train acc 98.01%, f1 0.9801, precision 0.9802, recall 0.9801, auc 0.9801
epoch 5701, loss 0.0473, train acc 98.06%, f1 0.9806, precision 0.9805, recall 0.9807, auc 0.9806
epoch 5801, loss 0.0461, train acc 98.11%, f1 0.9812, precision 0.9811, recall 0.9812, auc 0.9811
epoch 5901, loss 0.0449, train acc 98.17%, f1 0.9817, precision 0.9816, recall 0.9819, auc 0.9817
epoch 6001, loss 0.0437, train acc 98.21%, f1 0.9822, precision 0.9819, recall 0.9824, auc 0.9821
epoch 6101, loss 0.0424, train acc 98.27%, f1 0.9827, precision 0.9825, recall 0.9829, auc 0.9827
epoch 6201, loss 0.0412, train acc 98.32%, f1 0.9832, precision 0.9830, recall 0.9834, auc 0.9832
epoch 6301, loss 0.0400, train acc 98.39%, f1 0.9839, precision 0.9837, recall 0.9841, auc 0.9839
epoch 6401, loss 0.0387, train acc 98.46%, f1 0.9846, precision 0.9843, recall 0.9848, auc 0.9846
epoch 6501, loss 0.0375, train acc 98.52%, f1 0.9852, precision 0.9849, recall 0.9855, auc 0.9852
epoch 6601, loss 0.0362, train acc 98.58%, f1 0.9858, precision 0.9855, recall 0.9862, auc 0.9858
epoch 6701, loss 0.0350, train acc 98.65%, f1 0.9865, precision 0.9862, recall 0.9867, auc 0.9865
epoch 6801, loss 0.0338, train acc 98.71%, f1 0.9871, precision 0.9868, recall 0.9873, auc 0.9871
epoch 6901, loss 0.0326, train acc 98.77%, f1 0.9877, precision 0.9874, recall 0.9880, auc 0.9877
epoch 7001, loss 0.0314, train acc 98.83%, f1 0.9883, precision 0.9880, recall 0.9886, auc 0.9883
epoch 7101, loss 0.0303, train acc 98.89%, f1 0.9889, precision 0.9887, recall 0.9891, auc 0.9889
epoch 7201, loss 0.0292, train acc 98.94%, f1 0.9894, precision 0.9892, recall 0.9897, auc 0.9894
epoch 7301, loss 0.0281, train acc 98.99%, f1 0.9899, precision 0.9897, recall 0.9901, auc 0.9899
epoch 7401, loss 0.0270, train acc 99.05%, f1 0.9905, precision 0.9903, recall 0.9906, auc 0.9905
epoch 7501, loss 0.0261, train acc 99.08%, f1 0.9908, precision 0.9906, recall 0.9910, auc 0.9908
epoch 7601, loss 0.0251, train acc 99.12%, f1 0.9912, precision 0.9911, recall 0.9913, auc 0.9912
epoch 7701, loss 0.0242, train acc 99.17%, f1 0.9917, precision 0.9916, recall 0.9918, auc 0.9917
epoch 7801, loss 0.0233, train acc 99.21%, f1 0.9922, precision 0.9921, recall 0.9923, auc 0.9921
epoch 7901, loss 0.0224, train acc 99.25%, f1 0.9925, precision 0.9925, recall 0.9925, auc 0.9925
epoch 8001, loss 0.0216, train acc 99.29%, f1 0.9929, precision 0.9929, recall 0.9930, auc 0.9929
epoch 8101, loss 0.0208, train acc 99.33%, f1 0.9933, precision 0.9931, recall 0.9935, auc 0.9933/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)

epoch 8201, loss 0.0200, train acc 99.36%, f1 0.9936, precision 0.9934, recall 0.9939, auc 0.9936
epoch 8301, loss 0.0193, train acc 99.40%, f1 0.9940, precision 0.9937, recall 0.9943, auc 0.9940
epoch 8401, loss 0.0185, train acc 99.43%, f1 0.9943, precision 0.9940, recall 0.9946, auc 0.9943
epoch 8501, loss 0.0178, train acc 99.46%, f1 0.9946, precision 0.9943, recall 0.9949, auc 0.9946
epoch 8601, loss 0.0171, train acc 99.48%, f1 0.9948, precision 0.9946, recall 0.9951, auc 0.9948
epoch 8701, loss 0.0164, train acc 99.51%, f1 0.9951, precision 0.9948, recall 0.9954, auc 0.9951
epoch 8801, loss 0.0158, train acc 99.53%, f1 0.9953, precision 0.9950, recall 0.9956, auc 0.9953
epoch 8901, loss 0.0151, train acc 99.57%, f1 0.9957, precision 0.9954, recall 0.9960, auc 0.9957
epoch 9001, loss 0.0145, train acc 99.59%, f1 0.9959, precision 0.9956, recall 0.9962, auc 0.9959
epoch 9101, loss 0.0140, train acc 99.61%, f1 0.9961, precision 0.9958, recall 0.9964, auc 0.9961
epoch 9201, loss 0.0134, train acc 99.63%, f1 0.9963, precision 0.9960, recall 0.9965, auc 0.9963
epoch 9301, loss 0.0128, train acc 99.65%, f1 0.9965, precision 0.9963, recall 0.9967, auc 0.9965
epoch 9401, loss 0.0123, train acc 99.67%, f1 0.9967, precision 0.9965, recall 0.9969, auc 0.9967
epoch 9501, loss 0.0118, train acc 99.69%, f1 0.9969, precision 0.9968, recall 0.9970, auc 0.9969
epoch 9601, loss 0.0113, train acc 99.71%, f1 0.9971, precision 0.9969, recall 0.9972, auc 0.9971
epoch 9701, loss 0.0108, train acc 99.73%, f1 0.9973, precision 0.9971, recall 0.9974, auc 0.9973
epoch 9801, loss 0.0104, train acc 99.74%, f1 0.9974, precision 0.9973, recall 0.9976, auc 0.9974
epoch 9901, loss 0.0100, train acc 99.75%, f1 0.9975, precision 0.9974, recall 0.9977, auc 0.9975
/home/junda/.conda/envs/thenv/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_abalone19/standlization_data/abalone19_std_train_2.csv
./test_abalone19/standlization_data/abalone19_std_test_2.csv
MLP_concat_Mirror_10000
concat_pos_num_40_1
./test_abalone19/model_MLP_concat_Mirror_10000/record_1/MLP_concat_Mirror_10000_2
./test_abalone19/result_MLP_concat_Mirror_10000_concat_pos_num_40_1/record_1/
----------------------



the AUC is 0.4939759036144578

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
train_mlp_3_1.sh: line 27: 20106 Terminated              python3 ./classifier_MLP/train_MLP.py dataset_name=abalone19 dataset_index=2 record_index=1 device_id=3 train_method=MLP_concat_Mirror_8000
